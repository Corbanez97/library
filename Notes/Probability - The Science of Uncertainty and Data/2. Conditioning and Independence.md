* Conditioning
	Revising a model based on new information;
	Divide-and-conquer tools.
* Independence
	The occurrence or non-occurrence of a given event are determined by factors that are completely unrelated;
	Allows us to build complex models from simple ones. This comes from the fact that usually complex systems are composed from many unrelated factors.

## Conditioning and Baye's rule

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-2-conditioning-and-bayes-rule/). 

![[L02.pdf]]

### Conditional probabilities

Given two possible non-disjoint events, $A$ and $B$,  in a sample space $\Omega$ we shall define the probability of $A$ given that $B$ occurred as
$$P(A|B) = \frac{P(A\cap B)}{P(B)} \Leftrightarrow P(B) > 0.$$The definition above captures the idea of rescalling our sample space. If $\Omega$ is finite and we have a [[1. Probability models and axioms#Discrete uniform law and Uniform probability law|discrete uniform probability law]], then $P(A|B)$ is simply the number of elements of $A$ inside $B$ over the total elements of $B$.

![[L02E1.png]]

***GPT Addendum:***
	1. If $\Omega$ is finite and we have a [[1. Probability models and axioms#Discrete uniform law and Uniform probability law|discrete uniform probability law]], and if $ùêµ \ne \emptyset$, then the conditional probability law on $ùêµ$, given that $ùêµ$ occurred, is also discrete uniform.
	    This statement is¬†**True**. In a [[1. Probability models and axioms#Discrete uniform law and Uniform probability law|discrete uniform probability law]], all outcomes are equally likely. When event $B$ occurs, the outcomes within $B$ maintain their original proportions from the overall sample space $\Omega$. Therefore, the conditional probability law on $B$, given that $B$ occurred, remains discrete uniform.
	2. If $\Omega$ is finite and we have a [[1. Probability models and axioms#Discrete uniform law and Uniform probability law|discrete uniform probability law]], and if $B\neq \emptyset$, then the conditional probability law on $\Omega$, given that $B$ occurred, is also discrete uniform.
	    This statement is¬†**False**. The reason is that if an outcome is not in event $B$, it has zero conditional probability. This means not all outcomes in $\Omega$ can have the same conditional probability, so the conditional probability law on $\Omega$, given that $B$ occurred, cannot remain [[1. Probability models and axioms#Discrete uniform law and Uniform probability law|discrete uniform]].

### Properties of conditional probability

Conditional probabilities share *axioms* with [[1. Probability models and axioms#Probability axioms|ordinary probabilities]].
* $P(A|B) \ge 0$;
* $P(\Omega |B)$ = 1;
* $P(B|B) = 1$;
* $P(A \cup C | B) = P(A|B) + P(C|B) \Leftrightarrow A \cap C = \emptyset$. 
Since the *axioms* remain the same, their derived properties are also the same.

We can also show that $P(A^c|B) = 1 - P(A|B).$
**Proof:**
	Take that $B = (B\cap A) \cup (B \cap A^c)$. Applying the additivity *axiom*,
	$$P(B) = P(B \cap A) + P(B \cap A^c).$$
	Dividing both sides by $P(B)$ we find that
	$$
	1 = \frac{P(B\cap A)}{P(B)} + \frac{P(B\cap A^c)}{P(B)},
	$$
	and from the [[2. Conditioning and Independence#Conditional probabilities|definition of conditional probabilities]] we know that
	$$
	1 = P(A|B) + P(A^c|B).
	$$
$\square$
### Radar example

At an air base, staff members utilize a radar system, pointed skyward, to detect aircraft flying overhead. The base‚Äôs residents are well aware that there‚Äôs a $5\%$ chance of an airplane traversing their airspace. However, the radar system is not infallible. It can either trigger a false alarm or fail to alert when an airplane is indeed present. After working with this radar for some time, the staff has determined that it falsely alarms $10\%$ of the time and fails to alert for $1\%$ of the actual aircraft passages. Given this information, what is the probability that an airplane is indeed passing over the base when the radar sounds an alarm??

Say that event $A$ is the passing of an aircraft and $B$ is the radar giving us a signal we can visualize these probabilities with the following diagram:

![[radar_example.png]]

The logic behind each of these branches is that first we measure an event and then the other. The probability of the first branch, for instance, is $P(A)$ and then $P(B|A)$, *i.e.*, [[2. Conditioning and Independence#The multiplication rule|the multiplication of each individual probability]], $P(A \cap B) = 0.99 \times 0.05$. We are searching for $P(A|B)$. Based on the [[2. Conditioning and Independence#Conditional probabilities|conditional probability rule]],                                         
$$P(A|B) = \frac{P(A\cap B)}{P(B)}.$$
To find the total probability of $B$, we can sum the probability of $B$ given $A$ and $B$ given $A^c$. Therefore, $P(B) = P(A^c \cap B) + P(A \cap B) = 0.1445$. Replacing these values into the [[2. Conditioning and Independence#Conditional probabilities|conditional probability rule]] we find that $P(A|B) \approx 0.34$.

### The multiplication rule

The probability o two events happening together is simply the probability of their intersection, $P(A\cap B)$. The multiplication rule is constructed on top of [[2. Conditioning and Independence#Conditional probabilities|conditional probabilities]] as a manner to calculate the probability of $A$ **and** $B$ happening.
$$
P(A\cap B) = P(A|B) \times P(B).
$$

### Total probability theorem

Given a sample space partitioned into three events, $A_1$, $A_2$, and $A_3$. Have another event $B$ of which $B \cap A_i \ne \emptyset$ for every $i$. By the additivity *axiom*
$$P(B) = P(B \cap A_1) + P(B \cap A_2) + P(B \cap A_3).$$
This expression shows us that the total probability of an event $B$ happening equals the sum of the probabilities of the different ways $B$ may occur,*i.e.*,
$$P(B) = \sum_i P(A_i)P(B|A_i).$$
Since the sum of every probability of our partitions is equal to $1$, the expression above is the weighted sum of each scenario where $B$ occurs.
![[L02E4.png]]

### Baye's rule and inference

Let's explore the example [[2. Conditioning and Independence#Total probability theorem|above]] a bit further. Say that the event $B$ did happen, what does this tell us about each partition of our sample space? 

Thinking again of the [[2. Conditioning and Independence#Total probability theorem|total probability theorem]] as a weighted sum, the occurrence of $B$ defines a range of the most and less probable partitions. If the intersection of $B$ with $A_i$ is small, observing $B$ leads us to think that $A_i$ is less likely to had happened. Therefore, observing $B$ lets us revise our beliefs about other events also occurring.

Mathematically, given that $B$ occurred:
$$P(A_i|B) = \frac{P(A_i \cap B)}{\sum_j P(A_j) P(B|A_j)}.$$

Having observed $B$, we in are able to incorporate new evidence into our model and infer new $P(A_i)$.

![[L02E5.png]]![[L02E5S.jpg]]

It is important to understand that positive result on the test made us revise our beliefs on the probability of a person having the disease. Now, $P(\text{Having the disease}) = 0.0187$ which is $18.7$ times greater than our initial probability. In other words, we are $18.7$ times more sure that this particular individual has the disease we tested.
## Independence

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-3-independence/).

![[L03.pdf]]

Say that we have a biased coin of which the probability of head is $P(H) = p$ and the probability of tails is $P(T) = 1- p$. We will experiment with multiple tosses of this biases coin. This is a case where the conditional probability and unconditional probability are equal disregarded of the number of tosses. 

For instance, $P(H_2|H_1) = P(H_2|T_1) = p$, *i.e.*, the conditional probability of heads in the second toss is equal regardless of the firs outcome. Notwithstanding the outcome of the first toss, the [[2. Conditioning and Independence#Total probability theorem|total probability]] $P(H_2) = P(H_1)P(H_2|H_1) + P(T_1)P(H_2|T_1) = p$.

The fact that $P(H_2|H_1) = P(H_2|T_1) = P(H_2)$ show us that these two events are **independent**. The occurrence of $H_2$ provides no new information if it is more likely that the first toss was $T_1$ or $H_1$. 

Generally, two events are said to be **independent** if $P(A|B) = P(A)$. Independent events have a simples [[2. Conditioning and Independence#The multiplication rule|multiplication rule]] that shall be used as their definition:
$$
P(A \cap B) = P(A)\cdot P(B).
$$
The above definition is symmetric, therefore, $P(B|A) = P(B)$.

![[L03E2.png]]

### Independence of complements

If $P(A\cap B) = P(A)\cdot P(B)$ then $P(A\cap B^c) = P(A) \cdot P(B^c)$.
**Proof:**
	The event $A$ can be written as the union between its intersection with any other event and its complementar. 
	$$
	A = (A \cap B) \cup (A\cap B^c).
	$$
	By the [[1. Probability models and axioms#Probability axioms|additivity axiom]] the probability of $A$ is
	$$
	P(A) = P(A\cap B)+P(A\cap B^c) - P(A\cap B \cap A \cap B^c).
	$$
	The intersection of these sets is the $\emptyset$, since $A\cap A = A$ and $B\cap B^c = \emptyset$. Therefore,
	$$
	P(A) = P(A\cap B) + P(A \cap B^c).
	$$
	Since $P(A\cap B) = P(A)\cdot P(B)$,
	$$
	P(A) = P(A)P(B) + P(A \cap B^c),
	$$
	$$P(A\cap B^c) = P(A) \cdot (1 - P(B)) = P(A)\cdot P(B).$$
$\square$

### Conditional independence

*Conditional independence, given $C$, is defined as independence under the probability law $P(\cdot | C)$.* 

Two events are conditionally independent if
$$
P(A\cap B|C) = P(A|C) \cdot P(B|C).
$$

![[conditional_independence_example.png]]

Say that the events $A$ and $B$ in the sample space pictured above, $\Omega$, are independent. Notice that given the measurement of $C$, $A$ and $B$ are extremely dependent. Therefore, [[2. Conditioning and Independence#Independence|general independence]] does not imply conditional independence.

Moreover, one can say that $A$ and $B$ are independent over the complement of $C$. In other words, conditionally independence over given $C$ does not imply in conditional independence over its complementar.

![[L03E5.png]]

### Conditioning may affect independence

Consider this scenario: You are presented with two coins that appear identical. You know that one coin, which we‚Äôll call Coin $A$, has a $90\%$ chance of landing on heads, while the other coin, Coin $B$, has only a $10\%$ chance of landing on heads. Given their identical appearance, you have an equal probability of selecting either coin. The question then arises: Are the coin tosses independent?

The answer is *no*. For instance, if you want to calculate the [[2. Conditioning and Independence#Total probability theorem|total probability]] of the eleventh toss being heads, you would evaluate $P(\text{toss 11} = H)$ as follows:
$$P(\text{toss 11} = H) = P(A)P(H_{11}|A) + P(B)P(H_{11}|B) = 0.5 \, .$$
However, if you want to determine the probability of the eleventh toss resulting in heads, given that the first ten tosses also resulted in heads, the situation changes. In this case, the evidence from the first ten tosses strongly suggests that you have picked Coin $A$. Therefore, in this scenario, the coin tosses are not independent.

### Independence of a collection of events

Events $A_1$, $A_2$, ..., $A_n$ are called *independent* if:
$$P(A_{i}\cap A_{j} \cap A_{k} \cap \dots \cap A_{m}) = P(A_{i})P(A_{k})P(A_{k})\dots P(A_{m}) \; \forall \; i, j, k, \dots ,m.$$

![[L03E6.png]]

### Pairwise independence

Consider two independent fair coin tosses. The event where the two tosses had the same result, $C$, has a probability $P(C) = 0.5$. Let's analyze three events, $H_1$, $H_2$, and $C$, where $H_i$ is the event of which heads turn in the $i^{th}$ toss. Are they independent? 

Computing $P(H_1 \cap C)$ we find that
$$
P(H_1 \cap C) = P(H_1 \cap H_2) = \frac{1}{4}
$$
equals
$$
P(H_1)\cdot P(C) = \frac{1}{4}.
$$
This means that $H_1$ and $C$ are independent. From the symmetrical property of our tosse, the same can be said for $H_2$ and $C$.

However, the probability $P(H_1 \cap H_2 \cap C) \ne P(H_1)P(H_2)P(C)$. In other words, the pairwise independence between $H_1$, $H_2$, and $C$ does not imply overall independence.

### Examples

#### Reliability of a system

Picture a system consisting of three independent units that is said to be functional if there is a viable path between input and output. A path is valid if the unit is up and the probability of the $i^{\text{th}}$  unit being up is $P(U_i) = p_i$.

First, consider a system in the image below where three units are in a series. What is the probability of the system being functional? 

![[reliability_example_series.png]]

Since there is only one path between input and output, the only possible way for this system to work is if all three units are up, *i.e.*, $P(U_1 \cap U_2 \cap U_3)$. And, given that these events are independent of each other, $P(U_1 \cap U_2 \cap U_3) = p_1 \cdot p_2 \cdot p_3$.

Now, consider a system of three parallel units. If either one of the units is up, the system will be functional. The probability of either the first, second, or third unit being up is $P(U_1 \cup U_2 \cup U_3)$.

![[reliability_example_parallel.png]]

Since the independence of these three events imply the independence of its complementar, using the [[A. Mathematical background overview#De Morgan's laws|De Morgan's laws]] we show that
$$
P(U_1 \cup U_2 \cup U_3) = 1 - P((U_1 \cup U_2 \cup U_3)^c) = 1 - P(U_1^c \cap U_2^c \cap U_3^c).
$$
$U_i^c = D_i$ is the case when the $i^{\text{th}}$ unit is down, therefore,
$$
P(U_1 \cup U_2 \cup U_3) = 1 - P(D_1 \cap D_2 \cap D_3) = 1 - P(D_1)P(D_2)P(D_3).
$$
Knowing that $D_i$ is complementar to $U_i$ the probability of the parallel system being up is
$$
P(U_1 \cup U_2 \cup U_3) = 1 - (1 - p_1)(1-p_2)(1-p_3).
$$

#### The king's sibling

A king comes from a family of two children. What is the probability that his sibling is female?

First we assume that the number of children the family would have was predetermined to be two and that boys have precedence to the throne. Secondly, we know that $P(\text{boy}) = P(\text{girl}) = \frac{1}{2}$ and these are independent events. 

One might guess that the probability of the king's sibling being a female is half, however, our initial sample space is altered from the simple fact that there is a king.

The original sample space:

| BB | BG |
| ---- | ---- |
| **GB** | **GG** |
Becomes:

| BB | BG |
| ---- | ---- |
| **GB** | **X** |
Therefore, the probability of the king's sibling being a girl, $P(G| \text{There is a king}) = \frac{2}{3}$.

### Solved problems

The coin tossing puzzle.

![[L03SP3.png]]![[L03SP3S.jpg]]

As we can see the probabilities will be at least equal for every possible bias of the coin.

<iframe src="https://www.desmos.com/calculator/wlnjyoz3yh?embed" width="700" height="600" style="border: 1px solid #ccc" frameborder=0></iframe>

We can generalize this problem breaking our probability tree into three distinct events, $A$, $B$, and $C.$ If $B \subset C$ and $A \cap B = A\cap C$, then $P(A|B) \geq P(A|C)$. 

#### The Monty Hall Problem

You already know this problem. Will you be able to understand why switching is always the best choice??

The probability tree below helps a lot to understand every probable event. If you stay, you will get the car $1/3$ of the time. However, if you switch doors, you will get the car $2/3$ of the time.

![[L03SP4S.png]]
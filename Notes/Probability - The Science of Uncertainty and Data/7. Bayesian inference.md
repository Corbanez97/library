## The Bayesian inference framework

![[L14.pdf]]

Let an unknown called $\Theta$ be treated as a r.v. with distribution $p_{\Theta}$ called *prior*. Moreover, given an observation $X$ we model our observations according to a [[4. Discrete random variables#Conditional *PMFs*|conditional distribution]] $p_{X|\Theta}$. Once we measure the r.v. $X = x$, we can find the *posterior*  $p_{\Theta|X}(\theta | X = x)$ using the [[4. Discrete random variables#Conditional *PMFs*|Bayes rule]]
$$
p_{\Theta|X}(\theta| X = x) = \frac{p_{X|\Theta}(X = x, \theta)p_\Theta(\theta)}{p_X(X = x)}.
$$
Notice how the *posterior* is a distribution of $\Theta$ over all possible $\theta$ values given $X = x$.

Furthermore, now that we have the *posterior* distribution of $\Theta$ one might ask what is a probable value of $\theta$. One way to find this value is using the *maximum a posteriori probability*  (*MAP*)
$$
p_{\Theta|X}(\theta^*| x) = \underset{\theta}{\text{max}} \big[p_{\Theta|X}(\theta|x)\big].
$$
Another estimation is simply calculating the expected value of the *posterior*
$$
E[\Theta|X = x].
$$
This method is called the *least mean squares* estimation. Either way, we are applying a set of rules to our r.v. $X$ to estimate the value of $\Theta$. We shall call the process applied to $X$ as the estimator 
$$
\hat{\Theta} = g(X).
$$

One way to measure how accurate is our estimate is through the conditional probability error,
$$
P(\hat{\theta} \neq \Theta | X = x).
$$
When measuring erros as such, the *maximum a posteriori probability* estimation yields the best results. Overall, the probability error $P(\hat{\Theta} \neq \Theta)$  can be found through the [[2. Conditioning and Independence#Total probability theorem|total probability theorem]]. Therefore
$$
P(\hat{\Theta} \neq \Theta) = \sum_x P(\hat{\Theta}\neq \Theta | X = x) p_X(x)
$$
or 
$$
P(\hat{\Theta} \neq \Theta) = \sum_x P(\hat{\Theta}\neq \Theta | \Theta = \theta) p_\Theta(\theta).
$$
![[L14E3S.png]]

### [Bayesian Hypothesis Testing](https://www.probabilitycourse.com/chapter9/9_1_8_bayesian_hypothesis_testing.php)

Suppose that we need to decide between two hypotheses $H_0$ and $H_1$. In the Bayesian setting, we assume that we know prior probabilities of $H_0$ and $H_1$. That is, we know $P(H_0) = p_0$ and $P(H_1) = p_1$, where $p_0 + p_1 = 1$. We observe the random variable (or the random vector) $Y$. We know the distribution of $Y$ under the two hypotheses, i.e., we know $f_Y(y|H_0)$ and $f_Y(y|H_1)$. Using Bayes' rule, we can obtain the posterior probabilities of $H_0$ and $H_1$:
$$
P(H_0|Y=y)P(H_1|Y=y) = \frac{f_Y(y|H_0)P(H_0)}{f_Y(y)} = \frac{f_Y(y|H_1)P(H_1)}{f_Y(y)}
$$

One way to decide between $H_0$ and $H_1$ is to compare $P(H_0|Y=y)$ and $P(H_1|Y=y)$, and accept the hypothesis with the higher posterior probability. This is the idea behind the *maximum a posteriori* ($MAP$) test. Here, since we are choosing the hypothesis with the highest probability, it is relatively easy to show that the error probability is minimized.

To be more specific, according to the $MAP$ test, we choose $H_0$ if and only if:
$$
P(H_0|Y=y) \geq P(H_1|Y=y)
$$

In other words, we choose $H_0$ if and only if $f_Y(y|H_0)P(H_0) \geq f_Y(y|H_1)P(H_1)$. Note that as always, we use the PMF instead of the PDF if $Y$ is a discrete random variable. We can generalize the MAP test to the case where you have more than two hypotheses. In that case, again we choose the hypothesis with the highest posterior probability.

### Inferring the unknown bias of a coin and the Beta distribution

Let the bias of a coin be a r.v. $\Theta$ with *prior PDF* $f_\Theta$. It is reasonable to assume that $f_\Theta \sim \text{unif}[0, 1]$ since we are completely ignorant about the coin bias. We proceed by flipping this coin $n$ times, of which $K$ is the number of heads. The *posterior PDF* is given by the [[5. Continuous random variables#Conditioning on a random variable; Independence; Bayes' rule|Bayes' rule]]
$$
\begin{align}
f_{\Theta|K}(\theta|k) =  \frac{f_\Theta(\theta) \cdot p_{K|\Theta}(k|\theta)}{p_K(k)}, \\ \\
p_K(k) = \int f_\Theta(\theta') p_{K|\Theta}(k|\theta')d\theta'.
\end{align}
$$
Notice the distinction between the usage of *PMF* and *PDF*, since $K$ is a discrete and $\Theta$ is a continuous, respectively. Moreover, we know that $K$ is described by the [[4. Discrete random variables#Binomial random variable with *parameters* $n; p in [0, 1]$|binomial distribution]], therefore
$$
f_{\Theta|K}(\theta|k) =  \frac{1 \cdot \binom{n}{k}\theta^k (1 - \theta)^{n-k}}{p_K(k)} = \frac{1}{d(n ,k)} \cdot \theta^k (1-\theta)^{n-k} \;\; \forall \;\theta \in [0, 1],
$$
where we grouped the terms independent of $\theta$ as a normalizing factor. The remaining terms of the right-most side is the *Beta distribution* with parameter $(k + 1, n- k + 1)$. However, if our *prior PDF* is a Beta distribution, 
$$
f_\Theta(\theta) = \frac{1}{c} \theta^\alpha (1-\theta)^\beta,
$$
the *posterior* will again be described by a normalizing constant times a new Beta distribution.
$$
f_{\Theta|K}(\theta|k) = d(n ,k) \cdot \theta^{k+\alpha} (1-\theta)^{n-k + \beta}.
$$
To make point estimates, we can proceed with the calculation of $\hat{\theta}_{MAP}$ or $\hat{\theta}_{LMS}$. To find the *MAP* estimate, we can calculate the maximum value of the logarithm of $f_{\Theta|K}$ since the logarithm is a monotonically increasing function which will not change the estimate $\hat{\theta}_{MAP}$. The estimate and the estimator is
$$
\hat{\theta}_{MAP} = \frac{k}{n}, \;\; \hat{\Theta}_{MAP} = \frac{K}{n}.
$$
Calculating $\hat{\theta}_{LMS}$ is simply some straight-forward algebra. Given
$$
\int_0^1 \theta^\alpha (1- \theta)^\beta = \frac{\alpha! \beta!}{(\alpha + \beta + 1)!},
$$
we can calculate the normalization factor $1/d(n,k)$. Then, we compute $E[\Theta | K = k]$ by integrating
$$
E[\Theta | K = k] = \int_0^1 \theta f_{\Theta|K}(\theta|k) d\theta.
$$
This calculation yields
$$
\hat{\theta}_{LMS} = \frac{k + 1}{n +2}.
$$
***GPT Addendum:***
	 The Beta function is defined as:
	 $$
	 B(\alpha, \beta) = \int_0^1 \theta^{\alpha-1} (1-\theta)^{\beta-1} d\theta
	 $$
	 The Gamma function is defined as:
	 $$
	 \Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt
	 $$Now, consider the integral:
	 $$
	 I = \int_0^\infty \int_0^\infty t^{\alpha-1} u^{\beta-1} e^{-(t+u)} dt du
	 $$We can change the order of integration and then change variables to polar coordinates with $t = r \cos^2(\theta)$ and $u = r \sin^2(\theta)$. This gives us:
	 $$
	 I = \int_0^\infty \int_0^{\pi/2} r^{\alpha+\beta-1} e^{-r} (\cos^2(\theta))^{\alpha-1} (\sin^2(\theta))^{\beta-1} r dr d\theta
	 $$This integral separates into two parts, one dependent on $r$ and one dependent on $\theta$:$$I = \left( \int_0^\infty r^{(\alpha+\beta)-1} e^{-r} dr \right) \left( \int_0^{\pi/2} (\cos^2(\theta))^{\alpha-1} (\sin^2(\theta))^{\beta-1} d\theta \right)
	 $$
	 The first integral is just the definition of the Gamma function, $\Gamma(\alpha+\beta)$, and the second integral is the definition of the Beta function, $B(\alpha, \beta)$. Therefore, we have:
	 $$
	 I = \Gamma(\alpha+\beta) B(\alpha, \beta)
	 $$
	 But we can also express $I$ in terms of the Gamma function alone:
	 $$
	 I = \int_0^\infty t^{\alpha-1} e^{-t} dt \int_0^\infty u^{\beta-1} e^{-u} du = \Gamma(\alpha) \Gamma(\beta)
	 $$
	 Setting these two expressions for $I$ equal to each other gives the relationship between the Beta and Gamma functions:
	 $$
	 \Gamma(\alpha) \Gamma(\beta) = \Gamma(\alpha+\beta) B(\alpha, \beta)
	 $$Solving for $B(\alpha, \beta)$ gives the desired result:
	 $$
	 B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}
	 $$
$\square$

## Linear models with normal noise

![[L15.pdf]]

Let our measurement be 
$$
X = \Theta + W,
$$
where $\Theta$ and $W$ are both independent standard normal. Using the [[5. Continuous random variables#Conditioning on a random variable; Independence; Bayes' rule|continuous form of the Baye's rule]] we can find the *posterior PDF*, $f_{\Theta|X}(\theta|x)$, since
$$
\begin{align}
X = \Theta + W \Rightarrow X\sim N(\Theta, 1),
\\ \\
\therefore f_{X|\Theta}(x|\theta) = N(\theta, 1)
\end{align}
$$
and
$$
f_X(x) = \int_{-\infty}^{\infty}f_{\Theta}(\theta) f_{X|\Theta}(x|\theta)d\theta.
$$
From the [[5. Continuous random variables#Normal (Gaussian) random variables|form of the normal distribution]] ,given a fixed value of $x$, we can compute the *posterior distribution* as
$$
f_{\Theta|X}(\theta|x) = \frac{c}{f_X(x)} e^{-\frac{1}{2}\theta^2}e^{-\frac{1}{2}(x - \theta)^2} = h(x)e^{-g(\theta^2, \theta, x)}.
$$
In this equation, we can consolidate all terms dependent on $x$ and constants into a single constant $h(x)$ that depends on the value of $x$. Additionally, the exponent is a function of $\theta^2$, $\theta$, and $x$. This structure demonstrates that the *posterior PDF* of a normal distribution retains the form of a normal distribution.

Moreover, we can estimate the parameter $\theta$ by calculating the $\hat{\theta}_{MAP} = \hat{\theta}_{LMS} = E[\Theta | X = x]$. Given that $f_{\Theta|X}(\theta|x)$ reaches its maximum when the exponent $g(\theta^2, \theta, x)$ reaches its minimum yields
$$
\frac{d g(\theta^2, \theta, x)}{d\theta}\Bigg|_{\hat{\theta}_{MAP}} = 0.
$$
In this case we can show that $\hat{\theta}_{MAP} = \frac{x}{2}$ and that its estimator is $\hat{\Theta} = \frac{X}{2}$.

### The case of multiple observations

Let $\{X_i\}$ be a sample of $n$ observations
$$
X_i = \Theta + W_i,
$$
where
$$
\begin{align}
\Theta \sim N(x_0, \sigma_0^2) \;,\; W_i \sim N(0, \sigma^2_i),
\\ \\
\text{cov}(\Theta, W_i) = \text{cov}(W_i, W_j) = 0.
\end{align}
$$
Additionally,  given $\Theta = \theta$,
$$
X_i = \theta + W_i \sim N(\theta, \sigma^2_i ).
$$
Since all $W_i$ are independent of each other, this will also be true to $X_i$, therefore,
$$
f_{X|\Theta} (x| \theta) = f_{X_1, X_2, ..., X_n|\Theta}(x_1, x_2, ..., x_n | \theta) = \prod_{i=1}^n f_{X_i|\Theta}(x_i| \theta).
$$
Using [[5. Continuous random variables#Conditioning on a random variable; Independence; Bayes' rule|Baye's rule]] our *posterior PDF* in terms of these [[5. Continuous random variables#Normal (Gaussian) random variables|normals distributions]] will be
$$
f_{\Theta|X}(\theta|x) = \frac{1}{f_X(x)}c_0e^{-\frac{(\theta - x_0)^2}{2\sigma_0^2}}\prod_{i = 1}^nc_ie^{-\frac{(x_i - \theta)^2}{2\sigma_i^2}}.
$$
This expression depends on the vector of measurements $\{X_i = x_i\}$ and their known distributions. Again, we can see that this expression results in function of our conditioning r.v. and an exponent of a quadratic form of our r.v. of interest, which is also a normal distribution. Consolidating the expression results in
$$
f_{\Theta|X}(\theta|x) = h(x)\exp\left[-\sum_{i = 0}^n\frac{(\theta - x_i)^2}{2\sigma_i^2}\right],
$$
with a peak at
$$
\frac{d}{d\theta}\left[\sum_{i = 0}^n\frac{(\theta - x_i)^2}{2\sigma_i^2}\right]_{\hat{\theta}} = 0.
$$
Solving this differential equation for $\hat{\theta}$ yields
$$
\hat{\theta} = \frac{\sum_{i = 0}^n\frac{x_i}{\sigma_i^2}}{\sum_{i = 0}^n\frac{1}{\sigma_i^2}}.
$$
One important fact about the expression above is that it is composed of our *prior* knowledge, $x_0$, and our sample of observations, $\{x_i\}$. Moreover, the behavior of this estimation is self-leveling, meaning, if a given measurement was very "noisy" its importance to the estimation is toned-down.

### The mean squared error

Let the estimation of a standard normal r.v. $\Theta$ be $\hat{\Theta}$ given a set of observations $X_i = \Theta + W_i$. The mean squared error is defined as
$$
\text{MSE} = E\left[\left(\Theta - \hat{\Theta}\right)^2\Bigg| X = x\right].
$$
In the conditional universe where $X = x$ the estimator $\hat{\Theta} =\hat{\theta}$. However, since $\hat{\theta} = E\left[\Theta|X = x\right]$ the $\text{MSE}$ is simply the $\text{var}(\Theta| X = x)$,
$$
\text{var}(\Theta|X = x) = \frac{1}{\sum_{i= 0}^n \frac{1}{\sigma_i^2}}.
$$
Additionally, the *unconditional $\text{MSE}$* will be the same since the distribution for $X$ is normalized, *i.e.*,
$$
E\left[\left(\Theta - \hat{\Theta}\right)^2\right] = \int E\left[\left(\Theta - \hat{\Theta}\right)^2\Bigg| X = x\right] f_X(x)dx
 = \frac{1}{\sum_{i= 0}^n \frac{1}{\sigma_i^2}}\int f_X(x)dx = \frac{1}{\sum_{i= 0}^n \frac{1}{\sigma_i^2}}.$$

### The case of multiple parameters: trajectory estimation

The trajectory of a vertical throw is given by a quadratic function of time
$$
x(t) = \theta_0 + \theta_1t + \theta_2t^2.
$$
The quadratic function has parameters determined by the random variables $\{\Theta_j\}$ all independent from each other and each with a given *prior* $f_{\Theta_j}(\theta_j)$. We can estimate each parameter $\theta_j$ by sampling the position $x(t_i)$ as our observations $\{X_i\}$. We can model our measurement at a time $t_i$ as 
$$
X_i = \Theta_0 + \Theta_1t_i + \Theta_2t_i^2 + W_i,
$$
where $W_i \sim f_{W_i}$ is the error of our measurement which is independent from $\Theta_j$ or any other $W_k$.

<iframe src="https://www.desmos.com/calculator/fzjwnqdcgi?embed" width="700" height="700" style="border: 1px solid #ccc" frameborder=0></iframe>

First, assuming that $\Theta_j \sim N(0, \sigma_j^2)$ and $W_i \sim N(0 ,\sigma^2)$ the distribution for $X_i$ given $\Theta = \theta$ is
$$
\begin{align}
X_i \sim N(\theta_0 + \theta_1t_i + \theta_2t_i^2, \sigma^2) = f_{X_i|\Theta}(x_i|\theta) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left[\frac{-\left(x_i -\theta_0-\theta_1 t_i - \theta_2 t_i^2\right)^2}{2\sigma^2}\right],
\\
\\
\therefore f_X(x) = \prod_{i=0}^nf_{X_i|\Theta}(x_i|\theta).
\end{align}
$$

Second, since the three parameters are independent from each other,
$$
f_\Theta(\theta) = \prod_{j = 0}^2f_{\Theta_j}(\theta_j).
$$
With the *PDF*'s for the vectors $\Theta$ and $X$ we can calculate the *posterior PDF* using the [[5. Continuous random variables#Conditioning on a random variable; Independence; Bayes' rule|Baye's rule]]. Grouping every therm independent of $\theta_j$ into $h(x)$ results in
$$
f_{\Theta|X}(\theta|x) =h(x)\exp\left[-\frac{1}{2}\left(\frac{\theta_0^2}{\sigma_0^2} + \frac{\theta_1^2}{\sigma_1^2} + \frac{\theta_2^2}{\sigma_2^2}\right) + \frac{1}{2\sigma^2}\sum_{i = 0}^n\left(x_i -\theta_0-\theta_1 t_i - \theta_2 t_i^2\right)^2\right].
$$
Now, to find $\hat{\theta}_{MAP}$ we differentiate the exponent with respect to the vector $\theta$ and set it to zero. This will yield us three distinct linear equations for each component of the vector $(\theta_0, \theta_1, \theta_2)$ which when solved results in each component of our estimation.

## Least mean squares ($LMS$) estimation

![[L16.pdf]]

### Estimation in the absence of observations

Let $\Theta$ be an unknown r.v. with *prior* $f_{\Theta}(\theta) \sim \text{unif}[2, 6]$ show bellow.

<iframe src="https://www.desmos.com/calculator/gupucu8ykb?embed" width="700" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>

Given that there is no observation available, the *posterior* will be the same as the *prior*. Furthermore, the $\hat{\Theta}_{MAP}$ is degenerated since $f_{\Theta}(\theta)$ is a constant. However, $\hat{\Theta}_{LMS}$ infers a unique value of $\theta$,*i.e.*, $\hat{\theta}_{LMS} = 4$.

A criterion to determine which estimator is best suited for our model is the minimization of the *[[7. Bayesian inference#The mean squared error|Mean Squared Error]]*
$$
MSE = E\left[\left(\Theta - \hat{\theta}\right)^2\right].
$$Differentiating the $MSE$ according to $\hat{\theta}$ and equating to zero yields
$$
\hat{\theta} = E\left[\Theta\right],
$$
meaning that the $MSE$ is minimal when $\hat{\theta}=\hat{\theta}_{LMS}$, *ergo*, *Least Mean Square*.

Additionally, when calculating $MSE$ with $\hat{\theta} = E[\Theta]$ one finds that
$$
\text{min}(MSE) = E\left[\left(\Theta - E[\Theta]\right)^2\right] = \text{var}(\Theta).
$$
### $LMS$ estimation of $\Theta$ given $X$

Given an observation $X = x$, the $\hat{\Theta}_{LMS}$ is the estimator that minimizes the *conditional* $MSE$, *i.e.*,
$$
\hat{\Theta}_{LMS} = E\left[\Theta | X = x\right] \; \text{minimizes}\; E\left[\left(\Theta - \hat{\theta}\right)^2 \Bigg| X =x \right].
$$
#### $LMS$ performance evaluation

The $MSE$ of the $LMS$ estimator given an observation $X = x$ is conditional variance of $\Theta$.
$$
MSE = E\left[\left(\Theta - E[\Theta|X = x]\right)^2 \Big | X = x\right] = \text{var}(\Theta|X = x).
$$
However, in the absence of any observation, what we are really calculating in the performance of the overall design. And since we are not in a conditional set,
$$
MSE = E\left[\left(\Theta - E[\Theta]\right)^2\right] = E\left[\text{var}(\Theta)\right].
$$
### $LMS$ estimation with multiple observations or unknowns

Let our sample be a set of observations $X = (X_1, X_2, X_3, ..., X_n)$ modeled by $f_{X|\Theta}(x|\theta)$. The $LMS$ estimate is given by
$$
E[\Theta|X_1 = x_1, X_2 = x_2, X_3 = x_3, ..., X_n = x_n].
$$
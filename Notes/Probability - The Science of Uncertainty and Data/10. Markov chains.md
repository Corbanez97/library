## Markov processes

Unlike [[9. Bernoulli and Poisson processes|Bernoulli and Poisson processes]] where the future states of a system is independent of the past, *i.e.*, the process has a *memorylessness* property, Markov processes are used to model systems where the future is not only dependent of the past but it can be predicted from past stated of the system.

![[L24.pdf]]

### Checkout counter example

Let a queue at a checkout counter be modeled as a [[9. Bernoulli and Poisson processes#The Bernoulli process|Bernoulli process]] with parameter $p$. Additionally, let the checkout of a given customer be modeled as a [[4. Discrete random variables#Geometric random variable with *parameter* $p 0 < p leq 1$|geometric r.v.]] with parameter $q$. 

Therefore, at each time step a new customer may apear with a probability of $p$ and a customer that is being served at the counter may leave with a probability of $q$. The arrival and the checkout are [[2. Conditioning and Independence#Independence|independent]] events.

We define the state of our system $X_n$  as the number of customer in queue at a time $n$. For example, the probability of $X_n = 2$ transition to $X_{n+1} = 3$ is the probability of a customer arriving, $p$, and no customer checking-out, $1-q$. 

### Discrete-time finite state Markov chains

Let $X_n$ be a r.v. that refers to the state of a system after $n$ transitions. $X_n$ belongs to a finite set of possible states. At each state $i$ we define a set of transition probabilities $p_{ij}$ as
$$
p_{ij} = P(X_{n+1} = j | X_n = i).
$$
The probability is time homogeneous if
$$
p_{ij} = P(X_{n+1} = j | X_n = i) = P(X_{1} = j | X_0 = i).
$$
Morevover,
$$
\sum_jp_{ij} = 1.
$$
The Markov property/assumption is that given a current state the past doesn't matter, *i.e.*,
$$
p_{ij} = P(X_{n+1} = j | X_n = i) = P(X_{n+1} = j | X_n = i, X_{n-1}, X_{n-2}, ..., X_0)
$$
### $n$-step transitions probabilities

One might wonder given a initial state $X_0$ what is the probability of a state $X_n$. The n-step transition probability is defined as 
$$
r_{ij}(n) = P(X_n = j | X_0 = i),
$$
such that
$$
\sum_jr_{ij}(n) = 1.
$$

One way to figure out the $n$-step transition probability in terms of individual transition probabilities $p_{ij}$ it to consecutively break all n-step into the individual steps. For instance,
$$
r_{ij}(n) = r_{ik}(n-1)\cdot p_{kj},
$$
where $k$ is any possible state of our system.
![[n-step-transition_probabilities.png]]
 We can use the [[2. Conditioning and Independence#Total probability theorem|total probability theorem]] to take into account every possible state at step $n-1$. Therefore,
$$
r_{ij}(n) = \sum_k r_{ik}(n-1)p_{kj}.
$$Given a random initial state of our system, the probability of $X_n = j$ can also me calculated using the $n$-step transition as
$$
P(X_n = j) = \sum_iP(X_0 = i)r_{ij}(n).
$$
A state $i$ is termed as a *recurrent state* if there always exists a path to return to it. Conversely, a state $j$ is labeled as a *transient state* if, after a certain number of steps, it becomes inaccessible from the starting point. A set of *recurrent states* that are accessible to each other is called a *recurrent class*.

The long term behavior of a Markov chain with multiple *recurrent classes* connected through *transient states* will greatly depend on the initial state of the system.

## Steady-state behavior of Markov chains

How does the $n$-step transition behaves as we watch the system over time? Does $\lim_{n \rightarrow \infty}r_{ij}(n) = \pi_j$ converge? Additionally, does the initial state $i$ affects the value of convergence? Some nice examples can help us understand why and how a Markov chain may, or may not, reach a steady-state. 

![[L25.pdf]]

### Periodic states in a recurrent class

The states in a recurrent class are periodic if they can be grouped into $d>1$ groups so that all transitions from one group lead to the next group.

***GPT Addendum:***
	In a Markov chain, a state is said to be periodic if it returns to itself after a certain number of steps with a probability greater than zero. This number of steps is called the period of the state. If every state in a Markov chain is periodic, the chain itself is termed periodic. Here are some key points about periodic Markov chains and states:
	1. **Period of a State**:
	   - The period of a state $i$ in a Markov chain is denoted by $d(i)$.
	   - It is defined as the greatest common divisor (GCD) of all possible numbers of steps $n$ such that $P_{ii}^{(n)} > 0$, where $P_{ii}^{(n)}$ is the probability of going from state $i$ to state $i$ in $n$ steps.
	2. **Periodic State**:
	   - A state $i$ is said to be periodic if its period $d(i)$ is greater than 1.
	   - A periodic state $i$ has a cyclic behavior in which it returns to itself after a certain number of steps.
	3. **Periodic Markov Chains**:
	   - If every state in a Markov chain is periodic, the chain itself is called a periodic Markov chain.
	   - Periodic Markov chains exhibit complex behavior where the system cycles through different states in a predictable manner.
	4. **Irreducibility and Periodicity**:
	   - Irreducible Markov chains can have either periodic or aperiodic states.
	   - If a Markov chain is irreducible and contains a periodic state, then all states in the chain are periodic with the same period.
	5. **Absorbing States**:
	   - In some Markov chains, certain states may be absorbing, meaning once the system enters these states, it remains there indefinitely.
	   - Absorbing states can have periods of 1 since they don't transition out of themselves.
	6. **Examples**:
	   - Consider a simple two-state Markov chain with transitions such that $P_{11} = P_{22} = 0$ and $P_{12} = P_{21} = 1$. Both states $1$ and $2$ are periodic with a period of $2$.
	   - In a more complex chain, if a state has multiple paths to return to itself with different lengths, the period is the GCD of these lengths.
	Understanding periodic Markov chains and states is crucial in analyzing the long-term behavior and stability of stochastic systems modeled by Markov processes. Periodicity adds an element of predictability and structure to the otherwise random transitions between states.

### Steady-state probabilities

Does $\lim_{n \rightarrow \infty}r_{ij}(n) = \pi_j$ converge? The answer to this question will be yes when the Markov chain has only one recurrent class which is not periodical.

Given a convergent Markov chain, the limit of the $n$-step transition probability as $n\rightarrow \infty$ is
$$
\pi_j = \sum_k \pi_k p_{kj}.
$$
This is known as the *balance equation* of our chain. Additionally, since these $\pi_j$ are probabilities they are also normalized, *i.e.*,
$$
\sum_j \pi_j = 1.
$$
From the normalization and definition of the steady-state probability we get a set of solvable equations for all $\pi_j$.

To practically solve such systems of equations it is important to note that the steady-state probability of transient state is zero. This information will greatly reduce the variables in our system to only those that are present in the non-periodical recurrent class.

### Birth-death processes

***GPT Addendum:***
	These processes model systems where entities are born and die, making them particularly relevant in various fields such as population dynamics, queueing theory, and biochemical kinetics. At the core of a birth-death process is the idea of transitions between discrete states, where transitions occur only in two directions: birth and death. This simplicity allows for elegant mathematical analysis and yields insights into the long-term behavior of the system.

![[birth-death_processes.png]]

Birth-death processes have a recursion relation that is easier to calculate than the general steady-state probability. Given that the frequency of transitions from the $i^{\text{th}}$ state to the $i^{\text{th}}+1$ state is the same as the inverse transition, we know that
$$
\pi_{i+1} = \pi_i \cdot \frac{p_i}{q_{i+1}} \; \forall \; 0 \leq i \leq m.
$$Additionally, solving the normalization of the steady-state probabilities for $\pi_0$ yields
$$
\pi_0 + \pi_0 \cdot \frac{p_0}{q_{1}} + \left(\pi_0 \cdot \frac{p_0}{q_{1}}\right) \cdot \frac{p_1}{q_2} + \left(\pi_0 \cdot \frac{p_0}{q_{1}} \cdot \frac{p_1}{q_2}\right)\cdot \frac{p_2}{q_3}+\dots = 1.
$$
Therefore, the normalization of steady-state probabilities gives us a trivial form for $\pi_0$ and the recursion relation enables us to find any $\pi_i$ for $i\geq 1$.

There is a special case of these birth-death processes where $p_i = p$ and $q_i = q$. We define
$$
\rho = \frac{p}{q}
$$
as a general measure of our processes going to either direction. If $\rho > 1$ our system tends to a greater state. However, if $\rho < 1$ our system is bounded to lesser states. Moreover, in such cases, our recursion relation becomes
$$
\pi_i = \pi_0 \rho^i \; \forall \; i = 0, 1, \dots, m
$$
and the normalization yields
$$
\pi_0 = \frac{1}{\sum_{i=0}^m \rho^i }.
$$
For instance, if $\rho = 1$ our system is known as a symmetrical random walk. This means every state has equal probability $\pi_0$ valued
$$
\pi_0 = \frac{1}{1+m}.
$$

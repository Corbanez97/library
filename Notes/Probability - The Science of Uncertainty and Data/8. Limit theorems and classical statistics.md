## Inequalities, convergence, and the Weak Law of large numbers

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-19-weak-law-of-large-numbers/)

![[L18.pdf]]

### Markov inequality

The *Markov inequality* enables us to use a bit of information about a distribution to learn something about the probability of "extreme events". For instance, if $X \geq 0$ and $E[0]$ is a small number, then $X$ is unlikely to be very large. In a mathematically formal way, 
$$
P(X \geq a) \leq \frac{E[X]}{a}.
$$
**Proof:**
	 Let $X$ be a non-negative r.v. such that
	 $$
	 E[X] = \int_0^{\infty}x f_X(x)dx.
	 $$
	 Given a value $a$
	 $$
	 \int_0^{\infty}xf_X(x)dx \geq \int_a^{\infty}xf_X(x)dx.
	 $$
	 However, over the interval of integration $x \geq a$, therefore,
	 $$
	 E[X] = \int_0^{\infty}xf_X(x)dx \geq\int_a^{\infty}xf_X(x)dx \geq a\int_a^{\infty}f_X(x)dx.
	 $$
	 Finally, comparing the two extremes of the inequality above we prove the *Markov Inequality*
	 $$
	 P(X \geq a) \leq \frac{E[X]}{a}
	 $$
$\square$

![[L18E1S.png]]

### Chebyshev inequality

Consider a r.v. $X$ with a finite mean $\mu$ and variance $\sigma^2$. If the variance is small, then $X$ is very unlikely to be too far from the man, *i.e.*,
$$
P\left(|X- \mu| \geq c\right)\leq \frac{\sigma^2}{c^2}.
$$
**Proof:**
	 From the [[8. Limit theorems and classical statistics#Markov inequality|Markov inequality]] it is know that
	 $$
	 P\left(\left(X -\mu\right)^2 \geq c^2\right) \leq \frac{E\left[\left(X - \mu\right)^2\right]}{c^2}.
	 $$
	 Where on the right-hand side
	 $$
	 P\left(\left(X -\mu\right)^2 \geq c^2\right)  = P(|X - \mu| \geq c)
	 $$
	 and on the left-hand side
	 $$
	 E\left[(X - \mu)^2\right] = \sigma^2.
	 $$
	 Therefore,
	 $$
	 P(|X - \mu| \geq c) \leq \frac{\sigma^2}{c^2}.
	 $$
$\square$

From this inequality one can show that
$$
P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}.
$$
This means that the probability of a r.v. being $k$ standard deviations away from the mean is less than, or equal, to $k^{-2}$.

### Convergence "in probability"

A sequence of $Y_n$ *converges in probability* to a number $a$ if for any $\varepsilon > 0$,
$$
\lim_{n \rightarrow\infty} P(|Y_n - a| \geq \varepsilon) = 0.
$$
Given $X_n \rightarrow a$ and $Y_n \rightarrow b$, if $g$ is a continuous function then
$$
g(X_n) \rightarrow g(a).
$$
Moreover,
$$
X_n + Y_n \rightarrow a + b.
$$
However, $E[X_n]$ need not to converge to $a$, *i.e.*, the convergence of r.v.'s does not imply the convergence of expectations.

### The Weak Law of Large Numbers ($WLLN$)

Let $X_1$, $X_2$, ..., $X_n$ be *independent and identically distributed* (*i.i.d.*) r.v.'s with finite mean $\mu$ and variance $\sigma^2$. The sample mean is defined as
$$
M_n = \frac{1}{n}\sum_{i=1}^n X_i.
$$
From the [[4. Discrete random variables#Linearity of expectation|linearity of expectation]] one can calculate the expected value of $M_n$ as
$$
E[M_n] = \frac{1}{n}\sum_{i=1}^nE[X_i] = \mu,
$$
which mean that the expected value of the sample mean is the same as the true mean of each observation. Additionally, the variance of the sample is
$$
\text{var}(M_n) = \frac{1}{n^2}\text{var}\left(\sum_{i = 1}^n X_i\right) = \frac{1}{n^2}\sum_{i = 1}^n \text{var}\left(X_i\right) = \frac{\sigma^2}{n}.
$$
Applying the [[8. Limit theorems and classical statistics#Chebyshev inequality|Chebyshev inequality]] and letting $n \rightarrow \infty$ we show that
$$
P(|M_n - \mu| \geq \varepsilon) \leq \frac{\sigma^2}{n \varepsilon^2} \rightarrow 0.
$$
This is the *Weak Law of Large Numbers* ($WLLN$) and it dictates that the sample mean $M_n$ in unlikely to be far off from the true mean $\mu$ for large $n$.

Another way to look at the $WLLN$ is given an event $A$ with $P(A) = p$ and $X_i$ being an [[4. Discrete random variables#The simplest random variable Bernoulli with *parameter* $p in {0, 1 }$|indicator r.v.]] of event $A$, $M_n$ is the **empirical frequency** of $A$.

### The pollster's problem

Say that $p$ is a fraction of a population that will vote *"yes"* in a referendum. One can infer the value of $p$ by defining a [[4. Discrete random variables#The simplest random variable Bernoulli with *parameter* $p in {0, 1 }$|indicator r.v.]] such as
$$
i^\text{th} \;\text{randomly selected person polled}\; : X_i = \begin{cases}1, \; \text{if \it{yes},}\\0, \; \text{otherwise.}\end{cases} 
$$
The selection process is [[5. Continuous random variables#Conditioning on a random variable; Independence; Bayes' rule|independently and uniformly distributed]]. The expected value of $X_i$ is $p$ and its variance is $p(1-p)$.

However, once we begin the pooling process our sample may or may not be representative of our population. One can only infer a probable value of $p$ from the sample mean $M_n$ with a margin of error. Additionally, the Chebyshev inequality is our only tool to say for sure that the probability of our error being greater than a given value $c$ is small.

For instance, say we would like our error to be $|M_n - p| < 0.01$. By adjusting the size of our sample $n$ we can make $P(|M_n - p| \geq 0.01)$ as small as we need. Since the maximum variance of the Bernoulli r.v. is $0.25$
$$
P(|M_n - p| \geq 0.01) \leq \frac{0.25}{n \cdot(0.01)^2}.
$$
## The Central Limit Theorem ($CLT$)

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-20-the-central-limit-theorem/)

![[L19.pdf]]

Let $\{X_i\}$ be a set of $n$ *i.i.d.* r.v.'s with finite mean $\mu$ and variance $\sigma^2$. Their sum $S_n$ has a variance of $n\sigma^2$ and the sample mean $M_n = \frac{S_n}{n}$ has a variance of $\frac{\sigma^2}{n}$. Notice that in the limit of $n \rightarrow \infty$ our distribution of $M_n$ becomes degenerate to $\mu$. Therefore, a more interesting way to define a composite distribution is
$$
\frac{S_n}{\sqrt{n}}
$$
such that its variance is $\sigma^2$. Furthermore, if we define a r.v. 
$$
Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}
$$
it has null expected value and variance equal to one. Since these statistics are constants their values shall not change as $n$ tends to infinity. The *Central Limit Theorem* ($CLT$) dictates that
$$
\lim_{n\rightarrow\infty}P(Z_n \leq z) = P(Z \leq z) \;| Z \sim N(0, 1).
$$

### Example

Say that $X_i$ is the weight of *i.i.d.* packages which are all [[5. Continuous random variables#Exponential random variable with *parameter* $ lambda > 0$|exponentially distributed]] with parameter $\lambda = 1/2$. What is the probability that we load a container with $100$ packages with a total weight greater than $210$?

We need to find
$$
P(S_n \geq 210) = P\left(\frac{S_n - n\mu}{\sigma \sqrt{n}} \geq \frac{210 - n\mu}{\sigma \sqrt{n}}\right).
$$
It is know that for the exponential distribution $\mu = \sigma = 1/\lambda = 2$, therefore, by identifying $Z_n$ on the right-hand side of the expression
$$
P(Z_n \geq 0.5) \approx P(Z \geq 0.5).
$$
Using the [standard normal table](https://en.wikipedia.org/wiki/Standard_normal_table)
$$
P(Z_n \geq 0.5) \approx 1 - P(Z \leq 0.5) = 1 - \Phi(0.5)
$$
Where $\Phi(0.5)$ is the *CDF* of the [[5. Continuous random variables#Normal (Gaussian) random variables|normal distribution]].

### The [Moivre-Laplace Central Limit Theorem](https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem) to the binomial

One way to approximate the value of the [[4. Discrete random variables#Binomial random variable with *parameters* $n; p in [0, 1]$|binomial distribution]] is through the evaluation of an [[5. Continuous random variables#Normal (Gaussian) random variables|normal distribution]] around the desired point. For instance, say that
$$
P(S_n = a) \approx P(a-1/2\leq S_n\leq a + 1/2),
$$
given that $S_n$ is the sum of a [[4. Discrete random variables#The simplest random variable Bernoulli with *parameter* $p in {0, 1 }$|Bernoulli r.v.]] with parameters $p$ and $q = (1-p)$. Moreover, since the expected value and variance of each of these Bernoulli r.v.'s yield a *CDF* of a standard normal if summed and transformed to 
$$
Z_n = \frac{S_n - np}{\sqrt{npq}},
$$
we can tweak the probability of the range in which $S_n$ falls so that the original probability can be estimated by the standard normal table.
## An introduction to classical statistics

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-23-classical-statistical-inference-i/)

![[L20.pdf]]


We learned how to make inference using the [[7. Bayesian inference#The Bayesian inference framework|Bayes rule]]. In that framework we had two unknown r.v. $\Theta$ and $X$, our interest of inference and our observation respectively. We had our  *prior PDF*'s, $f_{\Theta}$ and $f_{X|\Theta}$, and we are able to find the *posterior PDF* $f_{\Theta|X}$.

However, suppose that $\Theta$ is universal constant. It does not seems right to impose a stochastic characteristic to it. Moreover, we do not wish to impose any bias to $\Theta$'s real value by defining *priors*. 

To avoid such mistakes, we will define $\theta$ as an unknown constant. To find this constant we will define a function of our data, $g(X)$, as our estimator, $\hat{\Theta}$. It is important to notice how $\hat{\Theta}$ is a r.v. and the realization of our sample data, $g(x) = \hat{\theta}$, is our estimate of the unknown constant.

Additionally, the *PDF* for $X$ is not modeled conditionally to $\theta$ since $\theta$ is not a random. We must create many models for $X$, $f_X(x ; \theta)$, each for a distinct value of our unknown quantity.

### Estimators

*Given a certain sample $\{x_i\}$ related to a particular distribution $P(\{x_i\})$, we say $f(\{x_i\})$ is a consistent estimator of $\theta$ -being $\theta$ a particular property of $P$- if $\lim_{N \to \infty} f(\{x_i\})= \theta$. Furthermore, $f(\{x_i\})$ will not be biased when $\langle f(\{x_i\}) \rangle = \theta$. Finally, $f(\{x_i\})$ will be efficient if its variance is sufficiently small.* -- Practical Data Analysis course.

Take for instance the estimation of the mean of a distribution from which we take $n$ observations $X_i$. The sample mean is an estimator that has all three properties above.

### Confidence interval

An $1-\alpha$ *confidence interval* is an interval $\left[\hat{\Theta}^-, \hat{\Theta}^+\right]$ such that $P\left(\hat{\Theta}^-\leq \theta \leq \hat{\Theta}^+\right) \geq 1 - \alpha$ for all $\theta$. The interpretation of this definition is subtle. What this probability is saying is that a fraction of $1-\alpha$ of all our distinct samples will yield a confidence interval of which $\theta \in \left[\hat{\Theta}^-, \hat{\Theta}^+\right]$. We are not imposing any probability to our unknown constant.

![[Neyman_Construction_Confidence_Intervals.png]]

#### Confidence intervals for the estimation of the mean

Let $\hat{\Theta}_n$ be the sample mean estimator. Using the [[8. Limit theorems and classical statistics#The Central Limit Theorem ($CLT$)|central limit theorem]] we can approximate the probability of $\hat{\Theta}_n$ being far from the unknown constant mean $\theta$ as
$$
P\left(\frac{|\hat{\Theta}_n - \theta |}{\frac{\sigma}{\sqrt{n}}} \leq z\right) \approx \Phi(z)
$$
where $\Phi(z)$ is the *CDF* of the standard normal evaluated at $z$. The above probability is the same as saying
$$
P\left(\hat{\Theta}_n - \frac{z\sigma}{\sqrt{n}} \leq \theta \leq \hat{\Theta}_n + \frac{z\sigma}{\sqrt{n}}\right) \approx \Phi(z).
$$
This way we can identify the confidence intervals as
$$
\Theta^\pm = \hat{\Theta}_n \pm \frac{z\sigma}{\sqrt{n}} \; \forall \; z: \Phi(z) = 1- \frac{\alpha}{2} .
$$
Notice how the value in which we evaluate the standard normal table is related to $1 - \alpha/2$ because of the symmetry of normal distribution.

### Maximum Likelihood estimation

The *Maximum Likelihood estimation* is a method that picks $\theta$ that makes the sample $X$ at hand most likely. This means
$$
\hat{\theta}_{ML} = \text{arg} \; \underset{\theta}{\text{max}} \; p_X(x ; \theta).
$$
*Suppose we have a accurate functional form of a distribution $P(x_i|\theta)$, what should we expect from the likelihood $\mathcal{L}(\{x_i\}|\theta)$? Seems reasonable to think that a correct assumption will give us a maximal value of the likelihood function. Maximum Likelihood Estimation is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. [Wiki](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)We shall define a estimator $\hat{\theta} = Max\; \mathcal{L}(\{x_i\}|\theta)$, simply calculated by*
$$
\frac{\partial \mathcal{L}}{\partial \theta} \bigg |_{\hat{\theta}} = 0\;.
$$
*Furthermore, given the scale of number of which we are working with it is common practice to work with $\chi^2 = -2 \ln \mathcal{L}$. Then, the above estimator becomes $\hat{\theta} = Min\; (-2 \ln \mathcal{L})$.*
*In case we have more than one dimension, the maximization -or minimization- must be absolute, i.e., the extreme value of every parameter. In other words, the gradient of the likelihood should be null. Otherwise, you will need to formulate a hypothesis for sets of parameters, since it is clearly difficult to have complete knowledge about the entirety of the space.*
*Additionally, say we have a likelihood of a sample $x_i$, given a set of parameters $\vec{\theta}$ and $\vec{\psi}$. These parameter are inversible functions of each other, i.e., $\vec{\theta}(\vec{\psi})$ and $\vec{\psi}(\vec{\theta})$. If we try to maximize $\chi^2$, it is easy to see that a estimator is independent of the parametrization.* -- Practical Data Analysis course.
Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-1-probability-models-and-axioms/).
## Sample Space

![[L01.pdf]]

A probabilistic model consists of two steps: 1. Describe the possible outcome; 2. Describe the beliefs about the likelihood of the outcome.

The describe all possible outcomes we create a set $\Omega$ called the sample space. The elements of this set should be **mutually exclusive**, *i.e.*, if this element is selected it should mean that all the others were not selected. They should also be **collectively exhaustive**, *i.e.*, all elements should represents every possible outcome of the experiment. Finally, they should be at the "right" granularity, which mean that each element should contain only the information relevant to the experiment.

![[L01E1.png]]
For instance, the first sample space is legitimate because it cover all possible outcomes of the experiment.
	*In a legitimate sample space, it is not necessary for all elements to share the same level of detail or granularity to satisfy the criteria of exhaustiveness. What is important is that the sample space collectively covers all possible outcomes of the experiment. Each outcome must be mutually exclusive and collectively exhaustive with respect to the experiment being considered. Additional details that do not affect the outcome of the experiment (such as weather conditions in a coin flip) do not need to be uniformly described for all elements in the sample space.* **RANDOM**, Guy 

### Examples of sample spaces

#### Discrete and finite
The table below represents a grid with possible outcomes of two tetrahedral dice rolls. The X-axis represents the first roll and the Y-axis represents the second roll. 

|  | 1 | 2 | 3 | 4 |
| ---- | ---- | ---- | ---- | ---- |
| 4 | 1,4 | 2,4 | 3,4 | 4,4 |
| 3 | 1,3 | 2,3 | 3,3 | 4,3 |
| 2 | 1,2 | 2,2 | 3,2 | 4,2 |
| 1 | 1,1 | 2,1 | 3,1 | 4,1 |
Below, we find a sequential description -tree representation- of the same experiment, however we are now taking into consideration the order of the steps of the experiment.

![[sequential_tree.png]] 

#### Continuous and infinite
A random number in the unitary square in the first quadrant of the Cartesian Plane. In this example, of sample space is $(x, y)$ such that $0 \leq x, y \leq 1$.

![[L01E2.png]]

## Probability axioms

We want to specify which outcomes are more likely and which are less likely to occur. This problem quickly shows itself as impossible for continuous sample spaces. For instance, following the example in [[1. Probability models and axioms#Continuous and infinite|above]], if we want to find the probability of $(x,y) = (0,0)$ with infinite precision, $p$ would be zero. 

Consequently, we define an **event** as a subset of the sample space and assign probabilities to these events. If the outcome fall inside the set $A$, then we say that the event $A$ happened.
$$
P(A)\;\text{is the probability of an event}\;A\,|\, A\subset\Omega
$$
We define the probability $P$ with a set of *axioms*:
* Nonnegativity: $P(A) \geq 0$;
* Normalization: $P(\Omega) = 1$;
* (Finite) Additivity: $\text{If } A \cap B = \emptyset, \text{ then } P(A \cup B) = P(A) + P(B)$.
	 This means that if two events are disjoint, the probability of either one happening is the sum of their probability.

![[L01E3.png]]

The first consequence of these *axioms* is that probabilities of an event $P(A)$ is always less or equal than $1$.
**Proof:**
	Say that $A$ and $A^c$ are complementary events:
	$$
	A \cup A^c = \Omega,
	$$
	$$
	A \cap A^c = \emptyset.
	$$
	Given the normalization of probabilities, we find that
	$$
	P(A \cup A^c) = P(\Omega) = 1.
	$$
	Moreover, since $A$ and $A^c$ are disjoint, following the additivity *axiom*
	$$
	P(A) = 1 - P(A^c).
	$$
	However, knowing that probability is nonnegative, $P(A) \leq 1$.
$\square$ 
A simple consequence of the *axioms* is that the probability of the empty set is zero, *i.e.*, $P(\emptyset) = 0$.
**Proof:**
	 Be the $\Omega$ our sample space, its complementary will be $\emptyset$. Using the additivity *axiom* we can show that
	 $$
	 1 = P(\Omega) + P(\emptyset) .
	 $$
	 Since our probabilities are normalized $P(\emptyset) = 0$.
$\square$
For a finite number of disjoint events, one can write their joint probabilities as
$$
P(A_1 \cup A_2 \cup A_3 \cup ... \cup A_k) = \sum_{i = 1}^k P(A_i).
$$
This can be used to extrapolate the probability of a set of elements in the sample space. Say we have a set of elements $\{ s_1, s_2, s_3, ..., s_k\}$. We can imagine events consisting of single elements, $A_i = {s_i}$, and by using the additivity we show that 
$$
P(\{s_1\} \cup \{s_2\} \cup ... \cup \{s_k\}) = \sum_{i = 0}^kP(\{s_i\}),
$$
and abusing of the notation we might simplify the expression above to
$$
P(s_1, s_2, s_3, \dots, s_k) = \sum_{i = 1}^k P(s_i).
$$![[L01E4.png]]

If a event in contained within another, $A \subset B$, then $P(A) \leq P(B)$.
**Proof**:
	Being $A$ a subset of $B$, we can rewrite $B$ as 
	$$
	B = A \cup (B \cap A^c),
	$$
	and by using the additivity *axiom*
	$$
	P(B) = P(A) + P(B\cap A^c).
	$$
	Since the probability is nonnegative, $P(B) \geq P(A)$.
$\square$
For two non-disjoint events,*i.e.*, $A \cap B \neq \emptyset$, we can show that $P(A\cup B) = P(A) + P(B) - P(A\cap B)$.
**Proof**:
	 The set $A \cup B$ can also be written as the part of $A$ that do not intersect with $B$, plus the part of $B$ that does not intersect with $A$, plus the the intersection between $A$ and $B$. 
	 $$
	 (A\cup B) = (A \cap B^c) \cup (A\cap B) \cup (B \cap A^c).
	 $$
	 Let us call each of these new disjoint subsets $a$, $b$, and $c$, respectively. In terms of these new subsets, 
	 $$
	 P(A\cup B) = P(a) + P(b) + P(c).
	 $$
	 By adding and subtracting $P(b)$, we show that
	 $$
	 P(A \cup B) = (P(a) + P(b)) + (P(b) + P(c)) - P(b) = P(A) + P(B) - P(A\cap B).
	 $$
$\square$
Furthermore, a corollary from the consequence above is that $P(A\cup B) \leq P(A) + P(B)$, also known as the *union bound*. 

### Discrete uniform law and Uniform probability law

Say our discrete sample space $\Omega$ consists of $n$ equally likely elements. The probability of any given element is $\frac{1}{n}$. Furthermore, if a event $A$ contains $k$ elements the *discrete uniform law* says that
$$
P(A) = k \cdot \frac{1}{n}.
$$
However, if our sample space is not discrete, we shall use its area as a baseline for calculation, *i.e.*,
$$
P(A) = Area(A)\cdot \frac{1}{Area(\Omega)}.
$$
This is the *uniform probability law*. The algorithm to calculate the probability of an event is simple
1. Specify the sample space;
2. Specify the probability law;
3. Identify an event of interest;
4. Calculate!

### Discrete and infinite sample space

Imagine the following experiment. We want to know how many coin tosses it takes to roll heads. The sample space of this experiment is infinite since we can get head in the first toss, fifth, fifty, thousandth, and so on so forth. The is no guarantee that we will observe head in any specific iteration, therefore, our sample space is infinite even though it is discrete.

***GPT Addendum:***
	The experiment you've described is an example of a geometric distribution, where you are interested in the number of trials it takes for a success to occur (rolling heads in this case). The probability mass function (PMF) of a geometric distribution is given by:
	$$
	P(A = k) = (1-p)^{k-1}p,
	$$
	where  $p$ is the probability of success on a single trial, and $k$ is the number of trials.
	The PMF captures the idea that the number of trials needed for the first success follows a geometric progression, with decreasing likelihood of success as the number of trials increases.the equation captures the probability of having a sequence of $k−1$ failures followed by a success on the $k$-th trial. Note that the first success occurs on the $k$-th trial, and the preceding $k−1$ trials are all failures. 
	In our case,  $p$ is the probability of rolling heads on a single coin toss. Assuming a fair coin, $p = 0.5$. Therefore, the PMF becomes:
	$$
	P(A = k) = 0.5^{k-1}\times 0.5.
	$$
	To derive the probability of observing heads within a certain number of tosses, you can use the cumulative distribution function (CDF):
	$$
	P(A = k) = 1 - (1-p)^k
	$$
	For example, the probability of getting heads within the first three tosses would be:
	$$
	P(A \leq 3) = 1 - (1- 0.5)^3 = 0.875.
	$$
	So, there is an $87.5\%$ chance of observing heads within the first three tosses. In summary, you can use the geometric distribution and its PMF or CDF to calculate the probabilities associated with the number of coin tosses needed to observe heads in this experiment.

To work with this class of sample spaces we will introduce another *axiom*, the *Countable Additivity Axiom* states that if $A_1$, $A_2$, $A_3$,... is an infinite **sequence** of disjoint events, then
$$
P(A_1 \cup A_2 \cup A_3 \cup \dots) = \sum_{i = 1}^\infty P(A_i).
$$Additivity holds only for [[A. Mathematical background overview#Countable and uncountable sets|countable]] sequences of events. For instance, the unit square **is not countable**. therefore the *axiom* above is not valid it.
![[L01E5.png]]

## Interpretations of probability theory

Simply put probability is just a branch of mathematics. A set of *axioms* from which we derive *theorems*. 

However, one common interpretation is that the probability $P(A)$ is the frequency of which $A$ is the outcome of an experiment. This the foundation of the *frequentist* interpretation of probability theory.

Another possible interpretation emerge with probabilities such as $P(\text{raining tomorrow})$. Because of these types of question, probability theory is often interpreted as a description of beliefs. This is the *Bayesian* interpretation. 

At the minimum, probability gives us core rules to analyze phenomena with uncertain outcomes. A set of rules for consistent reasoning.

The evaluate and refine our predictions created from probability theory, we shall use statistics. In statistics we shall use data from the real world to create models from which the probabilities emerge.

![[probability_theory_statistics_diagram.png]]

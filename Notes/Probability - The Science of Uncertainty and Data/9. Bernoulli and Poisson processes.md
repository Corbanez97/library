Bernoulli and Poisson processes are stochastic memoryless processes in the discrete and continuous case respectively.

## The Bernoulli process

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-13-bernoulli-process/).

![[L21.pdf]]

A Bernoulli process is simply a sequence of independent [[4. Discrete random variables#The simplest random variable Bernoulli with *parameter* $p in {0, 1 }$|Bernoulli trials]], $X_i$, modeled with time-homogeneity, *i.e.*, the probability of $X_i$ being a success is the same independent of $i$. 
The Bernoulli process is the simplest example of an *Stochastic Process*. An stochastic process is a sequence of infinite r.v.'s with defined expected value, variance, marginal and joint.


![[L21E1S.png]]

### Fresh-start property

Given a Bernoulli process with parameter $p$ that has been happening for any amount of trials $N$ the independence of each individual trial assures us that whenever we begin our observations the stochastic process will continue to be a Bernoulli process. 

Moreover, if $N$ is defined as a geometric r.v. its [[4. Discrete random variables#Conditioning a geometric random variable|memorylessness]] characteristic assures us that the process will remain Bernoulli.

However, notice that if $N$ is defined with prior knowledge of the subsequent trials, the rest of the process will not retain its original characteristics.

### The time of the $k\text{th}$ success/arrival

Using a [[4. Discrete random variables#Geometric random variable with *parameter* $p 0 < p leq 1$|geometric r.v.]] one can model the time of the first success of subsequent trials of a [[4. Discrete random variables#The simplest random variable Bernoulli with *parameter* $p in {0, 1 }$|Bernoulli r.v.]], *i.e.*, a the first success/arrival of a Bernoulli process. 

Lets define $Y_k$ as the time of the $k\text{th}$ success. The time interval between two consecutive successes $T_k = Y_k - Y_{k-1} | k \geq 2$. Additionally, $Y_k$ can be seen as the sum of all time intervals from $1$ to $k$, *i.e.*
$$
Y_k = \sum_{i =1}^k T_i.
$$
From the [[9. Bernoulli and Poisson processes#Fresh-start property|fresh-start property]] we know that all $T_i$ are *i.i.d.* geometric r.v.'s with parameter $p$. Therefore,
$$
E[Y_k] = \frac{k}{p}
$$
and
$$
\text{var}(Y_k) = k\frac{(1-p)}{p^2}.
$$

The probability of the $k\text{th}$ success happening at time $t$ can be written as 
$$
P(Y_k = t) = P(k-1 \; \text{successes in the interval} \; t-1)\cdot P(\text{success}),
$$
where the first part is a [[4. Discrete random variables#Binomial random variable with *parameters* $n; p in [0, 1]$|binomial r.v.]] with parameters $(p, k-1, t-1)$ and the second part is simply $p$. Finally, the *PMF* for the $k\text{th}$ success is
$$
P(Y_k = t) = \binom{k-1}{t-1}p^k(1-p)^{t-1}.
$$
### The merging of independent Bernoulli processes

Let $X_t$ and $Y_t$ be two independent Bernoulli processes happening simultaneously with parameter $p$ and $q$ respectively. We define
$$
Z_t = \begin{cases}
	1 \; \text{if either r.v. resulted in a success}\\ 0\;\text{if both r.v.'s resulted in a failure}
\end{cases},
$$
or
$$
Z_t = \begin{cases}
	1 \; \text{w.p.}\; p + q - pq\\ 0\; \text{w.p.} \; (1-p - q +pq)\end{cases}.
$$
Therefore, the merged process is also a Bernoulli process with parameter $m = p + q -pq$.

### Poisson approximation to binomial

Picture a [[4. Discrete random variables#Binomial random variable with *parameters* $n; p in [0, 1]$|binomial r.v.]] with parameters $(n ,p)$ where $n \rightarrow \infty$ and $p \rightarrow 0$. Its *PMF* can be written in terms of $\lambda = np$ as
$$
p_{S}(k) = \frac{n!}{(n-k)!k!}\cdot \left(\frac{\lambda}{n}\right)^k\left(1 - \frac{\lambda}{n}\right)^n\left(1 - \frac{\lambda}{n}\right)^{-k}.
$$
The limit of this *PMF* as $n$ tends to infinity
$$
\lim_{n \rightarrow\infty} p_S(k) = \frac{\lambda^k}{k!}e^{-\lambda}
$$
is called the *Poisson PMF*.

## The Poisson process

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-14-poisson-process-i/).

![[L22.pdf]]

The Poisson process is simply a continuous expansion of the Bernoulli process. Given any interval of time, $\tau$, a Poisson process consists of $k$ independent arrivals/successes with a probability $P(k , \tau)$. This is the time homogeneity characteristic of this process. For all possible $k$
$$
\sum_{k = 0}^\infty P(k, \tau) = 1.
$$
However, to avoid simultaneity of successes in a point in time, one must define
$$
P(k, \delta) \approx \begin{cases}
	1 - \lambda\delta \quad\; \text{if} \; k =0\\
	\lambda\delta \quad\quad\quad \text{if} \; k =1\\
	0 \quad\quad\quad\;\; \text{if} \; k > 1\\
\end{cases}\;.
$$
$\lambda$ is the arrival rate at any given time interval.

A Poisson process has a *PMF* dependent of the number of arrivals and the time interval
$$
P(k ,\tau) = \frac{(\lambda\tau)^ke^{-\lambda \tau}}{k!}.
$$
The expected value of $k$ is
$$
E[k] = \lambda \tau
$$
and its variance is
$$
\text{var}(k) = \lambda \tau.
$$
These values can be found using the limit shown in the [[9. Bernoulli and Poisson processes#Poisson approximation to binomial|Poisson approximation]] for the expected value and variance of the binomial distribution.

### The time $T_1$ until the first arrival

Say that $T_1$ is the time of the first arrival for a given a Poisson process. This information can be encoded in the *CDF*
$$
F_{T_1}(t) = P(T_1 \leq t) = 1 - P(T_1 > t).
$$
However, the probability of the first arrival being greater than a given time is equal to the probability of no arrival in the interval $t$, therefore
$$
P(T_1 \leq t) = 1 - P(0, t) = 1 - e^{-\lambda t}.
$$
By [[5. Continuous random variables#Cumulative distribution function (*CDF*)|taking the derivative]] of the *CDF* one can find the *PDF* for this r.v., hence
$$
f_{T_1}(t) = \lambda e^{-\lambda t} \;\; \forall \; t\geq 0.
$$
This is the known exponential distribution.

### The time $Y_k$ of the $k\text{th}$ arrival

This is known as the *Erlang* distribution
$$
f_{Y_k}(y) = \lambda\frac{(\lambda y)^{k-1}e^{-\lambda y}}{(k-1)!}.
$$
Since the Poisson process is a limit of the Bernoulli process, the memorylessness and fresh-start property are sustained. Being so, we can use the same arguments used in the Bernoulli process to find the expected value and variance of $Y_k$. 
Therefore,
$$
E[Y_k] = \frac{k}{\lambda}
$$
and
$$
\text{var}(Y_k) = \frac{k}{\lambda^2}.
$$
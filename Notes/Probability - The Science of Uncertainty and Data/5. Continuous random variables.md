## Probability density function

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-8-continuous-random-variables/). 

![[L08.pdf]]

The probability density function (*PDF*) is a defined as the function which describes the probability of a random variable $X:\Omega \rightarrow \mathcal{R}$ being valued $x$, where $x$ can be a range of continuous values in $\mathcal{R}$. Since we are dealing with a continuum of values, the probability will be attributed to a possible interval of values $a\leq x \leq b$,
$$
P(a \leq X \leq b) = \int_a^b f_X(x)dx: f_X(x)\;\text{is the }\textit{PDF}.
$$
The *PDF* is positive-valued and normalized. We shall define a *continuous random variable* as any random variable which is probability can be measured by the expression above. Moreover, since the probability of a given $x = a$ is zero, the probability of a open set or a closet set is the same.
$$
P(a \leq X \leq b) = P(a < X < b).
$$

### Means and variances

Similar to how we defined [[4. Discrete random variables#Expectation of a random variable|expectation of a discrete random variable]], the expected value of a continuous random variable is the weighted sum of each possible value of the *PDF*, *i.e.*,
$$
E[X] = \int_{-\infty}^\infty x f_X(x)dx.
$$
Here we are assuming that
$$
\int_{-\infty}^\infty|x|f_X(x)dx < \infty.
$$
Moreover, the expected value has the same interpretation as before. It is the average in large number of independent repetitions of the experiment. The properties of $E[X]$ given that $X$ is a continuou r.v. are the same as if it were discrete, while making sure to replace any discrete sum by a integral.

Finally, since the variance of a r.v. is the expected value of a function of $x$, the properties are also the same.

### Exponential random variable with *parameter* $\lambda > 0$

An exponential random variable is defined by its *PDF*,
$$
f_X(x) = 
\begin{cases}
	\lambda e^{-\lambda x} \; \forall \; x \geq 0 \\
	0  \;\;\;\;\;\;\;\; \forall \; x < 0 \\
\end{cases}
.
$$
The probability of $X$ being greater or equal to some value $a$ is
$$
P(X\geq a) = \int_a^\infty \lambda e^{-\lambda x}dx = e^{-\lambda x}\bigg|_a^\infty = e^{-\lambda a}.
$$
The expected value of $X$ can be done by parts from
$$
E[X] =\int_0^\infty x\lambda e^{-\lambda x}dx = -\lim_{x\rightarrow\infty}xe^{-\lambda x} + \lim_{x\rightarrow 0}xe^{-\lambda x} + \int_0^\infty e^{-\lambda x}dx,
$$
where both limits on the right-hand side tend to zero. Solving the right-most integral we find
$$
E[X] =\frac{1}{\lambda}. 
$$
Similarly, we compute $E[X^2]$, yielding
$$
E[X^2] = \frac{2}{\lambda^2}.
$$
These results can be used to find the variance of the exponential random variable.
$$
\text{var}(X) = \frac{1}{\lambda^2}
$$
The exponential random variable is the extrapolation of the [[4. Discrete random variables#Geometric random variable with *parameter* $p 0 < p leq 1$|geometric random variable]] to the continuum. However, instead of modeling the number of trials one must wait for a success, we are modeling the time itself. 

### Cumulative distribution function (*CDF*)

The *CDF* is defined as
$$
F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x)dx.
$$
From the definition, one can also find the *PDF* from the *CDF* by deriving it.
$$
\frac{d  F_X}{dx}(x) = f_X(x),
$$
given that $F_X(x)$ is defined in the desired value of evaluation. $F_X$ is a non-decreasing function that tends to $1$ as $x \rightarrow \infty$ and tends to $0$ as $x \rightarrow -\infty$.

![[L08E5S.png]]

### Normal (Gaussian) random variables

Firstly, let's define the standard normal, $N(0, 1)$, *PDF*'s.
$$
N(0, 1): f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}.
$$
The notation $N(0, 1)$ is used here because we have a normal distribution with zero mean and variance equal $1$. The general normal $N(\mu, \sigma^2)$ defined by the *PDF*
$$
N(\mu, \sigma^2): f_X(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},
$$
has $E[X] = \mu$ and variance equal to $\sigma^2$.

One special fact about normal r.v.'s is that linear transformations such as $Y = aX + b$ are also normal r.v.'s. Say $X$ is $N(\mu, \sigma^2)$, then $Y$ will be $N(a\mu + b, a^2 \sigma^2)$ as expected by the [[4. Discrete random variables#Linearity of expectation|linearity of expectations]] and [[4. Discrete random variables#Properties of the variance|properties of the variance]].

#### Standard normal tables

Since there are no closed form for the *CDF* of $N(0, 1)$, it is customary to use [tables](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2024/block-v1:MITx+6.431x+1T2024+type@sequential+block@Standard_normal_table/block-v1:MITx+6.431x+1T2024+type@vertical+block@ch8-s5-tab1) with calculated values. With such tables we are able to find $P(X \leq x)$ for standard normal r.v.'s. However, if we want to find these probabilities for general normal r.v.'s we can use the fact that linear function of a normal r.v. remains normal. Therefore, if $X$ is $N(\mu, \sigma^2)$
$$
Y = \frac{X - \mu}{\sigma} : N(0, 1).
$$
For each value in the table, we can compute the expression above.
## Conditioning on an event; Multiple *r.v.'s*

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-9-multiple-continuous-random-variables/). 

![[L09.pdf]]

### Conditioning a continuous random variable on an event

By now, it should be apparent that all aspects of [[4. Discrete random variables|discrete random variables]] can be extrapolated to the continuum. Therefore, just as we *[[4. Discrete random variables#Conditional *PMFs*|condition PMFs]]*, in discrete cases, we can also condition the *PDF* of a continuous random variable. For bins of length $\delta$
$$
f_{X|A}(x) \cdot \delta = P(x \leq X \leq x + \delta | A),
$$
*i.e.*, the probability of $X \in (x, x + \delta)$ given the event $A$ is the area under the curve $f_{X|A}(x)$. Moreover, the probability of $X \in B$ is
$$
P(X \in B | A) = \int_B f_{X|A}(x)dx.
$$

From the first expression, one can show that
$$
f_{X|A}(x) =  
\begin{cases}
	\frac{f_{X}(x)}{P(A)}, \; \text{if} \; x\in A \\
	0, \;\;\;\;\;\;\; \text{if} \; x\notin A
\end{cases}.
$$
This comes from the [[2. Conditioning and Independence#Conditional probabilities|definition of conditional probabilities]]. If $x\notin A$, then their intersection will be zero. However, if $x\in A$, we can pick $\delta$ as small as we want so the interval $(x, x+\delta)$ is in $A$ and their intersection is simply $f_X(x)$.

### Memorylessness of the exponential *PDF*

Consider the following refined scenario:

You find yourself in a store, on a quest for a light bulb. The proprietor, a quirky mathematician, informs you that the lifespan of their light bulbs can be modeled using an [[5. Continuous random variables#Exponential random variable with *parameter* $ lambda > 0$|exponential PDF]]. He then poses an intriguing question: Would you prefer to purchase a new light bulb or a used one? What would be the optimal choice in this situation?

The probability that a new light bulb with lifetime $T$ will burnout after a time $x$ is 
$$
P(T > x) = e^{-\lambda x} \;\forall\; x \geq 0.
$$
For a light bulb used for a time $t$, we define a new r.v. $X = T- t$. The probability of the used light bulb burning out at a time $x$ given our new specifications is
$$
P(X > x | T > t) = \frac{P(T- t > x , T > t)}{P(T > t)}.
$$
The numerator on the right-hand side is simply $P(T > t + x)$. Therefore, replacing both parts of the fraction by the definition of $P(T > x)$ we find
$$
P(X > x | T > t) = \frac{e^{-\lambda(t + x)}}{e^{-\lambda t}} = e^{-\lambda x}.
$$
The lifetime of a new light bulb and an used one are the same. Both cases are probabilistically identical. Therefore, the light bulb does not remember the time is has been used and the probability of it burning in the next time $\delta$ is always the same.

### Total probability and expectation theorems

The definition of the [[2. Conditioning and Independence#Total probability theorem|total probability theorem]] says that the overall probability of an event $B$ equals the sum of the conditional probability given $A_i$ times the probability of $A_i$. For a range of $X$ values we can write
$$
P(X < x) = \sum_i P(A_i)P(X < x|A_i).
$$
We have already defined that $P(X < x)$ is the *[[5. Continuous random variables#Cumulative distribution function (*CDF*)|CDF]]* of $X$. Therefore,
$$
F_X(x) = \sum_iP(A_i)F_{X|A_i}(x),
$$
which by deriving both sides yield
$$
f_X(x) = \sum_i P(A_i)f_{X|A_i}(x).
$$
This is the total probability theorem for a continuous random variable. From this theorem we may multiply both sides by $x$ and integrate both sides over $x$. This will result in the [[4. Discrete random variables#Total expectation theorem|total expectation theorem]] for continuous random variables.
$$
E[X] = \sum_i P(A_i)E[X|A_i].
$$
### Mixed distributions

Say that Alice has one dollar and with probability $1/2$ she will play the lottery and win a random value in $\text{unif}[0, 2]$. However, the contrary is that she will not play and keep her one dollar. Therefore, Alice's money after one trial is
$$
X = 
\begin{cases}
	\text{unif}[0, 2] \;\; \text{with probability }\frac{1}{2}\\
	1 \;\;\;\;\;\;\;\;\;\;\;\;\;\; \text{with probability }\frac{1}{2}
\end{cases}
.
$$
Notice that $X$ is neither discrete nor continuous. The prior is true because with a probability of one half $X$ takes a continuum of values and the latter is true because $P(X = 1) = 1/2$.

In such cases, it is natural to describe the mixed random variable $X$ in terms of the *CDF* since this function is defined both for discrete and continuous random variables. Moreover, since we used *CDF* to describe our random variable, the expressions in [[5. Continuous random variables#Total probability and expectation theorems|above]] are also valid.

### Jointly continuous r.v.'s and joint *PDFs*

The *joint PDF* is defined as
$$
f_{X, Y}(x, y) \geq 0.
$$
The probability of these joint continuous random variables falling into the event $B$ is described by
$$
P\big((X, Y) \in B \big) = \int_{(x, y)\in B}f_{X, Y}(x, y)dxdy.
$$
Again, this probability is normalized adding up to $1$.

In the infinitesimal case, 
$$
P(a \leq X \leq a + \delta, c \leq Y \leq c + \delta) \approx f_{X, Y}(a, c) \cdot \delta^2.
$$
![[L09E5S.png]]

#### From the joint to the marginals

So far, the process of transforming from *PMF* to *PDF* notation is simply replacing the sum by an integral. For instance, to find the marginals of a *joint PDF* you just integrate over all possible values of the joint r.v., *i.e.*,
$$
f_X(x) = \int f_{X, Y}(x, y) dy.
$$
Every other property of discrete r.v.'s can be used in continuous r.v.'s being careful to replace the sum by an integral if there is one.

### The *joint CDF*

Similarly to how we defined the *[[5. Continuous random variables#Cumulative distribution function (*CDF*)|CDF]]* as the probability of our random variable $X$ taking values below $x$, we can do the same for more than one continuous random variable.
$$
F_{X, Y}(x, y) = P(X \leq x, Y \leq y) = \int_{-\infty}^y\int_{-\infty}^x f_{X, Y}(s, t)dsdt.
$$
Analogously,
$$
f_{X, Y}(x, y) = \frac{\partial^2 F_{X, Y}}{\partial y \partial x}(x,y).
$$
## Conditioning on a random variable; Independence; Bayes' rule

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-10-continuous-bayes-rule-derived-distributions/). 

![[L10.pdf]]

### Conditional *PDFs*, given another random variable

The *[[4. Discrete random variables#Conditioning on a random variable; Independence of r.v.'s|conditional PMF]]* given another r.v is
$$
p_{X|Y}(x|y) = \frac{p_{X,Y}(x, y)}{p_Y(y)},
$$
where $p_{X|Y}(x|y) = P(X = x | Y =y)$. However, in the continuous case the same could impose some problems since the probability of a continuous random variables being equal to a specific values is always zero.

Say $f_{X|A}(x) \cdot \delta \approx P(x \leq X \leq x + \delta | A)$, if we define $A = \{y \leq Y \leq y + \epsilon\}$, we can use the definition of conditional probabilities to see that
$$
P(x \leq X \leq x + \delta | y \leq Y \leq y + \epsilon) = \frac{f_{X, Y}(x, y)\delta\epsilon}{f_Y(y)\epsilon}.
$$
We define the conditional *PDF* of a r.v. given another continuous r.v. as
$$
f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}.
$$
Moreover, the probability is defined as
$$
P(X \in A | Y = y) = \int_A f_{X|Y}(x|y)dx.
$$The conditional *PDF* is always positive. Additionally, if we integrate on both sides we can see that it is also normalized,
$$
\int_{-\infty}^\infty f_{X|Y}(x|y)dx = \frac{\int_{-\infty}^\infty f_{X, Y}(x, y)dx}{f_Y(y)} = 1.
$$

***GPT Addendum:***
	 You're correct that for continuous random variables, the probability of the variable taking on a specific value is zero. This is because there are infinitely many possible values the variable can take on.
	 To define the conditional probability of a continuous random variable given another, we use the concept of conditional density functions.
	 For two continuous random variables $X$ and $Y$, the conditional probability density function of $X$ given $Y = y$ is defined as:
	$$f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}$$
	where:
	-$f_{X,Y}(x, y)$ is the joint probability density function of $X$ and $Y$,
	-$f_Y(y)$ is the marginal probability density function of $Y$.
	This definition is analogous to the definition of conditional probability for discrete random variables, but we use density functions instead of probabilities.
	Then, the conditional probability of $X$ being in a set $A$ given $Y = y$ is defined as:
	$$P(X \in A| Y = y) = \int_{A} f_{X|Y}(x|y) dx$$
	This gives us a way to calculate conditional probabilities for continuous random variables. In the context of conditional probability, when we write $Y = y$, it's a shorthand notation. It doesn't mean that $Y$ equals $y$ with probability 1. Instead, it means that we are considering the behavior of the random variable $X$ in the hypothetical scenario where $Y$ is known to be $y$.

### Total probability and total expectation theorems

The [[2. Conditioning and Independence#Total probability theorem|total probability theorem]] emerges with ease from the definition of conditional probabilities of continuous random variables. The integral of the *joint PDF* on a given r.v. is the *marginal PDF* for its joint. Therefore,
$$
\int_{\mathcal{R}}f_{X,Y}(x, y)dy = \int_{\mathcal{R}}f_Y(y)f_{X|Y}(x, y)dy = f_X(x).
$$
Where the left-hand side is the total probability theorem for continuous r.v.'s. Additionally, the [[4. Discrete random variables#Total expectation theorem|total expectation theorem]] can be rewritten to take into account the continuity of our variables by simply replacing the sum by integrals and the *PMF* by *PDF*. The expected value of $X$ given that $Y = y$ is
$$
E[X|Y = y] = \int_\mathcal{R} x f_{X|Y}(x|y)dx.
$$
Multiplying both sides by $f_Y(y)$ and integrating over $y$ yields
$$
\int_\mathcal{R} f_Y(y)E[X|Y = y]dy = \int_\mathcal{R}f_Y(y)\int_\mathcal{R} x f_{X|Y}(x|y)dx\;dy.
$$
Switching the variables of integration we can rearrange the left-hand side to
$$
\int_\mathcal{R}x \int_\mathcal{R} f_Y(y)f_{X|Y}(x|y) dy \; dx = \int_\mathcal{R} x f_X(x)dx = E[X].
$$
Comparing the last two equation we get the total expectation theorem for continuous random variables
$$
E[X] = \int_\mathcal{R}f_Y(y)E[X|Y = y]dy.
$$
### Independence

By analogy with the [[4. Discrete random variables#Independence of random variables|discrete case]] we will define independence of continuous random variables as
$$
f_{X, Y}(x,y) = f_X(x)\cdot f_Y(y) \ \forall \ x, y.
$$
This is equivalent as saying that
$$
f_{X|Y}(x|y) = f_X(x)\ \forall\ y : f_Y(y) > 0 \ \forall \ x.
$$
Therefore, knowing something about $Y$ does not tell anything about $X$. The properties of [[4. Discrete random variables#Independence and expectations|expectation]] and [[4. Discrete random variables#Independence and variance|variance]] for discrete random variables still holds for the continuous cases.

### Independent normals

![[bell_surface.png]]
Say that $X$ and $Y$ are both $N(0, 1)$, their joint *PDF* is
$$
f_{X, Y}(x, y) = \frac{1}{2\pi}\exp\bigg[-\frac{1}{2}\big(x^2 + y^2\big)\bigg].
$$
This function consists of concentric circumference centered at the origin. However, if either of the independent random variables have non-zero mean or variance different then $1$ their joint *PDF* will be
$$
f_{X, Y}(x, y) = \frac{1}{2\pi \sigma_x \sigma_y}\exp\bigg[-\frac{(x -\mu_x)^2}{2\sigma_x^2} -\frac{(y -\mu_y)^2}{2\sigma_y^2}\bigg].
$$
In this case, instead of circles we will have elipses centered at $(\mu_x, \mu_y)$.

### Baye's rule

We can rewrite the [[2. Conditioning and Independence#Baye's rule and inference|Baye's rule]] in terms of *PMF* and *PDF* simply by using the symmetrical property of joint distributions. Since
$$
f_{X, Y}(x, y) = f_X(x)f_{Y|X}(x|y) = f_Y(y)f_{X|Y}(y|x),
$$
then solving for $f_{X|Y}(x|y)$ yields
$$
f_{X|Y}(x|y) = \frac{f_X(x)f_{Y|X}(x|y)}{f_{Y}(y)}.
$$
Furthermore, we can find the marginal $f_Y(y)$ using the [[5. Continuous random variables#Total probability and expectation theorems|total probability theorem]].

However, there are cases where a discrete r.v. is conditioned by a continuous r.v. and vice-versa. Therefore, the Baye's rule can describe a mix of *PMF* and *PDF*. For instance, given a discrete r.v. $K$ and a continuous r.v. $Y$, the *PMF* $p_{K|Y}(k|y)$ is 
$$
p_{K|Y}(k|y) = \frac{p_K(k)f_{Y|K}(y|k)}{f_Y(y)}. 
$$
#### Example

You are receiving a binary signal from a wire. Ideally, your measurement $K$ could be either $1$ or $0$, both equally likely. However, in our real setup, our measurement is affected by some noise $W$ normally distributed, $N(0, 1)$. Therefore, the real measurement we are making is actually
$$
Y = K + W.
$$
We want to know how likely is that $K = 1$ given $Y = y$.

Firstly, we know that the prior *PMF* of $K$ is
$$
p_K(k) = \frac{1}{2}.
$$
The *PDF* of $Y$ given $K$ is the displacement of the standard normal given a value of $K$, therefore
$$
f_{Y|K}(y|k) = f_W(y-k) =  N(k, 1).
$$
If we apply the [[5. Continuous random variables#Total probability and expectation theorems|total probability theorem]] on the expression above over all values of $K$, we find the marginal $f_Y(y)$
$$
f_Y(y) = \frac{1}{2\sqrt{2\pi}}\bigg[\exp \bigg\{-\frac{(y + 1)^2}{2}\bigg\} +\exp\bigg\{-\frac{(y -1)^2}{2}\bigg\}\bigg].
$$
Replacing these expressions on the Baye's rule we solve for the posterior for $K=1$
$$
p_{K|Y}(1|y) = \frac{1}{1+e^{-2y}}.
$$
<iframe src="https://www.desmos.com/calculator/pqsbtnyzit?embed" width="700" height="400" style="border: 1px solid #ccc" frameborder=0></iframe>

We can see that for $p_{K|Y}(1|y)$ close to $1$, it is very likely that $K=1$. Analogously, for values close to $-1$, it is very unlikely that $K=1$, therefore, $K$ is probably $0$. Notice that if we measure $Y = 0$, both values of $K$ are equally likely. 

This is modeling a simple communication system of one bit of information with the interference of some random noise.
"In this unit, we introduce random variables, with a focus on the discrete case. We describe their distribution through probability mass functions. We also introduce expectations and variances, and some of their properties, as well as the concept of independence. These concepts are illustrated in the context of various common random variables."

## Probability mass functions and expectations

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-5-discrete-variables-probability-expectations/). 

![[L05.pdf]]
### Random variables

Random variable (**r.v.**) is a function that takes values of our sample space $\Omega$ and returns numerical values in $\mathcal{R}$.  For instance, say we have a class with four students $A$, $B$, $C$, and $D$. We have a function $W(A)$ that gives the weight of student $A$.

***GPT Addendum:***
	 A random variable, denoted typically by $X$, is formally defined as a function $X:\Omega \rightarrow \mathcal{R}$ in the context of probability theory and statistics. Here, $\Omega$ represents the sample space, which comprises all possible outcomes of a random experiment or process, and $\mathcal{R}$ denotes the set of real numbers. The random variable $X$ assigns a real-valued outcome to each element in the sample space $\Omega$. Consequently, the value of $X$ is not deterministic but rather subject to randomness inherent in the underlying experiment or process. This formal definition encapsulates the fundamental role of random variables in modeling uncertain phenomena and analyzing their probabilistic properties.

A function of one or several random variables is also a random variable, *i.e.*, $Z = X + Y$ is a random variable if $X$ and $Y$ are random variables. Random variables can be either discrete or continuous.

From now on we will use capital latim letters to denote random variables and its lowercase as the numerical values, *e.g.*, $X(A) = x$, $Y(A) = w$, $X + Y - x = w$. **Pay attention to notation!**

![[L05E1S.png]]

### Probability mass function (PMF) of a discrete r.v. $X$

Also known as the *[[1. Probability models and axioms#Discrete uniform law and Uniform probability law|probability law]]* or *probability distribution* of $X$, the probability mass function can be thought as an event derived from $\Omega$. Picture the following scenario.

![[PFM_example.png]]

From our sample space one might ask whats is the probability of $x=5$. Rolling back in our mathematical spaces, it is clear that
$$
p_X(x=5) = P(X=5) = P(a \cup b) = \frac{1}{2}.
$$
Notice how we introduced a new notation. The **PMF** of the random variable $X$ is $p_X(x)$ defined as
$$
p_X(x) = P(\omega \in \Omega : X(\omega) = x).
$$
The **PMF** inherits some [[1. Probability models and axioms#Probability axioms|properties of the probability]]. For instance,
$$
p_X(x) \geq 0
$$
and
$$
\sum_x p_X(x) = 1.
$$

![[L05E3S.png]]

#### The simplest random variable: Bernoulli with *parameter* $p \in \{0, 1\}$

The random variable $X$ is valued
$$
X = 
\begin{cases}
	1, \text{with probability} \;p \\ 0, \text{with probability} \; 1-p
\end{cases}
\;\;.
$$
The PMF for $X$ has only two values, $p_X(1) = p$ and $p_X(0) = 1-p$. This r.v. is modeling a trial that results in success and failures, *e.g.*, Heads or Tails, red balls or any other color, *etc*. For this reason, the **Bernoulli** with *parameter* $p \in \{0, 1\}$ is an indicator of an event $A$, *i.e.*, $I_A = 1 \Leftrightarrow A \; \text{occurs}$.

![[indicator_rv_diagram.png]]

The probability of the event $A$ is equal to the probability of our indicator being $1$.
$$
p_{I_A}(1) = P(I_A = 1) = P(A).
$$
#### Discrete uniform random variable with *parameters* $a$ and $b$

Say we define our sample space $\Omega$ as a subset of the $\mathcal{Z}$ consisting of 
$$
\Omega = \{\{a, a+1, a+2, ...,b\} |  (a, b) \in \mathcal{Z}\;\text{and}\;a \leq b\}.
$$
Our r.v. $X: X(\omega) = \omega$ is simply a selection of a random value from $\Omega$. $X$ has a PMF
$$
p_X(x) = \frac{1}{b-a+1},
$$
given that we have no reason to think that any number is more likely than the other. This case models complete ignorance of our outcomes.

Nevertheless, in the case where $a =b$, our r.v. is constant/deterministic since $p_X(x = a) = 1$.

#### Binomial random variable with *parameters* $n; p \in [0, 1]$

Given $n$ tosses of a coin with probability $P(Heads) = p$, our sample space $\Omega$ consists of sequences of length $n$ of $H = \text{Heads}$ or $T = \text{Tails}$. We define on $\Omega$ a random variable $X$ that is the number of $H$ observed.

![[binomial_rv_tree_diagram.png]]

The r.v. $X$ models the number of successes in a given number of [[2. Conditioning and Independence#Independence|independent]] trials. We can find its PMF using the [[3. Counting#Combinations|binomial probability]]. The value $x$ is the number of $H$ observed, therefore
$$
P(\text{x heads}) = P(X = x) = p_X(x).
$$
And as we know, the [[3. Counting#Combinations|binomial probability]] of $x$ heads is
$$
p_X(x) = \binom{n}{x}p^x(1-p)^{n-x}.
$$

#### Geometric random variable with *parameter* $p: 0 < p \leq 1$

Say we perform an experiment infinitely many times and that its outcomes are always [[2. Conditioning and Independence#Independence|independent]].
Moreover, we know that the possible outcomes for our experiment are only two, $A$ with a probability of $p$ or $B$ with a probability of $1-p$. These experiments give rise to a sample space $\Omega$ that consists of infinite sequences of $A$ and/or $B$. Let's define $X$ over $\Omega$ such that
$$
X(w) = x:x = \text{number of experiments until the first} \; A \; \text{was observed}.
$$
The PMF $p_X(x)$ is the probability that for the first $x-1$ experiments we found the complement of our target event, therefore
$$
p_X(x) = P(X = x) = P(\underset{x-1 \; \text{times}}{B \cap B \cap B \cap ...\cap B})\times P(A) = (1-p)^{x-1}p.
$$
![[L05E5S.png]]

### Expectation of a random variable

Consider the following scenario. You play a game $1000$ times in a day. The gain that you have at each play is the random variable
$$
X = 
\begin{cases}
	1, \text{with probability} \; 2/10 \\ 
	2, \text{with probability} \; 5/10 \\ 
	4, \text{with probability} \; 3/10 
\end{cases}
\;\;.
$$
The average gain is something around 
$$
g = \frac{200 + 2\cdot500 + 4\cdot 300}{1000},
$$
since we are gaining $1$ two tenths of the times, $2$ five tenths and $4$ three tenths. By rewriting the expression in terms of the value of the random variable and its probability we define the *Expected value*
$$
E[X] = \sum_x xp_X(x).
$$
#### Expected value of a uniform random variable

For a uniform distribution on $0, 1, 2, ..., n$, the PMF is a constant $p_X(x) = \frac{1}{n+1}$. Calculating the expected value we find
$$
E[X] = 0 \cdot \frac{1}{n+1} + 1\cdot \frac{1}{n+1} + ... + n \cdot \frac{1}{n+1} = \frac{1}{n+1} \cdot(0 + 1 + ...+ n) = \frac{1}{n+1}\cdot \frac{n(n+1)}{2} = \frac{n}{2}.
$$
This shows us that for any symmetrical random variable, the expected value is always the center of symmetry.

#### Expected value of a population

In a population with $n$ elements, the random variable $X$ is valued $x_i$ for the $i^{\text{th}}$ element. Picking an element at random has a probability of $\frac{1}{n}$, therefore the PMF of $X$ is
$$
p_X(x_i) = \frac{1}{n}.
$$
Given this PMF and our sample space the expected value of $X$ is simply the average of the population,*i.e.*,
$$
E[X] = \sum_i x_i \frac{1}{n} = \frac{1}{n} \sum_i x_i.
$$
### Elementary properties of expectations

* If $X\geq 0$, then $E[X] \geq 0$;
* If $a \leq X \leq b$, then $a \leq E[X] \leq b$;
* If $c$ is a constant, $E[c] = c$.

#### Expected value rule for calculating $E[g(X)]$

Let $X$ be a random variable and let $Y = g(X)$.  The expected value of $Y$ is
$$
E[Y] = E[g(X)] = \sum_x g(x)p_X(x).
$$
#### Linearity of expectation

Given a random variable $X$ and $Y : Y = aX + b$, the expected value of $Y$ is
$$
E[Y] = E[aX + b] = aE[X] + b.
$$
This is surely a intuitive property of the expected value, however we may prove it using the expected value rule.

**Proof:**
	 Given $X$ and $Y:Y=aX+b$,
	 $$
	 E[Y] = \sum_x g(x)p_X(x) = \sum_x (ax + b)p_X(x).
	 $$
	 Using the properties of summation,
	 $$
	 E[Y] = a\sum_x x p_X(x) + b\sum_x p_X(x).
	 $$
	 Since our PMF is normalized, we find that
	 $$
	 E[Y] = aE[X] + b.
	 $$
$\square$
## Variance; Conditioning of an event; Multiple r.v.'s

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-6-discrete-random-variable-examples-joint-pmfs/). 

![[L06.pdf]]

### Variance - a measure of the spread of a *PMF*

Assuming that $X$ is a random variable with mean $\mu = E[X]$, the variance of $X$ is defined as
$$
\text{var}(X) = E[(X - \mu)^2].
$$
The variance of $X$ is a positive quantity, that can be computed using the [[4. Discrete random variables#Expected value rule for calculating $E[g(X)]$|expected value rule]].

The variable $var(X)$ has units of $[X]^2$, therefore it is more intuitive to use the standard deviation
$$
\sigma_X = \sqrt{\text{var}(X)}.
$$
#### Properties of the variance

* $\text{var}(aX + b) = a^2 \text{var}(X)$;
* $\text{var}(X) = E[X^2] - E[X]^2$.

#### Variance of the Bernoulli *PMF*

Let $X$ be
$$
X = 
\begin{cases}
	1, \text{with probability} \;p \\ 0, \text{with probability} \; 1-p
\end{cases}
\;\;,
$$
such that
$$
E[X] = \sum_x x p_X(x) = 1\cdot p + 0\cdot (1-p) = p.
$$
Computing the variance of $X$ we find
$$
\text{var}(X) = \sum_x(x-E[X])^2 p_X(x) = (1 - p)^2 p + (0-p)^2(1-p) = p(1-p).
$$
Even tough the result looks simple, it tells us a great deal about the behavior of our experiments. Say a coin is fair with $p = 0.5$, the variance is maximized meaning that this is the most random case of coin tossing.

<iframe src="https://www.desmos.com/calculator/etf99oinx6?embed" width="700" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>

#### Variance of the uniform

Given a uniform random variable $X$ from $0$ to $n$, its variance is
$$
\text{var}(X) = E[X^2] - (E[X])^2 = \frac{1}{n+1} \cdot (0^2 + 1^2 + 2^2 + ... + n^2) - \bigg(\frac{n}{2}\bigg)^2 = \frac{1}{12}n(n+2).
$$
The proof of the sum of squares from $0$ to $n$ can be found in [this blog post](https://brilliant.org/wiki/sum-of-n-n2-or-n3/).
### Conditional PMF and expectation, given an event

The expected value of a given *PMF* and the expected value of the same *PMF* given that an event $A$ has occurred are parallel in their calculation, but differ in their probability distributions.

For a given *PMF*,
$$
p_X(x) = P(X = x),
$$the expected value is calculated as 
$$
E[X] = \sum x p_X(x).
$$This represents the average or mean value of the random variable $X$.

When an event $A$ occurs, we transition to conditional probabilities, resulting in a new *PMF*,
$$
p_{X|A}(x) = P(X = x | A).
$$The expected value then becomes 
$$
E[X | A] = \sum x p_{X|A}(x).
$$This represents the average or mean value of the random variable $X$ given that event $A$ has occurred.

Similarly, for a function 
$$
g(X),
$$its expectation changes from 
$$
E[g(X)] = \sum g(x)p_X(x)
$$to 
$$
E[g(X) | A] = \sum g(x)p_{X|A}(x),
$$reflecting the impact of event $A$ on the expectations.

In essence, conditioning on an event $A$ adjusts the probabilities and consequently the expectations, providing a new perspective on the behavior of the random variable $X$. This parallelism maintains the fundamental structure of expectation calculations while allowing for the incorporation of additional information through conditioning.

### Total expectation theorem

Let $\Omega$ be a sample space where the event $B$ can happen under any $A_i: i = 0, 1, ..., n$. The [[2. Conditioning and Independence#Total probability theorem|total probability theorem]] tells us that
$$
P(B) = \sum_i P(A_i)P(B|A_i).
$$
Now, say that the random variable $X$ defined over $\Omega$ is valued $x$ when the event $B$ occurs. We can rewrite the total probability theorem in terms of the *PMF*, yielding
$$
p_X(x) = \sum_i P(A_i)p_{X|A_i}(x).
$$
Multiplying both sides of the expression by $x$ and summing over all possible values of $x$ we find
$$
\sum_x xp_X(x) = \sum_x x \sum_i P(A_i)p_{X|A_i}(x) = \sum_i P(A_i) \sum_x x p_{X|A_i}(x).
$$
The left-hand side of the expression is the expected value of $X$ and the right-hand side is the expected value of $X$ given any $A_i$, therefore
$$
E[X]=\sum_i P(A_i)E[X|A_i].
$$
This is the total expectation theorem. This theorem tells us that the expected value of a random variable can be calculated under different scenarios by finding the expected value under each scenario, $A_i$ and weighting them based on its probability, $P(A_i)$.

Such property gives us the change to calculate complex expected values by *dividing and conquering* the complex scenario into simpler ones.

### Conditioning a geometric random variable

Let $X$ be a [[4. Discrete random variables#Geometric random variable with *parameter* $p 0 < p leq 1$|geometric random variable]] with parameter $p$. The *memorylessness* property is the fact that the number of remaining coin tosses, given that our experiment failed in the first $n$ times, is also geometric with parameter $p$. In other words,
$$
p_{X-n|X>n}(k) = p_{X}(k).
$$
We can calculate $E[X]$ by first using the [[4. Discrete random variables#Linearity of expectation|linearity property of expectation]],*i.e.*,
$$
E[X] = E[X - 1] + 1.
$$
Using the [[4. Discrete random variables#Total expectation theorem|total expectation theorem]] on the right-hand side of the equation we find that
$$
E[X] = 1 + p_X(1)E[X-1|X=1] + (1-p)E[X-1|X>1].
$$
The term $E[X - 1| X = 1]$ is equal to zero, since knowing $X = 1$ leads to $X-1 = 0$. Moreover, taking into account the *memorylessness* of the geometric r.v. we know that the term $E[X-1|X>1] = E[X]$. Replacing these values into the expression and solving por $E[X]$ yields
$$
E[X] = \frac{1}{p}.
$$
![[L06E6S.png]]

### Multiple random variables and joint *PMFs*

Given two random variables, $X:p_X$ and $Y:p_Y$, both defined over the same sample space $\Omega$, their *joint PMF* is
$$
p_{X,Y}(x,y) = P(X = x \cap Y = y).
$$
The *joint PMF* also is normalized, such that
$$
\sum_x \sum_y p_{X,Y}(x, y) = 1.
$$
Each *PMF* for the singular random variables $X$ and $Y$ are called *marginal PMF*. Therefore, given a *join PMF* the *marginals* are easily found by summing over the complementary random variable. For instance,
$$
p_X(x) = \sum_y p_{X, Y}(x, y).
$$
These characteristics are also true for any given number of random variables and their joint probability. Moreover, the [[4. Discrete random variables#Expected value rule for calculating $E[g(X)]$|expected value rule]] is sustained for *joint PMFs*.

### Linearity of expectations

Let $X:p_X(x)$ and $Y:p_Y(y)$ be random variables defined over $\Omega$. The expected value of $Z = X + Y$ is
$$
E[X + Y] = E[X] + E[Y].
$$
**Proof:**
	 Let $Z$ be the sum of $X$ and $Y$, therefore,
	 $$
	 E[Z] = E[X+Y]= \sum_x \sum_y (x+y)p_{X, Y}(x, y).
	 $$
	 Applying the distributive property of summation, we find
	 $$
	 E[X+Y]= \sum_x\sum_yxp_{X, Y}(x, y) + \sum_x\sum_yyp_{X, Y}(x, y). 
	 $$
	 Both terms on the right-hand side of the expression are simply the *marginal PMFs*, therefore
	 $$
	 E[X+Y] = \sum_yp_Y(y) + \sum_x p_X(x) = E[X] + E[Y].
	 $$
$\square$

This linearity also applies to any number of random variables, which means that
$$
E\bigg[\sum_i^nX_i\bigg] = \sum_i^n E[X_i].
$$
### The mean of the binomial

Given $X$, a [[4. Discrete random variables#Binomial random variable with *parameters* $n; p in [0, 1]$|binomial random variable]] with parameters $n$ and $p$, it is possible to subdivide it into a range of $n$ [[4. Discrete random variables#The simplest random variable Bernoulli with *parameter* $p in {0, 1 }$|indicator variables]]
$$
X = \sum_i^n X_i : X_i
\begin{cases}
	1, \text{with probability} \;p \\ 0, \text{with probability} \; 1-p
\end{cases}
\;\;.
$$
Using the [[4. Discrete random variables#Linearity of expectations|linearity of expectations]], the expected value of $X$ is 
$$
E[X] = E\bigg[\sum_i^nX_i\bigg] = np.
$$
**Note:** The binomial r.v. is the number of success in $n$ experiments, while the Bernoulli r.v. (indicator) is the success in one simple experiment. Therefore, the binomial r.v. is the sum of $n$ Bernoulli r.v.s.
## Conditioning on a random variable; Independence of r.v.'s

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-7-multiple-variables-expectations-independence/). 

![[L07.pdf]]

### Conditional *PMFs*

Let $A$ be the event in $\Omega$ in which $Y = y$, *i.e.*, $A = \{Y = y\}$. Additionally, given $X$ and its *PMF* conditional to $A$,
$$
p_{X|A}(x|A) = p_{X|Y}(x|y) = P(X = x | Y = y).
$$
Replacing the right-hand side of the equation by the definition of [[2. Conditioning and Independence#Conditional probabilities|conditional probabilities]] we find
$$
p_{X|Y}(x|y) = \frac{P(X = x, Y=y)}{P(Y = y)},
$$
where $P(X =x, Y=y)$ is the *joint PMF* and $P(Y = y)$ is the *marginal PMF*. Therefore, in terms of these $PMFs$
$$
p_{X|Y}(x|y) = \frac{p_{X, Y}(x, y)}{p_Y(y)} : p_Y(y) > 0.
$$
Notice how there is nothing new about conditioning a *PMF* to another *PMF*. We are simply deriving what we already know into a new notation by taking into account that $A = \{Y = y\}$. For instance, the expected value $X$ given $Y$ still is
$$
E[X|A=\{Y = y\}] = E[X|Y] = \sum_x x p_{X|Y}(x,y).
$$
The [[2. Conditioning and Independence#Total probability theorem|total probability]] and [[4. Discrete random variables#Total expectation theorem|total expectation theorem]] written in this new notation shows us another relationship between the *marginal PMF* and  *joint PMF*.
$$
p_X(x) = \sum_y p_Y(y)p_{X|Y}(x|y)
$$
and
$$
E[X] = \sum_y p_Y(y)E[X|Y=y].
$$

### Independence of random variables

A random variable $X$ is said to be [[2. Conditioning and Independence#Independence|independent]] of a event $A$ if
$$
p_{X|A}(x) = p_X(A) \; \forall \; x.
$$
Symmetrically,
$$
P(A|X = x) = P(A) \; \forall \; x.
$$
Given that the event $A$ can be though of the set where a random variable $\{Y = y\}$, the independence of two random variables is assured if
$$
p_{X, Y}(x, y) = p_X(x)p_Y(y) \; \forall \; x, y.
$$
This leads to the fact that the conditional *PMF* equals the *marginal PMF*,
$$
p_{X|Y}(x|y) = p_X(x) \; \forall \; y.
$$
The definition of independent random variables can be extrapolated to $n$ random variables.

***Gemini Addendum:***
	 A brief discussion about *conditional PMFs* and *joint PMFs* can be found in the [link](https://g.co/gemini/share/10113ffeed78).

#### Independence and expectations

If $X$ and $Y$ are independent random variables, the expected value of their product is
$$
E[X\cdot Y] = E[X]\cdot E[Y].
$$
**Proof:**
	 Let $X$ and $Y$ be independent random variables, *i.e.*, $p_{X, Y}(x, y) = p_X(x) \cdot p_Y(y)$. The expected value of their products
	 $$
	 E[X \cdot Y] = \sum_x \sum_y (x \cdot y) p_{X, Y}(x, y) = \sum_x x p_X(x) \cdot \sum_y y p_Y(y),
	 $$
	 is simply the product of their expected values.
	 $$
	 E[X \cdot Y] = E[X] \cdot E[Y].
	 $$
$\square$ 

A more general form of this independence is given by
$$
E[g(X) \cdot h(Y)] = E[g(X)] \cdot E[h(Y)]
$$

#### Independence and variance

In general, the variance of the sum of two random variables does not equal the sum of the variances. However, in the case where $X$ and $Y$ are independent from each other the following is true.
$$
\text{var}(X + Y) = \text{var}(X) + \text{var}(Y). 
$$
**Proof:**
	 Let $X$ and $Y$ be independent random variables. We can always translate their expected value given the [[4. Discrete random variables#Linearity of expectation|linearity property]] so that we redefine them as r.v.s with expected value equal to zero. Therefore,
	 $$
	 E[X] = E[Y] = 0.
	 $$
	 Calculating the variance of $X + Y$,
	 $$
	 \text{var}(X+Y) = E[(X+Y)^2] - E[(X +Y)]^2,
	 $$
	 where rightmost term of the equation is zero yields
	 $$
	 \text{var}(X+Y) = E[X^2] + 2E[X \cdot Y] + E[Y^2].
	 $$
	 Since $X$ and $Y$ are independent, the middle term of the right-hand side is $2E[X]\cdot E[Y] = 0$. Moreover, the remaining terms are the variance of each random variable.
	 $$
	 \text{var}(X + Y) = \text{var}(X) + \text{var}(Y).
	 $$
$\square$

### The variance of the binomial

Similarly to the method applied when calculating the [[4. Discrete random variables#The mean of the binomial|expected value of the binomial]], we may also subdivide the variance.
$$
\text{var}(X) = \text{var} \bigg(\sum_i^n X_i \bigg),
$$
where $X$ is a binomial random variable and $X_i$ are Bernoulli random variables. Given that on each trial the indicator variables $X_i$ are independent, the overall variance is
$$
\text{var}(X) = \sum_i^n \text{var}(X_i).
$$
Replacing the variance of the indicator variable by the one [[4. Discrete random variables#Variance of the Bernoulli *PMF*|previously calculated]] we find
$$
\text{var}(X) = np(1-p).
$$

### The hat problem

We have $n$ people each with a enumerated hat. They all throw their own hat into a box. Then, at random, each one pick a single hat. One could pick either their own hat or someone else's hat. The number of [[3. Counting#Basic counting principle|permutations]] of $n$ hats between $n$ people is $n!$. Since they are all equally possible, the probability of each permutation is $\frac{1}{n!}$. 

Let $X$ be the number of people who get their own hat back. To find the $E[X]$ we shall use the fact that each person either get their own hat or not. Therefore, the value of $X$ is simply
$$
X = \sum_i^n X_i : X_i = \begin{cases}
	1, \text{if} \;i\; \text{selects own hat with a probability of}\;\frac{1}{n}\\ 0, \text{otherwise}
\end{cases}\;\;.
$$
The $E[X_i]$ is the expected value of an indicator variable which is $\frac{1}{n}$, therefore, $E[X] = 1$.

With the expected value of $X$, we can now calculate the $\text{var}(X)$. However, since the variables $X_i$ are not independent, the $\text{var}(X) \neq \sum_i^n \text{var}(X_i)$. To find the variance of $X$, we begin with
$$
\text{var}(X) = E[X^2] - (E[X])^2 = E[X^2] - 1.
$$
The expected value of $X^2$ is found by squaring $\sum_i^n X_i$, which yields
$$
E[X^2] = E\bigg[\sum_i^nX_i^2\bigg] + E\Bigg[\sum_{i, j: i\neq j}^{n(n-1)}X_iX_j\Bigg].
$$
Both terms of the right-hand side are Bernoulli random variables. Therefore, the expected value of the sum is the sum of the expected values.
$$
E[X^2] = \sum_i^nE[X_i^2] + \sum_{i, j: i\neq j}^{n(n-1)}E[X_iX_j].
$$
As we already know, the expected value of a Bernoulli r.v. equals $P(X_i = 1)$. For $X_i^2$, this probability is the same as $X_i$, giving $E[X_i^2] = n^{-1}$. Now for the product $X_iX_j$, they will only equal to $1$ when both are different than $0$, *i.e.*, $P(X_i = 1 \cap X_j = 1)$. Using [[2. Conditioning and Independence#Baye's rule and inference|Baye's rule]]
$$
P(X_i = 1 \cap X_j = 1) = P(X_j = 1)P(X_i = 1| X_j = 1) = \frac{1}{n} \cdot \frac{1}{(n-1)}.
$$
Replacing these probabilities in the expected value expression results
$$
E[X^2] = \sum_i^n \frac{1}{n} + \sum_{i,j}^{n(n-1)}\frac{1}{n(n-1)} = 2.
$$
Finally, the variance of $X$ is
$$
\text{var}(X) = 1.
$$
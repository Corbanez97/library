## Derived distributions

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-11-derived-distributions-convolution-correlation/)
![[L11.pdf]]

Let $X$ by a discrete random variable and $Y = g(X)$. The values of $Y$ will have probability distributed accordingly with $X$ but shifted across the axis. For instance, if $Y$ is a linear function of $X$, then
$$
p_Y(y) = p_X\bigg(\frac{y-b}{a}\bigg).
$$

![[derived_distribution.png]]

### Linear function of a continuous r.v.

Now consider the scenario where $X$ is a continuous random variable. In this case, any linear transformation applied to $X$ should also consider the total area under the *PDF* of the transformed variable. For example, if we have a constant random variable, its *PDF* will undergo a transformation that stretches it along the horizontal axis and compresses it along the vertical axis. Importantly, this transformation is designed to preserve the total area under the *PDF* curve.

<iframe src="https://www.desmos.com/calculator/3qy24vr9y3?embed" width="700" height="400" style="border: 1px solid #ccc" frameborder=0></iframe>

Given $Y = aX + b$ the *CDF* of $Y$ is
$$
F_Y(y) = P(Y \leq y) = P(aX + b \leq y).
$$
For $a> 0$ we can solve for $X$ in the argument of the right-hand side and we find the *CDF* of $X$ for the value $\frac{y-b}{a}$. Therefore,
$$
F_Y(y) = F_X\bigg(\frac{y-b}{a}\bigg),
$$
from which the derivation leaves us with
$$
f_Y(y) = \frac{1}{a}f_X\bigg(\frac{y - b}{a}\bigg).
$$
For $a < 0$ solving for $X$ will turn the inequality resulting in
$$
f_Y(y) = -\frac{1}{a}f_X\bigg(\frac{y - b}{a}\bigg).
$$
However, since $a<0$, the overall sign will be positive again. Therefore, we may summarize both solutions as 
$$
f_Y(y) = \frac{1}{|a|}f_X\bigg(\frac{y-b}{a}\bigg).
$$
### A linear function of a normal r.v.

Given $X \sim N(\mu, \sigma^2)$, $Y = aX + b$ will also be normally distributed with $Y \sim N(a\mu + b, a^2 \sigma^2)$.
**Proof:**
	$$
	f_Y(y)\ =\ \frac{1}{\left|a\right|}\frac{1}{\sigma\ \sqrt{2\pi}}e^{\left(-\frac{1}{2}\frac{\left(\left(\frac{x-b}{a}\right)-\mu\right)^{2}}{\sigma^{2}}\right)} = \frac{1}{(\sigma|a|)\sqrt{2\pi}}\exp\left[-\frac{(y-b-a\mu)^2}{2(\sigma a)^2}\right] = N(a\mu + b, a^2\sigma^2).
	$$
$\square$
<iframe src="https://www.desmos.com/calculator/9ddpjgvtqv?embed" width="700" height="600" style="border: 1px solid #ccc" frameborder=0></iframe>
### The *PDF* of a general function

For any general $g(X)$ of a continuous r.v. we can find the resulting *PDF* by simply finding the *CDF* of $Y = g(X)$, 
$$
F_Y(y) = P(Y \leq y) = P(g(X) \leq y)).
$$Solving the argument of the probability for $X$ will give us the *CDF* of $X$. We can then just differentiate this equation to find $f_Y(y)$.

#### A general formula for *PDF* of $Y = g(X)$ when $g$ is monotonic

Given $Y = g(X)$ where $g$ is a monotonic function, *i.e.*,
$$
g(x) \leq g(x') \Leftrightarrow x\leq x',
$$
the *CDF* of $Y$ is
$$
F_Y(y) = P_Y(Y \leq y).
$$
For a monotonic increasing function or decreasing function, the general form of $f_Y(y)$ is
$$
f_Y(y) = f_X\big(g^{-1}(y)\big) \cdot \Bigg|\frac{dg^{-1}}{dy}(y)\Bigg|.
$$

#### A case of a nonmonotonic $g$

Say that $Y = g(X)$ and that $g(x) = x^2$. This function is not inversible, therefore, the general formula defined above does not work. In such cases, one must compute through the usual method for a general function. For instance, in the current scenario,
$$
F_Y(y) = P(Y \leq y) = P(x^2 \leq y).
$$
The inequality on the argument of the right-hand side of the expression will become a closed interval if solved for $x$ because $\sqrt{x^2} = |x|$, therefore
$$
F_Y(y) = P(-\sqrt{y}\leq x \leq \sqrt{y}) = F_X(\sqrt{y}) - F_X(-\sqrt{y}).
$$
Differentiating both sides of the expression over $y$ yields
$$
f_Y(y) = \frac{f_X(\sqrt{y}) + f_X(-\sqrt{y})}{2\sqrt{y}}.
$$

### A function of multiple r.v.'s: $Z = g(X, Y)$

By following the same methodology one can also find the *PDF* of $Z = g(X, Y)$. For example, let $Z = Y/X$ with $X$ and $Y$ being both $\text{unif}[0,1]$. We begin by finding the *CDF* of $Z$,
$$
F_Z(z) = P\left(\frac{Y}{X} \leq z\right).
$$
**NOW THING GO FREESTYLE!!** We can plot the on the $x\times y$ plane the function for $Y \leq z\cdot X$ and figure out the areas for which the expression holds. These areas are a function of $z$ and they represent the probability of this inequality, *ergo*, the *CDF*.

After we figure out $F_Z(z)$ for different possible ranges of $z$ we must differentiate to find $f_Z(z)$.
#### Example

Suppose that $X$ and $Y$ are described by a *joint PDF* which is uniform inside the unit circle, that is, the set of points that satisfy $x^2 + y^2 \leq 1$. In particular, the *joint PDF* takes the value of $\frac{1}{\pi}$ on the unit circle. Let $Z = \sqrt{X^2 + Y^2}$, which is the distance of the outcome $(X, Y)$ from the origin. The *PDF* of $Z$, for $z\in[0,1]$, takes the form $f_Z(z) = az^b$. Find $a$ and $b$.

To solve such problem we must understand what is the probability of $Z$ being less then a given value $z$, *i.e.*, find the *CDF* of $Z$. 
$$
F_Z(z) = P(Z \leq z) = P\left(\sqrt{X^2 + Y^2} \leq z\right).
$$
The right-most probability is the same as $P\left(Y^2 \leq z^2 - X^2 \right)$, which is just the area of said region, ${Y^2 \leq z^2 - X^2}$, over the area of the unit circle. The animation below shows how this area changes as we change $z$ from $0$ to $1$, and it is clear that we are simply changing the radius $z$ of a circle.

<iframe src="https://www.desmos.com/calculator/wg1kbstydl?embed" width="700" height="600" style="border: 1px solid #ccc" frameborder=0></iframe>

Therefore,
$$
F_Z(z) = \frac{\pi z^2}{\pi}.
$$
Differentiating with respect to $z$ we find the *PDF*
$$
f_Z(z) = 2z,
$$
where $a = 2$ and $b = 1$.
## Sums of independents r.v.'s; Covariance and Correlation

Click here to see the [Live Lecture Hall Lesson](https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-12-iterated-expectations-sum-of-a-random-number-of-random-variables/)

![[L12.pdf]]

Given $X$ and $Y$, both independent discrete r.v.'s with known *PMFs*, the probability of $Z = X + Y$ taking a certain value $z$ is given by the convolution formula
$$
p_Z(z) = \sum_x P_X(x)P_Y(z - x).
$$
However, if $X$ and $Y$ are continuous r.v.'s, then we must integrate over all possible values of $x$,
$$
f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z - x) dx.
$$
**Proof:**
	Given $X = x$, the conditional *PDF* of $Z = x + Y$ is
	$$
	f_{Z|X}(z | x) = f_{Y+x|X}(z|x).
	$$
	Since $X$ and $Y$ are independent r.v.' $f_{Y-x|X} = f_{Y-x}$. Moreover, we can rewrite the right-hand side as a shift over the r.v. $Y$ by a value $x$.
	$$
	f_{Z|X}(z|x) = f_Y(z -x).
	$$
	From the [[5. Continuous random variables#Conditioning on a random variable; Independence; Bayes' rule|Baye's rule]] we know that $f_{X, Z}(x, z) = f_X(x)f_{Z|X}(z|x)$. Replacing the conditional *PDF* by the expression above results in
	$$
	f_{X, Z}(x, z) = f_X(x)f_{Y}(z-x).
	$$
	Integrating over $x$ on both sides yields in the *marginal PDF* of $Z$
	$$
	f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z - x) dx.
	$$
$\square$

### The sum of independent normal r.v.'s

Given two independent r.v.'s
$$
\begin{cases}
X \sim N(\mu_x, \sigma^2_x)\\
\\
Y \sim N(\mu_y, \sigma^2_y)
\end{cases}\quad,
$$
their sum $Z = X + Y$ will also be normal
$$
Z \sim N(\mu_x + \mu_y, \sigma^2_x + \sigma_y^2).
$$
### Covariance

The covariance of two random variables is defined as 
$$
\text{cov}(X, Y) = E\left[(X - E[X]) \cdot (Y - E[Y])\right].
$$
This value measure how the growth of one r.v.'s is related to the growth of the other. One important corollary from this definition is that if $X$ and $Y$ are independent, then
$$
\begin{aligned}
\text{cov}(X, Y) = E\left[(X - E[X])\right]\cdot E\left[(Y - E[Y])\right] = \left(E[X]- E[X]\right)\cdot \left(E[Y] -E[Y]\right) \\\\\therefore \text{cov}(X, Y) = 0
\end{aligned}
$$
If we calculate the covariance of a r.v. with itself we can see that
$$
\text{cov}(X, X) =  E[(X - E[X])^2] = \text{var}(X).
$$
Using the [[4. Discrete random variables#Linearity of expectations|linearity of expectations]] the covariance can be written as
$$
\text{cov}(X, Y) = E[X\cdot Y] - E[X]\cdot E[Y].
$$
Moreover, for a linear function of $X$ and $Y$, the linearity property assures us that
$$
\begin{align} 
\text{cov}(a_xX + b_x, a_yY + b_y) = E[(a_xX + b_x - a_xE[X] -b_x)(a_yY + b_y - a_yE[Y]-b_y)],\\ \\
\text{cov}(a_xX + b_x, a_yY + b_y) = a_x\cdot a_y\cdot \text{cov}(X, Y).
\end{align}
$$
Finally, if $Y = Z + W$ then, in the zero mean case,
$$
\text{cov}(X, Z + W) = E[X (Z + W)] = E[XZ] + E[XW] = \text{cov}(X, Z) + \text{cov}(X, W).
$$
The same will be true for different means because of the linearity of expectations.

### The variance of a sum of random variables

Let $X_1$ and $X_2$ be r.v.'s with unknown dependency, the variance of their sum is given by
$$
\text{var}(X_1 + X_2) = \text{var}(X_1) + \text{var}(X_2) + 2\text{cov}(X_1, X_2).
$$
**Proof:**
	 By the [[4. Discrete random variables#Variance - a measure of the spread of a *PMF*|definition of the variance]]
	 $$
	 \text{var}(X_1 + X_2) = E\left[(X_1 + X_2 - E[X_1 + X_2])^2\right].
	 $$
	 Using the linearity of the expectation the right-hand side of the equation becomes
	$$
	 \begin{align}
		 \text{var}(X_1 + X_2) = E\left[\left(\left(X_1  - E[X_1]\right) + \left(X_2  - E[X_2]\right)\right)^2\right],\\ \\
		 \text{var}(X_1 + X_2) = E\left[\left(X_1  - E[X_1]\right)^2 + 2\left(X_1  - E[X_1]\right)\left(X_2  - E[X_2]\right) + \left(X_2  - E[X_2]\right)^2\right].
	\end{align}
	 $$
	 By splitting the right-hand expression into their respective expected values we can identify the variance of $X_1$ and $X_2$ and their covariance, yielding
	 $$
	 \text{var}(X_1 + X_2) = \text{var}(X_1) + \text{var}(X_2) + 2\text{cov}(X_1, X_2).
	 $$
$\square$

As we showed before, for independent r.v.'s the variance of their sum is the sum of their variances. The formula above is aligned with this fact since for two independent r.v.'s their covariance is null.

Additionally, the variance of the sum of an arbitrary number of r.v.'s is
$$
\text{var}\left(\sum_i^nX_i\right) = \sum_i^n\text{var}(X_i) + \sum_{i\neq j}^n\text{cov}(X_i,X_j).
$$
**Proof:**
	 Let a set of $n$ r.v.'s $X_i$ all have $E[X_i] = 0$, the variance of their sum
	 $$
	 \text{var}\left(\sum_i^nX_i\right) = E\left((X_1 + X_2 + \dots + X_n)^2\right) = E\left[\sum_i^nX_i^2 + \sum_{i\neq j}^n X_i X_j\right].
	 $$
	 The right-most expression is the sum of the variance for each $X_i$ and the sum of each possible combination of $X_i$ and $X_j$ for the zero mean case. Applying the linearity of expectation,
	 $$
	 \text{var}\left(\sum_i^nX_i\right) = \sum_i^n\text{var}(X_i) + \sum_{i\neq j}^n\text{cov}(X_i,X_j).
	 $$
$\square$

### The correlation coefficient

To better understand the relationship between two r.v.'s, it will be useful to define a dimensionless version of the covariance. The correlation coefficient is given by
$$
\rho(X, Y) \overset{\text{def}}{=} E\left[\frac{(X - E[X])}{\sigma_X} \cdot \frac{(Y - E[Y])}{\sigma_Y}\right] = \frac{\text{cov}(X, Y)}{\sigma_X\sigma_Y}.
$$
This coefficient is limited to the range $[-1, 1]$ giving us a absolute measure of association between $X$ and $Y$. Again, for independent r.v.'s this coefficient is null.

The correlation coefficient of $X$ with itself is trivially equal to $1$ and $\rho(X, -X) = -1$. Additionally, if $X$ and $Y$ are linearly related by any means then $|\rho| = 1$.

For any linear function $g(X) = a_x X + b_x$, the value of correlation coefficient will remain unaltered except for the additional sign of $a_x$,
$$
\rho(g(X), Y) = \frac{a_x\text{cov}(X, Y)}{|a_x|\sigma_x\sigma_y} = \text{sign}(a_x)\rho(X, Y).
$$
More than the mathematical definition of correlation, It is important to understand the meaning behind this metric. First, we have to keep in mind that correlation does not imply causation. However, the correlation often reflects a hidden factor between both r.v.'s.
## Conditional expectation and variance revisited; Sum of a random number of independent r.v.'s

![[L13.pdf]]

By defining the conditional expectation of a r.v.
$$
E[X | Y = y] \overset{\text{def}}{=}\sum_x x p_{X|Y}(x|y),
$$
we can also interpret this new expectation as another r.v.
$$
g(Y) = E[X|Y].
$$
In other words, the function $g$ of a r.v. $Y$ is a r.v. that takes the value $E[X|Y = y]$ when $Y$ takes the value $y$.

![[L13E1S.png]]
**Notes:**
	 Pay attention to the notation. Here, $E[X + Y| X]$ is the value of the r.v. $X + Y$ given the r.v. $X$ not the value of the r.v. $X$.

### The mean of $E[X | Y]$: Law of iterated expectation

Given the conditional expectation
$$
E[X|Y] = g(Y),
$$
we can find its mean values with
$$
E[E[X|Y]] = E[g(Y)].
$$
Using the [[4. Discrete random variables#Expectation of a random variable|definition of the expected value]] results in
$$
E[g(Y)] = \sum_y g(Y)p_Y(y) = \sum_y E[X|Y = y] p_Y(y).
$$
We already know the value of right-most expression from the [[4. Discrete random variables#Total expectation theorem|total expectation theorem]], therefore
$$
E[E[X|Y]] = E[X].
$$
***GPT Addendum:***
	 The Law of Total Expectation and the Law of Iterated Expectations are actually the same thing, just under different names. They are also referred to as Adam's Law, the Tower Rule, or the Smoothing Theorem. The Law of Total Expectation states that if $X$ is a random variable whose expected value $E(X)$ is defined, and $Y$ is any random variable on the same probability space, then the expected value of the conditional expected value of $X$ given $Y$ is the same as the expected value of $X$. In mathematical terms, this is expressed as:
	 $$
	 E(X) = E(E(X|Y))
	 $$
	 This law is often used when the probability space is partitioned into different events and we want to calculate the expectation of a random variable across the entire space. The Law of Iterated Expectations is a generalization of the Law of Total Expectation. It extends the concept to situations where we have more than one conditioning variable. For example, if we have two random variables $Y$ and $Z$, the Law of Iterated Expectations can be expressed as:
	 $$
	 E(X|Y) = E(E(X|Y, Z)|Y)
	 $$
	 In this case, we first condition on both $Y$ and $Z$, and then we take the expectation conditioning only on $Y$. So, to summarize, the Law of Total Expectation and the Law of Iterated Expectations are essentially the same concept. The latter is just a more general form that allows for additional conditioning variables.

### The law of total variance

Given an r.v. $X$ and a conditional $Y$
$$
\text{var}(X) = E\left[\text{var}(X|Y)\right] + \text{var}\left(E[X|Y]\right).
$$
This law can be thought as a comparison between possible conditions over $X$, *i.e.*,
$$
\text{var}(X) = (\text{average variability within a conditions}) + (\text{variablitiy between conditions}).
$$
**Proof:**
	 Since [[4. Discrete random variables#Properties of the variance|the variance can be written as]]
	 $$
	 \text{var}(X) = E\left[X^2\right] - E[X]^2,
	 $$
	 by conditioning the r.v. $X$ over $Y$ we find that
	 $$
	 \text{var}(X|Y = y) = E\left[X^2|Y = y\right] - E[X|Y= y]^2 \; \forall \; y.
	 $$
	 This is a new r.v. calculated for when $Y = y$, therefore in a more abstract notation
	 $$
	 \text{var}(X|Y) = E\left[X^2|Y\right] - E[X|Y]^2.
	 $$
	 Using the [[6. Further topics on random variables#The mean of $E[X Y]$ Law of iterated expectation|law of iterated expectations]] we can calculated the values of $E[\text{var}(X|Y)]$ and $\text{var}(E[X|Y])$.
	 $$
	 \begin{align}
	 E[\text{var}(X|Y)] = E\left[E\left[X^2|Y\right]\right] - E\left[E[X|Y]^2\right] = E\left[X^2\right] - E\left[E[X|Y]^2\right], \\ \\
	 \text{var}\left(E[X|Y]\right) = E\left[E[X|Y]^2\right] - E\left[E[X|Y]\right]^2 = E\left[E[X|Y]^2\right] - E[X]^2.
	 \end{align}
	 $$
	 Finally, summing both expressions yields
	 $$
	 E\left[\text{var}(X|Y)\right] + \text{var}\left(E[X|Y]\right) = E\left[X^2\right] - E[X]^2 = \text{var}(X).
	 $$
$\square$ 

### Sum of a random number of independent r.v.'s

Let $Y$ be a r.v. resultant from the sum of $N$ r.v.'s $X_i$, *i.e.*,
$$
Y = \sum_i^NX_i.
$$
Additionally, let $N$ be independent of $X$, then the expected value of $Y$ given $N = n$ is
$$
E[Y|N = n] = E\left[\sum_i^n X_i \Bigg | N = n \right] = E[X_1 + X_2 + ... + X_n] = n\cdot E[X].
$$
Therefore, this expectation can also be written as a r.v.
$$
E[Y|N] = N \cdot E[X].
$$
From the [[6. Further topics on random variables#The mean of $E[X Y]$ Law of iterated expectation|law of iterated expectations]]
$$
E[Y] = E\left[E[Y|N]\right] = E[N\cdot E[X]] = E[N]\cdot E[X].
$$
From the [[6. Further topics on random variables#The law of total variance|law of total variance]]
$$
\text{var}(Y) = E\left[\text{var}(Y|N)\right] + \text{var}\left(E[Y|N]\right).
$$
The variance of $Y$ given $N$ is simply the sum of the variances of $X_i$, $N\cdot \text{var}(X)$. Calculating the expectation of the variance and the variance of the expectation we find that
$$
\text{var}(Y) = E[N]\cdot \text{var}(X) + E[X]^2 \cdot \text{var}(N).
$$
## Simulation: Inverse Transform Sampling

Given a r.v. $X$ with known *CDF* $F_X$, we can generate a sample of data distributed accordingly to $f_X$ using a uniform distribution. A method called inverse transform sampling achieves this by calculating the inverse of $F_X$ for values in a uniform distribution $\text{unif}[0, 1]$. 

![[inverse_transform_sampling_example.gif]]

We begin by acknowledging the equality between the intervals  
$$
X \leq c \Longleftrightarrow \text{unif}[0,1] \leq F_X(c).
$$
Therefore,
$$
P(X\leq c) = P(\text{unif}[0, 1] \leq F_X(c)),
$$
where the right-hand side is the *CDF* of the uniform distribution which is simply $F_X(c)$. This assures us the the *CDF* of the constructed sample is indeed $F_X(c)$.

A computational method for the Inverse Transform Sampling can be found [here](https://github.com/Corbanez97/statistical-inference-snia/blob/main/notebooks/Sampler.ipynb).
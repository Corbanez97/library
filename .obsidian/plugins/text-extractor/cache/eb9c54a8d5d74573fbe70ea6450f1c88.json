{"path":"Books and Papers/Consciousness/Consciousness as a State of Matter.pdf","text":"Consciousness as a State of Matter Max Tegmark Dept. of Physics & MIT Kavli Institute, Massachusetts Institute of Technology, Cambridge, MA 02139 (Dated: Accepted for publication in Chaos, Solitons & Fractals March 17, 2015) We examine the hypothesis that consciousness can be understood as a state of matter, “percep- tronium”, with distinctive information processing abilities. We explore four basic principles that may distinguish conscious matter from other physical systems such as solids, liquids and gases: the information, integration, independence and dynamics principles. If such principles can identify conscious entities, then they can help solve the quantum factorization problem: why do conscious observers like us perceive the particular Hilbert space factorization corresponding to classical space (rather than Fourier space, say), and more generally, why do we perceive the world around us as a dynamic hierarchy of objects that are strongly integrated and relatively independent? Tensor factorization of matrices is found to play a central role, and our technical results include a theorem about Hamiltonian separability (deﬁned using Hilbert-Schmidt superoperators) being maximized in the energy eigenbasis. Our approach generalizes Giulio Tononi’s integrated information framework for neural-network-based consciousness to arbitrary quantum systems, and we ﬁnd interesting links to error-correcting codes, condensed matter criticality, and the Quantum Darwinism program, as well as an interesting connection between the emergence of consciousness and the emergence of time. I. INTRODUCTION A. Consciousness in physics A commonly held view is that consciousness is irrel- evant to physics and should therefore not be discussed in physics papers. One oft-stated reason is a perceived lack of rigor in past attempts to link consciousness to physics. Another argument is that physics has been man- aged just ﬁne for hundreds of years by avoiding this sub- ject, and should therefore keep doing so. Yet the fact that most physics problems can be solved without refer- ence to consciousness does not guarantee that this applies to all physics problems. Indeed, it is striking that many of the most hotly debated issues in physics today involve the notions of observations and observers, and we cannot dismiss the possibility that part of the reason why these issues have resisted resolution for so long is our reluc- tance as physicists to discuss consciousness and attempt to rigorously deﬁne what constitutes an observer. For example, does the non-observability of spacetime regions beyond horizons imply that they in some sense do not exist independently of the regions that we can observe? This question lies at the heart of the contro- versies surrounding the holographic principle, black hole complementarity and ﬁrewalls, and depends crucially on the role of observers [1, 2]. What is the solution to the quantum measurement problem? This again hinges cru- cially on the role of observation: does the wavefunction undergo a non-unitary collapse when an observation is made, are there Everettian parallel universes, or does it make no sense to talk about an an observer-independent reality, as argued by QBism advocates [3]? Is our per- sistent failure to unify general relativity with quantum mechanics linked to the diﬀerent roles of observers in the two theories? After all, the idealized observer in general relativity has no mass, no spatial extent and no eﬀect on what is observed, whereas the quantum observer no- toriously does appear to aﬀect the quantum state of the observed system. Finally, out of all of the possible fac- torizations of Hilbert space, why is the particular factor- ization corresponding to classical space so special? Why do we observers perceive ourselves are fairly local in real space as opposed to Fourier space, say, which according to the formalism of quantum ﬁeld theory corresponds to an equally valid Hilbert space factorization? This “quan- tum factorization problem” appears intimately related to the nature of an observer. The only issue there is consensus on is that there is no consensus about how to deﬁne an observer and its role. One might hope that a detailed observer deﬁnition will prove unnecessary because some simple properties such as the ability to record information might suﬃce; however, we will see that at least two more properties of observers may be necessary to solve the quantum factor- ization problem, and that a closer examination of con- sciousness may be required to identify these properties. Another commonly held view is that consciousness is unrelated to quantum mechanics because the brain is a wet, warm system where decoherence destroys quantum superpositions of neuron ﬁring much faster than we can think, preventing our brain from acting as a quantum computer [4]. In this paper, I argue that consciousness and quantum mechanics are nonetheless related, but in a diﬀerent way: it is not so much that quantum mechan- ics is relevant to the brain, as the other way around. Speciﬁcally, consciousness is relevant to solving an open problem at the very heart of quantum mechanics: the quantum factorization problem. B. Consciousness in philosophy Why are you conscious right now? Speciﬁcally, why are you having a subjective experience of reading these words, seeing colors and hearing sounds, while the inani-arXiv:1401.1219v3 [quant-ph] 18 Mar 2015 2 mate objects around you are presumably not having any subjective experience at all? Diﬀerent people mean dif- ferent things by “consciousness”, including awareness of environment or self. I am asking the more basic ques- tion of why you experience anything at all, which is the essence of what philosopher David Chalmers has termed “the hard problem” of consciousness and which has pre- occupied philosophers throughout the ages (see [5] and references therein). A traditional answer to this prob- lem is dualism — that living entities diﬀer from inani- mate ones because they contain some non-physical ele- ment such as an “anima” or “soul”. Support for dualism among scientists has gradually dwindled with the real- ization that we are made of quarks and electrons, which as far as we can tell move according to simple physical laws. If your particles really move according to the laws of physics, then your purported soul is having no eﬀect on your particles, so your conscious mind and its ability to control your movements would have nothing to do with a soul. If your particles were instead found not to obey the known laws of physics because they were being pushed around by your soul, then we could treat the soul as just another physical entity able to exert forces on particles, and study what physical laws it obeys, just as physicists have studied new forces ﬁelds and particles in the past. The key assumption in this paper is that consciousness is a property of certain physical systems, with no “se- cret sauce” or non-physical elements.1, This transforms Chalmers’ hard problem. Instead of starting with the hard problem of why an arrangement of particles can feel conscious, we will start with the hard fact that some arrangement of particles (such as your brain) do feel con- scious while others (such as your pillow) do not, and ask what properties of the particle arrangement make the diﬀerence. This paper is not a comprehensive theory of concious- ness. Rather, it is an investigation into the physical properties that conscious systems must have. If we un- 1 More speciﬁcally, we pursue an extreme Occam’s razor approach and explore whether all aspects of reality can be derived from quantum mechanics with a density matrix evolving unitarily ac- cording to a Hamiltonian. It this approach should turn out to be successful, then all observed aspects of reality must emerge from the mathematical formalism alone: for example, the Born rule for subjective randomness associated with observation would emerge from the underlying deterministic density matrix evo- lution through Everett’s approach, and both a semiclassical world and consciousness should somehow emerge as well, perhaps though processes generalizing decoherence. Even if quantum gravity phenomena cannot be captured with this simple quantum formalism, it is far from clear that gravitational, relativistic or non-unitary eﬀects are central to understanding consciousness or how conscious observers perceive their immediate surroundings. There is of course no a priori guarantee that this approach will work; this paper is motivated by the view that an Occam’s razor approach is useful if it succeeds and very interesting if it fails, by giving hints as to what alternative assumptions or ingredients are needed. derstood what these physical properties were, then we could in principle answer all of the above-mentioned open physics questions by studying the equations of physics: we could identify all conscious entities in any physical system, and calculate what they would perceive. How- ever, this approach is typically not pursued by physicists, with the argument that we do not understand conscious- ness well enough. C. Consciousness in neuroscience Arguably, recent progress in neuroscience has funda- mentally changed this situation, so that we physicists can no longer blame neuroscientists for our own lack of progress. I have long contended that consciousness is the way information feels when being processed in certain complex ways [6, 7], i.e., that it corresponds to certain complex patterns in spacetime that obey the same laws of physics as other complex systems. In the seminal pa- per “Consciousness as Integrated Information: a Provi- sional Manifesto” [8], Giulio Tononi made this idea more speciﬁc and useful, making a compelling argument that for an information processing system to be conscious, it needs to have two separate traits: 1. Information: It has to have a large repertoire of accessible states, i.e., the ability to store a large amount of information. 2. Integration: This information must be integrated into a uniﬁed whole, i.e., it must be impossible to decompose the system into nearly independent parts, because otherwise these parts would subjec- tively feel like two separate conscious entities. Tononi’s work has generated a ﬂurry of activity in the neuroscience community, spanning the spectrum from theory to experiment (see [9–13] for recent reviews), mak- ing it timely to investigate its implications for physics as well. This is the goal of the present paper — a goal whose pursuit may ultimately provide additional tools for the neuroscience community as well. Despite its successes, Tononi’s Integrated Information Theory (IIT)2 leaves many questions unanswered. If it is to extend our consciousness-detection ability to ani- mals, computers and arbitrary physical systems, then we need to ground its principles in fundamental physics. IIT takes information, measured in bits, as a starting point. But when we view a brain or computer through our physi- cists eyes, as myriad moving particles, then what physical properties of the system should be interpreted as logical 2 Since it’s inception [8], IIT has been further developed [12]. In particular, IIT 3.0 considers both the past and the future of a mechanism in a particular state (it’s so-called cause-eﬀect reper- toire) and replaces the Kullback-Leibler measure with a proper metric. 3 Many State of long-lived Information Easily Complex? matter states? integrated? writable? dynamics? Gas N N N Y Liquid N N N Y Solid Y N N N Memory Y N Y N Computer Y ? Y Y Consciousness Y Y Y Y TABLE I: Substances that store or process information can be viewed as novel states of matter and investigated with traditional physics tools. bits of information? I interpret as a “bit” both the po- sition of certain electrons in my computers RAM mem- ory (determining whether the micro-capacitor is charged) and the position of certain sodium ions in your brain (de- termining whether a neuron is ﬁring), but on the basis of what principle? Surely there should be some way of identifying consciousness from the particle motions alone, or from the quantum state evolution, even without this information interpretation? If so, what aspects of the behavior of particles corresponds to conscious integrated information? We will explore diﬀerent measures of inte- gration below. Neuroscientists have successfully mapped out which brain activation patterns correspond to certain types of conscious experiences, and named these patterns neural correlates of consciousness. How can we general- ize this and look for physical correlates of consciousness, deﬁned as the patterns of moving particles that are con- scious? What particle arrangements are conscious? D. Consciousness as a state of matter Generations of physicists and chemists have studied what happens when you group together vast numbers of atoms, ﬁnding that their collective behavior depends on the pattern in which they are arranged: the key diﬀer- ence between a solid, a liquid and a gas lies not in the types of atoms, but in their arrangement. In this pa- per, I conjecture that consciousness can be understood as yet another state of matter. Just as there are many types of liquids, there are many types of consciousness. However, this should not preclude us from identifying, quantifying, modeling and ultimately understanding the characteristic properties that all liquid forms of matter (or all conscious forms of matter) share. To classify the traditionally studied states of matter, we need to measure only a small number of physical pa- rameters: viscosity, compressibility, electrical conductiv- ity and (optionally) diﬀusivity. We call a substance a solid if its viscosity is eﬀectively inﬁnite (producing struc- tural stiﬀness), and call it a ﬂuid otherwise. We call a ﬂuid a liquid if its compressibility and diﬀusivity are small and otherwise call it either a gas or a plasma, de- pending on its electrical conductivity. What are the corresponding physical parameters that can help us identify conscious matter, and what are the key physical features that characterize it? If such param- eters can be identiﬁed, understood and measured, this will help us identify (or at least rule out) consciousness “from the outside”, without access to subjective intro- spection. This could be important for reaching consen- sus on many currently controversial topics, ranging from the future of artiﬁcial intelligence to determining when an animal, fetus or unresponsive patient can feel pain. If would also be important for fundamental theoretical physics, by allowing us to identify conscious observers in our universe by using the equations of physics and thereby answer thorny observation-related questions such as those mentioned in the introductory paragraph. E. Memory As a ﬁrst warmup step toward consciousness, let us ﬁrst consider a state of matter that we would character- ize as memory 3 — what physical features does it have? For a substance to be useful for storing information, it clearly needs to have a large repertoire of possible long- lived states or attractors (see Table I). Physically, this means that its potential energy function has a large num- ber of well-separated minima. The information storage capacity (in bits) is simply the base-2 logarithm of the number of minima. This equals the entropy (in bits) of the degenerate ground state if all minima are equally deep. For example, solids have many long-lived states, whereas liquids and gases do not: if you engrave some- one’s name on a gold ring, the information will still be there years later, but if you engrave it in the surface of a pond, it will be lost within a second as the water surface changes its shape. Another desirable trait of a memory substance, distinguishing it from generic solids, is that it is not only easy to read from (as a gold ring), but also easy to write to: altering the state of your hard drive or your synapses requires less energy than engraving gold. F. Computronium As a second warmup step, what properties should we ascribe to what Margolus and Toﬀoli have termed “com- 3 Neuroscience research has demonstrated that long-term mem- ory is not necessary for consciousness. However, even extremely memory-impaired conscious humans such as Clive Wearing [14] are able to retain information for several seconds; in this paper, I will assume merely that information needs to be remembered long enough to be subjectively experienced — perhaps 0.1 sec- onds for a human, and much less for entities processing informa- tion more rapidly. 4 putronium” [15], the most general substance that can process information as a computer? Rather than just remain immobile as a gold ring, it must exhibit com- plex dynamics so that its future state depends in some complicated (and hopefully controllable/programmable) way on the present state. Its atom arrangement must be less ordered than a rigid solid where nothing interest- ing changes, but more ordered than a liquid or gas. At the microscopic level, computronium need not be par- ticularly complicated, because computer scientists have long known that as long as a device can perform certain elementary logic operations, it is universal: it can be pro- grammed to perform the same computation as any other computer with enough time and memory. Computer vendors often parametrize computing power in FLOPS, ﬂoating-point operations per second for 64-bit numbers; more generically, we can parametrize computronium ca- pable of universal computation by “FLIPS”: the number of elementary logical operations such as bit ﬂips that it can perform per second. It has been shown by Lloyd [16] that a system with average energy E can perform a maximum of 4E/h elementary logical operations per sec- ond, where h is Planck’s constant. The performance of today’s best computers is about 38 orders of magnitude lower than this, because they use huge numbers of parti- cles to store each bit and because most of their energy is tied up in a computationally passive form, as rest mass. G. Perceptronium What about “perceptronium”, the most general sub- stance that feels subjectively self-aware? If Tononi is right, then it should not merely be able to store and pro- cess information like computronium does, but it should also satisfy the principle that its information is inte- grated, forming a uniﬁed and indivisible whole. Let us also conjecture another principle that conscious systems must satisfy: that of autonomy, i.e., that in- formation can be processed with relative freedom from external inﬂuence. Autonomy is thus the combination of two separate properties: dynamics and independence. Here dynamics means time dependence (hence informa- tion processing capacity) and independence means that the dynamics is dominated by forces from within rather than outside the system. Just like integration, autonomy is postulated to be a necessary but not suﬃcient condi- tion for a system to be conscious: for example, clocks and diesel generators tend to exhibit high autonomy, but lack substantial information storage capacity. H. Consciousness and the quantum factorization problem Table II summarizes the four candidate principles that we will explore as necessary conditions for consciousness. Our goal with isolating and studying these principles is Principle Deﬁnition Information A conscious system has substantial principle information storage capacity. Dynamics A conscious system has substantial principle information processing capacity. Independence A conscious system has substantial principle independence from the rest of the world. Integration A conscious system cannot consist of principle nearly independent parts. Autonomy A conscious system has substantial principle dynamics and independence. Utility An evolved conscious system records mainly principle information that is useful for it. TABLE II: Four conjectured necessary conditions for con- sciousness that we explore in this paper. The ﬁfth principle simply combines the second and third. The sixth is not a necessary condition, but may explain the evolutionary origin of the others. not merely to strengthen our understanding of conscious- ness as a physical process, but also to identify simple traits of conscious matter that can help us tackle other open problems in physics. For example, the only property of consciousness that Hugh Everett needed to assume for his work on quantum measurement was that of the infor- mation principle: by applying the Schr¨odinger equation to systems that could record and store information, he inferred that they would perceive subjective randomness in accordance with the Born rule. In this spirit, we might hope that adding further simple requirements such as in the integration principle, the independence principle and the dynamics principle might suﬃce to solve currently open problems related to observation. The last princi- ple listed in Table II, the utility principle, is of a diﬀer- ent character than the others: we consider it not as a necessary condition for consciousness, but as a potential unifying evolutionary explanation of the others. In this paper, we will pay particular attention to what I will refer to as the quantum factorization problem: why do conscious observers like us perceive the particu- lar Hilbert space factorization corresponding to classical space (rather than Fourier space, say), and more gener- ally, why do we perceive the world around us as a dy- namic hierarchy of objects that are strongly integrated and relatively independent? This fundamental problem has received almost no attention in the literature [18]. We will see that this problem is very closely related to the one Tononi confronted for the brain, merely on a larger scale. Solving it would also help solve the “physics- from-scratch” problem [7]: If the Hamiltonian H and the total density matrix ρ fully specify our physical world, how do we extract 3D space and the rest of our semi- classical world from nothing more than two Hermitian matrices, which come without any a priori physical in- terpretation or additional structure such as a physical space, quantum observables, quantum ﬁeld deﬁnitions, an “outside” system, etc.? Can some of this information 5 be extracted even from H alone, which is fully speciﬁed by nothing more than its eigenvalue spectrum? We will see that a generic Hamiltonian cannot be decomposed using tensor products, which would correspond to a de- composition of the cosmos into non-interacting parts — instead, there is an optimal factorization of our universe into integrated and relatively independent parts. Based on Tononi’s work, we might expect that this factoriza- tion, or some generalization thereof, is what conscious observers perceive, because an integrated and relatively autonomous information complex is fundamentally what a conscious observer is! The rest of this paper is organized as follows. In Sec- tion II, we explore the integration principle by quanti- fying integrated information in physical systems, ﬁnding encouraging results for classical systems and interesting challenges introduced by quantum mechanics. In Sec- tion III, we explore the independence principle, ﬁnding that at least one additional principle is required to ac- count for the observed factorization of our physical world into an object hierarchy in three-dimensional space. In Section IV, we explore the dynamics principle and other possibilities for reconciling quantum-mechanical theory with our observation of a semiclassical world. We dis- cuss our conclusions in Section V, including applications of the utility principle, and cover various mathematical details in the three appendices. Throughout the paper, we mainly consider ﬁnite-dimensional Hilbert spaces that can be viewed as collections of qubits; as explained in Ap- pendix C, this appears to cover standard quantum ﬁeld theory with its inﬁnite-dimensional Hilbert space as well. II. INTEGRATION A. Our physical world as an object hierarchy The problem of identifying consciousness in an arbi- trary collection of moving particles is similar to the sim- pler problem of identifying objects there. One of the most striking features of our physical world is that we perceive it as an object hierarchy, as illustrated in Figure 1. If you are enjoying a cold drink, you perceive ice cubes in your glass as separate objects because they are both fairly in- tegrated and fairly independent, e.g., their parts are more strongly connected to one another than to the outside. The same can be said about each of their constituents, ranging from water molecules all the way down to elec- trons and quarks. Zooming out, you similarly perceive the macroscopic world as a dynamic hierarchy of objects that are strongly integrated and relatively independent, all the way up to planets, solar systems and galaxies. Let us quantify this by deﬁning the robustness of an object as the ratio of the integration temperature (the energy per part needed to separate them) to the independence temperature (the energy per part needed to separate the parent object in the hierarchy). Figure 1 illustrates that all of the ten types of objects shown have robustness of ten or more. A highly robust object preserves its identity (its integration and independence) over a wide range of temperatures/energies/situations. The more robust an object is, the more useful it is for us humans to perceive it as an object and coin a name for it, as per the above- mentioned utility principle. Returning to the “physics-from-scratch” problem, how can we identify this object hierarchy if all we have to start with are two Hermitian matrices, the density ma- trix ρ encoding the state of our world and the Hamilto- nian H determining its time-evolution? Imagine that we know only these mathematical objects ρ and H and have no information whatsoever about how to interpret the various degrees of freedom or anything else about them. A good beginning is to study integration. Consider, for example, ρ and H for a single deuterium atom, whose Hamiltonian is (ignoring spin interactions for simplicity) H(rp, pp, rn, pn, re, pe) = (1) = H1(rp, pp, rn, pn) + H2(pe) + H3(rp, pp, rn, pn, re, pe), where r and p are position and momentum vectors, and the subscripts p, n and e refer to the proton, the neutron and the electron. On the second line, we have decom- posed H into three terms: the internal energy of the proton-neutron nucleus, the internal (kinetic) energy of the electron, and the electromagnetic electron-nucleus in- teraction. This interaction is tiny, on average involving much less energy than those within the nucleus: tr H3ρ tr H1ρ ∼ 10−5, (2) which we recognize as the inverse robustness for a typical nucleus in Figure 3. We can therefore fruitfully approx- imate the nucleus and the electron as separate objects that are almost independent, interacting only weakly with one another. The key point here is that we could have performed this object-ﬁnding exercise of dividing the variables into two groups to ﬁnd the greatest indepen- dence (analogous to what Tononi calls “the cruelest cut”) based on the functional form of H alone, without even having heard of electrons or nuclei, thereby identifying their degrees of freedom through a purely mathematical exercise. B. Integration and mutual information If the interaction energy H3 were so small that we could neglect it altogether, then H would be decompos- able into two parts H1 and H2, each one acting on only one of the two sub-systems (in our case the nucleus and the electron). This means that any thermal state would be factorizable: ρ ∝ e−H/kT = e−H1/kT e−H2/kT = ρ1ρ2, (3) 6 Object: Oxygen atom Robustness: 10 Independence T: 1 eV Integration T: 10 eV Object: Oxygen nucleus Robustness: 10 5 Independence T: 10 eV Integration T: 1 MeV Object: Proton Robustness: 200 Independence T: 1 MeV Integration T: 200 MeV Object: Neutron Robustness: 200 Independence T: 1 MeV Integration T: 200 MeV Object: Electron Robustness: 10 22? Independence T: 10 eV Integration T: 10 16 GeV? Object: Down quark Robustness: 10 17? Independence T: 200 MeV Integration T: 10 16 GeV? Object: Up quark Robustness: 10 17? Independence T: 200 MeV Integration T: 10 16 GeV? Object: Hydrogen atom Robustness: 10 Independence T: 1 eV Integration T: 10 eV Object: Ice cube Robustness: 10 5 Independence T: 3 mK Integration T: 300 K Object: Water molecule Robustness: 30 Independence T: 300 K Integration T: 1 eV ~ 10 4K { mgh/k B ~3mK per molecule FIG. 1: We perceive the external world as a hierarchy of objects, whose parts are more strongly connected to one another than to the outside. The robustness of an object is deﬁned as the ratio of the integration temperature (the energy per part needed to separate them) to the independence temperature (the energy per part needed to separate the parent object in the hierarchy). so the total state ρ can be factored into a product of the subsystem states ρ1 and ρ2. In this case, the mutual information I ≡ S(ρ1) + S(ρ2) − S(ρ) (4) vanishes, where S(ρ) ≡ −tr ρ log2 ρ (5) is the von Neumann entropy (in bits) — which is simply the Shannon entropy of eigenvalues of ρ. Even for non- thermal states, the time-evolution operator U becomes separable: U ≡ eiHt/ℏ = eiH1t/ℏe iH2t/ℏ = U1U2, (6) which (as we will discuss in detail in Section III) implies that the mutual information stays constant over time and no information is ever exchanged between the objects. In summary, if a Hamiltonian can be decomposed without an interaction term (with H3 = 0), then it describes two perfectly independent systems.4 4 Note that in this paper, we are generally considering H and ρ for the entire cosmos, so that there is no “outside” containing observers etc. If H3 = 0, entanglement between the two systems thus cannot have any observable eﬀects. This is in stark contrast to most textbook quantum mechanics considerations, where one studies a small subsystem of the world. 7 Let us now consider the opposite case, when a sys- tem cannot be decomposed into independent parts. Let us deﬁne the integrated information Φ as the mutual in- formation I for the “cruelest cut” (the cut minimizing I) in some class of cuts that subdivide the system into two (we will discuss many diﬀerent classes of cuts be- low). Although our Φ-deﬁnition is slightly diﬀerent from Tononi’s [8] 5, it is similar in spirit, and we are reusing his Φ-symbol for its elegant symbolism (unifying the shapes of I for information and O for integration). C. Maximizing integration We just saw that if two systems are dynamically inde- pendent (H3 = 0), then Φ = 0 at all time both for ther- mal states and for states that were independent (Φ = 0) at some point in time. Let us now consider the oppo- site extreme. How large can the integrated information Φ get? A as warmup example, let us consider the fa- miliar 2D Ising model in Figure 2 where n = 2500 mag- netic dipoles (or spins) that can point up or down are placed on a square lattice, and H is such that they pre- fer aligning with their nearest neighbors. When T → ∞, ρ ∝ e−H/kT → I, so all 2n states are equally likely, all n bits are statistically independent, and Φ = 0. When T → 0, all states freeze out except the two degenerate ground states (all spin up or all spin down), so all spins are perfectly correlated and Φ = 1 bit. For interme- diate temperatures, long-range correlations are seen to exist such that typical states have contiguous spin-up or spin-down patches. On average, we get about one bit of mutual information for each such patch crossing our cut (since a spin on one side “knows” about at a spin on the other side), so for bipartitions that cut the system into two equally large halves, the mutual information will be proportional to the length of the cutting curve. The “cru- elest cut” is therefore a vertical or horizontal straight line of length n1/2, giving Φ ∼ n1/2 at the temperature where typical patches are only a few pixels wide. We would sim- ilarly get a maximum integration Φ ∼ n1/3 for a 3D Ising system and Φ ∼ 1 bit for a 1D Ising system. Since it is the spatial correlations that provide the in- tegration, it is interesting to speculate about whether the conscious subsystem of our brain is a system near its critical temperature, close to a phase transition. Indeed, Damasio has argued that to be in homeostasis, a num- ber of physical parameters of our brain need to be kept within a narrow range of values [19] — this is precisely what is required of any condensed matter system to be near-critical, exhibiting correlations that are long-range 5 Tononi’s deﬁnition of Φ [8] applies only for classical systems, whereas we wish to study the quantum case as well. Our Φ is measured in bits and can grow with system size like an extrinsic variable, whereas his is an intrinsic variable akin representing a sort of average integration per bit. (providing integration) but not so strong that the whole system becomes correlated like in the right panel or in a brain experiencing an epileptic seizure. D. Integration, coding theory and error correction Even when we tuned the temperature to the most fa- vorable value in our 2D Ising model example, the inte- grated information never exceeded Φ ∼ n1/2 bits, which is merely a fraction n −1/2 of the n bits of information that n spins can potentially store. So can we do better? Fortunately, a closely related question has been carefully studied in the branch of mathematics known as coding theory, with the aim of optimizing error correcting codes. Consider, for example, the following set of m = 16 bit strings, each written as a column vector of length n = 8: M =            0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1            This is known as the Hamming(8,4)-code, and has Ham- ming distance d = 4, which means that at least 4 bit ﬂips are required to change one string into another [20]. It is easy to see that for a code with Hamming distance d, any (d − 1) bits can always be reconstructed from the others: You can always reconstruct b bits as long as eras- ing them does not make two bit strings identical, which would cause ambiguity about which the correct bit string is. This implies that reconstruction works when the Ham- ming distance d > b. To translate such codes of m bit strings of length n into physical systems, we simply created a state space with n bits (interpretable as n spins or other two-state systems) and construct a Hamiltonian which has an m- fold degenerate ground state, with one minimum cor- responding to each of the m bit strings in the code. In the low-temperature limit, all bit strings will re- ceive the same probability weight 1/m, giving an entropy S = log2 m. The corresponding integrated information Φ of the ground state is plotted in Figure 3 for a few examples, as a function of cut size k (the number of bits assigned to the ﬁrst subsystem). To calculate Φ for a cut size k in practice, we simply minimize the mutual infor- mation I over all ( n k ) ways of partitioning the n bits into k and (n − k) bits. We see that, as advertised, the Hamming(8,4)-code gives gives Φ = 3 when 3 bits are cut oﬀ. However, it gives only Φ = 2 for bipartitions; the Φ-value for bi- partitions is not simply related to the Hamming distance, and is not a quantity that most popular bit string codes are optimized for. Indeed, Figure 3 shows that for bipar- titions, it underperforms a code consisting of 16 random 8 More correlation Too little Too muchOptimum UniformRandom Less correlation FIG. 2: The panels show simulations of the 2D Ising model on a 50 × 50 lattice, with the temperature progressively decreasing from left to right. The integrated information Φ drops to zero bits at T → ∞ (leftmost panel) and to one bit as T → 0 (rightmost panel), taking a maximum at an intermediate temperature near the phase transition temperature. Bits cut offIntegrated informationHamming (8,4)-code (16 8-bit strings) 16 random 8-bit strings 128 8-bit strings with checksum bit 2 4 6 8 1 2 3 4 FIG. 3: For various 8-bit systems, the integrated information is plotted as a function of the number of bits cut oﬀ into a sub-system with the “cruelest cut”. The Hamming (8,4)-code is seen to give classically optimal integration except for a bi- partition into 4 + 4 bits: an arbitrary subset containing no more than three bits is completely determined by the remain- ing bits. The code consisting of the half of all 8-bit strings whose bit sum is even (i.e., each of the 128 7-bit strings fol- lowed by a parity checksum bit) has Hamming distance d = 2 and gives Φ = 1 however many bits are cut oﬀ. A random set of 16 8-bit strings is seen to outperform the Hamming (8,4)- code for 4+4-bipartitions, but not when fewer bits are cut oﬀ. unique bit strings of the same length. A rich and diverse set of codes have been published in the literature, and the state-of-the-art in terms of maximal Hamming dis- tance for a given n is continually updated [21]. Although codes with arbitrarily large Hamming distance d exist, there is (just as for our Hamming(8,4)-example above) no guarantee that Φ will be as large as d − 1 when the smaller of the two subsystems contains more than d bits. Moreover, although Reed-Solomon codes are sometimes billed as classically optimal erasure codes (maximizing d for a given n), their fundamental units are generally Bits cut offIntegrated information 256 random 16-bit words 5 10 15 2 4 6 8 128 random 14-bit words 64 12-bit words 32 10-bit words 16 8-bit words 8 6-bit words 4 4-bit words FIG. 4: Same as for previous ﬁgure, but for random codes with progressively longer bit strings, consisting of a random subset containing √ 2n of the 2 n possible bit strings. For better legibility, the vertical axis has been re-centered for the shorter codes. not bits but groups of bits (generally numbers modulo some prime number), and the optimality is violated if we make cuts that do not respect the boundaries of these bit groups. Although further research on codes maximizing Φ would be of interest, it is worth noting that simple ran- dom codes appear to give Φ-values within a couple of bits of the theoretical maximum in the limit of large n, as il- lustrated in Figure 4. When cutting oﬀ k out of n bits, the mutual information in classical physics clearly can- not exceed the number of bits in either subsystem, i.e., k and n − k, so the Φ-curve for a code must lie within the shaded triangle in the ﬁgure. (The quantum-mechanical case is more complicated, and we well see in the next sec- tion that it in a sense integrates both better and worse.) The codes for which the integrated information is plotted simply consist of a random subset containing 2 n/2 of the 2 n possible bit strings, so roughly speaking, half the bits encode fresh information and the other half provide the 9 2-logarithm of number of patterns usedIntegrated information (bits) 2 4 6 8 10 12 14 1 2 3 4 5 6 7 FIG. 5: The integrated information is shown for random codes using progressively larger random subsets of the 214 possible strings of 14 bits. The optimal choice is seen to be using about 27 bit strings, i.e., using about half the bits to encode information and the other half to integrate it. redundancy giving near-perfect integration. Just as we saw for the Ising model example, these ran- dom codes show a tradeoﬀ between entropy and redun- dancy, as illustrated in Figure 5. When there are n bits, how many of the 2n possible bit strings should we use to maximize the integrated information Φ? If we use m of them, we clearly have Φ ≤ log2 m, since in classical physics, Φ cannot exceed the entropy if the system (the mutual information is I = S1 + S2 − S, where S1 ≤ S and S2 ≤ S so I ≤ S). Using very few bit strings is therefore a bad idea. On the other hand, if we use all 2 n of them, we lose all redundancy, the bits become in- dependent, and Φ = 0, so being greedy and using too many bit strings in an attempt to store more informa- tion is also a bad idea. Figure 5 shows that the optimal tradeoﬀ is to use √2n of the codewords, i.e., to use half the bits to encode information and the other half to in- tegrate it. Taken together, the last two ﬁgures therefore suggest that n physical bits can be used to provide about n/2 bits of integrated information in the large-n limit. E. Integration in physical systems Let us explore the consequences of these results for physical systems described by a Hamiltonian H and a state ρ. As emphasized by Hopﬁeld [22], any physical system with multiple attractors can be viewed as an in- formation storage device, since its state permanently en- codes information about which attractor it belongs to. Figure 6 shows two examples of H interpretable as po- tential energy functions for a a single particle in two di- mensions. They can both be used as information storage devices, by placing the particle in a potential well and keeping the system cool enough that the particle stays in the same well indeﬁnitely. The egg crate potential V (x, y) = sin 2(πx) sin 2(πy) (top) has 256 minima and hence a ground state entropy (information storage ca- pacity) S = 8 bits, whereas the lower potential has only FIG. 6: A particle in the egg-crate potential energy land- scape (top panel) stably encodes 8 bits of information that are completely independent of one another and therefore not integrated. In contrast, a particle in a Hamming(8,4) poten- tial (bottom panel) encodes only 4 bits of information, but with excellent integration. Qualitatively, a hard drive is more like the top panel, while a neural network is more like the bottom panel. 16 minima and S = 4 bits. The basins of attraction in the top panel are seen to be the squares shown in the bottom panel. If we write the x− and y− coordinates as binary numbers with b bits each, then the ﬁrst 4 bits of x and y encode which square (x, y) is in. The information in the remaining bits encodes the location within this square; these bits are not useful for information storage because they can vary over time, as the particle oscillates around a mini- mum. If the system is actively cooled, these oscillations are gradually damped out and the particle settles toward the attractor solution at the minimum, at the center of its basin. This example illustrates that cooling is a physical example of error correction: if thermal noise adds small perturbations to the particle position, altering the least signiﬁcant bits, then cooling will remove these perturba- tions and push the particle back towards the minimum it came from. As long as cooling keeps the perturbations small enough that the particle never rolls out of its basin of attraction, all the 8 bits of information encoding its basin number are perfectly preserved. Instead of inter- preting our n = 8 data bits as positions in two dimen- sions, we can interpret them as positions in n dimensions, where each possible state corresponds to a corner of the n-dimensional hypercube. This captures the essence of many computer memory devices, where each bit is stored in a system with two degenerate minima; the least sig- niﬁcant and redundant bits that can be error-corrected via cooling now get equally distributed among all the di- mensions. 10 How integrated is the information S? For the top panel of Figure 6, not at all: H can be factored as a tensor product of 8 two-state systems, so Φ = 0, just as for typical computer memory. In other words, if the particle is in a particular egg crate basin, knowing any one of the bits specifying the basin position tells us nothing about the other bits. The potential in the lower panel, on the other hand, gives good integration. This potential retains only 16 of the 256 minima, corresponding to the 16 bit strings of the Hamming(8,4)-code, which as we saw gives Φ = 3 for any 3 bits cut oﬀ and Φ = 2 bits for symmetric bipartitions. Since the Hamming distance d = 4 for this code, at least 4 bits must be ﬂipped to reach another minimum, which among other things implies that no two basins can share a row or column. F. The pros and cons of integration Natural selection suggests that self-reproducing information-processing systems will evolve integration if it is useful to them, regardless of whether they are con- scious or not. Error correction can obviously be use- ful, both to correct errors caused by thermal noise and to provide redundancy that improves robustness toward failure of individual physical components such as neu- rons. Indeed, such utility explains the preponderance of error correction built into human-developed devices, from RAID-storage to bar codes to forward error cor- rection in telecommunications. If Tononi is correct and consciousness requires integration, then this raises an in- teresting possibility: our human consciousness may have evolved as an accidental by-product of error correction. There is also empirical evidence that integration is useful for problem-solving: artiﬁcial life simulations of vehicles that have to traverse mazes and whose brains evolve by natural selection show that the more adapted they are to their environment, the higher the integrated information of the main complex in their brain [23]. However, integration comes at a cost, and as we will now see, near maximal integration appears to be pro- hibitively expensive. Let us distinguish between the max- imum amount of information that can be stored in a state deﬁned by ρ and the maximum amount of information that can be stored in a physical system deﬁned by H. The former is simply S(ρ) for the perfectly mixed (T = ∞) state, i.e., log2 of the number of possible states (the num- ber of bits characterizing the system). The latter can be much larger, corresponding to log2 of the number of Hamiltonians that you could distinguish between given your time and energy available for experimentation. Let us consider potential energy functions whose k diﬀerent minima can be encoded as bit strings (as in Figure 6), and let us limit our experimentation to ﬁnding all the minima. Then H encodes not a single string of n bits, but a subset consisting of k out of all 2 n such strings, one for each minimum. There are ( 2 n k ) such subsets, so the information contained in H is S(H) = log2 ( 2 n k ) = log2 2 n! k!(2n − k)! ≈ ≈ log2 (2n) k kk = k(n − log2 k) (7) for k ≪ 2 n, where we used Stirling’s approximation k! ≈ kk. So crudely speaking, H encodes not n bits but kn bits. For the near-maximal integration given by the random codes from the previous section, we had k = 2 n/2, which gives S(H) ∼ 2 n/2 n 2 bits. For example, if the n ∼ 1011 neurons in your brain were maximally integrated in this way, then your neural network would require a dizzying 1010000000000 bits to describe, vastly more information than can be encoded by all the 1089 particles in our universe combined. The neuronal mechanisms of human memory are still unclear despite intensive experimental and theoretical ex- plorations [24], but there is signiﬁcant evidence that the brain uses attractor dynamics in its integration and mem- ory functions, where discrete attractors may be used to represent discrete items [25]. The classic implementa- tion of such dynamics as a simple symmetric and asyn- chronous Hopﬁeld neural network [22] can be conve- niently interpreted in terms of potential energy func- tions: the equations of the continuous Hopﬁeld network are identical to a set of mean-ﬁeld equations that mini- mize a potential energy function, so this network always converges to a basin of attraction [26]. Such a Hopﬁeld network gives a dramatically lower information content S(H) of only about 0.25 bits per synapse[26], and we have only about 10 14 synapses, suggesting that our brains can store only on the order of a few Terabytes of information. The integrated information of a Hopﬁeld network is even lower. For a Hopﬁeld network of n neurons with Hebbian learning, the total number of attractors is bounded by 0.14n [26], so the maximum information ca- pacity is merely S ≈ log2 0.14n ≈ log2 n ≈ 37 bits for n = 10 11 neurons. Even in the most favorable case where these bits are maximally integrated, our 1011 neurons thus provide a measly Φ ≈ 37 bits of integrated informa- tion, as opposed to about Φ ≈ 5 × 10 10 bits for a random coding. G. The integration paradox This leaves us with an integration paradox: why does the information content of our conscious experience ap- pear to be vastly larger than 37 bits? If Tononi’s informa- tion and integration principles from Section I are correct, the integration paradox forces us 6 to draw at least one of 6 Can we sidestep the integration paradox by simply dismissing the idea that integration is necessary? Although it remains contro- 11 the following three conclusions: 1. Our brains use some more clever scheme for encod- ing our conscious bits of information, which allows dramatically larger Φ than Hebbian Hopﬁeld net- works. 2. These conscious bits are much fewer than we might naively have thought from introspection, implying that we are only able to pay attention to a very modest amount of information at any instant. 3. To be relevant for consciousness, the deﬁnition of integrated information that we have used must be modiﬁed or supplemented by at least one additional principle. We will see that the quantum results in the next section bolster the case for conclusion 3. Interestingly, there is also support for conclusion 2 in the large psychophysical literature on the illusion of the perceptual richness of the world. For example, there is evidence suggesting that of the roughly 107 bits of information that enter our brain each second from our sensory organs, we can only be aware of a tiny fraction, with estimates ranging from 10 to 50 bits [27, 28]. The fundamental reason why a Hopﬁeld network is speciﬁed by much less information than a near-maximally integrated network is that it involves only pairwise cou- plings between neurons, thus requiring only ∼ n2 cou- pling parameters to be speciﬁed — as opposed to 2n pa- rameters giving the energy for each of the 2n possible states. It is striking how H is similarly simple for the standard model of particle physics, with the energy in- volving only sums of pairwise interactions between parti- cles supplemented with occasional 3-way and 4-way cou- plings. H for the brain and H for fundamental physics thus both appear to belong to an extremely simple sub- class of all Hamiltonians, that require an unusually small amount of information to describe. Just as a system im- plementing near-maximal integration via random coding is too complicated to ﬁt inside the brain, it is also too complicated to work in fundamental physics: Since the information storage capacity S of a physical system is approximately bounded by its number of particles [16] or by its area in Planck units by the Holographic principle [17], it cannot be integrated by physical dynamics that itself requires storage of the exponentially larger informa- tion quantity S(H) ∼ 2S/2 S 2 unless the Standard Model Hamiltonian is replaced by something dramatically more complicated. versial whether integrated information is a suﬃcient condition for consciousness as asserted by IIT, it appears rather obvious that it is a necessary condition if the conscious experience is uniﬁed: if there were no integration, the conscious mind would consist of two separate parts that were independent of one an- other and hence unaware of each other. An interesting theoretical direction for further research (pursuing resolution 1 to the integration paradox) is therefore to investigate what maximum amount of in- tegrated information Φ can be feasibly stored in a physi- cal system using codes that are algorithmic (such as RS- codes) rather than random. An interesting experimental direction would be to search for concrete implementa- tions of error-correction algorithms in the brain. In summary, we have explored the integration prin- ciple by quantifying integrated information in physical systems. We have found that although excellent integra- tion is possible in principle, it is more diﬃcult in prac- tice. In theory, random codes provide nearly maximal integration, with about half of all n bits coding for data and the other half providing Ψ ≈ n bits of integration), but in practice, the dynamics required for implement- ing them is too complex for our brain or our universe. Most of our exploration has focused on classical physics, where cuts into subsystems have corresponded to parti- tions of classical bits. As we will see in the next section, ﬁnding systems encoding large amounts of integrated in- formation is even more challenging when we turn to the quantum-mechanical case. III. INDEPENDENCE A. Classical versus quantum independence How cruel is what Tononi calls “the cruelest cut”, di- viding a system into two parts that are maximally in- dependent? The situation is quite diﬀerent in classical physics and quantum physics, as Figure 7 illustrates for a simple 2-bit system. In classical physics, the state is speciﬁed by a 2×2 matrix giving the probabilities for the four states 00, 01, 10 and 11, which deﬁne an entropy S and mutual information I. Since there is only one pos- sible cut, the integrated information Φ = I. The point deﬁned by the pair (S, Φ) can lie anywhere in the “pyra- mid” in the ﬁgure, who’s top at (S, Φ) = (1, 1) (black star) gives maximum integration, and corresponds to per- fect correlation between the two bits: 50% probability for 00 and 11. Perfect anti-correlation gives the same point. The other two vertices of the classically allowed region are seen to be (S, Φ) = (0, 0) (100% probability for a sin- gle outcome) and (S, Φ) = (2, 0) (equal probability for all four outcomes). In quantum mechanics, where the 2-qubit state is de- ﬁned by a 4 × 4 density matrix, the available area in the (S, I)-plane doubles to include the entire shaded trian- gle, with the classically unattainable region opened up because of entanglement. The extreme case is a Bell pair state such as |ψ⟩ = 1 √2 (|↑⟩|↑⟩ + |↓⟩|↓⟩) , (8) which gives (S, I) = (0, 2). However, whereas there was only one possible cut for 2 classical bits, there are now in- 12 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 Entropy SMutual information I Possible only quantum-mechanically (entanglement) Possible classically Bell pair ( ) .4 .4 .2 .0( ) .91 .03 .03 .03 ( ) 1/3 1/3 1/3 0 ( ) 1 0 0 0 ( ) .7 .1 .1 .1 ( ) 1/2 0 0 1/2 ( ) 1/2 1/2 0 0 ( ) 1/4 1/4 1/4 1/4 ( ) .3 .3 .3 .1 Quantum integratedUnitary transformationUnitary transformation FIG. 7: Mutual information versus entropy for various 2-bit systems. The diﬀerent dots, squares and stars correspond to diﬀerent states, which in the classical cases are deﬁned by the probabilities for the four basis states 00, 01 10 and 11. Classical states can lie only in the pyramid below the upper black star with (S, I) = (1, 1), whereas entanglement allows quantum states to extend all the way up to the upper black square at (0, 2). However, the integrated information Φ for a quantum state cannot lie above the shaded green/grey region, into which any other quantum state can be brought by a unitary transformation. Along the upper boundary of this region, either three of the four probabilities are equal, or to two of them are equal while one vanishes. ﬁnitely many possible cuts because in quantum mechan- ics, all Hilbert space bases are equally valid, and we can choose to perform the factorization in any of them. Since Φ is deﬁned as I after the cruelest cut, it is the I-value minimized over all possible factorizations. For simplicity, we use the notation where ⊗ denotes factorization in the coordinate basis, so the integrated information is Φ = min U I(UρU†), (9) i.e., the mutual information minimized over all possi- ble unitary transformations U. Since the Bell pair of equation (8) is a pure state ρ = |ψ⟩⟨ψ|, we can unitarily transform it into a basis where the ﬁrst basis vector is |ψ⟩, making it factorizable: U     1 2 0 0 1 2 0 0 0 0 0 0 0 0 1 2 0 0 1 2     U† =    1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0    = ( 1 0 0 0 ) ⊗ ( 1 0 0 0 ) . (10) This means that Φ = 0, so in quantum mechanics, the cruelest cut can be very cruel indeed: the most entangled states possible in quantum mechanics have no integrated information at all! The same cruel fate awaits the most integrated 2- bit state from classical physics: the perfectly correlated mixed state ρ = 1 2 | ↑⟩⟨↑ | + 1 2 | ↓⟩⟨↓ |. It gave Φ = 1 bit classically above (upper black star in the ﬁgure), but a unitary transformation permuting its diagonal elements makes it factorable: U     1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2     U† =     1 2 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0     = ( 1 0 0 0 )⊗( 1 2 0 0 1 2 ) , (11) so Φ = 0 quantum-mechanically (lower black star in the ﬁgure). B. Canonical transformations, independence and relativity The fundamental reason that these states are more sep- arable quantum-mechanically is clearly that more cuts are available, making the cruelest one crueler. Interest- ingly, the same thing can happen also in classical physics. Consider, for example, our example of the deuterium atom from equation (1). When we restricted our cuts to simply separating diﬀerent degrees of freedom, we found that the group (rp, pp, rn, pn) was quite (but not com- pletely) independent of the group (re, pe), and that there was no cut splitting things into perfectly independent pieces. In other words, the nucleus was fairly indepen- dent of the electron, but none of the three particles was completely independent of the other two. However, if we allow our degrees of freedom to be transformed before the cut, then things can be split into two perfectly in- dependent parts! The classical equivalent of a unitary transformation is of course a canonical transformation (one that preserves phase-space volume). If we perform the canonical transformation where the new coordinates are the center-of-mass position rM and the relative dis- placements r ′ p ≡ rp − rM and r ′ e ≡ re − rM , and cor- respondingly deﬁne pM as the total momentum of the whole system, etc., then we ﬁnd that (rM , pM ) is com- pletely independent of the rest. In other words, the av- erage motion of the entire deuterium atom is completely decoupled from the internal motions around its center- of-mass. Interestingly, this well-known possibility of decompos- ing any isolated system into average and relative motions (the “average-relative decomposition”, for short) is equiv- alent to relativity theory in the following sense. The core of relativity theory is that all laws of physics (in- cluding the speed of light) are the same in all inertial frames. This implies the average-relative decomposition, since the laws of physics governing the relative motions of the system are the same in all inertial frames and hence independent of the (uniform) center-of-mass mo- tion. Conversely, we can view relativity as a special case 13 of the average-relative decomposition. If two systems are completely independent, then they can gain no knowl- edge of each other, so a conscious observer in one will be unaware of the other. The average-relative decompo- sition therefore implies that an observer in an isolated system has no way of knowing whether she is at rest or in uniform motion, because these are simply two diﬀerent allowed states for the center-of-mass subsystem, which is completely independent from (and hence inaccessible to) the internal-motions subsystem of which her conscious- ness is a part. C. How integrated can quantum states be? We saw in Figure 7 that some seemingly inte- grated states, such as a Bell pair or a pair of clas- sically perfectly correlated bits, are in fact not inte- grated at all. But the ﬁgure also shows that some states are truly integrated even quantum-mechanically, with I > 0 even for the cruelest cut. How inte- grated can a quantum state be? The following theo- rem, proved by Jevtic, Jennings & Rudolph [29], en- ables the answer to be straightforwardly calculated 7: ρ-Diagonality Theorem (ρDC): The mutual information always takes its min- imum in a basis where ρ is diagonal The ﬁrst step in computing the integrated information Φ(ρ) is thus to diagonalize the n × n density matrix ρ. If all n eigenvalues are diﬀerent, then there are n! pos- sible ways of doing this, corresponding to the n! ways of permuting the eigenvalues, so the ρDC simpliﬁes the continuous minimization problem of equation (9) to a dis- crete minimization problem over these n! permutations. Suppose that n = l × m, and that we wish to factor the n-dimensional Hilbert space into factor spaces of dimen- sionality l and m, so that Φ = 0. It is easy to see that this is possible if the n eigenvalues of ρ can be arranged into an l × m matrix that is multiplicatively separable (rank 1), i.e., the product of a column vector and a row vector. Extracting the eigenvalues for our example from equation (11) where l = m = 2 and n = 4, we see that ( 1 2 1 2 0 0 ) is separable, but ( 1 2 0 0 1 2 ) is not, and that the only diﬀerence is that the order of the four numbers has been permuted. More generally, we see that to ﬁnd the “cruelest cut” that deﬁnes the integrated in- formation Φ, we want to ﬁnd the permutation that makes the matrix of eigenvalues as separable as possible. It is 7 The converse of the ρDC is straightforward to prove: if Φ = 0 (which is equivalent to the state being factorizable; ρ = ρ1 ⊗ ρ2), then it is factorizable also in its eigenbasis where both ρ1 and ρ2 are diagonal. easy to see that when seeking the permutation giving maximum separability, we can without loss of generality place the largest eigenvalue ﬁrst (in the upper left corner) and the smallest one last (in the lower right corner). If there are only 4 eigenvalues (as in the above example), the ordering of the remaining two has no eﬀect on I. D. The quantum integration paradox We now have the tools in hand to answer the key ques- tion from the last section: which state ρ maximizes the integrated information Φ? Numerical search suggests that the most integrated state is a rescaled projection matrix satisfying ρ2 ∝ ρ. This means that some num- ber k of the n eigenvalues equal 1/k and the remain- ing ones vanish. 8 For the n = 4 example from Fig- ure 7, k = 3 is seen to give the best integration, with eigenvalues (probabilities) 1/3, 1/3, 1/3 and 0, giving Φ = log(27/16)/ log(8) ≈ 0.2516. For classical physics, we saw that the maximal at- tainable Φ grows roughly linearly with n. Quantum- mechanically, however, it decreases as n increases!9 In summary, no matter how large a quantum system we create, its state can never contain more than about a quarter of a bit of integrated information! This exacer- bates the integration paradox from Section II G, eliminat- ing both of the ﬁrst two resolutions: you are clearly aware of more than 0.25 bits of information right now, and this quarter-bit maximum applies not merely to states of Hop- ﬁeld networks, but to any quantum states of any system. Let us therefore begin exploring the third resolution: that our deﬁnition of integrated information must be modiﬁed or supplemented by at least one additional principle. 8 A heuristic way of understanding why having many equal eigen- values is advantageous is that it helps eliminate the eﬀect of the eigenvalue permutations that we are minimizing over. If the op- timal state has two distinct eigenvalues, then if swapping them changes I, it must by deﬁnition increase I by some ﬁnite amount. This suggests that we can increase the integration Φ by bringing the eigenvalues inﬁnitesimally closer or further apart, and repeat- ing this procedure lets us further increase Φ until all eigenvalues are either zero or equal to the same positive constant. 9 One ﬁnds that Φ is maximized when the k identical nonzero eigenvalues are arranged in a Young Tableau, which corre- sponds to a partition of k as a sum of positive integers k1 + k2 + ..., giving Φ = S(p) + S(p∗) − log2 k, where the probability vectors p and p∗ are deﬁned by pi = ki/k and p∗ i = k∗ i /k. Here k∗ i denotes the conjugate partition. For example, if we cut an even number of qubits into two parts with n/2 qubits each, then n = 2, 4, 6, ..., 20 gives Φ ≈ 0.252, 0.171, 0.128, 0.085, 0.085, 0.073, 0.056, 0.056, 0.051 and 0.042 bits, respectively. 14 E. How integrated is the Hamiltonian? An obvious way to begin this exploration is to con- sider the state ρ not merely at a single ﬁxed time t, but as a function of time. After all, it is widely assumed that consciousness is related to information processing, not mere information storage. Indeed, Tononi’s original Φ- deﬁnition [8] (which applies to classical neural networks rather than general quantum systems) involves time, de- pending on the extent to which current events aﬀect fu- ture ones. Because the time-evolution of the state ρ is determined by the Hamiltonian H via the Schr¨odinger equation ˙ρ = i[H, ρ], (12) whose solution is ρ(t) = eiHtρe−iHt, (13) we need to investigate the extent to which the cruelest cut can decompose not merely ρ but the pair (ρ, H) into independent parts. (Here and throughout, we often use units where ℏ = 1 for simplicity.) F. Evolution with separable Hamiltonian As we saw above, the key question for ρ is whether it it is factorizable (expressible as product ρ = ρ1 ⊗ ρ2 of matrices acting on the two subsystems), whereas the key question for H is whether it is what we will call addi- tively separable, being a sum of matrices acting on the two subsystems, i.e., expressible in the form H = H1 ⊗ I + I ⊗ H2 (14) for some matrices H1 and H2. For brevity, we will often write simply separable instead of additively separable. As mentioned in Section II B, a separable Hamiltonian H implies that both the thermal state ρ ∝ e−H/kT and the time-evolution operator U ≡ eiHt/ℏ are factorizable. An important property of density matrices which was pointed out already by von Neumann when he invented them [30] is that if H is separable, then ˙ρ1 = i[H1, ρ1], (15) i.e., the time-evolution of the state of the ﬁrst subsystem, ρ1 ≡ tr 2ρ, is independent of the other subsystem and of any entanglement with it that may exist. This is easy to prove: Using the identities (A12) and (A14) shows that tr 2 [H1 ⊗ I, ρ] = tr 2 {(H1 ⊗ I)ρ} − tr 2 {ρ(H1 ⊗ I)} = H1ρ1 − ρ1H2 = [H1, ρ1]. (16) Using the identity (A10) shows that tr 2 [I ⊗ H2, ρ] = 0. (17) Summing equations (16) and (17) completes the proof. G. The cruelest cut as the maximization of separability Since a general Hamiltonian H cannot be written in the separable form of equation (14), it will also include a third term H3 that is non-separable. The independence principle from Section I therefore suggests an interest- ing mathematical approach to the physics-from-scratch problem of analyzing the total Hamiltonian H for our physical world: 1. Find the Hilbert space factorization giving the “cruelest cut”, decomposing H into parts with the smallest interaction Hamiltonian H3 possible. 2. Keep repeating this subdivision procedure for each part until only relatively integrated parts remain that cannot be further decomposed with a small interaction Hamiltonian. The hope would be that applying this procedure to the Hamiltonian of our standard model would reproduce the full observed object hierarchy from Figure 1, with the fac- torization corresponding to the objects, and the various non-separable terms H3 describing the interactions be- tween these objects. Any decomposition with H3 = 0 would correspond to two parallel universes unable to communicate with one another. We will now formulate this as a rigorous mathemat- ics problem, solve it, and derive the observational conse- quences. We will ﬁnd that this approach fails catastroph- ically when confronted with observation, giving interest- ing hints regarding further physical principles needed for understanding why we perceive our world as an object hierarchy. H. The Hilbert-Schmidt vector space To enable a rigorous formulation of our problem, let us ﬁrst brieﬂy review the Hilbert-Schmidt vector space, a convenient inner-product space where the vectors are not wave functions |ψ⟩ but matrices such as H and ρ. For any two matrices A and B, the Hilbert-Schmidt inner product is deﬁned by (A, B) ≡ tr A†B. (18) For example, the trace operator can be written as an inner product with the identity matrix: tr A = (I, A). (19) This inner product deﬁnes the Hilbert-Schmidt norm (also known as the Frobenius norm) ||A|| ≡ (A, A) 1 2 = (tr A†A) 1 2 =  ∑ ij |Aij| 2   1 2 . (20) 15 If A is Hermitian (A† = A), then ||A|| 2 is simply the sum of the squares of its eigenvalues. Real symmetric and antisymmetric matrices form or- thogonal subspaces under the Hilbert-Schmidt inner product, since (S, A) = 0 for any symmetric matrix S (satisfying S t = S) and any antisymmetric matrix A (satisfying At = −A). Because a Hermitian matrix (sat- isfying H † = H) can be written in terms of real sym- metric and antisymmetric matrices as H = S + iA, we have (H1, H2) = (S1, S2) + (A1, A2), which means that the inner product of two Hermitian matrices is purely real. I. Separating H with orthogonal projectors By viewing H as a vector in the Hilbert-Schmidt vec- tor space, we can rigorously deﬁne an decomposition of it into orthogonal components, two of which are the sep- arable terms from equation (14). Given a factorization of the Hilbert space where the matrix H operates, we deﬁne four linear superoperators 10 Πi as follows: Π0H ≡ 1 n (tr H) I (21) Π1H ≡ ( 1 n2 tr 2 H ) ⊗ I2 − Π0H (22) Π2H ≡ I1 ⊗ ( 1 n1 tr 1 H) − Π0H (23) Π3H ≡ (I − Π1 − Π2 − Π3)H (24) It is straightforward to show that these four linear op- erators Πi form a complete set of orthogonal projectors, i.e., that 3∑ i=0 Πi = I, (25) ΠiΠj = Πiδij, (26) (ΠiH, ΠjH) = ||ΠiH|| 2δij. (27) This means that any Hermitian matrix H can be de- composed as a sum of four orthogonal components Hi ≡ ΠiH, so that its squared Hilbert-Schmidt norm can be decomposed as a sum of contributions from the four com- 10 Operators on the Hilbert-Schmidt space are usually called su- peroperators in the literature, to avoid confusions with operators on the underlying Hilbert space, which are mere vectors in the Hilbert-Schmidt space. ponents: H = H0 + H1 + H2 + H3, (28) Hi ≡ ΠiH, (29) (Hi, Hj) = ||Hi|| 2δij, (30) ||H|| 2 = ||H0|| 2+||H1|| 2+||H2|| 2+||H3|| 2. (31) We see that H0 ∝ I picks out the trace of H, whereas the other three matrices are trace-free. This trace term is of course physically uninteresting, since it can be eliminated by simply adding an unobservable constant zero-point en- ergy to the Hamiltonian. H1 and H2 corresponds to the two separable terms in equation (14) (without the trace term, which could have been arbitrarily assigned to ei- ther), and H3 corresponds to the non-separable residual. A Hermitian matrix H is therefore separable if and only if Π3H = 0. Just as it is customary to write the norm or a vector r by r ≡ |r| (without boldface), we will denote the Hilbert-Schmidt norm of a matrix H by H ≡ ||H||. For example, with this notation we can rewrite equation (31) as simply H 2 = H 2 0 + H 2 1 + H 2 2 + H 2 3 . Geometrically, we can think of n × n Hermitian matri- ces H as points in the N -dimensional vector space RN , where N = n×n (Hermiteal matrices have n real numbers on the diagonal and n(n − 1)/2 complex numbers oﬀ the diagonal, constituting a total of n + 2 × n(n − 1)/2 = n2 real parameters). Diagonal matrices form a hyperplane of dimension n in this space. The projection operators Π0, Π1, Π2 and Π3 project onto hyperplanes of dimen- sion 1, (n − 1), (n − 1) and (n − 1)2, respectively, so separable matrices form a hyperplane in this space of di- mension 2n − 1. For example, a general 4 × 4 Hermitian matrix can be parametrized by 10 numbers (4 real for the diagonal part and 6 complex for the oﬀ-diagonal part), and its decomposition from equation (28) can be written as follows: H =    t+a+b+v d+w c+x y d ∗ +w∗ t+a−b−v z c−x c∗ +x∗ z∗ t−a+b−v d−w y∗ c∗ −x∗ d ∗ −w∗ t−a−b+v    = =    t 0 0 0 0 t 0 0 0 0 t 0 0 0 0 t   +    a 0 c 0 0 a 0 c c ∗ 0 −a 0 0 c ∗ 0 −a   +    b d 0 0 d ∗ −b 0 0 0 0 b d 0 0 d ∗ −b   + +    v w x y w∗ −v z −x x∗ z∗ −v −w y∗ −x∗ −w∗ v    (32) We see that t contributes to the trace (and H0) while the other three components Hi are traceless. We also see that tr 1H2 = tr 2H1 = 0, and that both partial traces vanish for H3. 16 J. Maximizing separability We now have all the tools we need to rigorously max- imize separability and test the physics-from-scratch ap- proach described in Section III G. Given a Hamiltonian H, we simply wish to minimize the norm of its non- separable component H3 over all possible Hilbert space factorizations, i.e., over all possible unitary transforma- tions. In other words, we wish to compute ˚E ≡ min U ||Π3H||, (33) where we have deﬁned the integration energy ˚E by anal- ogy with the integrated information Φ. If ˚E = 0, then there is a basis where our system separates into two paral- lel universes, otherwise ˚E quantiﬁes the coupling between the two parts of the system under the cruelest cut. The Hilbert-Schmidt space allows us to interpret the minimization problem of equation (33) geometrically, as illustrated in Figure 8. Let H∗ denote the Hamiltonian in some given basis, and consider its orbit H = UHU† under all unitary transformations U. This is a curved hy- persurface whose dimensionality is generically n(n − 1), i.e., n lower than that of the full space of Hermitian ma- trices, since unitary transformation leave all n eigenval- ues invariant.11 We will refer to this curved hypersur- face as a subsphere, because it is a subset of the full n2- dimensional sphere: the radius H (the Hilbert-Schmidt norm ||H||) is invariant under unitary transformations, but the subsphere may have a more complicated topology than a hypersphere; for example, the 3-sphere is known to topologically be the double cover of SO(3), the matrix group of 3 × 3 orthonormal transformations. We are interested in ﬁnding the most separable point H on this subsphere, i.e., the point on the subsphere that is closest to the (2n − 1)-dimensional separable hyperplane. In our notation, this means that we want to ﬁnd the point H on the subsphere that minimizes ||Π3H||, the Hilbert- Schmidt norm of the non-separable component. If we perform inﬁnitesimal displacements along the subsphere, ||Π3H|| thus remains constant to ﬁrst order (the gradient vanishes at the minimum), so all tangent vectors of the subsphere are orthogonal to Π3H, the vector from the separable hyperplane to the subsphere. Unitary transformations are generated by anti- Hermitian matrices, so the most general tangent vector δH is of the form δH = [A, H] ≡ AH − HA (34) for some anti-Hermitian n × n matrix A (any matrix sat- isfying A† = −A). We thus obtain the following simple 11 n×n-dimensional Unitary matrices U are known to form an n×n- dimensional manifold: they can always be written as U = eiH for some Hermitian matrix H, so they are parametrized by the same number of real parameters (n × n) as Hermitian matrices. condition for maximal separability: (Π3H, [A, H]) = 0 (35) for any anti-Hermitian matrix A. Because the most gen- eral anti-Hermitian matrix can be written as A = iB for a Hermitian matrix B, equation (35) is equivalent to the condition (Π3H, [B, H]) = 0 for all Hermitian matrices B. Since there are n2 anti-Hermitian matrices, equa- tion (35) is a system of n 2 coupled quadratic equations that the components of H must obey. Tangent vector δH=[A,H] Integration energy E=||Π3Η|| Non-separable component Π 3ΗSeparable hyperplane: Π3Η=0 Subsphere H=UH *U †{ FIG. 8: Geometrically, we can view the integration energy as the shortest distance (in Hilbert-Schmidt norm) between the hyperplane of separable Hamiltonians and a subsphere of Hamiltonians that can be unitarily transformed into one another. The most separable Hamiltonian H on the subsphere is such that its non-separable component Π3 is orthogonal to all subsphere tangent vectors [A, H] generated by anti- Hermitian matrices A. K. The Hamiltonian diagonality theorem Analogously to the above-mentioned ρ-diagonality the- orem, we will now prove that maximal separability is at- tained in the eigenbasis. H-Diagonality Theorem (HDT): The Hamiltonian is always maximally separa- ble (minimizing ||H3||) in the energy eigenba- sis where it is diagonal. As a preliminary, let us ﬁrst prove the following: Lemma 1: For any Hermitian positive semideﬁnite matrix H, there is a diagonal matrix H∗ giving the same subsystem eigenvalue spectra, λ(Π1H∗) = λ(Π1H), λ(Π2H∗) = λ(Π2H), and whose eigenvalue spectrum is majorized by that of H, i.e., λ(H) ≻ λ(H∗). 17 Proof: Deﬁne the matrix H ′ ≡ UHU †, where U ≡ U1 ⊗ U2, and U1 and U2 are unitary matrices diagonal- izing the partial trace matrices tr 2H and tr 1H, respec- tively. This implies that tr 1H ′ and tr 2H′ are diagonal, and λ(H ′) = λ(H). Now deﬁne the matrix H∗ to be H′ with all oﬀ-diagonal elements set to zero. Then tr 1H∗ = tr 1H ′ and tr 2H∗ = tr 2H ′, so λ(Π1H∗) = λ(Π1H) and λ(Π2H∗) = λ(Π2H). Moreover, since the eigenvalues of any Hermitian positive semideﬁnite matrix majorize its diagonal elements [31], λ(H∗) ≺ λ(H′) = λ(H), which completes the proof. Lemma 2: The set S(H) of all diagonal matrices whose diagonal elements are majorized by the vector λ(H) is a convex subset of the subsphere, with boundary points on the surface of the subsphere that are diagonal matrices with all permutations of λ(H). Proof: Any matrix H∗ ∈ S(H) must lie either on the subsphere surface or in its interior, because of the well-known result that for any two positive semideﬁnite Hermitian matrices of equal trace, the majorization con- dition λ(H∗) ≺ λ(H) is equivalent to the former lying in the convex hull of the unitary orbit of the latter [32]: H∗ = ∑ i piUiHU† i , pi ≥ 0, ∑ i pi = 1, UiU† i = I. S(H) contains the above-mentioned boundary points, because they can be written as UHU† for all unitary matrices U that diagonalize H, and for a diagonal matrix, the corresponding H∗ is simply the matrix itself. The set S(H) is convex, because the convexity condition that pλ1 + (1 − p)λ2 ≻ λ if λ1 ≻ λ, λ2 ≻ λ, 0 ≤ p ≤ 1 follows straight from the deﬁnition of ≻. Lemma 3: The function f (H) ≡ ||Π1H|| 2 + ||Π2H|| 2 is convex, i.e., satisﬁes f (paHa + pbHb) ≤ paf (Ha) + pbf (Hb) for any constants satisfying pa ≥ 0, pb ≥ 0, pa + pb = 1. Proof: If we arrange the elements of H into a vec- tor h and denote the action of the superoperators Πi on h by matrices Pi, then f (H) = |P1h| 2 + |P2h| 2 = h †(P† 1P1 + P † 2P2)h. Since the matrix in parenthesis is symmetric and positive semideﬁnite, the function f is a positive semideﬁnite quadratic form and hence convex. We are now ready to prove the H-diagonality theorem. This is equivalent to proving that f (H) takes its maxi- mum value on the subsphere in Figure 8 for a diagonal H: since both ||H|| and ||H0|| are unitarily invariant, mini- mizing ||H3|| 2 = ||H|| 2 − ||H0|| 2 − f (H) is equivalent to maximizing f (H). Let O(H) denote the subphere, i.e., the unitary orbit of H. By Lemma 1, for every H ∈ O(H), there is an H∗ ∈ S(H) such that f (H) = f (H∗). If f takes its maximum over S(H) at a point H∗ which also belongs to O(H), then this is therefore also the maximum of f over O(H). Since the function f is convex (by Lemma 3) and the set S(H) is convex (by Lemma 2), f cannot have any local maxima within the set and must take its maximum value at at least one point on the boundary of the set. As per Lemma 2, these boundary points are diagonal matrices with all permutations of the eigenvalues of H, so they also belong to O(H) and therefore constitute maxima of f over the subsphere. In other words, the Hamiltonian is always maximally separable in its energy eigenbasis, q.e.d. This result holds also for Hamiltonians with negative eigenvalues, since we can make all eigenvalues positive by adding an H0-component without altering the opti- mization problem. In addition to the diagonal optimum, there will generally be other bases with identical values of ||H3||, corresponding to separable unitary transforma- tions of the diagonal optimum. We have thus proved that separability is always max- imized in the energy eigenbasis, where the n × n matrix H is diagonal and the projection operators Πi deﬁned by equations (21)-(24) greatly simplify. If we arrange the n = lm diagonal elements of H into an l × m matrix H, then the action of the linear operators Πi is given by simple matrix operations: H0 ≡ QlHQm, (36) H1 ≡ PlHQm, (37) H2 ≡ QlHPm, (38) H3 ≡ PlHPm, (39) where Pk ≡ I − Qk, (40) (Qk)ij ≡ 1 k (41) are k × k projection matrices satisfying P 2 k = Pk, Q 2 k = Qk, PkQk = QkPk = 0, Pk + Qk = I. (To avoid confu- sion, we are using boldface for n × n matrices and plain font for smaller matrices involving only the eigenvalues.) For the n = 2 × 2 example of equation (32), we have P2 = ( 1 2 1 2 1 2 1 2 ) , Q2 = 1 2 ( 1 2 − 1 2 − 1 2 1 2 ) , (42) and a general diagonal H is decomposed into four terms H = H0 + H1 + H2 + H3 as follows: H = ( t t t t )+( a a −a −a )+( b −b b −b )+( v −v −v v ) . (43) As expected, only the last matrix is non-separable, and the row/column sums vanish for the two previous matri- ces, corresponding to vanishing partial traces. Note that we are here choosing the n basis states of the full Hilbert space to be products of basis states from the two factor spaces. This is without loss of generality, since any other basis states can be transformed into such product states by a unitary transformation. Finally, note that the theorem above applies only to ex- act ﬁnite-dimensional Hamiltonians, not to approximate discretizations of inﬁnite-dimensional ones such as are frequently employed in physics. If n is not factorizable, the H-factorization problem can be rigorously mapped 18 onto a physically indistinguishable one with a slightly larger factorizable n by setting the corresponding new rows and columns of the density matrix ρ equal to zero, so that the new degrees of freedom are all frozen out — we will discuss this idea in more detail in in Section IV F. L. Ultimate independence and the Quantum Zeno paradox FIG. 9: If the Hamiltonian of a system commutes with the interaction Hamiltonian ([H1, H3] = 0), then decoherence drives the system toward a time-independent state ρ where nothing ever changes. The ﬁgure illustrates this for the Bloch Sphere of a single qubit starting in a pure state and ending up in a fully mixed state ρ = I/2. More general initial states end up somewhere along the z-axis. Here H1 ∝ σz, generating a simple precession around the z-axis. In Section III G, we began exploring the idea that if we divide the world into maximally independent parts (with minimal interaction Hamiltonians), then the observed ob- ject hierarchy from Figure 1 would emerge. The HDT tells us that this decomposition (factorization) into max- imally independent parts can be performed in the energy eigenbasis of the total Hamiltonian. This means that all subsystem Hamiltonians and all interaction Hamiltonians commute with one another, corresponding to an essen- tially classical world where none of the quantum eﬀects associated with non-commutativity manifest themselves! In contrast, many systems that we customarily refer to as objects in our classical world do not commute with their interaction Hamiltonians: for example, the Hamil- tonian governing the dynamics of a baseball involves its momentum, which does not commute with the position- dependent potential energy due to external forces. As emphasized by Zurek [33], states commuting with the interaction Hamiltonian form a “pointer basis” of classically observable states, playing an important role in understanding the emergence of a classical world. The fact that the independence principle automatically leads to commutativity with interaction Hamiltonians might therefore be taken as an encouraging indication that we are on the right track. However, whereas the pointer states in Zurek’s examples evolve over time due to the system’s own Hamiltonian H1, those in our independence-maximizing decomposition do not, because they commute also with H1. Indeed, the situation is even worse, as illustrated in Figure 9: any time- dependent system will evolve into a time-independent one, as environment-induced decoherence [34–37, 39, 40] drives it towards an eigenstate of the interaction Hamil- tonian, i.e., an energy eigenstate.12 The famous Quantum Zeno eﬀect, whereby a system can cease to evolve in the limit where it is arbitrar- ily strongly coupled to its environment [41], thus has a stronger and more pernicious cousin, which we will term the Quantum Zeno Paradox or the Independence Para- dox. Quantum Zeno Paradox: If we decompose our universe into maximally independent objects, then all change grinds to a halt. In summary, we have tried to understand the emer- gence of our observed semiclassical world, with its hi- erarchy of moving objects, by decomposing the world into maximally independent parts, but our attempts have failed dismally, producing merely a timeless world remi- niscent of heat death. In Section II G, we saw that using the integration principle alone led to a similarly embar- rassing failure, with no more than a quarter of a bit of integrated information possible. At least one more prin- ciple is therefore needed. IV. DYNAMICS AND AUTONOMY Let us now explore the implications of the dynamics principle from Table II, according to which a conscious system has the capacity to not only store information, but also to process it. As we just saw above, there is an interesting tension between this principle and the inde- pendence principle, whose Quantum Zeno Paradox gives the exact opposite: no dynamics and no information pro- cessing at all. 12 For a system with a ﬁnite environment, the entropy will eventu- ally decrease again, causing the resumption of time-dependence, but this Poincar´e recurrence time grows exponentially with envi- ronment size and is normally large enough that decoherence can be approximated as permanent. 19 We will term the synthesis of these two competing prin- ciples the autonomy principle: a conscious system has substantial dynamics and independence. When explor- ing autonomous systems below, we can no longer study the state ρ and the Hamiltonian H separately, since their interplay is crucial. Indeed, we well see that there are in- teresting classes of states ρ that provide substantial dy- namics and near-perfect independence even when the in- teraction Hamiltonian H3 is not small. In other words, for certain preferred classes of states, the independence principle no longer pushes us to simply minimize H3 and face the Quantum Zeno Paradox. A. Probability velocity and energy coherence To obtain a quantitative measure of dynamics, let us ﬁrst deﬁne the probability velocity v ≡ ˙p, where the prob- ability vector p is given by pi ≡ ρii. In other words, vk = ˙ρkk = i[H, ρ]kk. (44) Since v is basis-dependent, we are interested in ﬁnding the basis where v2 ≡ ∑ k v2 k = ∑ k ( ˙ρkk)2 (45) is maximized, i.e., the basis where the sums of squares of the diagonal elements of ˙ρ is maximal. It is easy to see that this basis is the eigenbasis of ˙ρ: v2 = ∑ k ( ˙ρkk)2 = ∑ jk ( ˙ρjk) 2 − ∑ j̸=k( ˙ρjk) 2 = || ˙ρ|| 2 − ∑ j̸=k( ˙ρjk) 2 (46) is clearly maximized in the eigenbasis where all oﬀ- diagonal elements in the last term vanish, since the Hilbert-Schmidt norm || ˙ρ|| is the same in every basis; || ˙ρ|| 2 = tr ˙ρ2, which is simply the sum of the squares of the eigenvalues of ˙ρ. Let us deﬁne the energy coherence δH ≡ 1 √2 || ˙ρ|| = 1 √2 ||i[H, ρ]|| = √ −tr {[H, ρ]2} 2 = √ tr [H2ρ2 − HρHρ]. (47) For a pure state ρ = |ψ⟩⟨ψ|, this deﬁnition implies that δH ≡ ∆H, where ∆H is the energy uncertainty ∆H = [ ⟨ψ|H 2|ψ⟩ − ⟨ψ|H|ψ⟩2]1/2 , (48) so we can think of δH as the coherent part of the en- ergy uncertainty, i.e., as the part that is due to quantum rather than classical uncertainty. Since || ˙ρ|| = ||[H, ρ]|| = √2δH, we see that the maxi- mum possible probability velocity v is simply vmax = √2 δH, (49) so we can equivalently use either of v or δH as conve- nient measures of quantum dynamics. 13 Whimsically speaking, the dynamics principle thus implies that en- ergy eigenstates are as unconscious as things come, and that if you know your own energy exactly, you’re dead. Although it is not obvious from their deﬁnitions, these quantities vmax and δH are independent of time (even though ρ generally evolves). This is easily seen in the energy eigenbasis, where − i ˙ρmn = [H, ρ]mn = ρmn(Em − En), (51) where the energies En are the eigenvalues of H. In this basis, ρ(t) = eiHtρ(0)e −iHt simpliﬁes to ρ(t)mn = ρ(0)mnei(Em−En)t, (52) This means that in the energy eigenbasis, the probabili- ties pn ≡ ρnn are invariant over time. These quantities constitute the energy spectral density for the state: pn = ⟨En|ρ|En⟩. (53) In the energy eigenbasis, equation (48) reduces to δH 2 = ∆H 2 = ∑ n pnE2 n − ( ∑ n pnEn )2 , (54) which is time-invariant because the spectral density pn is. For general states, equation (47) simpliﬁes to δH 2 = ∑ mn |ρmn|2En(En − Em). (55) This is time-independent because equation (52) shows that ρmn changes merely by a phase factor, leaving |ρmn| invariant. In other words, when a quantum state evolves unitarily in the Hilbert-Schmidt vector space, both the position vector ρ and the velocity vector ˙ρ retain their lengths: both ||ρ|| and || ˙ρ|| remain invariant over time. B. Dynamics versus complexity Our results above show that if all we are interested in is maximizing the maximal probability velocity vmax, then we should ﬁnd the two most widely separated eigenval- ues of H, Emin and Emax, and choose a pure state that involves a coherent superposition of the two: |ψ⟩ = c1|Emin⟩ + c2|Emax⟩, (56) 13 The ﬁdelity between the state ψ(t) and the initial state ψ0 is deﬁned as F (t) ≡ ⟨ψ0|ψ(t)⟩, (50) and it is easy to show that ˙F (0) = 0 and ¨F (0) = −(∆H)2, so the energy uncertainty is a good measure of dynamics in that it also determines the ﬁdelity evolution to lowest order, for pure states. For a detailed review of related measures of dynam- ics/information processing capacity, see [16]. 20 FIG. 10: Time-evolution of Bloch vector tr σ ˙ρ1 for a single qubit subsystem. We saw how minimizing H3 leads to a static state with no dynamics, such as the left example. Maximizing δH, on the other hand, produces extremely simple dynamics such as the right example. Reducing δH by a modest factor of order unity can allow complex and chaotic dynamics (center); shown here is a 2-qubit system where the second qubit is traced out. where |c1| = |c2| = 1/ √2. This gives δH = (Emax − Emin)/2, the largest possible value, but produces an ex- tremely simple and boring solution ρ(t). Since the spec- tral density pn = 0 except for these two energies, the dynamics is eﬀectively that of a 2-state system (a sin- gle qubit) no matter how large the dimensionality of H is, corresponding to a simple periodic solution with fre- quency ω = Emax − Emin (a circular trajectory in the Bloch sphere as in the right panel of Figure 10). This vi- olates the dynamics principle as deﬁned in Table II, since no substantial information processing capacity exists: the system is simply performing the trivial computation that ﬂips a single bit repeatedly. To perform interesting computations, the system clearly needs to exploit a signiﬁcant part of its energy spectrum. As can be seen from equation (52), if the eigenvalue diﬀerences are irrational multiples of one an- other, then the time evolution will never repeat, and ρ will eventually evolve through all parts of Hilbert space allowed by the invariants |⟨Em|ρ|En⟩|. The reduction of δH required to transition from simple periodic motion to such complex aperiodic motion is quite modest. For example, if the eigenvalues are roughly equispaced, then changing the spectral density pn from having all weight at the two endpoints to having approximately equal weight for all eigenvalues will only reduce the energy coherence δH by about a factor √3, since the standard deviation of a uniform distribution is √3 times smaller than its half-width. C. Highly autonomous systems: sliding along the diagonal What combinations of H, ρ and factorization produce highly autonomous systems? A broad and interesting class corresponds to macroscopic objects around us that move classically to an excellent approximation. The states that are most robust toward environment- induced decoherence are those that approximately com- mute with the interaction Hamiltonian [36]. As a simple but important example, let us consider an interaction Hamiltonian of the factorizable form H3 = A ⊗ B, (57) and work in a system basis where the interaction term A is diagonal. If ρ1 is approximately diagonal in this basis, then H3 has little eﬀect on the dynamics, which becomes dominated by the internal subsystem Hamilto- nian H1. The Quantum Zeno Paradox we encountered in Section III L involved a situation where H1 was also diagonal in this same basis, so that we ended up with no dynamics. As we will illustrate with examples below, classically moving objects in a sense constitute the op- posite limit: the commutator ˙ρ1 = i[H1, ρ1] is essentially as large as possible instead of as small as possible, con- tinually evading decoherence by concentrating ρ around a single point that continually slides along the diagonal, as illustrated in Figure 11. Decohererence rapidly sup- presses oﬀ-diagonal elements far from this diagonal, but leaves the diagonal elements completely unaﬀected, so 21 Dynamics from H 1 slides along diagonal ρij≠0 High-decoher ence subspace High-decoher ence subspace Dynamics from H 3 suppresses of f-diagonal elements ρ ij Dynamics from H 3 suppresses of f-diagonal elements ρ ij Low-decoher ence subspace i j FIG. 11: Schematic representation of the time-evolution of the density matrix ρij for a highly autonomous subsystem. ρij ≈ 0 except for a single region around the diagonal (red/grey dot), and this region slides along the diagonal un- der the inﬂuence of the subsystem Hamiltonian H1. Any ρij- elements far from the diagonal rapidly approach zero because of environment-decoherence caused by the interaction Hamil- tonian H3. there exists a low-decoherence band around the diagonal. Suppose, for instance, that our subsystem is the center- of-mass position x of a macroscopic object experiencing a position-dependent potential V (x) caused by coupling to the environment, so that Figure 11 represents the den- sity matrix ρ1(x, x ′) in the position basis. If the potential V (x) has a ﬂat (V ′ = 0) bottom of width L, then ρ1(x, x ′) will be completely unaﬀected by decoherence for the band |x′ − x| < L. For a generic smooth potential V , the deco- herence suppression of oﬀ-diagonal elements grows only quadratically with the distance |x′ − x| from the diagonal [4, 35], again making decoherence much slower than the internal dynamics in a narrow diagonal band. As a speciﬁc example of this highly autonomous type, let us consider a subsystem with a uniformly spaced en- ergy spectrum. Speciﬁcally, consider an n-dimensional Hilbert space and a Hamiltonian with spectrum Ek = [ k − n − 1 2 ] ℏω = kℏω + E0, (58) k = 0, 1, ..., n − 1. We will often set ℏω = 1 for simplic- ity. For example, n = 2 gives the spectrum {− 1 2 , 1 2 } like the Pauli matrices divided by two, n = 5 gives {−2, −1, 0, 1, 2} and n → ∞ gives the simple Harmonic oscillator (since the zero-point energy is physically irrele- vant, we have chosen it so that tr H = ∑ Ek = 0, whereas the customary choice for the harmonic oscillator is such that the ground state energy is E0 = ℏω/2). If we want to, we can deﬁne the familiar position and momentum operators x and p, and interpret this system as a Harmonic oscillator. However, the probability ve- locity v is not maximized in either the position or the momentum basis, except twice per oscillation — when the oscillator has only kinetic energy, v is maximized in the x-basis, and when it has only potential energy, v is maximized in the p-basis, and when it has only poten- tial energy. If we consider the Wigner function W (x, p), which simply rotates uniformly with frequency ω, it be- comes clear that the observable which is always chang- ing with the maximal probability velocity is instead the phase, the Fourier-dual of the energy. Let us therefore deﬁne the phase operator Φ ≡ FHF †, (59) where F is the unitary Fourier matrix. Please remember that none of the systems H that we consider have any a priori physical interpretation; rather, the ultimate goal of the physics-from-scratch program is to derive any interpretation from the mathematics alone. Generally, any thus emergent interpretation of a subsys- tem will depend on its interactions with other systems. Since we have not yet introduced any interactions for our subsystem, we are free to interpret it in whichever way is convenient. In this spirit, an equivalent and sometimes more convenient way to interpret our Hamiltonian from equation (58) is as a massless one-dimensional scalar par- ticle, for which the momentum equals the energy, so the momentum operator is p = H. If we interpret the par- ticle as existing in a discrete space with n points and a toroidal topology (which we can think of as n equispaced points on a ring), then the position operator is related to the momentum operator by a discrete Fourier transform: x = FpF†, Fjk ≡ 1 √N ei jk 2πn . (60) Comparing equations (59) and (60), we see that x = Φ. Since F is unitary, the operators H, p, x and Φ all have the same spectrum: the evenly spaced grid of equation (58). As illustrated in Figure 12, the time-evolution gen- erated by H has a simple geometric interpretation in the space spanned by the position eigenstates |xk⟩, k = 1, ...n: the space is unitarily rotating with frequency ω, so after a time t = 2π/nω, a state |ψ(0)⟩ = |xk⟩ has been rotated such that it equals the next eigenvector: |ψ(t)⟩ = |xk+1⟩, where the addition is modulo n. This means that the system has period T ≡ 2π/ω, and that |ψ⟩ rotates through each of the n basis vectors during each period. Let us now quantify the autonomy of this system, start- ing with the dynamics. Since a position eigenstate is a Dirac delta function in position space, it is a plane wave in momentum space — and in energy space, since H = p. 22 x z y z ω x y z ˆ ˆ ˆ ω 2xˆ 1xˆ 5xˆ 8xˆ7xˆ 4xˆ 6xˆ 3xˆ ω FIG. 12: For a system with an equispaced energy spectrum (such as a truncated harmonic oscillator or a massless particle in a discrete 1-dimensional periodic space), the time-evolution has a simple geometric interpretation in the space spanned by the eigenvectors ˆxk of the phase operator FHF, the Fourier dual of the Hamiltonian, corresponding to unitarily rotating the entire space with frequency ω, where ℏω is the energy level spacing. After a time 2π/nω, each basis vector has been rotated into the subsequent one, as schematically illustrated above. (The orbit in Hilbert space is only planar for n ≤ 3, so the ﬁgure should not be taken too literally.) The black star denotes the α = 1 apodized state described in the text, which is more robust toward decoherence. This means that the spectral density is pn = 1/n for a position eigenstate. Substituting equation (58) into equa- tion (54) gives an energy coherence δH = ℏω √ n2 − 1 12 . (61) For comparison, ||H|| = ( n−1∑ k=0 E2 k )1/2 = ℏω √ n(n2 − 1) 12 = √n δH. (62) Let us now turn to quantifying independence and de- coherence. The inner product between the unit vector |ψ(0)⟩ and the vector |ψ(t)⟩ ≡ eiHt|ψ(0)⟩ into which it evolves after a time t is fn(φ) ≡ ⟨ψ|e iH φ ω |ψ⟩ = 1 n n−1∑ k=0 e iEkφ = e−i n−1 2 φ n−1∑ k=0 eikφ = 1 n e−i n−1 2 φ 1 − einφ 1 − eiφ = sin nφ n sin φ , (63) where φ ≡ ωt. This inner product fn is plotted in Fig- ure 13, and is seen to be a sharply peaked even function satisfying fn(0) = 1, fn(2πk/n) = 0 for k = 1, ..., n − 1 and exhibiting one small oscillation between each of these zeros. The angle θ ≡ cos−1 fn(φ) between an initial vec- tor φ and its time evolution thus grows rapidly from 0 ◦ to 90◦, then oscillates close to 90◦ until returning to 0 ◦ after a full period T . An initial state |ψ(0)⟩ = |xk⟩ therefore evolves as ψj(t) = fn(ωt − 2π[j − k]/n) 1.0 -0.2 0.8 0.4 0.2 0.6 -100°-150° 150°100°50°-50°Penalty functionApodized Non-apodized FIG. 13: The wiggliest (heavy black) curve shows the inner product of a position eigenstate with what it evolves into a time t = φ/ω later due to our n = 20-dimensional Hamilto- nian with energy spacings ℏω. When optimizing to minimize the square of this curve using the 1 − cos φ penalty func- tion shown, corresponding to apodization in the Fourier do- main, we instead obtain the green/light grey curve, resulting in much less decoherence. in the position basis, i.e., a wavefunction ψj sharply peaked for j ∼ k + nωt/2π (mod n). Since the density matrix evolves as ρij(t) = ψi(t)ψj(t) ∗, it will therefore be small except for i ∼ j ∼ k + nωt/2π (mod n), corre- sponding to the round dot on the diagonal in Figure 11. In particular, the decoherence-sensitive elements ρjk will be small far from the diagonal, corresponding to the small values that fn takes far from zero. How small will the decoherence be? Let us now develop the tools needed to quantify this. 23 D. The exponential growth of autonomy with system size Let us return to the most general Hamiltonian H and study how an initially separable state ρ = ρ1 ⊗ ρ2 evolves over time. Using the orthogonal projectors of Section III I, we can decompose H as H = H1 ⊗ I + I ⊗ H2 + H3, (64) where tr 1H3 = tr 2H3 = 0. By substituting equa- tion (64) into the evolution equation ˙ρ1 = tr 2 ˙ρ = itr 2[H, ρ] and using various partial trace identities from Section A to simplify the resulting three terms, we obtain ˙ρ1 = i tr 2 [H, ρ1 ⊗ ρ2] = i [H1 + H∗, ρ1], (65) where what we will term the eﬀective interaction Hamil- tonian H∗ ≡ tr 2 {(I ⊗ ρ2)H3} (66) can be interpreted as an average of the interaction Hamil- tonian H3, weighted by the environment state ρ2. A similar eﬀective Hamiltonian is studied in [42–44]. Equa- tion (65) implies that the evolution of ρ1 remains unitary to ﬁrst order in time, the only eﬀect of the interaction H3 being to replace H1 from equation (15) by an eﬀective Hamiltonian H1 + H∗. The second time derivative is given by ¨ρ1 = tr 2 ˙ρ = −tr 2[H, [H, ρ]], and by analogously substituting equa- tion (64) and using partial trace identities from Section A to simplify the resulting nine terms, we obtain − ¨ρ1 = tr 2[H, [H, ρ1 ⊗ ρ2]] = = [H1, [H1, ρ1]] − i [K, ρ1] + + [H1, [H∗, ρ1]] + [H∗, [H1, ρ1]] + + tr 2[H3, [H3, ρ1 ⊗ ρ2]], (67) where we have deﬁned the Hermitian matrix K ≡ i tr 2{(I ⊗ [H2, ρ2])H3}. (68) To qualify independence and autonomy, we are inter- ested in the extent to which H3 causes entanglement and makes the time-evolution of ρ1 non-unitary. When think- ing of ρ as a vector in the Hilbert-Schmidt vector space that we reviewed in Section III H, unitary evolution pre- serves its length ||ρ||. To provide geometric intuition for this, let us deﬁne dot and cross product notation analo- gous to vector calculus. First note that (A†, [A, B]) = tr AAB − tr ABA = 0, (69) since a trace of a product is invariant under cyclic per- mutations of the factors. This shows that a commuta- tor [A, B] is orthogonal to both A† and B † under the Hilbert-Schmidt inner product, and a Hermitian matrix H is orthogonal to its commutator with any matrix. This means that it we restrict ourselves to the Hilbert- Schmidt vector space of Hermitian matrices, we obtain an interesting generalization of the standard dot and cross products for 3D vectors. Deﬁning A · B ≡ (A, B), (70) A × B ≡ i[A, B], (71) we see that these operations satisfy all the same prop- erties as their familiar 3D analogs: the scalar (dot) product is symmetric (B · A = tr B †A = tr AB† = A · B), while the vector (cross) product is antisym- metric (A × B = B × A), orthogonal to both factors ([A × B] · A = [A × B] · B = 0), and produces a result of the same type as the two factors (a Hermitian matrix). In this notation, the products of an arbitrary Hermi- tian matrix A with the identity matrix I are I · A = tr A, (72) I × A = 0, (73) and the Schr¨odinger equation ˙ρ = i[H, ρ] becomes simply ˙ρ = H × ρ. (74) Just as in the 3D vector analogy, we can think of this as generating rotation of the vector ρ that preserves its length: d dt ||ρ|| 2 = d dt ρ · ρ = 2 ˙ρ · ρ = 2(H × ρ) · ρ = 0. (75) A simple and popular way of quantifying whether evo- lution is non-unitary is to compute the linear entropy Slin ≡ 1 − tr ρ2 = 1 − ||ρ|| 2, (76) and repeatedly diﬀerentiating equation (76) tells us that ˙Slin = −2ρ · ˙ρ, (77) ¨Slin = −2(|| ˙ρ|| 2 + ρ · ¨ρ), (78) ... S lin = −6 ˙ρ · ¨ρ − 2ρ · ... ρ . (79) Substituting equations (65) and (67) into equations (77) and (78) for ρ1, we ﬁnd that almost all terms cancel, leaving us with the simple result ˙Slin 1 = 0, (80) ¨Slin 1 = 2 tr {ρ1 tr 2 [H3, [H3, ρ]]} − 2||[H∗, ρ1]|| 2. (81) This means that, to second order in time, the entropy production is completely independent of H1 and H2, de- pending only on quadratic combinations of H3, weighted by quadratic combinations of ρ. We ﬁnd analogous re- sults for the Shannon entropy S: If the density matrix is initially separable, then ˙S1 = 0 and ¨S1 depends not on the full Hamiltonian H, but only on its non-separable component H3, quadratically. We now have the tools we need to compute the auton- omy of our “diagonal-sliding” system from the previous 24 subsection. As a simple example, let us take H1 to be our Hamiltonian from equation (58) with its equispaced energy spectrum, with n = 2b, so that we can view the Hilbert space as that of b coupled qubits. Equation (61) then gives an energy coherence δH ≈ ℏω √12 2 b, (82) so the probability velocity grows exponentially with the system size b. We augment this Hilbert space with one additional “environment” qubit that begins in the state | ↑⟩, with internal dynamics given by H2 = ℏω2σx, and couple it to our subsystem with an interaction H3 = V (x) ⊗ σx (83) for some potential V ; x is the position operator from equation (60). As a ﬁrst example, we use the sinusoidal potential V (x) = sin(2πx/n), start the ﬁrst subsystem in the position eigenstate |x1⟩ and compute the linear entropy Slin 1 (t) numerically. As expected from our qualitative arguments of the pre- vious section, Slin 1 (t) grows only very slowly, and we ﬁnd that it can be accurately approximated by its Taylor ex- pansion around t = 0 for many orbital periods T ≡ 2π/ω: Slin 1 (t) ≈ ¨Slin 1 (0) t 2/2, where ¨Slin 1 (0) is given by equa- tion (81). Figure 14 shows the linear entropy after one orbit, Slin 1 (T ), as a function of the number of qubits b in our subsystem (top curve in top panel). Whereas equation (83) showed that the dynamics increases expo- nentially with system size (as 2 b), the ﬁgure shows that Slin 1 (T ) decreases exponentially with system size, asymp- totically falling as 2 −4b as b → ∞. Let us deﬁne the dynamical timescale τdyn and the in- dependence timescale τind as τdyn = ℏ δH , (84) τind = [ ¨Slin 1 (0)]−1/2. (85) Loosely speaking, we can think of τdyn as the time our system requires to perform an elementary information processing operation such as a bit ﬂip [16], and τind as the time it takes for the linear entropy to change by of order unity, i.e., for signiﬁcant information exchange with the environment to occur. If we deﬁne the autonomy A as the ratio A ≡ τind τdyn , (86) the autonomy of our subsystem thus grows exponen- tially with system size, asymptotically increasing as A ∝ 2 2b/2 −b = 2 3b as b → ∞. As illustrated by Figure 11, we expect this exponen- tial scaling to be quite generic, independent of interaction details: the origin of the exponential is simply that the size of the round dot in the ﬁgure is of order 2b times smaller than the size of the square representing the full density matrix. The independence timescale τind is ex- ponentially large because the dot, with its non-negligible elements ρij, is exponentially close to the diagonal. The dynamics timescale τdyn is exponentially small because it is roughly the time it takes the dot to traverse its own di- ameter as it moves around at some b-independent speed in the ﬁgure. This exponential increase of autonomy with system size makes it very easy to have highly autonomous sys- tems even if the magnitude H3 of the interaction Hamil- tonian is quite large. Although the environment contin- ually “measures” the position of the subsystem through the strong coupling H3, this measurement does not de- cohere the subsystem because it is (to an exponen- tially good approximation) a non-demolition measure- ment, with the subsystem eﬀectively in a position eigen- state. This phenomenon is intimately linked to the quan- tum Darwinism paradigm developed by Zurek and collab- orators [40], where the environment mediates the emer- gence of a classical world by acting as a witness, stor- ing large numbers of redundant copies of information about the system state in the basis that it measures. We thus see that systems that have high autonomy via the “diagonal-sliding” mechanism are precisely objects that dominate quantum Darwinism’s “survival of the ﬁttest” by proliferating imprints of their states in the environ- ment. E. Boosting autonomy with optimized wave packets In our worked example above, we started our subsys- tem in a position eigenstate |x1⟩, which cyclically evolved though all other position eigenstates. The slight decoher- ence that did occur thus originated during the times when the state was between eigenstates, in a coherent super- positions of multiple eigenstates quantiﬁed by the most wiggly curve in Figure 13. Not surprisingly, these wiggles (and hence the decoherence) can be reduced by a better choice of initial state |ψ⟩ = ∑ k ψk|xk⟩ = ∑ k ˆψk|Ek⟩ for our subsystem, where ψk and ˆψk are the wavefunction amplitudes in the position and energy bases, respectively. Equation (63) then gets generalized to gn(φ) ≡ ⟨x1|eiH φ ω |ψ⟩ = e−i n−1 2 φ n−1∑ k=0 ˆψkeikφ. (87) Let us choose the initial state |ψ⟩ that minimizes the quantity ∫ π −π |gn(θ)| 2w(θ)dθ (88) for some penalty function w(θ) that punishes states giv- ing large unwanted |g(θ)| far from θ = 0. This gives a simple quadratic minimization problem for the vec- tor of coeﬃcients ˆψk, whose solution turns out to be the 25 4Entropy increase during first orbit 10 -6 1 10-2 10 -4 10 -8 10-6 10-2 10-4 10-8 10 -10 System qubits 2 5 104 63 987 Sinusoidal potential Gaussian potential α = 0α = 2α = 3α = 4 α = 1 α = 0 α = 1Entropy increase during first orbit FIG. 14: The linear entropy increase during the ﬁrst orbit, ¨Slin 1 (2π/ω), is plotted for as a function of the subsystem size (number of qubits b). The interaction potential V (x) is sinusoidal (top) and Gaussian (bottom), and the diﬀerent apodization schemes used to select the initial state are labeled by their corresponding α-value, where α = 0 corresponds to no apodization (the initial state being a position eigenstate). Some lines have been terminated in the bottom panel due to insuﬃcient numerical precision. last (with smallest eigenvalue) eigenvector of the Toeplitz matrix whose ﬁrst row is the Fourier series of w(θ). A convenient choice of penalty function 1 − cos φ (see Fig- ure 13), which respects the periodicity of the problem and grows quadratically around its φ = 0 minimum. In the n → ∞ limit, the Toeplitz eigenvalue problem sim- pliﬁes to Laplace’s equation with a ˆψ(φ) = cos φ 2 winning eigenvector, giving ψk ≡ ∫ π −π cos(kφ) ˆφ(φ)dφ = cos(πk) 1 − 4k2 . (89) The corresponding curve gn(φ) is plotted is Figure 13, and is seen to have signiﬁcantly smaller wiggles away from the origin at the cost of a very slight widening of the central peak. Figure 14 (top panel, lower curve) shows that this choice signiﬁcantly reduces decoherence. What we have eﬀectively done is employ the standard signal processing technique known as apodization. Aside from the irrelevant phase factor, equation (87) is simply the Fourier transform of ˆψ, which can be made narrower by making ˆψ smoothly approach zero at the two end- points. In the n → ∞ limit, our original choice corre- sponded to ˆψ = 1 for −π ≤ φ ≤ π, which is discontin- uous, whereas our replacement function ˆψ = cos φ 2 van- ishes at the endpoints and is continuous. This reduces the wiggling because Riemann-Lebesgue’s lemma implies that the Fourier transform of a function whose ﬁrst d derivatives are continuous falls oﬀ faster than k−d. By instead using ˆψ(α)(φ) = (cos φ 2 ) α for some integer α ≥ 0, we get α continuous derivatives, so the larger we choose α, the smaller the decoherence-inducing wiggles, at the cost of widening the central peak. The ﬁrst ﬁve cases give ψ(0) k = δ0k, (90) ψ(1) k = cos(πk) 1 − 4k2 , (91) ψ(2) k = δ0k + 1 2 δ1,|k|, (92) ψ(3) k = cos(πk) (1 − 4k2)(1 − 4 9 k2) , (93) ψ(4) k = δ0k + 2 3 δ1,|k| + 1 6 δ2,|k|, (94) and it is easy to show that the α → ∞ limit corresponds to a Gaussian shape. Which apodization is best? This depends on the in- teraction H3. For our sinusoidal interaction potential (Figure 14, top), the best results are for α = 1, when the penalty function has a quadratic minimum. When switching to the roughly Gaussian interaction potential V (x) ∝ e4 cos(2πx/n) (Figure 14, bottom), the results are instead seen to keep improving as we increase α, pro- ducing dramatically less decoherence than for the sinu- soidal potential, and suggesting that the optical choice is the α → ∞ state: a Gaussian wave packet. Gaussian wave packets have long garnered interest as models of ap- proximately classical states. They correspond to general- ized coherent states, which have shown to be maximally robust toward decoherence in important situations in- volving harmonic oscillator interactions [45]. They have also been shown to emerge dynamically in harmonic os- cillator environments, from the accumulation of many independent interactions, in much the same way as the central limit theorem gives a Gaussian probability distri- bution to sums of many independent contributions [46]. Our results suggest that Gaussian wave packets may also emerge as the most robust states towards decoherence from short-range interactions with exponential fall-oﬀ. F. Optimizing autonomy when we can choose the state: factorizable eﬀective theories Above we explored speciﬁc examples of highly au- tonomous systems, motivated by approximately classical systems that we ﬁnd around us in nature. We found that there are combinations of ρ, H and Hilbert space factor- ization that provide excellent autonomy even when the 26 interaction H3 is not small. We will now see that, more generally, given any H and factorization, there are states ρ that give perfect factorization and inﬁnite autonomy. The basic idea is that for states such that some of the spectral density invariants pk vanish, it makes no diﬀer- ence if we replace the corresponding unused eigenvalues of H by others to make the Hamiltonian separable. Consider a subspace of the full Hilbert space deﬁned by a projection operator Π. A projection operator satisﬁes Π 2 = Π = Π †, so its eigenvalues are all zero or one, and the latter correspond to our subspace of interest. Let us deﬁne the symbol ≏ to denote that operator equality holds in this subspace. For example, A − B ≏ 0 (95) means that Π(A − B)Π = 0. (96) Below will often chose the subspace to correspond to low- energy states, so the wave symbol in ≏ is intended to re- mind us that equality holds in the long wavelength limit. We saw that the energy spectral density pn of equa- tion (53) remains invariant under unitary time evolution, so any energy levels for which pn = 0 will never have any physical eﬀect, and the corresponding dimensions of the Hilbert space can simply be ignored as “frozen out”. This remains true even considering observation-related state projection as described in the next subsection. Let us therefore deﬁne Π = ∑ k θ(pn)|En⟩⟨En|, (97) where θ is the Heaviside step function (θ(x) = 1 if x > 0, vanishing otherwise) i.e., summing only over those en- ergy eigenstates for which the probability pn is non-zero. Deﬁning new operators in our subspace by ρ′ ≡ ΠρΠ, (98) H′ ≡ ΠHΠ, (99) (100) equation (97) implies that ρ′ = ∑ mn θ(pm)θ(pn)|Em⟩⟨Em|ρ|En⟩⟨En| = ∑ mn |Em⟩⟨Em|ρ|En⟩⟨En| = ρ, (101) Here the second equal sign follows from the fact that |⟨Em|ρ|En⟩| 2 ≤ ⟨Em|ρ|Em⟩⟨En|ρ|En⟩14, so that the left 14 This last inequality follows because ρ is Hermitian and positive semideﬁnite, so the determinant must be non-negative for the 2 × 2 matrix ⟨Ei|ρ|Ej ⟩ where i and j each take the two values k and l. hand side must vanish whenever either pm or pn vanishes — the Heaviside step functions therefore have no eﬀect in equation (101) and can be dropped. Although H ′ ̸= H, we do have H′ ≏ H, and this means that the time-evolution of ρ can be correctly computed using H ′ in place of the full Hamiltonian H: ρ(t) = Πρ(t)Π = Πe iHtΠρ(0)Πe−iHtΠ = eiH ′tρ(0)e−iH′t. The frozen-out part of the Hilbert space is therefore com- pletely unobservable, and we can act as though the sub- space is the only Hilbert space that exists, and as if H′ is the true Hamiltonian. By working only with ρ′ and H′ restricted to the subspace, we have also simpliﬁed things by reducing the dimensionality of these matrices. Sometimes, H ′ can possess more symmetry than H. Sometimes, H ′ can be separable even if H is not: H ≏ H ′ = H1 ⊗ I + I ⊗ H2 (102) To create such a situation for an arbitrary n×n Hamilto- nian, where n = n1n2, simply pick a state ρ such that the spectral densities pk vanish for all except n1 + n2 − 1 en- ergy eigenvectors. This means that in the energy eigenba- sis, with the eigenvectors sorted to place these n1 +n2 −1 special ones ﬁrst, ρ is a block-diagonal matrix vanishing outside of the upper left (n1 +n2 −1)×(n1 +n2 −1) block. Equation (52) shows that ρ(t) will retain this block form for all time, and that changing the energy eigenvalues Ek with k > n1 + n2 − 1 leaves the time-evolution of ρ unaf- fected. We can therefore choose these eigenvalues so that H becomes separable. For example, for the case where the Hilbert space dimensionality n = 9, suppose that pk vanishes for all energies except E0, E1, E2, E3, E4, and adjust the irrelevant zero-point energy so that E0 = 0. Then deﬁne H ′ whose 9 eigenvalues are   0 E1 E2 E3 E1 + E3 E2 + E3 E4 E1 + E4 E2 + E4   . (103) Note that H ′ ≏ H, and that although H is generically not separable, H ′ is separable, with subsystem Hamilto- nians H′ 1 = diag {0, E1, E2} and H ′ 2 = diag {0, E3, E4}. Subsystems 1 and 2 will therefore evolve as a parallel universes governed by H ′ 1 and H′ 1, respectively. G. Minimizing quantum randomness When we attempted to maximize the independence for a subsystem above, we implicitly wanted to maximize the ability to predict the subsystems future state from its present state. The source of unpredictability that we considered was inﬂuence from outside the subsystem, from the environment, which caused decoherence and in- creased subsystem entropy. Since we are interested in modeling also conscious sys- tems, there is a second independent source of unpre- dictability that we need to consider, which can occur even 27 if there is no interaction with the environment: “quantum randomness”. If the system begins in a single conscious state and unitarily evolves into a superposition of subjec- tively distinguishable conscious states, then the observer in the initial state has no way of uniquely predicting her future perceptions. A comprehensive framework for treating such situa- tions is given in [47], and in the interest of brevity, we will not review it here, merely use the results. To be able to state them as succinctly as possible, let us ﬁrst intro- duce notation for a projection process “pr ” that is in a sense dual to partial-tracing. For a Hilbert space that is factored into two parts, we deﬁne the following notation. We indicate the tensor product structure by splitting a single index α into an in- dex pair ii′. For example, if the Hilbert space is the ten- sor product of an m-dimensional and an n-dimensional space, then α = n(i − 1) + i ′, i = 1, ..., m, i ′ = 1, ..., n, α = 1, ..., mn, and if A = B ⊗ C, then Aαβ = Aii′jj′ = BijCi′j′. (104) We deﬁne ⋆ as the operation exchanging subsystems 1 and 2: (A⋆)ii′jj′ = Ai′ij′j (105) We deﬁne pr kA as the kth diagonal block of A: (pr k A)ij = Akikj For example, pr 1A is the m × m upper left corner of A. As before tr iA, denotes the partial trace over the i th subsystem: (tr 1 A)ij = ∑ k Akikj (106) (tr 2 A)ij = ∑ k Aikjk (107) The following identities are straightforward to verify: tr 1 A⋆ = tr 2 A (108) tr 2 A⋆ = tr 1 A (109) tr 1 A = ∑ k pr k A (110) tr 2 A = ∑ k pr k A⋆ (111) tr pr k A = (tr 2 A)kk (112) tr pr k A⋆ = (tr 1 A)kk (113) Let us adopt the framework of [47] and decompose the full Hilbert space into three parts corresponding to the subject (the conscious degrees of freedom of the ob- server), the object (the external degrees of freedom that the observer is interested in making predictions about) and the environment (all remaining degrees of freedom). If the subject knows the object-environment density matrix to be ρ, it obtains its density matrix for the object by tracing out the environment: ρo = tr e ρ. If the subject-object density matrix is ρ, then the sub- ject may be in a superposition of having many diﬀerent perceptions |sk⟩. Take the |sk⟩ to form a basis of the subject Hilbert space. The probability that the subject ﬁnds itself in the state |sk⟩ is pk = (tr 2 ρ)kk, (114) and for a subject ﬁnding itself in this state |sk⟩, the object density matrix is ρ (k) o = pr k ρ pk . (115) If ρ refers to a future subject-object state, and the sub- ject wishes to predict its future knowledge of the object, it takes the weighted average of these density matrices, obtaining ρo = ∑ k pkρ(k) o = ∑ k pr k ρ = tr s ρ, i.e., it traces out itself! (We used the identity (110) in the last step.) Note that this simple result is indepen- dent of whatever basis is used for the object-space, so all issues related to how various states are perceived become irrelevant. As proven in [48], any unitary transformation of a sep- arable ρ will increase the entropy of tr 1ρ. This means that the subject’s future knowledge of ρo is more un- certain than its present knowledge thereof. However, as proven in [47], the future subject’s knowledge of ρo will on average be less uncertain than it presently is, at least if the time-evolution is restricted to be of the measurement type. The result ρo = tr 1 ρ also holds if you measure the object and then forget what the outcome was. In this case, you are simply playing the role of an environment, resulting in the exact same partial-trace equation. In summary, for a conscious system to be able to pre- dict the future state of what it cares about (ρo) as well as possible, we must minimize uncertainty introduced both by the interactions with the environment (ﬂuctu- ation, dissipation and decoherence) and by measurement (“quantum randomness”). The future evolution can be better predicted for certain object states than for oth- ers, because they are more stable against both of the above-mentioned sources of unpredictability. The util- ity principle from Table II suggests that it is precisely these most stable and predictable states that conscious observers will perceive. The successful “predictability 28 sieve” idea of Zurek and collaborators [50] involves pre- cisely this idea when the source of unpredictability is environment-induced decoherence, so the utility principle lets us generalize this idea to include the second unpre- dictability source as well: to minimize apparent quantum randomness, we should pay attention to states whose dy- namics lets them remain relatively diagonal in the eigen- basis of the subject-object interaction Hamiltonian, so that our future observations of the object are essentially quantum non-demolition measurements. A classical computer is a ﬂagship example of a such a maximally causal system, minimizing its uncertainty about its future. By clever design, a small subset of the degrees of freedom in the computer, interpreted as bits, deterministically determine their future state with virtually no uncertainty. For my laptop, each bit corre- sponds to the positions of certain electrons in its memory (determining whether a micro-capacitor is charged). An ideal computer with zero error rate thus has not only complex dynamics (which is Turing-complete modulo re- source limitations), but also perfect autonomy, with its future state determined entirely by its own state, inde- pendently of the environment state. The Hilbert space factorization that groups the bits of this computer into a subsystem is therefore optimal, in the sense that any other factorization would reduce the autonomy. More- over, this optimal solution to the quantum factorization problem is quite sharply deﬁned: considering inﬁnites- imal unitary transformations away from this optimum, any transformation that begins rotating an environment bit into the system will cause a sharp reduction of the autonomy, because the decoherence rate for environment qubits (say a thermal collision frequency ∼ 10 15 Hz) is orders of magnitude larger than the dynamics rate (say the clock frequency ∼ 109 Hz). Note that H3 is far from zero in this example; the pointer basis corresponds to classical bit strings of which the environment performs frequent quantum non-demolition measurements. This means that if artiﬁcial intelligence researchers one day succeed in making a classical computer conscious, and if we turn oﬀ any input devices though which our outside world can aﬀect its information processing, then it will subjectively perceive itself as existing in a parallel universe completely disconnected from ours, even though we can probe its internal state from outside. If a future quantum computer is conscious, then it will feel like in a parallel universe evolving under the Hamiltonian H1(t) that we have designed for it — until the readout stage, when we switch on an interaction H3. H. Optimizing autonomy when the state is given Let us now consider the case where both H and ρ are treated as given, and we want to vary the Hilbert space factorization to attain maximal separability. H and ρ together determine the full time-evolution ρ(t) via the Schr¨odinger equation, so we seek the unitary transforma- tion U that makes Uρ(t)U† as factorizable as possible. For a pure initial state, exact factorability is equivalent to ρ1(t) being pure, with ||ρ1|| = 1 and vanishing linear entropy Slin = 1 − ||ρ1(t)|| 2, so let us minimize the linear entropy averaged over a range of times. As a concrete example, we minimize the function f (U) ≡ 1 − 1 m m∑ i=1 || tr 1 Uρ(ti)U†|| 2, (116) using 9 equispaced times ti ranging from t = 0 and t = 1, a random 4 × 4 Hamiltonian H, and a random pure state ρ(0). 0 2 4 6 8 10 0.2 0.4 0.6 0.8 1.0 TimeNorm ||ρ1||New factorizationOld factorization FIG. 15: The Hilbert-Schmidt norm ||ρ1|| is plotted for a ran- dom pure-state 2-qubit system when factorizing the Hilbert space in the original basis (black curve) and after a unitary transformation optimized to keep ρ1 as pure as possible for t ≤ 1 (red/grey curve). The result of numerically solving this optimization problem is shown in Figure 15, and we see that the new factorization keeps the norm ||ρ1|| visually indistinguish- able from unity for the entire time period optimized for. The optimization reduced the average Shannon entropy over this period from S ≈ 1.1 bits to S = 0.0009 bits. The reason that the optimization is so successful is presumably that it by adjusting N = n2 − n 2 1 − n2 2 = 16 − 4 − 4 = 8 real parameters15 in U, it is able to approximately zero out the ﬁrst N terms in the Taylor expansion of Slin(t), whose leading terms are given by equations (77)- (79). A series of similar numerical exper- iments indicated that such excellent separability could generally be found as long as the number of time steps ti was somewhat smaller than the number of free pa- rameters N but not otherwise, suggesting that separa- bility can be extended over long time periods for large 15 There are n2 parameters for U, but transformations within each of the two subspaces have no eﬀect, wasting n2 1 and n2 2 parame- ters. 29 n. However, because we are studying only unitary evolu- tion here, neglecting the important projection eﬀect from the previous section, it is unclear how relevant these re- sults are to our underlying goal. We have therefore not extended these numerical optimizations, which are quite time-consuming, to larger n. V. CONCLUSIONS In this paper, we have explored two problems that are intimately related. The ﬁrst problem is that of under- standing consciousness as a state of matter, “perceptro- nium”. We have focused not on solving this problem, but rather on exploring the implications of this view- point. Speciﬁcally, we have explored four basic principles that may distinguish conscious matter from other phys- ical systems: the information, integration, independence and dynamics principles. The second one is the physics-from-scratch problem: If the total Hamiltonian H and the total density ma- trix ρ fully specify our physical world, how do we ex- tract 3D space and the rest of our semiclassical world from nothing more than two Hermitian matrices? Can some of this information be extracted even from H alone, which is fully speciﬁed by nothing more than its eigen- value spectrum? We have focused on a core part of this challenge which we have termed the quantum factoriza- tion problem: why do conscious observers like us perceive the particular Hilbert space factorization corresponding to classical space (rather than Fourier space, say), and more generally, why do we perceive the world around us as a dynamic hierarchy of objects that are strongly inte- grated and relatively independent? These two problems go hand in hand, because a generic Hamiltonian cannot be decomposed using tensor prod- ucts, which would correspond to a decomposition of the cosmos into non-interacting parts, so there is some op- timal factorization of our universe into integrated and relatively independent parts. Based on Tononi’s work, we might expect that this factorization, or some gener- alization thereof, is what conscious observers perceive, because an integrated and relatively autonomous infor- mation complex is fundamentally what a conscious ob- server is. A. Summary of ﬁndings We ﬁrst explored the integration principle, and found that classical physics allows information to be essentially fully integrated using error-correcting codes, so that any subset containing up to about half the bits can be re- constructed from the remaining bits. Information stored in Hopﬁeld neural networks is naturally error-corrected, but 10 11 neurons support only about 37 bits of integrated information. This leaves us with an integration paradox: why does the information content of our conscious expe- rience appear to be vastly larger than 37 bits? We found that generalizing these results to quantum information exacerbated this integration paradox, allowing no more than about a quarter of a bit of integrated information — and this result applied not only to Hopﬁeld networks of a given size, but to the state of any quantum system of any size. This strongly implies that the integration prin- ciple must be supplemented by at least one additional principle. We next explored the independence principle and the extent to which a Hilbert space factorization can decom- pose the Hamiltonian H (as opposed to the state ρ) into independent parts. We quantiﬁed this using projection operators in the Hilbert-Schmidt vector space where H and ρ are viewed as vectors rather than operators, and proved that the best decomposition can always be found in the energy eigenbasis, where H is diagonal. This leads to a more pernicious variant of the Quantum Zeno Ef- fect that we termed the Quantum Zeno Paradox: if we decompose our universe into maximally independent ob- jects, then all change grinds to a halt. Since conscious observers clearly do not perceive reality as being static and unchanging, the integration and independence prin- ciples must therefore be supplemented by at least one additional principle. We then explored the dynamics principle, according to which a conscious system has the capacity to not only store information, but also to process it. We found the energy coherence δH ≡ √ 2 tr ˙ρ2 to be a conve- nient measure of dynamics: it can be proven to be time- independent, and it reduces to the energy uncertainty ∆H for the special case of pure states. Maximizing dy- namics alone gives boring periodic solutions unable to support complex information processing, but reducing δH by merely a modest percentage enables chaotic and complex dynamics that explores the full dimensionality of the Hilbert space. We found that high autonomy (a com- bination of dynamics and independence) can be attained even if the environment interaction is strong. One class of examples involves the environment eﬀectively perform- ing quantum-non-demolition measurements of the au- tonomous system, whose internal dynamics causes the non-negligible elements of the density matrix ρ to “slide along the diagonal” in the measured basis, remaining in the low-decoherence subspace. We studied such an example involving a truncated harmonic oscillator cou- pled to an external spin, and saw that it is easy to ﬁnd classes of systems whose autonomy grows exponentially with the system size (measured in qubits). Generalized coherent states with Gaussian wavefunctions appeared particularly robust toward interactions with steep/short- range potentials. We found that any given H can also be perfectly decomposed given a suitably chosen ρ that as- signs zero amplitude to some energy eigenstates. When optimizing the Hilbert space factorization for H and ρ jointly, it appears possible to make a subsystem history ρ1(t) close to separable for a long time. However, it is unclear how relevant this is, because the state projection 30 caused by observation also alters ρ1. B. How does a conscious entity perceive the world? What are we to make of these ﬁndings? We have not solved the quantum factorization problem, but our re- sults have brought it into sharper focus, and highlighted both concrete open sub-problems and various hints and clues from observation about paths forward. Let us ﬁrst discuss some open problems, then turn to the hints. For the physics-from-scratch problem of deriving how we perceive our world from merely H, ρ and the Schr¨odinger equation, there are two possibilities: either the problem is well-posed or it is not. If not, this would be very interesting, implying that some sort of additional structure beyond ρ and H is needed at the fundamen- tal level — some additional mathematical structure en- coding properties of space, for instance, which would be surprising given that this appears unnecessary in lattice Gauge theory (see Appendix C). Since we have limited our treatment to unitary non-relativistic quantum me- chanics, obvious candidates for missing structure relate to relativity and quantum gravity, where the Hamiltonian vanishes, and to mechanisms causing non-unitary wave- function collapse. Indeed, Penrose and others have spec- ulated that gravity is crucial for a proper understanding of quantum mechanics even on small scales relevant to brains and laboratory experiments, and that it causes non-unitary wavefunction collapse [51]. Yet the Occam’s razor approach is clearly the commonly held view that neither relativistic, gravitational nor non-unitary eﬀects are central to understanding consciousness or how con- scious observers perceive their immediate surroundings: astronauts appear to still perceive themselves in a semi- classical 3D space even when they are eﬀectively in a zero- gravity environment, seemingly independently of rela- tivistic eﬀects, Planck-scale spacetime ﬂuctuations, black hole evaporation, cosmic expansion of astronomically dis- tant regions, etc. If, on the other hand, the physics-from-scratch prob- lem is well-posed, we face crucial unanswered questions related to Hilbert space factorization. Why do we per- ceive electromagnetic waves as transferring information between diﬀerent regions of space, rather than as com- pletely independent harmonic oscillators that each stay put in a ﬁxed spatial location? These two viewpoints correspond to factoring the Hilbert space of the elec- tromagnetic ﬁeld in either real space or Fourier space, which are simply two unitarily equivalent Hilbert space bases. Moreover, how can we perceive a harmonic oscil- lator as an integrated system when its Hamiltonian can, as reviewed in Appendix B, be separated into completely independent qubits? Why do we perceive a magnetic sys- tem described by the 3D Ising model as integrated, when it separates into completely independent qubits after a unitary transformation?16 In all three cases, the answer clearly lies not within the system itself (in its internal dynamics H1), but in its interaction H3 with the rest of the world. But H3 involves the factorization problem all over again: whence this distinction between the system itself and the rest of the world, when there are countless other Hilbert space factorizations that mix the two? C. Open problems Based on our ﬁndings, three speciﬁc problems stand in the way of solving the quantum factorization problem and answering these questions, and we will now discuss each of them in turn. 1. Factorization and the chicken-and-egg problem What should we determine ﬁrst: the state or the fac- torization? If we are given a Hilbert space factorization and an environment state, we can use the predictability sieve formalism [50] to ﬁnd the states of our subsystem that are most robust toward decoherence. In some sim- ple cases, they are eigenstates of the eﬀective interaction Hamiltonian H∗ from equation (66). However, to ﬁnd the best factorization, we need information about the state. A clock is a highly autonomous system if we fac- tor the Hilbert space so that the ﬁrst factor corresponds to the spatial volume containing the clock, but if the state were diﬀerent such that the clock were somewhere else, we should factor out a diﬀerent volume. Moreover, if the state has the clock in a superposition of two macro- scopically diﬀerent locations, then there is no single op- timal factorization, but instead a separate one for each branch of the wavefunction. An observer looking at the clock would use the clock position seen to project onto the appropriate branch using equation (115), so the solu- tion to the quantum factorization problem that we should be looking for is not a single unique factorization of the Hilbert space. Rather, we need a criterion for identifying conscious observers, and then a prescription that deter- mines which factorization each of them will perceive. 2. Factorization and the integration paradox A second challenge that we have encountered is the extreme separability possible for both H and ρ. In the 16 If we write the Ising Hamiltonian as a quadratic function of σx-operators, then it is also quadratic in the annihilation and creation operators and can therefore be diagonalized after a Jordan-Wigner transform [49]. Note that such diagonalization is impossible for the Heisenberg ferromagnet, whose couplings are quadratic in all three Pauli matrices, because σ2 z -terms are quartic in the annihilation and creation operators. 31 introduction, we expressed hope that the apparent inte- gration of minds and external objects might trace back to the fact that for generic ρ and H, there is no Hilbert space factorization that makes ρ factorizable or H additively separable. Yet by generalizing Tononi’s ideas to quantum systems, we found that what he terms the “cruelest cut” is very cruel indeed, able to reduce the mutual informa- tion in ρ to no more than about 0.25 bits, and typically able to make the interaction Hamiltonian H3 very small as well. We saw in Section IV H that even the combined eﬀects ρ and H can typically be made close to separable, in the sense that there is a Hilbert space factorization where a subsystem history ρ1(t) is close to separable for a long time. So why do we nonetheless perceive out uni- verse as being relatively integrated, with abundant infor- mation available to us from near and far? Why do we not instead perceive our mind as essentially constituting its own parallel universe, solipsism-style, with merely expo- nentially small interactions with the outside world? We saw that the origin of this integration paradox is the vast- ness of the group of unitary transformations that we are minimizing over, whose number of parameters scales like n2 = 2 2b with the number of qubits b and thus grows ex- ponentially with system size (measured in either volume or number of particles). 3. Factorization and the emergence of time A third challenge involves the emergence of time. Al- though this is a famously thorny problem in quantum gravity, our results show that it appears even in non- relativistic unitary quantum mechanics. It is intimately linked with our factorization problem, because we are optimizing over all unitary transformations U, and time evolution is simply a one-dimensional subset of these transformations, given by U = e iHt. Should the opti- mal factorization be determined separately at each time, or only once and for all? In the latter case, this would appear to select only one special time when our universe is optimally separable, seemingly contrary to our obser- vations that the laws of physics are time-translation in- variant. In the former case, the continuous change in factorization will simply undo time evolution [18], mak- ing you feel that time stands still! Observationally, it is obvious that the optimal factorization can change at least somewhat with time, since our designation of objects is temporary: the atoms of a highly autonomous wooden bowling ball rolling down a lane were once dispersed (as CO2 and H2O in the air, etc.) and will eventually disperse again. An obvious way out of this impasse is to bring con- sciousness back to center-stage as in Section IV G and [4, 47, 48]. Whenever a conscious observer interacts with her environment and gains new information, the state ρ with which she describes her world gets updated accord- ing to equation (115), the quantum-mechanical version of Bayes Theorem [48]. This change in her ρ is non- unitary and therefore evades our timelessness argument above. Because she always perceives herself in a pure state, knowing the state of her mind, the joint state or her and the rest of the world is always separable. It there- fore appears that if we can one day solve the quantum factorization problem, then we will ﬁnd that the emer- gence of time is linked to the emergence of consciousness: the former cannot be fully understood without the latter. D. Observational hints and clues In summary, the quantum factorization problem is both very interesting and very hard. However, as op- posed to the hard problem of quantum gravity, say, where we have few if any observational clues to guide us, physics research has produced many valuable hints and clues rel- evant to the quantum factorization problem. The factor- ization of the world that we perceive and the quantum states that we ﬁnd objects in have turned out to be ex- ceptionally unusual and special in various ways, and for each such way that we can identify, quantify and under- stand the underlying principle responsible for, we will make another important stride towards solving the fac- torization problem. Let us now discuss the hints that we have identiﬁed upon so far. 1. The universality of the utility principle The principles that we listed in Table II were for con- scious systems. If we shift attention to non-conscious objects, we ﬁnd that although dynamics, independence and integration still apply in many if not most cases, the utility principle is the only one that universally applies to all of them. For example, a rain drop lacks signiﬁcant information storage capacity, a boulder lacks dynamics, a cogwheel can lack independence, and a sand pile lacks integration. This universality of the utility principle is hardly surprising, since utility is presumably the reason we evolved consciousness in the ﬁrst place. This suggests that we examine all other clues below through the lens of utility, to see whether the unusual circumstances in ques- tion can be explained via some implication of the utility principle. In other words, if we ﬁnd that useful conscious- ness can only exist given certain strict requirements on the quantum factorization, then this could explain why we perceive a factorization satisfying these requirements. 2. ρ is exceptional The observed state ρ of our universe is excep- tional in that it is extremely cold, with most of the Hilbert space frozen out — what principles might re- quire this? Perhaps this is useful for consciousness by allowing relatively stable information storage and by allowing large autonomous systems thanks to the 32 large available dynamic range in length scales (uni- verse/brain/atom/Planck scale)? Us being far from ther- mal equilibrium with our 300K planet dumping heat from our 6000K sun into our 3K space is clearly conducive to dynamics and information processing. 3. H is exceptional The Hamiltonian H of the standard model of particle physics is of the very special form H = ∫ Hr(r)d3r, (117) which is seen to be almost additively separable in the spatial basis, and in no other basis. Although equa- tion (117) superﬁcially looks completely separable just as H = ∑ i Hi, there is a coupling between inﬁnitesi- mally close spatial points due to spatial derivatives in the kinetic terms. If we replace the integral by a sum in equation (117) by discretizing space as in lattice gauge theory, we need couplings only between nearest-neighbor points. This is a strong hint of the independence princi- ple at work; all this near-independence gets ruined by a generic unitary transformation, making the factorization corresponding to our 3D physical space highly special; indeed, 3D space and the exact form of equation (117) could presumably be inferred from simply knowing the spectrum of H. H from equation (117) is also exceptional in that it contains mainly quadratic, cubic and quartic functions of the fermion and boson ﬁelds, which can in turn be expressed linearly or quadratically in terms of qubit rais- ing and lowering operators (see Appendix C). A generic unitary transformation would ruin this simplicity as well, introducing polynomials of enormous degree. What prin- ciple might be responsible for this? H from equation (117) is also exceptional by exhibiting tremendous symmetry: the form of Hr in invariant under both space and time translation, and indeed under the full Poincare group; using a factorization other than 3D space would ruin this symmetry. 4. The ubiquity of autonomy When discussing the integration paradox above, we worried about factorizations splitting the world into nearly independent parts. If there is a factorization with H3 = 0, then the two subsystems are independent for any state, for all time, and will act as two parallel universes. This means that if the only way to achieve high inde- pendence were to make H3 tiny, the integration paradox would indeed be highly problematic. However, we saw in Section IV that this is not at all the case: it is quite easy to achieve high independence for some states, at least temporarily, even when H3 is large. The independence principle therefore does not push us inexorably towards perceiving a more disconnected world than the one we are familiar with. The ease of approximately factoring ρ1(t) during a signiﬁcant time period as in Section IV H also appears unlikely to be a problem: as mentioned, our cal- culation answered the wrong question by studying only unitary evolution, neglecting projection. The take-away hint is thus that observation needs to be taken into ac- count to address this issue properly, just as we argued that it must be taken into account to understand the emergence of time. 5. Decoherence as enemy Early work on decoherence [34, 35] portrayed it mainly as an enemy, rapidly killing oﬀ most quantum states, with only a tiny minority surviving long enough to be observable. For example, a bowling ball gets struck by about 1025 air molecules each second, and a single strike suﬃces to ruin any macrosuperposition of the balls po- sition extending further than about an angstrom, the molecular De Broglie wavelength [35, 52]. The successful predictability sieve idea of Zurek and collaborators [50] states that we will only perceive those quantum states that are most robust towards decoherence, which in the case of macroscopic objects such as bowling balls selects roughly classical states with fairly well-deﬁned positions. In situations where the position basis emerges as special, this might thus trace back to the environmental interac- tions H3 (with air molecules etc.) probing the position, which might in turn traces back to the fact that H from equation (117) is roughly separable in the position basis. Note, however, that the general situation is more compli- cated, since the predictability sieve depends also on the state ρ, which might contain long-distance entanglement built up over time by the kinetic terms in equation (117). Indeed, ρ can describe a laboratory where a system is probed in a non-spatial basis, causing the predictablity sieve to favor, say, energy eigenstates. In terms of Table II, we can view the predictability sieve as an application of the utility principle, since there is clearly no utility in trying to perceive something that will be irrelevant 10 −25 seconds later. In summary, the hint from this negative view of decoherence is that we should minimize it, either by factoring to minimize H3 itself or by using robust states on which H3 essentially performs quantum non-demolition measurements. 6. Decoherence as friend Although quantum computer builders still view deco- herence as their enemy, more recent work on decoher- ence has emphasized that it also has a positive side: the Quantum Darwinism framework [40] emphasizes the role of environment interactions H3 as a valuable communica- tion channel, repeatedly copying information about the 33 states of certain systems into the environment 17, thereby helping explain the emergence of a consensus reality [53]. Quantum Darwinism can also be viewed as an applica- tion of the utility principle: it is only useful for us to try to be aware of things that we can get information about, i.e., about states that have quantum-spammed the environment with redundant copies of themselves. A hint from this positive view of environmental interactions is that we should not try to minimize H3 after all, but should instead reduce decoherence by the second mech- anism: using states that are approximate eigenstates of the eﬀective interaction H∗ and therefore get abundantly copied into the environment. Further work on Quantum Darwinism has revealed that such situations are quite exceptional, reaching the following conclusion [54]: “A state selected at random from the Hilbert space of a many-body system is over- whelmingly likely to exhibit highly non-classical correla- tions. For these typical states, half of the environment must be measured by an observer to determine the state of a given subsystem. The objectivity of classical reality — the fact that multiple observers can agree on the state of a subsystem after measuring just a small fraction of its environment — implies that the correlations found in nature between macroscopic systems and their environ- ments are very exceptional.” This gives a hint that the particular Hilbert space factorization we observe might be very special and unique, so that using the utility prin- ciple to insist on the existence of a consensus reality may have large constraining power among the factorizations — perhaps even helping nail down the one we actually observe. E. Outlook In summary, the hypothesis that consciousness can be understood as a state of matter leads to fascinating in- terdisciplinary questions spanning the range from neu- roscience to computer science, condensed matter physics and quantum mechanics. Can we ﬁnd concrete exam- ples of error-correcting codes in the brain? Are there brain-sized non-Hopﬁeld neural networks that support much more than 37 bits of integrated information? Can a deeper understanding of consciousness breathe new life into the century-old quest to understand the emergence of a classical world from quantum mechanics, and can it even help explain how two Hermitian matrices H and ρ lead to the subjective emergence of time? The quests to better understand the internal reality of our mind and the external reality of our universe will hopefully assist one another. Acknowledgments: The author wishes to thank Christoph Koch, Meia Chita-Tegmark, Russell Hanson, Hrant Gharibyan, Seth Lloyd, Bill Poirier, Matthew Pusey, Harold Shapiro and Marin Soljaˇci´c and for help- ful information and discussions, and Hrant Gharibyan for mathematical insights regarding the ρ- and H- diagonality theorems. This work was supported by NSF AST-090884 & AST-1105835. 17 Charles Bennett has suggested that Quantum Darwinism would be more aptly named “Quantum Spam”, since the many redun- dant imprints of the system’s state are normally not further re- produced. [1] A. Almheiri, D. Marolf, J. Polchinski, and J. Sully, JHEP 2, 62 (2013). [2] T. Banks, W. Fischler, S. Kundu, and J. F. Pedraza, arXiv:1401.3341 (2014). [3] S. Saunders, J. Barrett, A. Kent, and D. Wallace, Many Worlds? Everett, Quantum Theory, & Reality (Oxford, Oxford Univ. Press, 2010). [4] M. Tegmark, PRE 61, 4194 (2000). [5] D. J. Chalmers, J. Consc. Studies 2, 200 (1995). [6] P. Hut, M. Alford, and M. Tegmark, Found. Phys. 36, 765 (2006, physics/0510188). [7] M. Tegmark, Found.Phys. 11/07, 116 (2007). [8] G. Tononi, Biol. Bull. 215, 216, http://www.biolbull. org/content/215/3/216.full (2008). [9] S. Dehaene, Neuron 70, 200 (2011). [10] G. Tononi, Phi: A Voyage from the Brain to the Soul (New York, Pantheon, 2012). [11] A. Casali et al., Sci. Transl. Med 198, 1 (2013). [12] M. Oizumi, L. Albantakis, and Tononi G, PLoS comp. bio, e1003588 (2014). [13] S. Dehaene et al., Current opinion in neurobiology 25, 76 (2014). [14] B. A. Wilson and D. Wearing 1995, in Broken memories: Case studies in memory impairment, ed. R. Campbell and M. A. Conway (Malden: Blackwell) [15] I. Amato, Science 253, 856 (1991). [16] S. Lloyd, Nature 406, 1-47 (2000). [17] G. t’Hooft, arXiv:gr-qc/9310026 (1993). [18] J. Schwindt, arXiv:1210.8447 [quant-ph] (2012). [19] A. Damasio, Self Comes to Mind: Constructing the Con- scious Brain (New York, Vintage, 2010). [20] R. W. Hamming, The Bell System Technical Journal 24, 2 (1950). [21] M. Grassl, http://i20smtp.ira.uka.de/home/grassl/ codetables/ [22] J. J. Hopﬁeld, Proc. Natl. Acad. Sci. 79, 2554 (1982). [23] N. J. Joshi, G. Tononi, and C. Koch, PLOS Comp. Bio. 9, e1003111 (2013). [24] O. Barak et al., Progr. Neurobio. 103, 214 (2013). [25] Yoon K et al., Nature Neuroscience 16, 1077 (2013). [26] D. J. C McKay, Information Theory, Inference, and Learning Algorithms (Cambridge, Cambridge University Press, 2003). [27] K. K¨upfm¨uller, Nachrichtenverarbeitung im Menschen, in Taschenbuch der Nachrichtenverarbeitung, K. Stein- buch, Ed., 1481-1502 (1962). 34 [28] T. Nørretranders, The User Illusion: Cutting Conscious- ness Down to Size (New York, Viking, 1991). [29] S. Jevtic, Jennings D, and T. Rudolph, PRL 108, 110403 (2012). [30] J. von Neumann., Die mathematischen Grundlagen der Quantenmechanik (Berlin., Springer, 1932). [31] A. W. Marshall, I. Olkin, and B,. Inequalities: Theory of Majorization and Its Applications, 2nd ed. Arnold (New York, Springer, 2011). [32] S. Bravyi, Quantum Inf. and Comp. 4, 12 (2004). [33] W. H. Zurek, quant-ph/0111137 (2001). [34] H. D. Zeh, Found.Phys. 1, 69 (1970). [35] E. Joos and H. D. Zeh, Z. Phys. B 59, 223 (1985). [36] W. H. Zurek, S. Habib, and J. P. Paz, PRL 70, 1187 (1993). [37] D. Giulini, E. Joos, C. Kiefer, J. Kupsch, I. O. Sta- matescu, and H. D. Zeh, Decoherence and the Appear- ance of a Classical World in Quantum Theory (Springer, Berlin, 1996). [38] W. H. Zurek, Nature Physics 5, 181 (2009). [39] M. Schlosshauer, Decoherence and the Quantum-To- Classical Transition (Berlin, Springer, 2007). [40] W. H. Zurek, Nature Physics 5, 181 (2009). [41] E. C. G Sudarshan and B. Misra, J. Math. Phys. 18, 756 (1977). [42] R. Omn`es, quant-ph/0106006 (2001). [43] J. Gemmer and G. Mahler, Eur. Phys J. D 17, 385 (2001). [44] T. Durt, Z. Naturforsch. 59a, 425 (2004). [45] W. H. Zurek, S. Habib, and J. P. Paz, PRL 70, 1187 (1993). [46] M. Tegmark and H. S. Shapiro, Phys. Rev. E 50, 2538 (1994). [47] H. Gharibyan and M. Tegmark, arXiv:1309.7349 [quant- ph] (2013). [48] M. Tegmark, PRD 85, 123517 (2012). [49] Nielsen 2005, http://michaelnielsen.org/blog/ archive/notes/fermions_and_jordan_wigner.pdf [50] D. A. R Dalvit, J. Dziarmaga, and W. H. Zurek, PRA 72, 062101 (2005). [51] R. Penrose, The Emperor’s New Mind (Oxford, Oxford Univ. Press, 1989). [52] M. Tegmark, Found. Phys. Lett. 6, 571 (1993). [53] M. Tegmark, Our Mathematical Universe: My Quest for the Ultimate Nature of Reality (New York, Knopf, 2014). [54] C. J. Riedel, W. H. Zurek, and M. Zwolak, New J. Phys. 14, 083010 (2012). [55] S. Lloyd, Programming the Universe (New York, Knopf, 2006). [56] Z. Gu and X. Wen, Nucl.Phys. B 863, 90 (2012). [57] X. Wen, PRD 68, 065003 (2003). [58] M. A. Levin and X. Wen, RMP 77, 871 (2005). [59] M. A. Levin and X. Wen, PRB 73, 035122 (2006). [60] M. Tegmark and L. Yeh, Physica A 202, 342 (1994). Appendix A: Useful identities involving tensor products Below is a list of useful identities involving tensor mul- tiplication and partial tracing, many of which are used in the main part of the paper. Although they are all straightforward to prove by writing them out in the in- dex notation of equation (104), I have been unable to ﬁnd many of them in the literature. The tensor product ⊗ is also known as the Kronecker product. (A ⊗ B) ⊗ C = A ⊗ (B ⊗ C) (A1) A ⊗ (B + C) = A ⊗ B + A ⊗ C (A2) (B + C) ⊗ A = B ⊗ A + C ⊗ A (A3) (A ⊗ B)† = A† ⊗ B † (A4) (A ⊗ B)−1 = A −1 ⊗ B −1 (A5) tr [A ⊗ B] = (tr A)(tr B) (A6) tr 1 [A ⊗ B] = (tr A)B (A7) tr 2 [A ⊗ B] = (tr B)A (A8) tr 1 [A(B ⊗ I)] = tr 1 [(B ⊗ I)A] (A9) tr 2 [A(I ⊗ B)] = tr 2 [(I ⊗ B)A] (A10) tr 1 [(I ⊗ A)B] = A(tr 1 B) (A11) tr 2 [(A ⊗ I)B] = A(tr 2 B) (A12) tr 1 [A(I ⊗ B)] = (tr 1 A)B (A13) tr 2 [A(B ⊗ I)] = (tr 2 A)B (A14) tr 1 [A(B ⊗ C)] = tr 1 [A(B ⊗ I)]C (A15) tr 2 [A(B ⊗ C)] = tr 2 [A(I ⊗ C)]B (A16) tr 1 [(B ⊗ C)A] = C tr 1 [(A ⊗ I)B] (A17) tr 2 [(B ⊗ C)A] = B tr 2 [(I ⊗ C)A] (A18) tr { [(tr 2 A) ⊗ I]B } = tr [(tr 2 A)(tr 2 B)] (A19) tr { [I ⊗ (tr 1 A)]B } = tr [(tr 1 A)(tr 1 B)] (A20) (A ⊗ B, C ⊗ D) = (A, C)(B, D) (A21) ||A ⊗ B|| = ||A|| ||B|| (A22) Identities A11-A14 are seen to be special cases of iden- tities A15-A18. If we deﬁne the superoperators T1 and T2 by T1A ≡ 1 n1 I ⊗ (tr 1A), (A23) T2A ≡ 1 n2 (tr 2A) ⊗ I, (A24) then identities A19-A20 imply that they are self-adjoint: (T1A, B) = (A, T1B), (T2A, B) = (A, T2B). They are also projection operators, since they satisfy T 2 1 = T1 and T 2 2 = T2. Appendix B: Factorization of Harmonic oscillator into uncoupled qubits If the Hilbert space dimensionality n = 2 b for some integer b, then the truncated harmonic oscillator Hamil- 35 tonian of equation (58) can be decomposed into b inde- pendent qubits: in the energy eigenbasis, H = b−1∑ j=0 Hj, Hj = 2 j ( 1 2 0 0 − 1 2 ) j = 2 j−1σz j , (B1) where the subscripts j indicate that an operator acts only on the jth qubit, leaving the others unaﬀected. For ex- ample, for b = 3 qubits, H = (2 0 0 −2 ) ⊗ I ⊗ I + I ⊗ (1 0 0 −1 ) ⊗ I + I ⊗ I ⊗ ( 1 2 0 0 − 1 2 ) =             − 7 2 0 0 0 0 0 0 0 0 − 5 2 0 0 0 0 0 0 0 0 − 3 2 0 0 0 0 0 0 0 0 − 1 2 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 3 2 0 0 0 0 0 0 0 0 5 2 0 0 0 0 0 0 0 0 7 2             , (B2) in agreement with equation (58). This factorization cor- responds to the standard binary representation of inte- gers, which is more clearly seen when adding back the trace (n − 1)/2 = (2 b − 1)/2: H + 7 2 = (4 0 0 0 ) ⊗ I ⊗ I + I ⊗ (2 0 0 0 ) ⊗ I + I ⊗ I ⊗ (1 0 0 0 ) =            0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 7            . (B3) Here we use the ordering convention that the most sig- niﬁcant qubit goes to the left. If we write k as k = b−1∑ j=0 kj2 j, where kj are the binary digits of k and take values 0 or 1, then the energy eigenstates can be written |Ek⟩ = b−1 ⊗ j=0(σ†)kj |0⟩, (B4) where |0⟩ is the ground state (all b qubits in the down state), the creation operator σ† = (0 1 0 0 ) (B5) raises a qubit from the down state to the up state, and (σ†) 0 is meant to be interpreted as the identity matrix I. For example, since the binary representation of 6 is “110”, we have |E6⟩ = σ† ⊗ σ† ⊗ I|0⟩ = |110⟩, (B6) the state where the ﬁrst two qubits are up and the last one is down. Since (σ†) kj ( 0 1 ) is an eigenvector of σz with eigenvalue (2kj − 1), i.e., +1 for spin up and −1 for spin down, equations (B1) and (B4) give H|Ek⟩ = Ek|Ek⟩, where Ek = b−1∑ j=0 2j−1(2kj − 1)|Ek⟩ = k − 2b − 1 2 (B7) in agreement with equation (58). The standard textbook harmonic oscillator corre- sponds to the limit b → ∞, which remains completely separable. In practice, a number of qubits b = 200 is large enough to be experimentally indistinguishable from b = ∞ for describing any harmonic oscillator ever en- countered in nature, since it corresponds to a dynamic range of 2200 ∼ 1060, the ratio between the largest and smallest potentially measurable energies (the Planck en- ergy versus the energy of a photon with wavelength equal to the diameter of our observable universe). So far, we have never measured any physical quantity to better than 17 signiﬁcant digits, corresponding to 56 bits. Appendix C: Emergent space and particles from nothing but qubits Throughout the main body of our paper, we have lim- ited our discussion to a Hilbert space of ﬁnite dimen- sionality n, often interpreting it as b qubits with n = 2 b. On the other hand, textbook quantum mechanics usually sets n = ∞ and contains plenty of structure additional to merely H and ρ, such as a continuous space and various fermion and boson ﬁelds. The purpose of this appendix is to brieﬂy review how the latter picture might emerge from the former. An introduction to this “it’s all qubits” approach by one of its pioneers, Seth Lloyd, is given in [55], and an up-to-date technical review can be found in [56]. As motivation for this emergence approach, note that a large number of quasiparticles have been observed such as phonons, holes, magnons, rotons, plasmons and polarons, which are known not to be fundamental particles, but instead mere excitations in some underlying substrate. This raises the question of whether our standard model particles may be quasiparticles as well. It has been shown that this is indeed a possibility for photons, electrons and quarks [57–59], and perhaps even for gravitons [56], with the substrate being nothing more than a set of qubits without any space or other additional structure. In Appendix B, we saw how to build a harmonic oscil- lator out of inﬁnitely many qubits, and that a truncated harmonic oscillator built from merely 200 qubits is exper- imentally indistinguishable from an inﬁnite-dimensional 36 one. We will casually refer to such a qubit collection de- scribing a truncated harmonic oscillator as a “qubyte”, even if the number of qubits it contains is not precisely 8. As long as our universe is cold enough that the very highest energy level is never excited, a qubyte will behave identically to a true harmonic oscillator, and can be used to deﬁne position and momentum operators obeying the usual canonical commutation relations. To see how space can emerge from qubits alone, con- sider a large set of coupled truncated harmonic oscillators (qubytes), whose position operators qr and momentum operators pr are labeled by an index r = (i, j, k) consist- ing of a triplet of integers — r has no a priori meaning or interpretation whatsoever except as a record-keeping device used to specify the Hamiltonian. Grouping these operators into vectors p and q, we choose the Hamilto- nian H = 1 2 |p| 2 + 1 2 qtAq, (C1) where the coupling matrix A is translationally invariant, i.e., Arr′ = ar′−r, depending only on the diﬀerence r′ − r between two index vectors. For simplicity, let us treat the lattice of index vectors r as inﬁnite, so that A is diagonalized by a 3D Fourier transform. (Alternatively, we can take the lattice to be ﬁnite and the matrix A to be circulant, in which case A is again diagonalized by a Fourier transform; this will lead to the emergence of a toroidal space.) Fourier transforming our qubyte lattice preserves the canonical commutation relations and corresponds to a unitary transformation that decomposes H into indepen- dent harmonic oscillators. As in [60], the frequency of the oscillator corresponding to wave vector κ is ω(k) 2 = ∑ r are−iκ·r. (C2) For example, consider the simple case where each oscilla- tor has a self-coupling µ and is only coupled to its 6 near- est neighbors by a coupling γ: a1,0,0 = a−1,0,0 = a0,1,0 = a0,−1,0 = a0,0,1 = a0,0,−1 = −γ2, a0,0,0 = µ 2 + 6γ2. Then ω(κ) 2 = µ 2 + 4γ2 ( sin 2 κx 2 + sin2 κy 2 + sin2 κz 2 ) , (C3) where κx, κy and κz lie in the interval [π, π]. If we were to interpret the lattice points as existing in a three- dimensional space with separation a between neighboring lattice points, then the physical wave vector k would be given by k = κ a . (C4) Let us now consider a state ρ where all modes except long-wavelength ones with |κ| ≪ 1 are frozen out, in the spirit of our own relatively cold universe. Using the ≏ symbol from Section IV F, we then have H ≏ H ′, where H ′ is a Hamiltonian with the isotropic dispersion relation ω2 = µ 2 + γ2 (κ 2 x + κ 2 y + κ 2 z) = µ 2 + γ2κ 2, (C5) i.e., where the discreteness eﬀects are absent. Comparing this with the standard dispersion relation for a relativistic particle, ω2 = µ 2 + (ck) 2, (C6) where c is the speed of light, we see that the two agree if the lattice spacing is a = c γ . (C7) For example, if the lattice spacing is the Planck length, then the coupling strength γ is the inverse Planck time. In summary, this Hilbert space built out of qubytes, with no structure whatsoever except for the Hamiltonian H, is physically indistinguishable from a system with quan- tum particles (scalar bosons of mass µ) propagating in a continuous 3D space with the same translational and rotational symmetry that we normally associate with inﬁnite Hilbert spaces, so not only did space emerge, but continuous symmetries not inherent in the original qubit Hamiltonian emerged as well. The 3D structure of space emerged from the pattern of couplings between the qubits: if they had been presented in a random order, the graph of which qubits were coupled could have been ana- lyzed to conclude that everything could be simpliﬁed into a 3D rectangular lattice with nearest-neighbor couplings. Adding polarization to build photons and other vec- tor particles is straightforward. Building simple fermion ﬁelds using qubit lattices is analogous as well, except that a unitary Jordan-Wigner transform is required for con- verting the qubits to fermions. Details on how to build photons, electrons, quarks and perhaps even gravitons are given in [56–59]. Lattice gauge theory works simi- larly, except that here, the underlying ﬁnite-dimensional Hilbert space is viewed not as the actual truth but as a numerically tractable approximation to the presumed true inﬁnite-dimensional Hilbert space of quantum ﬁeld theory.","libVersion":"0.3.2","langs":""}
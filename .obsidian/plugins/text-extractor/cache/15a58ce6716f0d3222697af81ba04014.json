{"path":"Notes/Probability - The Science of Uncertainty and Data/slides/L05.pdf","text":"LECTURE 5: Discrete random variables: probability mass functions and expectations • Readings: Sections 2.1-2.3, start 2.4 Lecture outline LECTURE 5: Discrete random variables: probability mass functions and expectations • Readings: Sections 2.1-2.3, start 2.4 Lecture outline • Random variables: the idea and the deﬁnition –Discrete: take values in ﬁnite or countable set • Probability mass function (PMF) • Random variable examples – Bernoulli – Uniform – Binomial – Geometric • Expectation (mean, average) and its properties – The expected value rule – Linearity • Random variables: the idea and the deﬁnition –Discrete: take values in ﬁnite or countable set • Probability mass function (PMF) • Random variable examples – Bernoulli – Uniform – Binomial – Geometric • Expectation (mean, average) and its properties – The expected value rule – Linearity • Random variables: the idea and the deﬁnition –Discrete: take values in ﬁnite or countable set • Probability mass function (PMF) • Random variable examples – Bernoulli – Uniform – Binomial – Geometric • Expectation (mean) and its properties – The expected value rule – Linearity Random variables: the idea • An assignment of a value (number) to every possible outcome • Mathematically: A function from the sample space Ω to the real numbers – discrete or continuous values • Can have several random variables deﬁned on the same sample space • Notation: – random variable X – numerical value x Random variables: the idea Random variables: the formalism • A random variable (”r.v.”) associates a value (a number) to every pos- sible outcome • Mathematically: A function from the sample space Ω to the real numbers – it can take discrete or continuous values • We can have several random variables deﬁned on the same sample space • A function of one or several random variables is also a random variable Random variables: the idea Random variables: the formalism • A random variable (”r.v.”) associates a value (a number) to every possible outcome • Mathematically: A function from the sample space Ω to the real numbers – it can take discrete or continuous values • We can have several random variables deﬁned on the same sample space • A function of one or several random variables is also a random variable Random variables: the idea Random variables: the formalism • A random variable (”r.v.”) associates a value (a number) to every possible outcome • Mathematically: A function from the sample space Ω to the real numbers • It can take discrete or continuous values • We can have several random variables deﬁned on the same sample space • A function of one or several random variables is also a random variable Random variables: the idea Random variables: the formalism • A random variable (”r.v.”) associates a value (a number) to every possible outcome • Mathematically: A function from the sample space Ω to the real numbers • It can take discrete or continuous values • We can have several random variables deﬁned on the same sample space • A function of one or several random variables is also a random variable Random variables: the idea Random variables: the formalism • A random variable (”r.v.”) associates a value (a number) to every possible outcome • Mathematically: A function from the sample space Ω to the real numbers • It can take discrete or continuous values • We can have several random variables deﬁned on the same sample space • A function of one or several random variables is also a random variable • Notation: random variable X numerical value x meaning of X + Y : meaning of X ≥ 0 – random variable X – numerical value x • Notation: random variable X numerical value x – meaning of X + Y : – meaning of X ≥ 0” – random variable X – numerical value x Random variables: the idea Random variables: the formalism • A random variable (“r.v.”) associates a value (a number) to every possible outcome • Mathematically: A function from the sample space Ω to the real numbers • It can take discrete or continuous values • We can have several random variables deﬁned on the same sample space • A function of one or several random variables is also a random variable Probability mass function (PMF) of X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • pX(x) ≥ 0 ￿x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • pX(x) ≥ 0 ￿x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Discrete uniform law − Assume Ω consists of n equally likely elements − Assume A consists of m elements Then : P(A)= number of elements of A number of elements of Ω = m n • Just count... Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd 345 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd 345 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd 345 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • X ab cd 345 prob = 1 4 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X 345 Die roll example X = First roll 1 2 3 4 4 3 2 Y = Second roll 1 • Let B be the event: min(X, Y )=2 • Let M =max(X, Y ) • P(M =1 | B)= • P(M =2 | B)= Sample space: discrete/ﬁnite example • Two rolls of a tetrahedral die – Sample space vs. sequential description X = First roll 1 2 3 4 4 3 2 Y = Second roll 1 1 2 3 4 1,1 1,2 1,3 1,4 4,4 • A continuous sample space: (x, y) such that 0 ≤ x, y ≤ 1 x 1 1 y Probability calculation: discrete/ﬁnite example Example X = First roll 1 2 3 4 4 3 2 Y = Second roll 1 • Let every possible outcome have probability 1/16 • P(X = 1) = PMF calculation – collect all possible outcomes for which X is equal to x – add their probabilities – repeat for all x • Example: Two independent rolls of a fair tetrahedral die F : result of ﬁrst throw S: result of second throw X = min(F, S) 12 3 4 4 3 2 1 F = First roll S = Second roll pX(2) = PMF calculation – collect all possible outcomes for which X is equal to x – add their probabilities – repeat for all x • Example: Two independent rolls of a fair tetrahedral die F : ﬁrst roll S: second roll X = min(F, S) Z = X + Y 12 3 4 4 3 2 1 F = First roll S = Second roll pX(2) = PMF calculation • repeat for all z:ﬁve – collect all possible outcomes for which Z is equal to z – add their probabilities • Example: Two independent rolls of a fair tetrahedral die F : ﬁrst roll S: second roll X = min(F, S) Z = X + Y Find pZ(z) 12 3 4 4 3 2 1 F = First roll S = Second roll PMF calculation • repeat for all z: – collect all possible outcomes for which Z is equal to z – add their probabilities • Example: Two independent rolls of a fair tetrahedral die F : ﬁrst roll S: second roll X = min(F, S) Z = X + Y Find pZ(z) 12 3 4 4 3 2 1 F = First roll S = Second roll PMF calculation • repeat for all z: – collect all possible outcomes for which Z is equal to z – add their probabilities • Example: Two independent rolls of a fair tetrahedral die F : ﬁrst roll S: second roll X = min(F, S) Z = X + Y Find pZ(z) 12 3 4 4 3 2 1 F = First roll S = Second roll123456789 pX (2) = PMF calculation • repeat for all z: – collect all possible outcomes for which Z is equal to z – add their probabilities z • Example: Two independent rolls of a fair tetrahedral die F : ﬁrst roll S: second roll X = min(F, S) Z = X + Y Find pZ(z) n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 The simplest random variable: Bernoulli with parameter p ∈ [0, 1] pX(x)=    1, w.p. p 0, w.p. 1 − p Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 The simplest random variable: Bernoulli with parameter p ∈ [0, 1] X =    1, w.p. p 0, w.p. 1 − p Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely The simplest random variable: Bernoulli with parameter p ∈ [0, 1] X =    1, w.p. p 0, w.p. 1 − p • Models a trial that results in success/failure, Heads/Tails, etc. Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. The simplest random variable: Bernoulli with parameter p ∈ [0, 1] X =    1, w.p. p 0, w.p. 1 − p • Models a trial that results in success/failure, Heads/Tails, etc. • Indicator r.v. of an event A: IA =1 iﬀ A occurs Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. Discrete uniform random variable; parameters a, b Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X :— X (ω)= ω • Model of: complete ignorance ab Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X:— X(ω)= ω • Model of: complete ignorance ab Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X :— X (ω)= ω • Model of: complete ignorance ab Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X :— X (ω)= ω • Model of: complete ignorance ab Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant r.v. Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. Amodelbasedon conditionalprobabilities • 3 tosses of a biased coin: P(H)= p, P(T )=1 − p p p p p p p p 1 - p 1 - p 1 - p 1 - p 1 - p 1 - p 1 - p HHH HHT HTH HTT THH THT TTH TTT P(THT )= P(1 head) = P(ﬁrst toss is H | 1 head) = Binomial random variable; parameters n, p • Parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX (k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Binomial random variable; parameters n, p • Parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X : number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Binomial random variable; parameters n, p • Parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Binomial random variable; parameters n, p • Parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Binomial random variable; parameters n, p • Parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Binomial random variable; parameters n, p • Parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Binomial random variable; parameters: integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 Binomial random variable; parameters: positive integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 pX(x)= ￿n k ￿ pk(1 − p)n−k, for k =0, 1,... ,n Binomial random variable; parameters: positive integer n; p ∈ [0, 1] • Experiment: n independent tosses of a coin with P(Heads) = p • Sample space: Set of sequences of H and T, of length n • Random variable X: number of Heads observed • Model of: number of successes in a given number of independent trials 01 2 pX(k)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,..., n n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin with P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿ p k(1 − p) n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿ p k(1 − p) n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿ p k(1 − p) n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿ p k(1 − p) n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= k P(no Heads ever) 01 2 pX(x)= ￿n k ￿ p k(1 − p) n−k, for k =0, 1,... ,n 123456789 pX (2) = Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= kp =1/3 P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Expectation • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of repetitions of the experiment (to be substantiated later in this course) • Example: Uniform on 0, 1,..., n Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of repetitions of the experiment (to be substantiated later in this course) • Example: Uniform on 0, 1,..., n Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of repetitions of the experiment (to be substantiated later in this course) • Example: Uniform on 0, 1,..., n Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ • Example: Uniform on 0, 1,..., n 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) E[X]=0 × 1 n +1 +1 × 1 n +1 + ··· + n × 1 n +1 = • Interpretation: Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation of a uniform r.v. • Uniform on 0, 1,... ,n 1 n +1 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) Expectation • Motivation: Play a game 1000 times. Random gain at each play described by: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of repetitions of the experiment (to be substantiated later in this course) • Example: Uniform on 0, 1,..., n Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 2/10 2, w.p. 5/10 4, w.p. 3/10 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretation: Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation of a Bernoulli r.v. pX(x)=    1, w.p. p 0, w.p. 1 − p Expectation of a uniform r.v. • Uniform on 0, 1,... ,n 1 n +1 • Interpretation: Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation of a Bernoulli r.v. pX(x)=   1, w.p. p 0, w.p. 1 − p If X is the indicator of an event A, X = IA: Expectation of a uniform r.v. • Uniform on 0, 1,... ,n 1 n +1 n = 3, p =0.5 n = 3, p =0.2 n = 10, p =0.5 n = 3, p =0.2 n = 100, p =0.5 n = 100, p =0.1 The simplest random variable: Bernoulli with parameter p ∈ [0, 1] X =    1, w.p. p 0, w.p. 1 − p Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation examples • Uniform on 0, 1,... ,n 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) E[X]=0 × 1 n +1 +1 × 1 n +1 + ··· + n × 1 n +1 = Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation examples • Uniform on 0, 1,... ,n 1 n +1 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation examples • Uniform on 0, 1,... ,n 1 n +1 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation examples • Uniform on 0, 1,... ,n 1 n +1 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation examples • Uniform on 0, 1,... ,n 1 n +1 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of independent repetitions of the experiment (to be substantiated later in this course) • Caution: If we have an inﬁnite sum, it needs to be well-deﬁned. We assume ￿ x |x| pX(x) < ∞ Expectation of a uniform r.v. • Uniform on 0, 1,... ,n 1 n +1 1 x pX(x ) . . . 0 n- 1 n 1/(n+1) Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of repetitions of the experiment (to be substantiated later in this course) • Example: Uniform on 0, 1,..., n Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) Expectation as a population average • n students • Quiz scores : x1,... ,xn • Experiment: pick a student at random, all equally likely • Random variable X: quiz score of selected student – asume the xi are distinct pX(xi)= Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Expectation as a population average • n students • Weight of ith student: : xi • Experiment: pick a student at random, all equally likely • Random variable X: weight of selected student – asume the xi are distinct pX(xi)= Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Expectation as a population average • n students • Weight of ith student: xi • Experiment: pick a student at random, all equally likely • Random variable X: weight of selected student – asume the xi are distinct pX(xi)= Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Expectation as a population average • n students • Weight of ith student: xi • Experiment: pick a student at random, all equally likely • Random variable X: weight of selected student – asume the xi are distinct pX(xi)= Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Expectation as a population average • n students • Weight of ith student: xi • Experiment: pick a student at random, all equally likely • Random variable X: weight of selected student – asume the xi are distinct pX(xi)= Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Expectation as a population average • n students • Weight of ith student: xi • Experiment: pick a student at random, all equally likely • Random variable X: weight of selected student – assume the xi are distinct pX(xi)= Elementary properties of expectations • If X ≥ 0, then E[X] ≥ 0 • If X ∈ {a, b}, then a ≤ E[X] ≤ b • If c is a constant, E[c]= c Elementary properties of expectations • If X ≥ 0, then E[X] ≥ 0 • If X ∈ {a, b}, then a ≤ E[X] ≤ b • If c is a constant, E[c]= c Elementary properties of expectations • If X ≥ 0, then E[X] ≥ 0 • If a ≤ X ≤ b, then a ≤ E[X] ≤ b • If c is a constant, E[c]= c Elementary properties of expectations • If X ≥ 0, then E[X] ≥ 0 • If a ≤ X ≤ b, then a ≤ E[X] ≤ b • If c is a constant, E[c]= c Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) • Interpretations: – Center of gravity of PMF – Average in large number of repetitions of the experiment (to be substantiated later in this course) • Example: Uniform on 0, 1,..., n Expectation/mean of a random variable • Motivation: Play a game 1000 times. Random gain at each play described by: • “Average” gain: X =    1, w.p. 1/6 2, w.p. 1/2 4, w.p. 1/3 • Deﬁnition: E[X]= ￿ x xpX(x) The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX (k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • 345 Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX (k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX (x) y 2 g prob 0.1 0.2 0.3 0.4 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Averaging over y: • Averaging over x: • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Averaging over y: • Averaging over x: • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Averaging over y: • Averaging over x: • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Averaging over y: • Averaging over x: • Caution: In general, E[g(X)] ￿= g(E[X]) Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Averaging over y: • Averaging over x: • Caution: In general, E[g(X)] ￿= g(E[X]) • E[X2]= Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= The expected value rule, for calculating E[g(X)] Properties of expectations • Let X be a r.v. and let Y = g(X) – Hard: E[Y ]= ￿ y ypY (y) – Easy: E[Y ]= E[g(X)] = ￿ x g(x)pX(x) y 2 g prob 0.1 0.2 0.3 0.4 • Averaging over y: • Averaging over x: • Caution: In general, E[g(X)] ￿= g(E[X]) • E[X2]= Linearity of expectation: E[aX + b]= aE[X]+ b • Intuitive • Proof, based on the expected value rule: Proof: Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= Linearity of expectation: E[aX + b]= aE[X]+ b Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= Linearity of expectation: E[aX + b]= aE[X]+ b • Intuitive • Proof: Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]= Linearity of expectation: E[aX + b]= aE[X]+ b • Intuitive • Derivation, based on the expected value rule: Proof: Properties: If α, β are constants, then: • E[α]= • E[αX]= • E[αX + β]=","libVersion":"0.3.2","langs":""}
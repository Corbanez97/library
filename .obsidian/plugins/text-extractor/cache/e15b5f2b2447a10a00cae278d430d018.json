{"path":"Books and Papers/Masters Points of Interest/Exact Phase Transitions in Deep Learning.pdf","text":"Exact Phase Transitions in Deep Learning Liu Ziyin1, Masahito Ueda1,2,3 1Department of Physics, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033 2Institute for Physics of Intelligence, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033 3RIKEN Center for Emergent Matter Science (CEMS), Wako, Saitama 351-0198, Japan May 26, 2022 Abstract This work reports deep-learning-unique ﬁrst-order and second-order phase transitions, whose phe- nomenology closely follows that in statistical physics. In particular, we prove that the competition between prediction error and model complexity in the training loss leads to the second-order phase tran- sition for nets with one hidden layer and the ﬁrst-order phase transition for nets with more than one hidden layer. The proposed theory is directly relevant to the optimization of neural networks and points to an origin of the posterior collapse problem in Bayesian deep learning. Understanding neural networks is a fundamental problem in both theoretical deep learning and neuro- science. In deep learning, learning proceeds as the parameters of diﬀerent layers become correlated so that the model responds to an input in a meaningful way. This is reminiscent of an ordered phase in physics, where the microscopic degrees of freedom behave collectively and coherently. Meanwhile, regularization eﬀectively prevents the overﬁtting of the model by reducing the correlation between model parameters in a manner similar to the eﬀect of an entropic force in physics. One thus expects a phase transition in the model behavior from the regime where the regularization is negligible to the regime where it is dominant. In the long history of statistical physics of learning [10, 23, 18, 2], a series of works studied the under-to- overparametrization (UO) phase transition in the context of linear regression [12, 13, 23, 9]. Recently, this type of phase transition has seen a resurgence of interest [8, 16]. One recent work by [15] deals with the UO transition in a deep linear model. However, the UO phase transition is not unique to deep learning because it appears in both shallow and deep models and also in non-neural-network models [3]. To understand deep learning, we need to identify what is unique about deep neural networks. In this work, we address the fundamental problem of the loss landscape of a deep neural network and prove that there exist phase transitions in deep learning that can be described precisely as the ﬁrst- and second-order phase transitions with a striking similarity to physics. We argue that these phase transitions can have profound implications for deep learning, such as the importance of symmetry breaking for learning and the qualitative diﬀerence between shallow and deep architectures. We also show that these phase transitions are unique to machine learning and deep learning. They are unique to machine learning because they are caused by the competition between the need to make predictions more accurate and the need to make the model simpler. These phase transitions are also deep-learning unique because they only appear in “deeper” models. For a multilayer linear net with stochastic neurons and trained with L2 regularization, 1. we identify an order parameter and eﬀective landscape that describe the phase transition between a trivial phase and a feature learning phase as the L2 regularization hyperparameter is changed (Theo- rem 3); 2. we show that ﬁnite-depth networks cannot have the zeroth-order phase transition (Theorem 2); 3. we prove that: 1arXiv:2205.12510v1 [cs.LG] 25 May 2022 (a) depth-0 nets (linear regression) do not have a phase transition (Theorem 1); (b) depth-1 nets have the second-order phase transitions (Theorem 4); (c) depth-D nets have the ﬁrst-order phase transition (Theorem 5) for D > 1; (d) inﬁnite-depth nets have the zeroth-order phase transition (Theorem 6). The theorem statements and proofs are presented in the Supplementary Section B. To the best of our knowledge, we are the ﬁrst to identify second-order and ﬁrst-order phase transitions in the context of deep learning. Our result implies that one can precisely classify the landscape of deep neural models according to the Ehrenfest classiﬁcation of phase transitions. Results Formal framework. Let ℓ(w, a) be a diﬀerentiable loss function that is dependent on the model parameter w and a hyperparameters a. The loss function ℓ can be decomposed into a data-dependent feature learning term ℓ0 and a data-independent term aR(w) that regularizes the model at strength a: ℓ(w, a) = Ex[ℓ0(w, x)] + aR(w). (1) Learning amounts to ﬁnding the global minimizer of the loss: ¢¨¨ ¦ ¨¨¤ L(a) ∶= minw ℓ(w, a); w∗ ∶= arg minw ℓ(w, a). (2) Naively, one expects L(a) to change smoothly as we change a. If L changes drastically or even discontinuously when one perturb a, it becomes hard to tune a to optimize the model performance. Thus, that L(a) is well- behaved is equivalent to that a is an easy-to-tune hyperparameter. We are thus interested in the case where the tuning of a is diﬃcult, which occurs when a phase transition comes into play. It is standard to treat the ﬁrst term in Eq. (1) as an energy. To formally identify the regularization term as an entropy, its coeﬃcient must be proportional to the temperature: aR(w) = T 2σ2 R(w), (3) where σ2 controls the ﬂuctuation of w at zero temperature. We note that this identiﬁcation is consistent with many previous works, where the term that encourages a lower model complexity is identiﬁed as an “entropy” [9, 22, 4, 6, 15]. In this view, learning is a balancing process between the learning error and the model complexity. Intuitively, one expects phase transitions to happen when one term starts to dominate the other, just like thermodynamic phase transitions that take place when the entropy term starts to dominate the energy. In this setting, the partition function is Z(a) = ∫ dw exp[−ℓ(w, a)~T ]. We consider a special limit of the partition function, where both T and 2σ2 are made to vanish with their ratio held ﬁxed at T ~2σ2 = γ. In this limit, one can ﬁnd the free energy with the saddle point approximation, which is exact in the zero- temperature limit: F (a) = lim T →0, σ2→0, T ~2σ2=γ −T log S dw exp[−ℓ(w, a)~T ] = min w ℓ(w, a). (4) We thus treat L as the free energy. Deﬁnition 1. L(a) is said to have the nth-order phase transition in a at a = a ∗ if n is the smallest integer such that dn dan L(a)Sa=a∗ is discontinuous. We formally deﬁne the order parameter and eﬀective loss as follows. 2 machine learning statistical physics training loss free energy prediction error internal energy regularization negative entropy learning process symmetry breaking norm of model (b) order parameter feature learning regime ordered phase trivial regime disordered phase noise required for learning latent heat Table 1: Left table: the correspondence between machine learning and statistical physics. Right table: the correspondence between a learning process and symmetry breaking. Deﬁnition 2. b = b(w) ∈ R is said to be an order parameter of ℓ(w, a) if there exists a function ¯ℓ such that for all a, minw ¯ℓ(b(w), a) = L(a), where ¯ℓ is said to be an eﬀective loss function of ℓ. In other words, an order parameter is a one-dimensional quantity whose minimization on ¯ℓ gives L(a). The existence of an order parameter suggests that the original problem ℓ(w, a) can eﬀectively be reduced to a low-dimensional problem that is much easier to understand. Physical examples are the average magnetization in the Ising model and the average density of molecules in a water-to-vapor phase transition. A dictionary of the corresponding concepts between physics and deep learning is given in Table 1. Our theory deals with deep linear nets, the primary minimal model for deep learning. It is well-established that the landscape of a deep linear net can be used to understand that of nonlinear networks [11, 7, 14]. The most general type of deep linear nets, with L2 regularization and stochastic neurons, has the following loss: ExEϵ(1),ϵ(2),...,ϵ(D) ™ ﬂ d0,d0,d0,...d0 Q i,i1,i2,...,iD UiD ϵ(D) iD ...ϵ (2) i2 W (2) i2i1ϵ(1) i1 W (1) i1i xi − yﬁ Ł 2 ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ L0 + γSSU SS 2 2 + D Q i=1 γSSW (i)SS 2 F ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ L2 reg. , (5) where x is the input data, y the label, U and W (i) the model parameters, D the network depth, ϵ the noise in the hidden layer (e.g., dropout), d0 the width of the model, and γ the weight decay strength. We build on the recent results established in [24]. Let b ∶= SSU SS~d0. Ref. [24] shows that all the global minima of Eq. (5) must take the form U = f (b) and Wi = fi(b), where f and fi are explicit functions of the hyperparameters. Ref. [24] further shows that there are two regimes of learning, where, for some range of γ, the global minimum is uniquely given by b = 0, and for some other range of γ, some b > 0 gives the global minimum. When b = 0, the model outputs a constant 0, and so this regime is called the “trivial regime,” and the regime where b = 0 is not the global minimum is called the “feature learning regime.” In this work, we prove that the transition between these two regimes corresponds to a phase transition in the Ehrenfest sense (Deﬁnition 1), and therefore one can indeed refer to these two regimes as two diﬀerent phases. No-phase-transition theorems. The ﬁrst result we prove is that there is no phase transition in any hyperparameter (γ, E[xxT ], E[xy], E[y2]) for a simple linear regression problem. In our terminology, this corresponds to the case of D = 0. The fact that there is no phase transition in any of these hyperparameters means that the model’s behavior is predictable as one tunes the hyperparameters. In the parlance of physics, a linear regressor operates within the linear-response regime. Theorem 2 shows that a ﬁnite-depth net cannot have zeroth-order phase transitions. This theorem can be seen as a worst-case guarantee: the training loss needs to change continuously as one changes the hyperparameter. We also stress that this general theorem applies to standard nonlinear networks as well. Indeed, if we only consider the global minimum of the training loss, the training loss cannot jump. However, in practice, one can often observe jumps because the gradient-based algorithms can be trapped in local minima. The following theory oﬀers a direct explanation for this phenomenon. Phase Transitions in Deeper Networks. Theorem 4 shows that the quantity b is an order parameter describing any phase transition induced by the weight decay parameter in Eq. (5). Let b = SSU SS~du, A0 ∶= E[xxT ], and ai be the i-th eigenvalue of A0. The eﬀective loss landscape is ¯ℓ(b, γ) ∶= − Q i d 2D 0 b2DE[x′y] 2 i dD 0 (σ2 + d0)Daib2D + γ + Ex[y2] + γDd2 0b2, (6) 3 Figure 1: Eﬀective landscape given in Eq. (6) for D = 1 (left) and D = 2 (right). For D = 1, zero is either the global minimum or a local maximum. Note that the shape of the loss resembles that of the Landau free energy for the second-order phase transition. For D = 2, the landscape becomes more complicated, featuring the emergence of local minima. In particular, zero is always a local minimum. where x′ is a rotation of x. See Figure 1 for an illustration. The complicated landscape for D > 1 implies that neural networks are susceptible to initialization schemes and entrapment in meta-stable states is common (see Supplementary Section A.1). Theorem 5 shows that when D = 1 in Eq. (5), there is a second-order phase transition precisely at γ = SSE[xy]SS. (7) In machine learning language, γ is the regularization strength and SSE[xy]SS is the signal. The phase transition occurs precisely when the regularization dominates the signal. In physics, γ and SSE[xy]SS are proportional to the temperature and energy, respectively. The phase transition occurs exactly when the entropy dominates the energy. Also, the phase transition for a depth-1 linear net is independent of the number of parameters of the model. For D > 1, the size of the model does play a role in inﬂuencing the phase transition. However, γ remains the dominant variable controlling this phase transition. This independence of the model size is an advantage of the proposed theory because our result becomes directly relevant for all model sizes, not just the inﬁnitely large ones that the previous works often adopt. For D ≥ 2, we show that there is a ﬁrst-order phase transition between the two phases at some γ > 0. However, an analytical expression for the critical point is not known. In physics, ﬁrst-order phase transitions are accompanied by latent heat. Our theory implies that this heat is equivalent to the amount of random noise we have to inject into the model parameters to escape from a local to the global minimum for a deep model. We illustrate the phase transitions studied in Figure 2. We also experimentally demonstrate that the same phase transitions take place in deep nonlinear networks with the corresponding depths (Supplementary Section A.3). While inﬁnite-depth networks are not used in practice, they are important from a theoretical point of view [20] because they can be used for understanding a (very) deep network that often appears in the deep learning practice. Our result shows that the limiting landscape has a zeroth-order phase transition at γ = 0. In fact, zeroth-order phase transitions do not occur in physics, and it is a unique feature of deep learning. Relevance of symmetry breaking. The phase transitions we studied also involve symmetry breaking. This can be seen directly from the eﬀective landscape in Eq. (6). The loss is unaltered as one ﬂip the sign of b, and therefore the loss is symmetric in b. Figure 3 illustrates the eﬀect and importance of symmetry breaking on the gradient descent dynamics. Additionally, this observation may also provide an alternative venue for studying general symmetry-breaking dynamics because the computation with neural networks is both accurate and eﬃcient. Mean-Field Analysis. The crux of our theory can be understood by applying a simpliﬁed “mean-ﬁeld” analysis of the loss function in Eq. (5). Let each weight matrix be approximated by a scalar U = bD+1, Wi = bi, ignore the stochasticity due to ϵi, and let x be one-dimensional. One then obtains a simpliﬁed mean-ﬁeld loss: Ex < @ @ @ @ >„c0x D+1 M i=1 bi − y‚ 2= A A A A ? + γ D Q i=1 cib 2 i , (8) 4 Figure 2: Phase transitions in a linear net. In agreement with the theory, a depth-0 net has no phase transition. A depth-1 net has a second-order phase transition at approximately γ = 0.45, close to the theoretical value of SSE[xy]SS, and a depth-2 net has a ﬁrst-order phase transition at roughly γ = 0.15. The qualitative diﬀerences between networks of diﬀerent depths are clearly observed in the data. Left: Training loss of a network with 0 (linear regression), 1, and 2 hidden layers. Clearly, a depth-0 net shows no phase transition. A depth-1 net has a second-order phase transition at approximately γ = 0.45, and a depth-2 net has a ﬁrst-order phase transition at roughly γ = 0.15. Middle: Magnitude of the regularization term at convergence. As discussed in the main text, this term corresponds to the entropy term T S. We see that for D > 1, there is a jump (discontinuity) in T S from a ﬁnite value to 0. This jump corresponds to the latent heat of the ﬁrst-order phase transition process. Right: Order parameter as a function of γ. The inset shows that b precisely scales as t0.5 with t ∶= −(γ − γ∗) in the vicinity of the phase transition, in agreement with the standard Landau theory. where ci’s are constants. The ﬁrst term can be interpreted as a special type of (D + 1)-body interaction. We now perform a second mean-ﬁeld approximation, where all the bi take the same value b: ℓ ∝ c′ 0E[x2]b2D+2 − c ′ 1E[xy]bD+1 + γc ′ 2b2 + const. (9) Here, c ′ 0, c ′ 1 and c′ 2 are structural constants, only depending on the model (depth, width, etc). The ﬁrst and the third terms monotonically increase in b, encouraging a smaller b. The second term monotonically decreases in bD+1E[xy], encouraging a positive correlation between b and the feature E[xy]. The leading and lowest-order terms regularize the model, while the intermediate term characterizes learning. For D = 0, the loss is quadratic and has no transition. For D = 1, the loss is identical to the Landau free energy, and a phase transition occurs when the second-order term ﬂips sign: c ′ 2γ = c ′ 1E[xy]. For D > 1, the origin is always a local minimum, dominated by the quadratic term. This leads to a ﬁrst-order phase transition. When D → ∞, the leading terms become discontinuous in b, and one obtains a zeroth-order phase transition. This simple analysis highlights one important distinction between physics and machine learning: in physics, the most common type of interaction is a two-body interaction, whereas, for machine learning, the common interaction is many-body and tends to inﬁnite-body as D increases. One implication is that L2 regularization may be too strong for deep learning because it creates a trivial phase. Our result also suggests a way to avoid the trivial phase. Instead of regularizing by γSSwSS 2 2, one might consider γSSwSS d+2 2 , which is the lowest-order regularization that does not lead to a trivial phase. The eﬀectiveness of this suggested method is conﬁrmed in Supplementary Section A.2. Posterior Collapse in Bayesian Deep Learning. Our results also identify an origin of the well-known problem posterior collapse problem in Bayesian deep learning. Posterior collapse refers to the learning failure where the learned posterior distribution coincides with the prior, and so no learning has happened even after training [5, 1, 17]. Our results oﬀer a direct explanation for this posterior collapse problem. In the Bayesian interpretation, the training loss in Eq. (5) is the exact negative log posterior, and the trivial phase exactly corresponds to the posterior collapse: the global minimum of the loss is identical to the global maximum of the prior term. Our results thus imply that (1) posterior collapse is a unique problem of deep learning because it does not occur in shallow models, and (2) posterior collapse happens as a direct consequence of the competition between the prior and the likelihood. This means that it is not a good idea to assume a Gaussian prior for the deep neural network models. The suggested ﬁx also leads to a clean and Bayesian-principled solution to the posterior collapse problem by using a prior log p(w) ∝ −SSwSS D+2 2 . 5 Figure 3: Dynamics of training, where the model is initialized at the origin and the learning proceeds under gradient descent with injected Gaussian noise. Before training, the models lie roughly in the trivial phase because the model is initialized close to the origin and has not learned anything yet. However, for the feature learning phase, any global minimum must choose a speciﬁc b ≠ 0, and so the actual solution does not feature the symmetry in b: a symmetry breaking in b must take place for the learning to happen. The recent work of Ref. [21] showed that the symmetries in the loss could become diﬃcult obstacles in the training of a neural network, and our result complements this view by identifying a precise deep-learning-relevant symmetry to be broken. Left: the training loss L; except for D = 0, where no symmetry breaking occurs, the dynamics exhibits a wide plateau that hinders learning emerging at initialization. Middle: a zoom-in of the left panel when L is close to the initialized value (≈ 0.2). For D = 1, the loss decreases monotonically. For D > 1, in sharp contrast, the loss ﬁrst increases slowly and then decreases precipitously, a signature of escaping from a local minimum: the height of the peak may be interpreted as the latent heat of the phase transition since this is the ”energy barrier” for the system to overcome in order to undergo the ﬁrst-order phase transition. Right: time evolution of the order parameter b: one sees that D = 0 shows a fast increase of b from the beginning. For D ≥ 1, the initial stage is dominated by slow diﬀusion, where b increases as the square root of time. The diﬀusion phase only ends after a long period, before a fast learning period begins. One also notices that in the fast learning period, the slope of b versus time is diﬀerent for diﬀerent depths, with deeper models considerably faster than shallower ones. The inset shows the corrected order parameter ˜b ∶= b − D√τ , where τ is the training step, and D is the diﬀusion constant of the noisy gradient descent. One sees that ˜b stays zero over an extended period of time for D > 0. Discussion The striking similarity between phase transitions in neural networks and statistical-physics phase transitions lends a great impetus to a more thorough investigation of deep learning through the lens of thermodynamics and statistical physics. We now outline a few major future steps: 1. Instead of classiﬁcation by analyticity, can we classify neural networks by symmetry and topological invariants? 2. What are other possible phases for a nonlinear network? Does a new phase emerge? 3. Can we ﬁnd any analogy of other thermodynamic quantities such as volume and pressure? More broadly, can we establish thermodynamics for deep learning? 4. Can we utilize the latent heat picture to devise better algorithms for escaping local minima in deep learning? This work shows that the Ehrenfest classiﬁcation of phase transitions aligns precisely with the number of layers in deep neural networks. We believe that the statistical-physics approach to deep learning will bring about fruitful developments in both ﬁelds of statistical physics and deep learning. 6 References [1] Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R. A., and Murphy, K. (2018). Fixing a broken elbo. In International Conference on Machine Learning, pages 159–168. PMLR. [2] Bahri, Y., Kadmon, J., Pennington, J., Schoenholz, S. S., Sohl-Dickstein, J., and Ganguli, S. (2020). Statistical mechanics of deep learning. Annual Review of Condensed Matter Physics, 11:501–528. [3] Belkin, M., Hsu, D., and Xu, J. (2020). Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167–1180. [4] Benedek, G. M. and Itai, A. (1991). Learnability with respect to ﬁxed distributions. Theoretical Computer Science, 86(2):377–389. [5] Dai, B. and Wipf, D. (2019). Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789. [6] Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in cognitive sciences, 13(7):293–301. [7] Hardt, M. and Ma, T. (2016). Identity matters in deep learning. arXiv preprint arXiv:1611.04231. [8] Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2019). Surprises in high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560. [9] Haussler, D., Kearns, M., Seung, H. S., and Tishby, N. (1996). Rigorous learning curve bounds from statistical mechanics. Machine Learning, 25(2):195–236. [10] Hopﬁeld, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554–2558. [11] Kawaguchi, K. (2016). Deep learning without poor local minima. Advances in Neural Information Processing Systems, 29:586–594. [12] Krogh, A. and Hertz, J. A. (1992a). Generalization in a linear perceptron in the presence of noise. Journal of Physics A: Mathematical and General, 25(5):1135. [13] Krogh, A. and Hertz, J. A. (1992b). A simple weight decay can improve generalization. In Advances in neural information processing systems, pages 950–957. [14] Laurent, T. and Brecht, J. (2018). Deep linear networks with arbitrary loss: All local minima are global. In International conference on machine learning, pages 2902–2907. PMLR. [15] Li, Q. and Sompolinsky, H. (2021). Statistical mechanics of deep linear neural networks: The backprop- agating kernel renormalization. Physical Review X, 11(3):031059. [16] Liao, Z., Couillet, R., and Mahoney, M. W. (2020). A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent. Advances in Neural Information Processing Systems, 33:13939–13950. [17] Lucas, J., Tucker, G., Grosse, R., and Norouzi, M. (2019). Don’t blame the elbo! a linear vae perspective on posterior collapse. [18] Martin, C. H. and Mahoney, M. W. (2017). Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior. arXiv preprint arXiv:1710.09553. [19] Mianjy, P. and Arora, R. (2019). On dropout and nuclear norm regularization. In International Con- ference on Machine Learning, pages 4575–4584. PMLR. [20] Sonoda, S. and Murata, N. (2019). Transport analysis of inﬁnitely deep neural network. The Journal of Machine Learning Research, 20(1):31–82. 7 [21] Tanaka, H. and Kunin, D. (2021). Noether’s learning dynamics: Role of symmetry breaking in neural networks. Advances in Neural Information Processing Systems, 34. [22] Vapnik, V. (2006). Estimation of dependences based on empirical data. Springer Science & Business Media. [23] Watkin, T. L., Rau, A., and Biehl, M. (1993). The statistical mechanics of learning a rule. Reviews of Modern Physics, 65(2):499. [24] Ziyin, L., Li, B., and Meng, X. (2022). Exact solutions of a deep linear network. arXiv preprint arXiv:2202.04777. 8 Figure 4: Sensitivity of the obtained solution to the initialization of the model. We initialize the model around zero with standard deviation s. The experiment shows that a larger initialization variance (s = 0.3) aﬀords a preference of the nontrivial solution over the trivial one, while a smaller initialization leads to the opposite preference. Figure 5: The training loss L(γ) (left) and the model norm b (right) when we train with a regularization term of the form γSSwSS D+2, which is a theoretically justiﬁed ﬁx to the trivial learning problem. We see that the trivial phase disappears under this regularization. A Additional Experiments A.1 Sensitivity to the Initial Condition Our result suggests that the learning of a deeper network is quite sensitive to the initialization schemes we use. In particular, for D > 1, some initialization schemes converge to the trivial solutions more easily, while others converge to the nontrivial solution more easily. Figure 4 plots the converged loss of a D = 2 model for two types of initialization: (a) larger initialization, where the parameters are initialized around zero with the standard deviation s = 0.3 and (b) small initialization with s = 0.01. The value of s is thus equal to the expected norm of the model at initialization, and a small s means that it is initialized closer to the trivial phase and a larger s means that it is initialized closer to the learning phase. We see that across a wide range of γ, one of the initialization schemes gets stuck in a local minimum and does not converge to the global minimum. In light of the latent heat picture, the reason for the sensitivity to initial states is clear: one needs to inject additional energy to the system to leave the meta-stable state; otherwise, the system may become stuck for a very long time. The existing initialization methods are predominantly data-dependent. However, our result (also see [24]) suggests that the size of the trivial minimum is data-dependent, and our result thus highlights the importance of designing data-dependent initialization methods in deep learning. A.2 Removing the Trivial Phase We also explore our suggested ﬁx to the trivial learning problem. Here, instead of regularization the model by γSSwSS 2 2, we regularize the model by γSSwSS D+2 2 . The training loss and the model norm b are plotted in Figure 5. We ﬁnd that the trivial phase now completely disappears even if we go to very high γ. However, we note that this ﬁx only removes the local maximum at zero, but zero remains a saddle point from which it takes the system a long time to escape. 9 Figure 6: Phase transition of a fully connected tanh network. Top row shows the case of D = 1, exhibiting a second-order phase transition: the training loss L(γ) (left), ﬁrst derivative (middle), and the second derivative (right). Bottom row shows the case of D = 2, exhibiting a ﬁrst-order phase transition: the training loss L(γ) (left) and ﬁrst derivative L ′(γ) (middle). For D = 2, we initialize the model with three initialization at diﬀerent scales and use the minimum of the respective loss values as an empirical estimate of the actual global minimum. A.3 Nonlinear Networks We expect our theory to also apply to deep nonlinear networks that can be locally approximated by linear net at the origin, e.g., a network with tanh activations. As shown in Figure 6, the data shows that a tanh net also features a second-order phase transition for D = 1 and a ﬁrst-order phase transition for D = 2. One notable exception that our theory may not apply is the networks with the ReLU activation because these networks are not diﬀerentiable at zero (i.e., in the trivial phase). However, there are smoother (and empirically better) alternatives to ReLU, such as the swish activation function, to which the present theory should also be relevant. 10 B Main Results B.1 Theorem Statements For a simple ridge linear regression, the minimization objective is ℓ(W ) = Ex „Q i Wixi − y‚ 2 + γSSW SS 2. (10) Theorem 1. There is no phase transition in any hyperparameter (γ, A0, E[xy], E[y2]) in a simple ridge linear regression for any γ ∈ (0, ∞). The following result shows that for a ﬁnite depth, L(γ) must be continuous in γ. Theorem 2. For any ﬁnite D > 0 and γ ∈ [0, ∞), L(γ) has no zeroth-order phase transition with respect to γ. Note that this theorem allows the weight decay parameter to be 0, and so our results also extend to the case when there is no weight decay. The following theorem shows that there exists order parameters describing any phase transition induced by the weight decay parameter in Eq. (5). Theorem 3. Let b = SSU SS~du, and let ¯ℓ(b, γ) ∶= − Q i d 2D 0 b2DE[x′y] 2 i dD 0 (σ2 + d0)Daib2D + γ + Ex[y2] + γDd2 0b2. (11) Then, b is an order parameter of Eq. (5) for the eﬀective loss ¯ℓ. Here, the norm of the last layer is referred to as the order parameter. The meaning of this choice should be clear. The norm of the last layer is zero if and only if all weights of the last layer is zero, and the model is a trivial model. The model can only learn something when the order parameter is nonzero. Additionally, we note that the choice of the order parameter is not unique and there are other choices for the order parameter (e.g., the norm of any other layer, or the sum of the norms of all layers). The following theorem shows that when D = 1 in Eq. (5), there is a second-order phase transition with respect to γ. Theorem 4. Equation (5) has the second-order phase transition between the trivial and feature learning phases at 1 γ = SSE[xy]SS. (12) Now, we show that for D ≥ 2, there is a ﬁrst-order phase transition. Theorem 5. Let D ≥ 2. There exists a γ∗ > 0 such that the loss function Eq. (5) has the ﬁrst-order phase transition between the trivial and feature learning phases at γ = γ∗. Theorem 6. Let L (D)(γ) denote the loss function for a ﬁxed depth D as a function of γ. Then, for γ ∈ [0, ∞) and some constant r, L (D)(γ) → ¢¨¨ ¦ ¨¨¤ r if γ = 0; E[y2] otherwise. (13) The constant r is, in general, not equal to E[y2]. For example, in the limit σ → 0, r converges to the loss value of a simple linear regression, which is not equal to E[y2] as long as E[xy] ≠ 0. 1When the two layers have diﬀerent regularization strengths γu and γw, one can show that the phase transition occurs precisely at √γuγw = SSE[xy]SS. 11 B.2 Proof of Theorem 1 Proof. The global minimum of Eq. (10) is W∗ = (A0 + γI) −1E[xy]. (14) The loss of the global minimum is thus L = Ex „Q i Wixi − y‚ 2 + γSSW SS 2 (15) = W T A0W − 2W T E[xy] + E[y2] + γSSW SS 2 (16) = E[xy] T A0 (A0 + γI)2 E[xy] − 2E[xy]T 1 A0 + γI E[xy] + E[y2] + γE[xy]T 1 (A0 + γI)2 E[xy] (17) = −E[xy] T (A0 + γI) −1E[xy] + E[y2], (18) which is inﬁnitely diﬀerentiable for any γ ∈ (0, ∞) (note that A0 is always positive semi-deﬁnite by deﬁnition). j B.3 Proof of Theorem 2 Proof. For any ﬁxed and bounded w, ℓ(w, γ) is continuous in γ. Moreover, ℓ(w, γ) is a monotonically increasing function of γ. This implies that L(γ) is also an increasing function of γ (but may not be strictly increasing). We now prove by contradiction. We ﬁrst show that L(γ) is left-continuous. Suppose that for some D, L(γ) is not left-continuous in γ at some γ∗. By deﬁnition, we have L(γ∗ − ϵ) = min w ℓ(w, γ∗ − ϵ) ∶= ℓ(w′, γ∗ − ϵ), (19) where w′ is one of the (potentially many) global minima of L(γ∗ − ϵ). Since L(γ) is not left-continuous by assumption, there exists δ > 0 such that for any ϵ > 0, L(γ∗ − ϵ) < L(γ∗) − δ, (20) which implies that ℓ(w′, γ∗ − ϵ) = L(γ∗ − ϵ) < L(γ∗) − δ ≤ ℓ(w′, γ∗) − δ. (21) Namely, the left-discontinuity implies that for all ϵ > 0, ℓ(w′, γ∗ − ϵ) ≤ ℓ(w′, γ∗) − δ. (22) However, by deﬁnition of ℓ(w, γ), we have ℓ(w, γ) − ℓ(w, γ − ϵ) = ϵSSwSS 2. (23) Thus, by choosing ϵ < δ~SSwSS 2, the relation in (21) is violated. Thus, L(γ) must be left-continuous. In a similar manner, we can prove that L is right-continuous. Suppose that for some D, L(γ) is not right-continuous in γ at some γ∗. Let γ > 0. By deﬁnition, we have L(γ∗ + ϵ) = min w ℓ(w, γ∗ + ϵ) ∶= ℓ(w′, γ∗ + ϵ), (24) where w′ is one of the (potentially many) global minima of L(γ∗ + ϵ). Since L(γ) is not right-continuous by assumption, there exists δ > 0 such that for any ϵ > 0, L(γ∗ + ϵ) > L(γ∗) + δ, (25) which implies that ℓ(w′, γ∗ + ϵ) = L(γ∗ + ϵ) > L(γ∗) + δ ≥ ℓ(w′, γ∗) + δ. (26) 12 Namely, the right-discontinuity implies that for all ϵ > 0, ℓ(w′, γ∗ + ϵ) ≥ ℓ(w′, γ∗) + δ. (27) However, by deﬁnition of ℓ(w, γ), we have ℓ(w, γ + ϵ) − ℓ(w, γ) = ϵSSwSS 2. (28) Thus, by choosing ϵ < δ~SSwSS 2, the relation in (26) is violated. Thus, L(γ) must be right-continuous. Therefore, L(γ) is continuous for all γ > 0. By deﬁnition, this means that there is no zeroth-order phase transition in γ for L. Additionally, note that the above proof does not require γ ≠ 0, and so we have also shown that L(γ) is right-continuous at γ = 0. j B.4 Proof of Theorem 3 Proof. By Theorem 3 of Ref. [24], any global minimum of Eq. (5) is given by the following set of equations for some b ≥ 0: ¢¨¨¨¨¨ ¦ ¨¨¨¨¨¤ U = √d0brD; W (i) = brir T i−1; W (1) = r1E[xy] T d D− 1 2 0 b D \u0001dD 0 (σ2 + d0)Db2DA0 + γ\u0006−1 , (29) where ri = (±1, ..., ±1) is an arbitrary vertex of a di-dimensional hypercube for all i. Therefore, the global minimum must lie on a one-dimensional space indexed by b ∈ [0, ∞). Let f (x) specify the model as f (x) ∶= d,d1,d2,...dD Q i,i1,i2,...,iD UiD ϵ(D) iD ...ϵ (2) i2 W (2) i2i1ϵ(1) i1 W (1) i1i x, (30) and let η denote the set of all random noises ϵi. Substituting Eq. (29) in Eq. (5), one ﬁnds that within this subspace, the loss function can be written as ℓ(w, γ) = ExEη(f (x) − y)2 + L2 reg. (31) = Ex,η[f (x)2] − 2Ex,η[yf (x)] + Ex[y2] + L2 reg. (32) = Q i d 3D 0 (σ2 + d0)Db4DaiE[x′y]2 i [dD 0 (σ2 + d0)Daib2D + γ]2 − 2 Q i d 2D 0 b2DE[x′y] 2 i dD 0 (σ2 + d0)Daib2D + γ + Ex[y2] + L2 reg., (33) where the L2 reg. term is L2 reg. = γDd2 0b2 + γ Q i d2D 0 b 2DSSE[x′y]iSS 2 [dD 0 (σ2 + d0)Db2Dai + γ]2 . (34) Combining terms, we can simplify the expression for the loss function to be − Q i d 2D 0 b2DE[x′y]2 i [dD 0 (σ2 + d0)Daib2D + γ] + Ex[y2] + γDd2 0b2. (35) We can now deﬁne the eﬀective loss by ¯ℓ(b, γ) ∶= − Q i d 2D 0 b2DE[x′y]2 i [dD 0 (σ2 + d0)Daib2D + γ] + Ex[y2] + γDd2 0b2. (36) Then, the above argument shows that, for all γ, min b ¯ℓ(b, γ) = min w ℓ(w, γ). (37) By deﬁnition 2, b is an order parameter of ℓ with respect to the eﬀective loss ¯ℓ(b, γ). This completes the proof. j 13 B.5 Two Useful Lemmas Before continuing the proofs, we ﬁrst prove two lemmas that will simplify the following proofs signiﬁcantly. Lemma 1. If L(γ) is diﬀerentiable, then for at least one of the global minima b∗, d dγ L(γ) = Q i d 2D 0 b2D ∗ E[x′y]2 i [dD 0 (σ2 + d0)Daib2D ∗ + γ]2 + Dd 2 0b 2 ∗ ≥ 0. (38) Proof. Because L is diﬀerentiable in γ, one can ﬁnd the derivative for at least one of the global minima b∗ d dγ L(γ) = d dγ ¯ℓ(b ∗(γ), γ) (39) = ∂ ∂b∗ ¯ℓ(b∗, γ) ∂b ∗ ∂γ + ∂ ∂γ ¯ℓ(b∗, γ) (40) = ∂ ∂γ ¯ℓ(b∗, γ) (41) = Q i d 2D 0 b2D ∗ E[x′y]2 i [dD 0 (σ2 + d0)Daib2D ∗ + γ]2 + Dd 2 0b 2 ∗ ≥ 0, (42) where we have used the optimality condition ∂ ∂b∗ ¯ℓ(b ∗(γ), γ) = 0 in the second equality. j B.6 Proof of Theorem 4 Proof. By deﬁnition 1, it suﬃces to only prove the existence of phase transitions on the eﬀective loss. For D = 1, the eﬀective loss is ¯ℓ(b, γ) = −d1b2E[xy]T [b2(σ2 + d1)A + γI]−1E[xy] + E[y2] + γd1b 2. (43) By Theorem 1 of Ref. [24], the phase transition, if exists, must occur precisely at γ = SSE[xy]SS. To prove that γ = SSE[xy]SS has a second-order phase transition, we must check both its ﬁrst derivative and second derivative. When γ → SSE[xy]SS from the right, we ﬁnd that the all derivatives of L(γ) are zero because the loss is identically equal to E[y2]. We now consider the derivative of L when γ → SSE[xy]SS from the left. We ﬁrst need to ﬁnd the minimizer of Eq. (43). Because Eq.(43) is diﬀerentiable, its derivative in b must be equal to 0 at the global minimum − 2γd1bE[xy] T [b2(σ2 + d1)2A + γI]−2E[xy] + 2γd1b = 0. (44) Finding the minimizer b is thus equivalent to ﬁnding the real roots of a high-order polynomial in b. When γ ≥ SSE[xy]SS, the solution is unique [24]: b2 0 = 0, (45) where we labeled the solution with the subscript 0 to emphasize that this solution is also the zeroth-order term of the solution in a perturbatively small neighborhood of γ = SSE[xy]SS. From this point, we deﬁne a shifted regularization strength: ∆ ∶= γ − SSE[xy]SS. When ∆ < 0, the condition (44) simpliﬁes to E[xy] T [b 2(σ2 + d1)A + γI] −2E[xy] = 1. (46) Because the polynomial is not singular in ∆, one can Taylor expand the (squared) solution b 2 in ∆: b(γ)2 = β0 + β1∆ + O(∆2). (47) We ﬁrst Substitute (47) in (44) to ﬁnd 2 β0 = 0. (48) 2Note that alternatively, β0 = 0 is implied by the no-zeroth-order transition theorem. 14 One can then again substitute Eq. (47) in Eq. (44) to ﬁnd β1. To the ﬁrst order in b2, Eq. (44) reads 1 γ2 SSE[xy]SS 2 − 2b2 (σ2 + d1) γ3 SSE[xy]SS 2 A0 = 1 (49) ⇐⇒ −2β1∆ (σ2 + d1) γ3 SSE[xy]SS 2 A0 = 2 ∆ SSE[xy]SS (50) ⇐⇒ β1 = − 1 (σ2 + d1) SSE[xy]SS 2 SSE[xy]SS2 A0 (51) Substituting this ﬁrst-order solution to Lemma 1, we obtain that d dγ L(γ)Sγ=SSE[xy]SS− ∼ b2 ∗ = 0 = d dγ L(γ)Sγ=SSE[xy]SS+ . (52) Thus, the ﬁrst-order derivative of L(γ) is continuous at the phase transition point. We now ﬁnd the second-order derivative of L(γ). To achieve this, we also need to ﬁnd the second-order term of b 2 in γ. We expand b 2 as b(γ) 2 = 0 + β1∆ + β2∆2 + O(∆3). (53) To the second order in b2, (44) reads 1 γ2 SSE[xy]SS 2 − 2b2 (σ2 + d1) γ3 SSE[xy]SS 2 A0 + 3b 4 (σ2 + d1) 2 γ4 SSE[xy]SS 2 A2 0 = 1 (54) ⇐⇒ γ2SSE[xy]SS 2 − 2b2(σ2 + d1)γSSE[xy]SS 2 A0 + 3b4(σ2 + d1) 2SSE[xy]SS 2 A2 0 = γ4 (55) ⇐⇒ ∆2E2 0 − 2β2∆2(σ2 + d1)E0E2 1 − 2β1∆2(σ2 + d1)E2 1 + 3β2 1 ∆2(σ2 + d1)2E2 2 = 6E2 0 ∆2 (56) ⇐⇒ β2 = 3β2 1 (σ2 + d1)2E2 2 − 5E2 0 − 2β1(σ2 + d1)E2 1 2(σ2 + d1)E0E2 1 , (57) where, from the third line, we have used the shorthand E0 ∶= SSE[xy]SS, E1 ∶= SSE[xy]SSA0, and E2 ∶= SSE[xy]SSA2 0. Substitute in β1, one ﬁnds that β2 = 3E0(E2 2 − E2 1 ) 2(σ2 + d1)E4 1 . (58) This allows us to ﬁnd the second derivative of L(γ). Substituting β1 and β2 into Eq. (43) and expanding to the second order in ∆, we obtain that L(γ) = −d1b2E[xy]T [b2(σ2 + d1)A + γI]−1E[xy] + E[y2] + γd1b 2 (59) = −d1(β1∆ + β2∆2)E[xy] T [(β1∆ + β2∆2)(σ2 + d1)A0 + γI] −1E[xy] + γd1(β1∆ + β2∆). (60) At the critical point, d 2 dγ2 L(γ)Sγ=SSE[xy]SS− = −d1β2E0 + d1β2 1 (σ2 + d1) E2 1 E2 0 + d1β1 + d1β1 + d1β2E0 (61) = 2d1β1 + d1β2 1 (σ2 + d1) E2 1 E2 0 (62) = d1β1 (63) = − d1 σ2 + d1 SSE[xy]SS 2 SSE[xy]SS2 A0 . (64) Notably, the second derivative of L from the left is only dependent on β1 and not on β2. d 2 dγ2 L(γ)Sγ=SSE[xy]SS− = − d1 σ2 + d1 SSE[xy]SS 2 SSE[xy]SS2 A0 < 0. (65) Thus, the second derivative of L(γ) is discontinuous at γ = SSE[xy]SS. This completes the proof. j Remark. Note that the proof suggests that close to the critical point, b ∼ √ ∆, in agreement with the Landau theory. 15 B.7 Proof of Theorem 5 Proof. By deﬁnition, it suﬃces to show that d dγ L(γ) is not continuous. We prove by contradiction. Suppose that d dγ L(γ) is everywhere continuous on γ ∈ (0, ∞). Then, by Lemma 1, one can ﬁnd the derivative for at least one of the global minima b∗ d dγ L(γ) = Q i d 2D 0 b2D ∗ E[x′y] 2 i [dD 0 (σ2 + d0)Daib2D ∗ + γ]2 + γDd2 0b2 ∗ ≥ 0. (66) Both terms in the last line are nonnegative, and so one necessary condition for d dγ L(γ) to be continuous is that both of these two terms are continuous in γ. In particular, one necessary condition is that γDd2 0b 2 ∗ is continuous in γ. By Proposition 3 of Ref. [24], there exist constants c0, c1 such that 0 < c0 ≤ c1, and ¢¨¨ ¦ ¨¨¤ b∗ = 0 if γ < c0; b∗ > 0, if γ > c1. (67) Additionally, if b∗ > 0, b∗ must be lower-bounded by some nonzero value [24]: b∗ ≥ 1 d0 „ γ SSE[xy]SS ‚ 1 D−1 > 1 d0 „ c1 SSE[xy]SS ‚ 1 D−1 > 0. (68) Therefore, for any D > 1, b∗(γ) must have a discontinuous jump from 0 to a value larger than 1 d0 − c0 SSE[xy]SS ‘ 1 D−1 , and cannot be continuous. This, in turn, implies that d dγ L(γ) jumps from zero to a nonzero value and cannot be continuous. This completes the proof. j B.8 Proof of Theorem 6 Proof. It suﬃces to show that a nonzero global minimum cannot exist at a suﬃciently large D, when one ﬁxes γ. By Proposition 3 of Ref. [24], when γ > 0, any nonzero global minimum must obey the following two inequalities: 1 d0 \u0004 γ SSE[xy]SS \t 1 D−1 ≤ b∗ ≤ \u0004 SSE[xy]SS d0(σ2 + d0)Damax \t 1 D+1 , (69) where amax is the largest eigenvalue of A0. In the limit D → ∞, the lower bound becomes 1 d0 \u0004 γ SSE[xy]SS \t 1 D−1 → 1 d0 . (70) The upper bound becomes \u0004 SSE[xy]SS d0(σ2 + d0)Damax \t 1 D+1 → 1 σ2 + d0 . (71) But for any σ2 > 0, 1 d0 < 1 σ2+d0 . Thus, the set of such b∗ is empty. On the other hand, when γ = 0, the global minimizer has been found in Ref. [19] and is nonzero, which implies that L(0) < E[y2]. This means that L(γ) is not continuous at 0. This completes the proof. j 16","libVersion":"0.3.2","langs":""}
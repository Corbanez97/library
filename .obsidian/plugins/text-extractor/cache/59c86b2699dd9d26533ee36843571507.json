{"path":"Books and Papers/Optics/ismar-2018-focusar_cam_ready_compressed.pdf","text":"See discussions, st ats, and author pr ofiles for this publication at: https://www.r esearchgate.ne t/publication/327631307 FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real and Virtual Article  in  IEEE Transactions on Visualization and Computer Graphics · September 2018 DOI: 10.1109/TV CG.2018.2868532 CITATIONS 70 READS 3,790 4 author s, including: Praneeth Chakravarthy University of North Carolina at Chapel Hill 53 PUBLICATIONS   1,023 CITATIONS    SEE PROFILE David Dunn Qualcomm 7 PUBLICATIONS   402 CITATIONS    SEE PROFILE Kaan Akşit University College London 95 PUBLICATIONS   1,600 CITATIONS    SEE PROFILE All content following this page was uploaded by Praneeth Chakravarthy on 09 No vember 2018. The user has requested enhancement of the downloaded file. FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real World and Virtual Imagery Praneeth Chakravarthula, David Dunn, Kaan Aks¸it and Henry Fuchs Life Fellow, IEEE Fig. 1. Left: The view of both real and virtual for a presbyopic user with distant vision, with the accommodation depth ﬁxed at 7 m, on a conventional AR HMD. The virtual bunny is at a mid-distance (1 m) together with the stamp (0.25 m), text book (1 m) and bicycle (5 m) arranged at near, medium and far distances respectively. Notice that both the real and virtual imagery appears blurred to the user as none of the objects are in the presbyopic user’s accommodation plane. Middle: A presbyopic user with near zero accommodation range looking through our auto-focus AR eyeglasses. Our prototype AR eyeglasses are capable of providing well-focused imagery of both real and virtual objects at all depths by independently adjusting for the user focus for viewing both real world and virtual imagery from the internal display, based on the user’s current eye accommodation state. Right: The well-focused view of both real and virtual objects of the same scene on our auto-focus AR eyeglasses, due to independent focus adjustments for both real and virtual. Notice that the letters on the textbook at the mid-distance (1 m) are in sharp focus, as well as the virtual bunny, which also is set to appear at the mid-distance. Abstract— We describe a system which corrects dynamically for the focus of the real world surrounding the near-eye display of the user and simultaneously the internal display for augmented synthetic imagery, with an aim of completely replacing the user prescription eyeglasses. The ability to adjust focus for both real and virtual stimuli will be useful for a wide variety of users, but especially for users over 40 years of age who have limited accommodation range. Our proposed solution employs a tunable-focus lens for dynamic prescription vision correction, and a varifocal internal display for setting the virtual imagery at appropriate spatially registered depths. We also demonstrate a proof of concept prototype to verify our design and discuss the challenges to building an auto-focus augmented reality eyeglasses for both real and virtual. Index Terms—Augmented Reality, Displays, Auto-focus, Focus accommodation, Prescription correction 1 INTRODUCTION Introduction to VR and AR: Enhancing Virtual Reality (VR) has been identiﬁed by the National Academy of Engineering (NAE) as one of the fourteen grand challenges for engineering in the 21st century, along with challenges such as “reverse engineer the brain”, “provide energy from fusion”, and “secure cyberspace” 1. Augmented Reality (AR) is the form of VR in which users see the synthetic “virtual world” imagery overlaid and merged with their real-world surroundings. Many • Praneeth Chakravarthula is with UNC Chapel Hill. E-mail: cpk@cs.unc.edu. • David Dunn is with UNC Chapel Hill. E-mail: dunn@unc.edu. • Kaan Aks¸it is with NVIDIA Research. E-mail: kaksit@nvidia.com. • Henry Fuchs is with UNC Chapel Hill. E-mail: fuchs@cs.unc.edu. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx 1http://www.engineeringchallenges.org/9140.aspx experts predict that VR, and especially AR, will be the next mass market platform taking the place of PC’s, laptops, and mobile phones. Vision of AR: If AR is indeed to be the next platform, then AR systems will have to be comfortable enough to be worn for long periods, perhaps all day, like a pair of ordinary eyeglasses. Otherwise people will just continue to carry their smartphones in their pockets. If AR does become like one’s everyday prescription eyeglasses, then it will be used for both 2D content (messages, reminders) and 3D geolocated content (directions to restaurants, virtual/remote participants walking around the room with local people; e.g. holoportation [32]). In all these situations, the user wants to have a comfortable view, simultaneously, of both real and virtual worlds. Many design and computational challenges have to be overcome for such eyeglass-style AR systems to become a reality: low latency tracking and rendering, low power, a display with wide ﬁeld of view in a compact form factor. The concentration of this paper is on a crucial, under-investigated aspect of AR systems: focal accommodation for real and augmented imagery. Fig. 2. Mismatch of virtual and real world focus cues in AR for existing conventional AR displays due to all virtual objects placed at a ﬁxed depth plane. [Left] Since the virtual images are placed at a farther depth plane than the book, book appears in focus and the virtual image tags on the book are out of focus [Right] The virtual image tags on the book appear sharp when focused at a farther depth, but now the book is out of focus. Importance of focus cues in VR: The distance to which the eyes must converge for both to ﬁxate upon the same point in space is called the vergence distance, and the distance to which the eyes must accommodate to bring the image of that point in space to sharp focus is the focal or accommodation distance. Vergence and accommodation are neurally coupled, i.e. as the vergence angle changes, the eye adjusts its accommodation depth, thus bringing the scene into focus. Proper matching between vergence and accommodation are important in VR. If not matched, then the conﬂict of vergence distance and accommoda- tion depth, called the vergence-accommodation conﬂict (VAC), causes fatigue and discomfort [14, 23, 39]. Importance of focus cues in AR: Proper matching between vergence distance and accommodation depth is even more important in AR than VR because in addition to the requirements of VR, in AR the real world has to be closely matched to the virtual world. If the virtual images co-located with real world objects are not at the same focal depth, the disparity in blur forces the user to change focus upon gazing at objects that should be completely sharp (see ﬁgure 2). VAC with prescription eyeglasses and adaptation: For users who need corrective lenses in their everyday lives (“near-sighted”, “far- sighted”), the situation is even more complex, because these users already have to deal with VAC even without AR or VR [39]. Consider a “near-sighted” user who can comfortably verge and accommodate to, say, 0.5 meters, but needs corrective lenses to focus clearly at objects at 10 meters. When he ﬁrst uses the corrective “distance” lenses, an object at 10 meters appears in focus (because to his eyes, it is at 0.5 meters), but he will verge to 0.5 meters, giving him “double vision.” Only after many hours, days or even weeks of wear, does his vision system gradually adapt to verging at 10 meters while still accommodating to 0.5 meters. Some users never become adapted to such a large VAC [4]. Over generations, opticians have empirically studied the range of VACs (“zone of clear single binocular vision”, “zones of comfort” [6, 11]) which are tolerable and to which most users can adapt2. Vision correction for presbyopes and AR: When donning a head mounted display (HMD), users requiring vision correction still need to wear their corrective lenses. A few AR displays, such as Lumus DK- 32 3, provide physical space between the user’s eyes and the display, for ﬁtting prescription lenses. For presbyopes (people over 40 years 2Refer to pg.20: https://www.aoa.org/documents/optometrists/CPG-17.pdf 3https://lumusvision.com/products/dev-kit-50/ of age), who account for about 40% of US population4, this does not solve the problem because the user’s range of focus is restricted by the focus range of the lenses being worn at any moment - for instance “reading” glasses or “driving” glasses. Installing bifocals, trifocals, or progressive lenses merely puts a particular distance in focus at one vertical angle, forcing the user to tilt their head up or down to bring in focus a real-world object that is at a particular distance. Inventors since at least Benjamin Franklin have tried to solve the problem of getting objects at all distances to be in focus, but even the most recent offerings require the user to turn a focus knob on the lens (e.g. Alvarez lens) to adjust the depth of the focal plane - an unacceptably awkward requirement for most users. New opportunity with AR displays: Today’s and future AR dis- plays offer an opportunity to ameliorate vergence-accommodation con- ﬂict by taking advantage of the capabilities already on AR displays: powerful processors and outward looking depth sensing for tracking and hand-based user interaction (e.g. Hololens, Meta). If rapid, ac- curate, and robust binocular eye tracking were added, a system could measure the user’s object of attention in both the real and virtual world. Then adaptive focus could be added to the real world (external) view and separate adaptive focus for the virtual world (internal) view to bring into focus both the real and virtual imagery. Such a display could also operate as auto-focus prescription eyeglasses, with the virtual content turned off. Current challenges for auto-focus AR glasses: Building an AR display that presents well-focused images, both real and virtual, and near and far, requires overcoming two major challenges: 1. Designing a display that can dynamically adjust focus for both the internal display (showing rendered synthetic content) and the external real world scene. We address this problem by using tunable-focus lenses for external scene and a varifocal beam combiner design for the internal display. 2. Robust eye tracking to determine the user’s current gaze ﬁxation. Note that we do not implement eye tracking in our current version of the prototype. 1.1 Contributions The most important insight of this work is that an augmented reality display needs to have the capability of dynamically supporting appro- priate focus cues not only for the virtual content but also for viewing the 4https://www.census.gov/prod/cen2010/briefs/c2010br-03.pdf real world, in order to cater to larger audience. Future AR displays are targeting an eyeglasses form factor and providing for on-board compu- tation integrating several outward looking sensors. With an auto-focus capability for both real and virtual imagery, the AR eyeglasses could potentially replace conventional user prescription eyeglasses. Speciﬁc contributions of our work are as follows: 1. We present a framework for analyzing the retinal blur caused by AR displays, both for real and virtual objects out of focus. We use this to understand and characterize the requirements for AR displays to present well-focused real and virtual imagery. 2. We propose, design and fabricate a prototype auto-focus AR eyeglasses from in-house custom 3D printed components as well as off-the-shelf consumer electronics, and drive the display in real-time. 3. We demonstrate that with independently adjustable focus, for viewing both real world as well as virtual content, perceived image quality improves for users with lower order aberrations in eye along with those with normal vision across all depths. Note that astigmatism is not corrected in the current version of the prototype. 2 BACKGROUND AND RELATED WORK 2.1 Zone of Comfort We propose to correct for the user’s vision for the real world as well as the virtual world. Of all correctable visual imperfections, refractive errors are the most common 5 and most pertinent to AR displays. Correcting such errors for viewing both digital and real content is of primary concern. As discussed in section , all corrective prescription lenses introduce a certain amount of VAC. People have known for a long time that there is a zone of clear single binocular vision and zones of comfort for tolerable VAC. [6, 11] Percival [34] and Sheard [37] proposed that the spectacle prescriptions should utilize prisms and lenses to place natural stimuli inside the zone of comfort and thus mitigating VAC in prescription glasses. However, people adapt to this VAC over time, with most people taking a couple of weeks to get used to the new prescription. It is to be noted that presbyopes with corrective lenses adapt to two or more different corrective lens powers simultaneously.6 This gives us more conﬁdence that people will be comfortable with dynamic focus. 2.2 Adjustable-focus lenses An AR HMD that dynamically adjusts for user accommodation state, both for real and virtual content, requires optical elements that are tunable in focus. Here we review designs of adjustable-focus lenses and investigate their characteristics. Lin and Chen [25] proposed electrically focus-tunable, polarizer- free, liquid crystal lenses for ophthalmic applications. By varying the applied voltage, a range of positive and negative focal lengths were achieved. Wei et al. [40] proposed an electroactive liquid lens design which is driven by an annular membrane, but suffered from a limited aperture size. A large aperture focus tunable liquid lens was achieved by changing the curvature of the lens using shape memory alloy spring by Hasan et al. [13]. Wei et al. [41] designed an electrically tunable liquid lens with an aspheric membrane and improved central and peripheral resolutions at high diopters. These designs, with spherical curvatures, suffer spherical aberrations along with gravity induced coma, which is common for all liquid lenses. Hasan et al. [12] proposed a piezo- actuated piston based mechanically tunable liquid lens, which can control coma aberration but compromises on the maximum achievable optical power of the lens. A range of tunable liquid lenses are also commercially available 7. An adaptive, focus-tunable lens for vision correction in AR needs to eliminate spherical, chromatic, comatic and 5http://www.who.int/mediacentre/factsheets/fs282/en/ 6Refer to pg.20 of https://www.aoa.org/documents/optometrists/CPG-17.pdf 7http://www.optotune.com/technology/focus-tunable-lenses other aberrations present in existing optical designs. Though current commercially available tunable-focus lenses have limited aperture sizes, the developments as mentioned here promise wide aperture tunable- focus lenses that could be used for prescription correction in near future. 2.3 Vergence Accommodation Conﬂict (VAC) Vergence and accommodation are neurally coupled. [10,36] This neural coupling is useful in the real world since vergence distance and ac- commodation distance are almost always equal. Several experimental studies have attempted to more fully understand the effects of VAC on viewing stereoscopic content in VR. Padmanaban et al. [33] evaluated the user comfort by changing the display focal state using focus tunable lenses based on the eye accommodation state measured by an autore- fractor device. An accommodation invariant display design was used by Konrad et al. [20], making the imagery appear always in focus while supporting vergence to mitigate VAC to an extent. Koulieris et al. [21] evaluated the feasibility of several existing designs for alleviating VAC and found that the most effective way to eliminate VAC is driving accommodation by getting focus cues correct or nearly correct. The study also showed that other proposed solutions like DoF rendering and monovision do not drive accommodation accurately and therefore do not minimize VAC in VR as much as one would hope. Advantages of mitigating VAC in virtual reality displays can be heavily user dependent - for example, users with focal imperfections might not see a signiﬁcant advantage as compared to users with normal vision [33]. The effects of VAC on AR have not, to our knowledge, been reported in the literature. AR poses additional challenges because the physical world is in view, and so the rendered synthetic content should match the depth of physical world. 2.4 Focus Supporting Near-eye Displays Light ﬁeld and Holographic displays: Light ﬁeld and/or Holo- graphic displays are ideal, as they support (at least conceptually) all focus cues, statically, for all states of user accommodation, (i.e. the user can focus at any depth at any time) but all have serious shortcomings. Lanman and Luebke [24] demonstrated a VR near-eye light ﬁeld display (NELD) design, extending on the concepts of integral imaging ﬁrst introduced by Lippmann [26]. Unfortunately, the perceived reso- lution of the imagery is an order of magnitude lower than the original display. Maimone and Fuchs [28] used a stack of LCDs to present time multiplexed attenuation patters to generate a light ﬁeld at the eye. Huang et al. [17] demonstrated a similar factorized compressive lightﬁeld display built on conventional VR NED design. A freeform see-through NED based on integral imaging method was proposed by Hua and Javidi [16] which supported lightﬁelds in a limited FoV (33.4°). Maimone et al. [30] introduced and demonstrated a very wide FoV (110°diagonal) see-through NED, but lack of eye-tracking limited the effective resolution and the available LCD components degraded the image quality with diffraction effects. More recent designs by Maimone et al. [29] and Shi et al. [38] use holography with ﬁne depth control and high image quality with a wide FoV, but have shortcomings with respect to eyebox size and required computation. Multifocal and Varifocal displays The lack of any fully useful design of light ﬁeld or holographic displays has encouraged researchers to explore other designs based on multi-focal or variable-focal ideas - either multiple depth planes displayed simultaneously or a single depth plane moved dynamically in depth with a computational blur (e.g. including chromatic aberration ocular blur [1, 5, 22]) approximation of items away from the current depth. Akeley et al [2] used multiple display planes to generate approximate focus cues, without eyetracking, on a large format display. Hu et al. [15] extended the idea of multi-plane desktop displays to near-eye displays by demonstrating a freeform optical design supporting high resolution imagery. Recent work of Matsuda et al. [31] demonstrate the idea of deforming a plane to non-planar surface to nearly match the scene depth in VR, thereby generating more correct focus cues, unfortunately with low frame rate (46 sec per frame) and limited color. Above designs have small FoV and are currently bulky. Recently Dunn et al. [9] demonstrated a varifocal display with a wider FoV (100°diagonal) using deformable beamsplitters, whereas Aks¸it et al. [3] used see-through holographic diffuser screens to achieve a similar varifocal capability with wide FoV. Other techniques like monovision and use of focus tunable lenses [19] have also been proposed for mitigating VAC in VR NEDs. Huang et al. [18] demonstrated that correction for refractive errors can be encoded into a light ﬁeld display by predistorting the image pre- sented, eliminating the need to wear glasses. But none of the existing display designs actively correct for the imperfections in the vision of the users for viewing real world and no one, to the best of our knowledge, has addressed the challenge of providing correct focus cues for both real and virtual scenes simultaneously. 3 FOCUSAR ARCHITECTURE In this section we present a framework for describing the requirements of auto-focus in augmented reality eye glasses. The mathematical model helps us to calculate the best possible conﬁguration for present- ing well-focused imagery for both real and virtual in an augmented reality display. Section 3.1 presents an in-eye image formation model and discusses the circle of confusion on the retinal plane, which is directly related to the visual acuity [27, 35]. Section 3.2 discusses image formation for users with normal vision as well as users with refractive errors in an AR setting, with the limitations of conventional AR head-mounted displays (HMDs). Section 4 explores the design requirements for the internal display to present correct virtual imagery and the external vision correction to correct the real world for an auto- focus AR display for all users. Note: The display contained in the HMD and used for displaying gen- erated virtual imagery is referred to as the internal display, the virtual image plane formed as virtual image, and the real world outside the HMD is referred to as external view henceforth. 3.1 Focused image formation of real world objects It is common knowledge that the eyes can focus at various distances by changing the shape of its lens, thereby changing the effective focal distance. This process of changing the focal power to bring objects into focus on the retina is called accommodation. However, one must note that the eye lens (crystalline lens) is not an ideal lens, but has (many) associated aberrations [27]. Real lenses like that of the eye do not focus all rays precisely to one point. Therefore, even at best focus, a point is imaged as a spot on the image plane, which is called a circle of confusion (CoC). In the human eye, with the cornea along with aqueous and vitreous humor, and crystalline lens acting together as lens and retina as image plane, the CoC is the spot a real world point makes on the retina. For a given point source distance and the focal state of the eye, the diameter of the blur spot formed in the retinal plane can be calculated from the principles of similar triangles. As shown in the ﬁgure 3 consider a situation where an eye brings into focus a point source located at a distance d f ocus from the crystalline lens, on the retinal plane located at a distance dretina. Let us suppose that the focal length of the eye lens for the above conﬁguration is f . Since the eye is focused at a distance d f ocus, another point source located at dstimulus will appear out of focus and is imaged on the retinal plane as a blur circle of diameter Dblur. The defocus disk or the blur circle on the retinal plane can be thought of as the image formed by the crystalline lens, of projection of the cone of light from the point source at distance dstimulus on the focus plane located at d f ocus. For a pupil opening of size Apupil, the diameter Dpro j of projection of the point source in the stimulus plane onto the focus plane can be calculated using similar triangles as Dpro j = Apupil | dstimulus − d f ocus | dstimulus (1) From geometric optics, it is known that the size of the image on the image plane is equal to the object size multiplied by magniﬁcation of the system, so the size of blur circle Dblur on the image plane is given by Dblur = Dpro jm (2) Fig. 3. A simple model of focused image formation on the retinal plane of the eye. For a given lens focal state and the size of the pupil opening, an object present outside of the focus plane causes a defocus blur on the retina. The size of this blur circle can be used to analyze the sharpness of the image formed on the retina, and this model can be used to understand and characterize the requirements of well-focused real and virtual imagery in an AR display. where the magniﬁcation m of the given system can be obtained as m = dretina d f ocus (3) Although the distance to the retina from the crystalline lens plays a pivotal role in determining the blur circle, as can be seen from the above equation, since the distance dretina can be assumed to be constant for all accommodation states of the eye, the magniﬁcation of the system can be derived from the lens equation as 1 f = 1 d f ocus + 1 dretina (4) dretina = f d f ocus d f ocus − f (5) m = f d f ocus − f (6) which depends on d f ocus and f , two important parameters that are to be considered for designing an AR display. However, note that for a real world object, changing f automatically results in a change in d f ocus. Substituting equation 6 in equation 2 gives the resultant diameter of the defocus disc as Dblur = Apupil | dstimulus − d f ocus | dstimulus ( f d f ocus − f ) (7) Notice that the blur circle depends on the focus plane, stimulus object plane, pupil opening and the current eye lens accommodation state (or the current focal length of the crystalline lens). For the stimulus to appear in sharp focus, the circle of confusion needs to be minimized on the retinal plane. We can deﬁne a maximum CoC which is the largest defocus spot that is indistinguishable from a point for a human eye when imaging a point source, as the point threshold. If the point threshold is c, then the images formed on the retinal plane are considered to be in sharp focus if Dblur ≤ c (8) The range of distances dstimulus that satisfy the equation 8 deﬁne the Depth of Field (DoF) of the human eye for the focus plane at d f ocus, within which the images formed on the retina are in focus. Observe that the DoF depends on three factors: the current focal state of the Fig. 4. Field of view comparison between trifocal glasses (top) and auto- focus AR HMD (bottom). A presbyopic person wearing trifocals need to tilt their head to view through the correct segment of the lens, to bring a range of distances into focus within a part of the total ﬁeld of view. On the other hand, auto-focus AR eyeglasses promise to dynamically adjust for the user’s vision providing well-focused imagery of both real and virtual objects, across the entire ﬁeld of view. crystalline lens, distance to the plane of focus of the eye and the size of the pupil opening. For objects to be well-focused on the retinal plane, from equation 8, there are two possibilities: • The size of the pupil opening is small enough such that equation 8 is satisﬁed for all depth ranges. Unfortunately, the size of the pupil varies heavily depending on the ambient light and several other physiological factors causing a varied depth of ﬁeld effect for a given eye lens power. An external, artiﬁcial pinhole aperture model can be employed, which has nearly inﬁnite depth of ﬁeld and hence everything appears in focus, but having a small aperture limits the incoming light causing the image formed on the retina to be very dim and thus is not practical. • A continuously varying focal length f , and hence the location of the plane of focus d f ocus, such that any stimulus located at dstimulus causes an acceptable blur disk, resulting in well-focused images. Notice that this ability of eye to change the shape of its lens results in dynamic focus adjustments and a continuously variable focal length, bringing a range of depths into sharp focus. 3.2 Image formation in AR for different user groups We have discussed the blur circle on the retinal plane and the circle of least confusion which is used to determine if the images formed on the retina are in sharp focus. Now we review the conventional AR HMD and its limitations, and build towards a more practical AR HMD design. Conventional AR HMDs place an augmented view over a real world view by using simple optics (e.g. beam splitter) reﬂecting an internal display (e.g. liquid crystal display (LCD)) worn on the forehead (e.g. Meta 2 8). More advanced designs use complex optics such as waveg- uides to optically place the virtual image at a distance from the eye, like Microsoft Hololens 9. In either case, notice that the augmented virtual image is placed at a ﬁxed distance from the eye, similar to that of a conventional VR HMD. We will now deﬁne two types of blur disks to independently analyze the images formed by the real world objects and the augmented images formed by the internal display. The “Real World Blur” (Dblur; real) is the blur caused by an external real object when the user’s eyes are 8http://www.metavision.com/ 9https://www.microsoft.com/en-us/hololens ﬁxated to the virtual image shown on the internal display, and is deﬁned as Dblur; real = Apupil | dstimulus; real − d f ocus; virtual | dstimulus; real ( f d f ocus; virtual − f ) (9) where dstimulus; real is the real object distance from the eye and d f ocus; virtual is the distance to the virtual image plane which the user focus is set to. For the real world object to appear sharp when the user is ﬁxated on the virtual image, Dblur; real ≤ c (10) The blur seen on the internal display when the user is ﬁxated on a real world target is hereby called the “Virtual Image Blur” Dblur; virtual and is deﬁned as Dblur; virtual = Apupil | dstimulus; virtual − d f ocus; real | dstimulus; virtual ( f d f ocus; real − f ) (11) where dstimulus; virtual is the distance at which the augmented virtual image appears, whereas d f ocus; real is the distance to the real world object to which the user’s focus is set. For the virtual image to appear sharp when the user is focused to a certain real world target at d f ocus; real, the virtual image blur should be such that Dblur; virtual ≤ c (12) When the user ﬁxates on the virtual imagery displayed by the internal display, the focus plane of their eye is set to the depth of the virtual image, i.e. d f ocus; virtual = dvirtual image This means that for a given maximum circle of confusion size c which is still perceivable as a single point for the eye, there is only a small range of real world stimuli distances dstimulus; real that satisfy equation 10, resulting in well-focused real world images. As one might guess, changing the focus to a real world object at dstimulus, i.e. d f ocus; real = dstimulus would increase the virtual image blur Dblur; virtual (the blur spot result- ing from the internal display) making the virtual image appear out of focus, which is indeed not intended. Moreover, users with refractive errors would not be able to see either one or both of virtual and real imagery without their prescription lenses. Though this is not a big problem for Myopic and Hyperopic users given that AR eyeglasses can have slots to slide in prescription lenses, the real issue arises for Presbyopic users since the use of bifocals or progressive lenses limit the effective ﬁeld of view per depth range as shown in ﬁgure 4. 3.2.1 Users with normal vision People with normal vision can accommodate a wide range of distances, meaning that the crystalline lens of their eyes can have a continuously varying focal length f . Therefore, it can be observed from equations 7 and 8 that much of the target depths can be brought into sharp focus, i.e. Dblur ≤ c for a wide range of real world stimuli. However, for a given virtual image plane, the range of real world object distances, dstimulus; real, that satisfy equation 10 is limited, meaning that not all real world objects can appear in focus simultaneously with the virtual imagery. To have both real and virtual in focus simultaneously, from equations 9 and 11, the distance at which the virtual image appears (or simply, the virtual image focus) needs to change on demand when the user accommodates to various distances in the real world. 3.2.2 Users with Myopia or Hyperopia Myopic or Hyperopic people experience a lens power offset from the normal range. Being unable to focus at far distances in the former case and near distances in the latter, they experience a shifted accommoda- tion range. From equation 7, if a ﬁxed term equal to their prescription is added to the range of values the focus of the crystalline lens f , it can User vision type External real world correction Internal AR display correction Normal-vision No correction Dynamic Myopic (‘near-sighted’) Static Dynamic (Offset by prescription) Hyperopic (‘far-sighted’) Static Dynamic (Offset by prescription) Presbyopic (‘limited accommodation’) Dynamic Static Table 1. Comparison of focus adjustment requirements for different users for viewing well focused imagery of both real and virtual objects at all distances. be observed that not all real world depths can appear in sharp focus. The standard method of correcting for Myopia or Hyperopia is by using negative and positive dioptric power prescription lenses respectively, thereby optically shifting the object of interest by a distance corre- sponding to the corrective lens power. However, in an AR setting, both real world blur and virtual image blur sizes need to be below the point threshold for both real and virtual to appear in focus simultaneously. Therefore, the power of the corneal-crystalline lens system needs to be corrected as Pcorrected = Peyelens + Pprescription (13) such that Dblur ≤ c for f = 1 Pcorrected , bringing the real world object into focus. Replacing f with fcorrected in equation 11 yields Dblur; virtual = Apupil | dstimulus; virtual − d f ocus; real | dstimulus; virtual ( fcorrected d f ocus; real − fcorrected ) (14) As can be clearly seen from the above equation 14, for the given fcorrected value the virtual image distance dstimulus;virtual needs to be adjusted such that Dblur;virtual ≤ c for a well-focused virtual image. Therefore, it is necessary that the focus is corrected for both real and virtual imagery, i.e. correcting for the offset eye focus range and then placing the virtual imagery at the new corrected optical depth for the eye. Similarly, it is to be observed that one can also place the virtual image at the same physical depth as the real world object, and correct the eye focus once for both. Choosing one of the above two approaches is largely a design choice for the AR display. 3.2.3 Users with Presbyopia Presbyopia is the refractive error caused by losing the plasticity of the crystalline lens, typically associated with aging of the eye. Due to the hardening of the lens, the eye loses its ability to change its shape resulting in a nearly ﬁxed focus plane, i.e. f = f f ixed (Note that some accommodation is retained in a presbyopic eye, in reserves 10). Similar to the focus correction as mentioned in section 3.2.2, presbyopia can be corrected with prescription lenses. However, unlike the case with myopia or hyperopia where some plasticity of the eye lens still remains, presbyopic users need different prescription correction for various depth ranges. Hence they generally choose multifocal lenses or progressive lenses, or sometimes employ monovision solutions. We want to point that the multifocals or progressive lenses offer a small ﬁeld of view (FoV) for near and far segments of the lens, and require the user to tilt their head to bring the gaze into the right segment to focus on an object. For an augmented reality HMD, a small FoV and constant head tilt is not something that is desired. With a ﬁxed eye focus, the blur equation for a presbyopic eye is as follows Dblur = Apupil | dstimulus − d f ocus; f ixed | dstimulus ( fcorrected d f ocus; f ixed − fcorrected ) (15) 10https://www.aoa.org/documents/optometrists/CPG-17.pdf where fcorrected is similar to equation 13. But notice that in case of myopia or hyperopia, since signiﬁcant accommodation is left in eye, a static focus correction for external view is sufﬁcient. In case of presbyopia, since the accommodation is nearly ﬁxed, the correction needs to be dynamic for well-focused real world images, i.e. Dblur ≤ c. On the other hand, the virtual image blur can be computed as Dblur; virtual = Apupil | dstimulus; virtual − d f ocus; f ixed | dstimulus; virtual ( f f ixed d f ocus; f ixed − f f ixed ) (16) Observe that since the focus of the eye and hence the focus plane d f ocus is ﬁxed, it is sufﬁcient to display the virtual imagery at the ﬁxed focal plane of the eye for a perfectly in-focus virtual image for a presbyopic user. One could also place all virtual imagery at the real world object depth and correct for both real and virtual at once. But this approach would only result in an extra complexity in driving the focus of the internal display, which can be avoided. In this section we have seen the various focus adjustment require- ments for users with and without any refractive errors for seeing well- focused imagery, both real and virtual, in an AR display. These insights are summarized in table 1. 4 FOCUSAR DISPLAY DESIGN From table 1 it can be seen that for an AR display to support focus cues for both real and virtual, we need independent adjustments for the virtual image depth and the real world vision correction. In this section we discuss our design choices for external dynamic prescription correction and focus supporting internal display. Vision correction for real world We have seen that while users with normal vision do not need any vision correction, users with any refractive aberrations in eye need external focus adjustments to bring real world objects into sharp focus. However, myopic and hyperopic users only need a static focus correction whereas presbyopic users need dynamic vision correction based on the depth of the object of interest. Therefore, for dynamically adjusting the external corrective lens power, we need a tunable-focus lens that can operate over a range of focal distances. With robust binocular eye gaze tracking, a multitude of outward looking cameras on the headset, and a prior knowledge of the degree of user’s refractive error in eye, we can determine the depth of the object of interest and adjust the focus of the external corrective lens accordingly, so as to bring the real world target into sharp focus. We note that commercial AR headsets like Microsoft Hololens already employ outward-looking cameras and trackers to analyze the ambient spatial environment, and future AR and VR HMDs are expected to have eye trackers integrated, so this work focuses on the optical correction systems. Focus supporting internal display The internal display should be capable of rendering objects at various depths while spatially registering them to the real world, providing the depth cues either statically or dynamically. Such an internal display could be one of the following two kinds: 1) lightﬁeld and holographic displays, which provide all depth cues statically by approximating the wavefront originating from PC DisplayPort™ 1.2 2560x1440 Frame Rate: 60 HzTopfoison TF60010A-V06\" DisplayOptotune Lens EL-10-30-TC-VIS-12D Human eye D e fo r m a b le Membrane Mirror Teensy 3.6 Microcontroller Pyle PLMRW8 8\" Speakers Release Internal Display USB 2.0 Motorola PX5010DP sure Sensor PeterPaul 72B11DGM Valve Optotune Electrical Lens Driver 4 USB 2.0-16 D Lens External Correction Fig. 5. An overview of our system and the components used. The two subsystems can be seen, on the left is the external vision correction system, and on the right is the internal augmented reality display system. a given point in space, or 2) varifocal display, which provides depth cues dynamically by bringing into focus one particular depth plane at any given instant. Note that providing depth cues statically ensures the correct retinal blur, whereas providing dynamic depth cues requires rendering objects away from the focus depth plane with appropriate amount of retinal blur. We would like to point that the internal display type is more of a design choice, and rendering will be dependent upon the internal display technology used. For Varifocal display a traditional rendering pipeline can be employed, with slight modiﬁcations to support com- putational blur and distortion correction. Lightﬁeld and holographic display pipelines are more complex and involve multi-viewport integral imaging and point based methods with Fresnel integration. Considering the computational complexity of rendering the appropriate wavefront at the eye, generating the synthetic scene spatially registered to the real world, and the current limitations on resolution and eyebox sizes respectively, we recommend a varifocal internal display similar to the one mentioned in Dunn et al. 2017 [9] which is currently computation- ally less expensive, offers wider ﬁeld of view and eyebox size, and a competitive resolution, all in real-time. To summarize, AR glasses with vision correction and dynamic focus capabilities can be said to have two tasks which need to be combined into a single system: 1. actively correcting the focus externally for real world viewing 2. actively correcting the focus of internal display and rendering virtual imagery with appropriate computational blur. 5 IMPLEMENTATION Here we present our hardware prototype of the proposed display design. It includes both vision correction for the real world objects and a focus supporting internal display for virtual objects, as well as a hardware and software for setting per user calibration and controlling each subsystem. 5.1 Hardware 5.1.1 External Vision Correction For correcting the external real world imagery, we have developed a vision correction module using tunable lenses capable of enabling sharp image formation at any depth for myopic, hyperopic and presbyopic users. Our prototype uses a fast electrically tunable lens as its core, with an additional offset lens enabling a total optic power range of -7.7 to +4 diopters(D). Our electrically tunable lens is an Optotune EL-10- 30-TC-VIS-12D, which has a static to static response time under 12 ms and a focal power range of 11.7 D. We drive the lens with an Optotune Electrical Lens Driver 4 which we control from a computer by using a custom control library written in Python programming language. The vision correction module is placed at 14 mm distance from the user’s eye enabling a monocular 37° FoV. This placement between the user and the internal display maximizes the available ﬁeld of view and enables the internal display to present virtual images which match the external real world depths because the vision correction module corrects focus for both real and virtual imagery simultaneously, which simpliﬁes the system as a whole. 5.1.2 Internal Display for Augmented Imagery Our internal display is a more reﬁned system from the varifocal display presented in Dunn et al. 2017 [7–9]. It relies on the technique of adjusting optical depth of a virtual image by dynamically adjusting the optical power of a semi-reﬂective membrane to match the gaze of a user. With respect to Dunn et al. 2017, we also have improved the form-factor of the internal display in our version. Our new prototype is based on the same optical conﬁguration, with improvements in optical quality and form-factor which have lead to a much smaller head-mounted volume (5.5 x 12.5 x 15.2 cm). We use a single Liquid Crystal Display (LCD) panel Topfoison TF60010A-V0 1440x2560 5.98” TFT LCD to provide imagery to both eyes. The deformable membranes for each eye are manufactured in house using the methodology described in our original proposal [9]. Our most recent implementation does not require air compressors and pressure regulators, instead, we use a Pyle PLMRW8 8” 400 Watt 4 Ohm Marine Subwoofer to modulate the air pressure in the membrane housing for each eye. A Motorola MPX5010DP pressure sensor pro- vides feedback on the current pressure differential between ambient atmosphere and inside our membrane housing, thus our system no longer uses power-draining cameras for pressure control. A PeterPaul 72B11DGM 12/DC solenoid valve allows for re-pressurizing the sys- tem as needed for leak correction, which in our observation typically occurs during continuous operation about thrice an hour. All pressure modules are connected with SMC Pneumatics ¼” OD Tubing, one touch ﬁttings, and T-junctions. We control the vacuum system with an Arduino Teensy 3.6 microcontroller, which uses a software PID con- troller to hold the membrane at the target depth based on the sensory inputs. 5.2 Software All software is implemented in-house in Python and OpenGL, except for controlling the microcontroller which is done using C. We use a custom developed library for simultaneously driving the Optotune focus tunable lens and the internal display, and use OpenGL with GLFW for rendering the synthetic imagery. Our software runs on an Intel Xeon CPU W5590 @ 3.33 GHz PC with an Nvidia GeFroce GTX 1080 Ti GPU and Linux operating system. 6 DISCUSSION In this section we provide an experimental assessment of our prototype auto-focus AR eyeglasses and discuss the capabilities and limitations of our proposed design. 6.1 Experimental Conﬁguration To test our prototype described in Section 5, a Canon Rebel T6i camera with EF 24-70 1:2.8 L USM lens was placed behind the vision cor- rection module looking through the display into the world, imitating the human eye. The distance between the camera lens and the vision correcting module was maintained close to 22 mm - slightly more than the typical distance between the eyes and eyeglasses. Due to the slightly larger camera distance, the ﬁeld of view in the captured images is worse than a user experiences. The aperture of the camera was set to f2.8. The camera setup is shown in ﬁgure A real world scene is created by placing a postage stamp at 0.25 m, a text book at 1 m and a bicycle at a distance of 5 m from the display. A polygonal model of the Stanford bunny is used for the virtual image and is rendered on the internal display using OpenGL. Two different kinds of users are simulated - user with normal Fig. 6. Prototype hardware and testing conﬁguration Left: Our prototype display with scene capture camera conﬁgured to approximate users with normal vision and presbyopia. Right: User side view of our prototype display consisting of vision correcting modules for external world focus adjustments, as well as deformable beamsplitter membranes to drive the focus of the internal display. vision and user with presbyopia having only distant vision. Speciﬁc details follow. 6.2 Results for User with Normal Vision To simulate a user with normal vision who has no loss in accommo- dation, the camera focus was continuously adjusted to bring various depths into focus while the vision correction module was turned off. Two different scenarios were simulated 1. user wearing a conventional HMD where the virtual image depth is ﬁxed at 1 m (i.e. at the text book) 2. user wearing a varifocal internal display where the virtual im- age depth was dynamically adjusted to match that of real world targets. The results captured in both these settings are reported in ﬁgure 7. Conventional AR HMD In the conventional HMD setting, it can observed that the virtual images clearly appear blurred when the user sets focus on the near postage stamp, since the virtual image depth does not match with the real world depth. However, since the virtual image depth is set to the depth of the text book, both the bunny and the text book can be seen in focus. This shows that conventional AR HMDs are not good enough for providing comfortable viewing experience of augmented content. Varifocal AR HMD On the other hand, when the internal display is switched to the varifocal mode, the virtual bunny could be set to near, medium and far distances matching the depths of the real world targets. It can be noticed that the bunny and the real world targets are always in focus at all depths. The bunny is scaled appropriately for near, medium and far distances to include perspective depth. 6.3 Results for Presbyopic User with Distant Vision To simulate a presbyopic user with distant vision, we set the camera focus ﬁxed at a distance of 7 m to simulate a hardened lens with nearly zero accommodation. The virtual bunny is rendered with appropriate scaling for near, medium and far distances. Four difference scenarios are simulated in this experiment 1. conventional AR HMD with no vision correction for external view and no dynamic focus adjustment for the internal display, with the focus of the virtual display set to medium distance (i.e. 1 m) 2. varifocal AR HMD, to simulate recent works on focus supporting near-eye displays, where the vision correction module is inactive but the virtual image depth can be adjusted dynamically Users with normal visionVarifocal AR HMDConventional AR HMD Near focus Mid focus Far focus Fig. 7. A user with normal vision is simulated by a camera with con- tinuously adjustable focus and a comparison between a conventional AR HMD and a varifocal AR HMD (Dunn et al. 2017) is made. The real world objects, postage stamp, text book and bicycle, are placed at near, mid and far distances of 0.25 m, 1 m and 5 m respectively. Top: A conventional AR HMD is mimicked by ﬁxing the virtual image plane at 1 m and the scene as viewed by a user with normal vision is simulated by adjusting the focus of the camera to various real world objects. It can be seen that various real world targets at near, medium and far distances are in focus, but the virtual bunny is in focus only at mid-distance. Bottom: With a varifocal display the virtual image depth can be dynamically set to match the depth of the real world object. Therefore it can be seen that both the real world objects as well as the virtual bunny are in focus for all near, mid and far distances. 3. adjusted static focus internal display with the depth of the virtual image adjusted to match that of the presbyopic user accommoda- tion (7 m in this case), but with external vision correction module inactive 4. our auto-focus AR eyeglasses mode, where the focus for both the internal display and the external real world view can be indepen- dently and dynamically adjusted All results from the above scenarios are reported in ﬁgure 8. Conventional AR HMD When the display is operated in conven- tional AR HMD mode, both real and virtual objects appear blurred as expected, since the virtual image depth is set at 1 m whereas the user focus is set to 7 m, beyond all the target distances either real or virtual. Varifocal AR HMD When the internal display is switched to var- ifocal mode the perceived image does not improve, unlike the case with previous experiment with the user having normal vision. This is because although the virtual image depths are spatially registered to the depths of the real world objects, the user’s accommodation does not match the depth of the objects. And unlike users with normal vision, presbyopes have nearly zero accommodation range. Adjusted Static Focus However, when the focus of the internal display is adjusted to be static at the accommodation depth of the user, the virtual imagery now comes into sharp focus. This is in accordance with the equation 12 discussed in section 3. However, the real world objects still appear out of focus. Our solution When the vision correction module of the display is turned on, the external corrective lens is also adjusted to match the user’s accommodation depth to that of the target object depth in the real world, while simultaneously adjusting the focus of the internal display to match the depth of the real world object. Therefore, it can be seen that both real and virtual imagery are in focus at all near, medium and far distances. Presbyopic user with fixed distant vision Near focus Mid focus Far focus Real VirtualConventional AR HMDNear focusFar focusDunn et al. 2017Near focusFar focusDunn et al. 2017Near focusFar focusOur solutionNear focusFar focus(focus set for presbyope) 0.25 m 1 m 5 m 7 m (stamp) (text book) (bicycle) (presbyope accommodation plane) Fig. 8. The views of real and virtual imagery as seen by a presbyopic user with distant vision, with the accommodation distance ﬁxed at 7 m is simulated by ﬁxing the camera focus at 7 m. The real world objects are placed at near, mid and far distances. Row 1: The virtual bunny focus is ﬁxed to a mid distance of 1 m to simulate a conventional AR HMD. The real and virtual objects at all near, mid and far distances appear out of focus for the presbyopic user in this case. Row 2: A Varifocal AR HMD (Dunn et al. 2017) can place virtual imagery at any depth dynamically. However, due to limited accommodation in a presbyope, providing correct virtual image depth does not result in well-focused virtual images. The presbyope cannot focus on real world objects without corrective lenses for each distance range, and hence both real and virtual imagery appear out of focus. Row 3: The distance of the virtual image plane is set to the ﬁxed distance matching the accommodation of the presbyopic user so the virtual images appear in focus whereas the real world objects all appear out of focus. Row 4: The view of both real and virtual for a presbyopic user as viewed from our prototype display. We adjust independently for the focus of both real world and the internal display so both the real world objects and the virtual bunny are in focus at all near, mid and far distances. These results verify that independent adjustments are needed for viewing both real and virtual content for presenting well-focused im- ages for a large pool of users. Also, with such auto-focus AR eyeglasses, the vision of presbyopes can be signiﬁcantly improved with the per- ceived image quality being close to that of a person with 20/20 vision at all depths. We do not discuss here the experiments related to near- sighted or far-sighted users since they fall within the spectrum of users with normal vision having a large range of accommodation and users with presbyopia with nearly zero accommodation range. Limitations The current implementation of the prototype has cer- tain limitations and are discussed here. Field of view The major limitation of our prototype is the available ﬁeld of view for viewing both real and virtual stimuli simultaneously. Although the internal display for viewing virtual imagery provides a ﬁeld of view of about 75° both horizontally and vertically, the small aperture size of 10 mm of the tunable focus lens limits the overall ﬁeld of view to 37°. However, we would like to point that the recent research on large aperture tunable focus lenses discussed in section 2.2 which is promising for an increased FoV in the near future. Eyetracking The current prototype does not integrate eyetracking due to hardware constraints - employing an eyetracker required users to be further away from the vision correction module, which decreased the usable ﬁeld of view. However, we plan to integrate eyetracking in the future version of our prototype with an increased aperture and FoV. 7 CONCLUSION AND FUTURE WORK We have demonstrated a proof of concept design for auto-focus aug- mented reality eyeglasses that can let a wide range of users appropri- ately focus on both real and virtual content simultaneously, a capability never before achieved to the best of our knowledge. We employ a deformable beamsplitter membrane varifocal display for presenting virtual imagery at spatially registered depths, and a tunable focus lens for dynamic prescription correction, when needed. Our early prototype demonstrates preliminary capabilities to display both rendered and real content in sharp focus for users with and without any refractive errors. However, the current system does not implement eyetracking and is limited by the ﬁeld of view. In future work, we plan to integrate eyetracking and outward looking cameras to make the system completely automatic, and increase the ﬁeld of view by employing wider aperture tunable focus lenses. We are excited by the possibility of future AR displays employing dynamic vision correction alongside focus supporting internal displays, enabling the promise of 20/20 vision at all distances, for all users, for both real world and virtual imagery. ACKNOWLEDGMENTS The authors thank Madhumita Mahadevan for fruitful discussions, and Jim Mahaney for helping with the physical setup. This research is supported in part by the BeingTogether Centre, a collaboration between Nanyang Technological University (NTU) Singapore and University of North Carolina (UNC) at Chapel Hill. The BeingTogether Centre is supported by the National Research Foundation, Prime Minister’s Ofﬁce, Singapore under its International Research Centres in Singapore Funding Initiative. REFERENCES [1] K. R. Aggarwala, E. S. Kruger, S. Mathews, and P. B. Kruger. Spectral bandwidth and ocular accommodation. JOSA A, 12(3):450–455, 1995. [2] K. Akeley, S. J. Watt, A. R. Girshick, and M. S. Banks. A stereo display prototype with multiple focal distances. In ACM transactions on graphics (TOG), vol. 23, pp. 804–813. ACM, 2004. [3] K. Aks¸it, W. Lopes, J. Kim, P. Shirley, and D. Luebke. Near-eye varifocal augmented reality display using see-through screens. ACM Transactions on Graphics (TOG), 36(6):189, 2017. [4] T. L. Alvarez, E. H. Kim, and B. Granger-Donetti. Adaptation to pro- gressive additive lenses: Potential factors to consider. Scientiﬁc Reports, 7(1):2529, 2017. [5] S. A. Cholewiak, G. D. Love, P. P. Srinivasan, R. Ng, and M. S. Banks. Chromablur: rendering chromatic eye aberration improves accommodation and realism. ACM transactions on graphics., 36(6):210, 2017. [6] F. C. Donders and W. D. Moore. On the anomalies of accommodation and refraction of the eye: With a preliminary essay on physiological dioptrics, vol. 22. New Sydenham Society, 1864. [7] D. Dunn, P. Chakravarthula, Q. Dong, K. Aks¸it, and H. Fuchs. 10-1: To- wards varifocal augmented reality displays using deformable beamsplitter membranes. In SID Symposium Digest of Technical Papers, vol. 49, pp. 92–95. Wiley Online Library, 2018. [8] D. Dunn, P. Chakravarthula, Q. Dong, and H. Fuchs. Mitigating vergence- accommodation conﬂict for near-eye displays via deformable beamsplit- ters. In Digital Optics for Immersive Displays, vol. 10676, p. 106760U. International Society for Optics and Photonics, 2018. [9] D. Dunn, C. Tippets, K. Torell, P. Kellnhofer, K. Aks¸it, P. Didyk, K. Myszkowski, D. Luebke, and H. Fuchs. Wide ﬁeld of view varifocal near-eye display using see-through deformable membrane mirrors. IEEE Transactions on Visualization and Computer Graphics, 23(4):1322–1331, 2017. [10] E. F. Fincham and J. Walton. The reciprocal actions of accommodation and convergence. The Journal of physiology, 137(3):488–508, 1957. [11] G. A. Fry. Further experiments on the accommodation-convergence rela- tionship*. Optometry & Vision Science, 16(9):325–336, 1939. [12] N. Hasan, A. Banerjee, H. Kim, and C. H. Mastrangelo. Tunable-focus lens for adaptive eyeglasses. Optics Express, 25(2):1221–1233, 2017. [13] N. Hasan, H. Kim, and C. H. Mastrangelo. Large aperture tunable-focus liquid lens using shape memory alloy spring. Optics express, 24(12):13334– 13342, 2016. [14] D. M. Hoffman, A. R. Girshick, K. Akeley, and M. S. Banks. Vergence– accommodation conﬂicts hinder visual performance and cause visual fatigue. Journal of vision, 8(3):33–33, 2008. [15] X. Hu and H. Hua. High-resolution optical see-through multi-focal- plane head-mounted display using freeform optics. Optics express, 22(11):13896–13903, 2014. [16] H. Hua and B. Javidi. A 3d integral imaging optical see-through head- mounted display. Optics express, 22(11):13484–13491, 2014. [17] F.-C. Huang, K. Chen, and G. Wetzstein. The light ﬁeld stereoscope: immersive computer graphics via factored near-eye light ﬁeld displays with focus cues. ACM Transactions on Graphics (TOG), 34(4):60, 2015. [18] F.-C. Huang, G. Wetzstein, B. A. Barsky, and R. Raskar. Eyeglasses-free display: towards correcting visual aberrations with computational light ﬁeld displays. ACM Transactions on Graphics (TOG), 33(4):59, 2014. [19] R. Konrad, E. A. Cooper, and G. Wetzstein. Novel optical conﬁgurations for virtual reality: Evaluating user preference and performance with focus- tunable and monovision near-eye displays. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp. 1211–1220. ACM, 2016. [20] R. Konrad, N. Padmanaban, K. Molner, E. A. Cooper, and G. Wetzstein. Accommodation-invariant computational near-eye displays. ACM Trans- actions on Graphics (TOG), 36(4):88, 2017. [21] G.-A. Koulieris, B. Bui, M. Banks, and G. Drettakis. Accommodation and comfort in head-mounted displays. ACM Transactions on Graphics, 36(4):11, 2017. [22] P. B. Kruger, S. Mathews, K. R. Aggarwala, and N. Sanchez. Chro- matic aberration and ocular focus: Fincham revisited. Vision Research, 33(10):1397–1411, 1993. [23] M. Lambooij, M. Fortuin, I. Heynderickx, and W. IJsselsteijn. Visual discomfort and visual fatigue of stereoscopic displays: A review. Journal of Imaging Science and Technology, 53(3):30201–1, 2009. [24] D. Lanman and D. Luebke. Near-eye light ﬁeld displays. ACM Transac- tions on Graphics (TOG), 32(6):220, 2013. [25] Y.-H. Lin and H.-S. Chen. Electrically tunable-focusing and polarizer- free liquid crystal lenses for ophthalmic applications. Optics express, 21(8):9428–9436, 2013. [26] G. Lippmann. Reversible events photos integrals. Academic Accounts- Academic, 146, 1908. [27] M. Lombardo and G. Lombardo. Wave aberration of human eyes and new descriptors of image optical quality and visual performance. Journal of cataract & refractive surgery, 36(2):313–331, 2010. [28] A. Maimone and H. Fuchs. Computational augmented reality eyeglasses. In Mixed and Augmented Reality (ISMAR), 2013 IEEE International Sym- posium on, pp. 29–38. IEEE, 2013. [29] A. Maimone, A. Georgiou, and J. S. Kollin. Holographic near-eye displays for virtual and augmented reality. ACM Transactions on Graphics (TOG), 36(4):85, 2017. [30] A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, and H. Fuchs. Pinlight displays: wide ﬁeld of view augmented reality eye- glasses using defocused point light sources. In ACM SIGGRAPH 2014 Emerging Technologies, p. 20. ACM, 2014. [31] N. Matsuda, A. Fix, and D. Lanman. Focal surface displays. ACM Transactions on Graphics (TOG), 36(4):86, 2017. [32] S. Orts-Escolano, C. Rhemann, S. Fanello, W. Chang, A. Kowdle, Y. Degt- yarev, D. Kim, P. L. Davidson, S. Khamis, M. Dou, V. Tankovich, C. Loop, Q. Cai, P. A. Chou, S. Mennicken, J. Valentin, V. Pradeep, S. Wang, S. B. Kang, P. Kohli, Y. Lutchyn, C. Keskin, and S. Izadi. Holoportation: Virtual 3d teleportation in real-time. In Proceedings of the 29th Annual Sympo- sium on User Interface Software and Technology, UIST ’16, pp. 741–754. ACM, New York, NY, USA, 2016. doi: 10.1145/2984511.2984517 [33] N. Padmanaban, R. Konrad, T. Stramer, E. A. Cooper, and G. Wetzstein. Optimizing virtual reality for all users through gaze-contingent and adap- tive focus displays. Proceedings of the National Academy of Sciences, p. 201617251, 2017. [34] A. S. Percival. The relasion of convergence to accommodation and its practical bearing. Ophthal. Rev., 11:313–328, 1892. [35] S. F. Ray. Applied photographic optics: Lenses and optical systems for photography, ﬁlm, video, electronic and digital imaging. Focal Press, 2002. [36] C. M. Schor. A dynamic model of cross-coupling between accommodation and convergence: simulations of step and frequency responses. Optometry & Vision Science, 69(4):258–269, 1992. [37] C. Sheard. The prescription of prisms. American Journal of Optometry, 11(10):364–378, 1934. [38] L. Shi, F.-C. Huang, W. Lopes, W. Matusik, and D. Luebke. Near-eye light ﬁeld holographic rendering with spherical waves for wide ﬁeld of view interactive 3d computer graphics. ACM Transactions on Graphics (TOG), 2017. [39] T. Shibata, J. Kim, D. M. Hoffman, and M. S. Banks. The zone of comfort: Predicting visual discomfort with stereo displays. Journal of vision, 11(8):11–11, 2011. [40] K. Wei, N. W. Domicone, and Y. Zhao. Electroactive liquid lens driven by an annular membrane. Optics letters, 39(5):1318–1321, 2014. [41] K. Wei, H. Huang, Q. Wang, and Y. Zhao. Focus-tunable liquid lens with an aspherical membrane for improved central and peripheral resolutions at high diopters. Optics express, 24(4):3929–3939, 2016. View publication stats","libVersion":"0.3.2","langs":""}
{"path":"Notes/Probability - The Science of Uncertainty and Data/slides/L06.pdf","text":"• Variance and its properties – Variance of the Bernoulli and uniform PMFs • Conditioning a r.v. on an event – Conditional PMF, mean, variance – Total expectation theorem • Geometric PMF – Memorylessness – Mean value • Multiple random variables – Joint and marginal PMFs – Expected value rule – Linearity of expectations • The mean of the binomial PMF LECTURE 6: Variance; Conditioning on an event; Multiple random variables • Readings: Sections 2.4-2.6 Lecture outline LECTURE 6: Variance; Conditioning on an event; Multiple random variables • Readings: Sections 2.4-2.6 Lecture outline Variance — a measure of the spread of a PMF Recall: E[g(X)] = ￿ x g(x)pX(x) • Second moment: E[X2]= ￿x x2pX(x) • Variance var(X)= E ￿(X − E[X])2￿ = ￿ x (x − E[X])2pX(x) = E[X2] − (E[X])2 Properties: • var(X) ≥ 0 • var(αX + β)= α2var(X) Variance — a measure of the spread of a PMF • Random variable X, with expectation E[X]= µ • Distance from the mean: X − µ • Average distance from the mean? • Possible (but cumbersome) measure of absolute distance from the mean: E[|X − µ|] • Deﬁnition of variance: var(X)= E[(X − µ)2] Recall: E[g(X)] = ￿ x g(x)pX(x) • Second moment: E[X2]= ￿ x x2pX(x) • Variance var(X)= E ￿ (X − E[X]) 2￿ = ￿ x (x − E[X]) 2pX(x) = E[X2] − (E[X]) 2 Properties: • var(X) ≥ 0 • var(αX + β)= α2var(X) Variance — a measure of the spread of a PMF • Random variable X, with expectation E[X]= µ • Distance from the mean: X − µ • Average distance from the mean? • Possible (but cumbersome) measure of absolute distance from the mean: E[|X − µ|] • Deﬁnition of variance: var(X)= E[(X − µ)2] Recall: E[g(X)] = ￿ x g(x)pX(x) • Second moment: E[X2]= ￿x x2pX(x) • Variance var(X)= E ￿(X − E[X])2￿ = ￿ x (x − E[X])2pX(x) = E[X2] − (E[X])2 Properties: • var(X) ≥ 0 • var(αX + β)= α2var(X) Variance — a measure of the spread of a PMF • Random variable X, with expectation E[X]= µ • Distance from the mean: X − µ • Average distance from the mean? • Possible (but cumbersome) measure of absolute distance from the mean: E[|X − µ|] • Deﬁnition of variance: var(X)= E[(X − µ)2] Recall: E[g(X)] = ￿ x g(x)pX(x) • Second moment: E[X 2]= ￿x x2pX(x) • Variance var(X)= E ￿(X − E[X])2￿ = ￿ x (x − E[X])2pX(x) = E[X 2] − (E[X])2 Properties: • var(X) ≥ 0 • var(αX + β)= α2var(X) var(X)= E[X2] − (E[X])2 var(X)= E ￿(X − E[X])2￿ = ￿ x (x − E[X])2pX(x) = E[X2] − (E[X])2 Standard deviation: σX = ￿var(X) var(X)= E[X 2] − (E[X])2 var(X)= E ￿(X − E[X])2￿ = ￿ x (x − E[X])2pX(x) = E[X 2] − (E[X])2 • Calculation, using the expected value rule, E[g(X)] = ￿ x g(x)pX(x) Standard deviation: σX = ￿var(X) Variance — a measure of the spread of a PMF • Random variable X, with expectation E[X]= µ • Distance from the mean: X − µ • Average distance from the mean? • Possible (but cumbersome) measure of absolute distance from the mean: E[|X − µ|] • Deﬁnition of variance: var(X)= E[(X − µ)2] Recall: E[g(X)] = ￿ x g(x)pX(x) • Second moment: E[X2]= ￿x x2pX(x) • Variance var(X)= E ￿(X − E[X])2￿ = ￿ x (x − E[X])2pX(x) = E[X2] − (E[X])2 Properties: • var(X) ≥ 0 • var(αX + β)= α2var(X) LECTURE 7: Conditional PMFs; independence of r.v.’s • Conditional PMFs – Conditional expectations – Total expectation theorem • Independence of r.v.’s • Properties of mean and variance under independence • The variance of the binomial • The hat problem: mean and variance • Random variable X,with mean µ = E[X] Properties of the variance: var(aX + b)= a2 var(X) • Notation: µ = E[X] • Let Y = X + b • Let Y = aX A useful formula: • var(X) ≥ 0 • var(αX + β)= α2var(X) Properties of the variance: var(aX + b)= a2 var(X) • Notation: µ = E[X] • Let Y = X + b • Let Y = aX A useful formula: • var(X) ≥ 0 • var(αX + β)= α2var(X) Properties of the variance: var(aX + b)= a2 var(X) • Notation: µ = E[X] • Let Y = X + b • Let Y = aX A useful formula: • var(X) ≥ 0 • var(αX + β)= α2var(X) Properties of the variance: var(aX + b)= a2 var(X) • Notation: µ = E[X] • Let Y = X + b • Let Y = aX A useful formula: • var(X) ≥ 0 • var(αX + β)= α2var(X) Properties of the variance: var(aX + b)= a2 var(X) • Notation: µ = E[X] • Let Y = X + b • Let Y = aX A useful formula: var(X)= E[X2] − ￿ E[X]￿2 • var(X) ≥ 0 • var(αX + β)= α2var(X) LECTURE 7: Conditional PMFs; independence of r.v.’s Properties of the variance var(aX + b)= a2var(X) • Conditional PMFs – Conditional expectations – Total expectation theorem • Independence of r.v.’s • Properties of mean and variance under independence • The variance of the binomial • The hat problem: mean and variance • Random variable X,with mean µ = E[X] LECTURE 7: Conditional PMFs; independence of r.v.’s Properties of the variance var(aX + b)= a2var(X) • Conditional PMFs – Conditional expectations – Total expectation theorem • Independence of r.v.’s • Properties of mean and variance under independence • The variance of the binomial • The hat problem: mean and variance • Random variable X,with mean µ = E[X] The simplest random variable: Bernoulli with parameter p ∈ [0, 1] X =    1, w.p. p 0, w.p. 1 − p • Models a trial that results in success/failure, Heads/Tails, etc. • Indicator r.v. of an event A: IA =1 iﬀ A occurs Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Special case: a = b constant/deterministic r.v. Variance of the Bernoulli Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = var(X)= E[X2] − (E[X])2 var(X)= E ￿(X − E[X])2￿ var(X)= ￿ x (x − E[X])2pX(x) = E[X2] − (E[X])2 var(X)= ￿ x (x − E[X])2pX(x) • Calculation, using the expected value rule, E[g(X)] = ￿ x g(x)pX(x) Standard deviation: σX = ￿var(X) var(X)= E[X2] − (E[X])2 var(X)= E ￿ (X − E[X])2￿ var(X)= ￿ x (x − E[X])2pX(x) = E[X2] − (E[X])2 var(X)= ￿ x (x − E[X])2pX(x) • Calculation, using the expected value rule, E[g(X)] = ￿ x g(x)pX(x) Standard deviation: σX = ￿ var(X) Variance of the uniform Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX(k) = P(X = k) = P(TT ··· TH) = (1 − p) k−1p, k =1, 2,... –geometric PMF Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X:— X(ω)= ω • Model of: complete ignorance ab Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X:— X(ω)= ω • Model of: complete ignorance ab Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X:— X(ω)= ω • Model of: complete ignorance ab Discrete uniform random variable; parameters a, b • Parameters: integers a, b; a ≤ b • Experiment: Pick one of a, a +1,... ,b at random; all equally likely • Sample space: {a, a +1,... ,b} • Random variable X: X(ω)= ω • Model of: complete ignorance ab 1 b − a +1 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X=number of coin tosses until ﬁrst head – assume independent tosses, P(H)= p> 0 pX (k) = P(X = k) = P(TT ··· TH) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P ￿ {ω ∈ Ω s.t.X(ω)= x} ￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 Variance of the uniform n Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Variance of the uniform n 1 n+1 Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Variance of the uniform n 1 n+1 01 Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Variance of the uniform n 1 n+1 01 Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Conditional PMF and expectation • Condition on an event A ⇒ use conditional probabilities • pX|A(x)= P(X = x | A) • E[X | A]= ￿ x xpX|A(x) 1 x pX (x ) 4 1/4 23 • Let A = {X ≥ 2} pX|A(x)= E[X | A]= Conditional PMF and expectation • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) 1 x pX (x ) 4 1/4 23 • Let A = {X ≥ 2} Conditional PMF and expectation • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) 1 x pX (x ) 4 1/4 23 • Let A = {X ≥ 2} Conditional PMF and expectation, given an event Example of conditiioning • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) Conditional PMF and expectation, given an event Example of conditioning • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) Conditional PMF and expectation, given an event Example of conditioning • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) Conditional PMF and expectation, given an event Example of conditioning • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) ￿ x pX(x)=1 ￿ x pX|A(x)=1 ￿ x pX(x)=1 ￿ x pX|A(x)=1 Conditional expectation E[X | A]= ￿ x xpX|A(x) E[X | Y = y]= ￿ x xpX|Y (x | y) • Expected value rule E[g(X)] = ￿ x g(x) pX(x) E[g(X) | A]= ￿ x g(x) pX|A(x) E[g(X) | Y = y]= ￿ x g(x) pX|Y (x | y) • Expected value rule E[g(X)] = ￿ x g(x) pX(x) E[g(X) | A]= ￿ x g(x) pX|A(x) E[g(X) | Y = y]= ￿ x g(x) pX|Y (x | y) Conditional PMF and expectation, given an event Example of conditioning • Condition on an event A ⇒ use conditional probabilities • pX(x)= P(X = x) E[X]= ￿ x xpX(x) • pX|A(x)= P(X = x | A) • E[X]= ￿ x xpX(x) • E[X | A]= ￿ x xpX|A(x) Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 1 x pX (x ) 4 1/4 23 01 2 3 4 • Let A = {X ≥ 2} pX|A(x)= E[X | A]= 1 x pX (x ) 4 1/4 23 01 2 3 4 • Let A = {X ≥ 2} pX|A(x)= E[X | A]= Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF 1 x pX (x ) 4 1/4 23 01 2 3 4 • Let A = {X ≥ 2} pX|A(x)= E[X | A]= 1 x pX (x ) 4 1/4 23 01 2 3 4 • Let A = {X ≥ 2} pX|A(x)= E[X | A]= Geometric PMF • X: number of independent coin tosses until ﬁrst head pX(k)=(1 − p)k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p)k−1p • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF 1 k pX (k) ... p(1-p)2 k pX |X>2(k) ... 3 p p k pX- 2|X>2(k) ... 1 p • Let A = {X ≥ 2} pX|A(x)= E[X | A]= var(X | A)= • Let A = {X ≥ 2} pX|A(x)= E[X | A]= var(X | A)= var(X)= • Let A = {X ≥ 2} pX|A(x)= E[X | A]= var(X | A)= var(X)= Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B)= P(A1)P(B | A1)+ ··· + P(An)P(B | An) pX(x)= P(A1)pX|A1(x)+ ··· + P(An)pX|An(x) E[X]= P(A1)E[X | A1]+ ··· + P(An)E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) pX(x)= P(A1)pX|A1(x)+ ··· + P(An)pX|An(x) E[X]= P(A1)E[X | A1]+ ··· + P(An)E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 A1 ∩ B A2 ∩ B A2 ∩ B • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 A1 ∩ B A2 ∩ B A3 ∩ B • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 A1 ∩ B A2 ∩ B A3 ∩ B • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B)= P(A1)P(B | A1)+ ··· + P(An)P(B | An) pX(x)= P(A1)pX|A1(x)+ ··· + P(An)pX|An(x) E[X]= P(A1)E[X | A1]+ ··· + P(An)E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) pX(x)= P(A1)pX|A1(x)+ ··· + P(An)pX|An(x) E[X]= P(A1)E[X | A1]+ ··· + P(An)E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) pX(x)= P(A1) pX|A1(x)+ ··· + P(An) pX|An(x) E[X]= P(A1)E[X | A1]+ ··· + P(An)E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) pX(x)= P(A1) pX|A1(x)+ ··· + P(An) pX|An(x) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total probability theorem • Divide and conquer • Partition of sample space into A1,A2,A3 A1 A2 A3 • Have P(B | Ai), for every i A1 B A2 A3 • One way of computing P(B): P(B)= P(A1)P(B | A1) + P(A2)P(B | A2) + P(A3)P(B | A3) Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) A1 ∩ {X = x} A2 ∩ {X = x} A3 ∩ {X = x} pX(x)= P(A1) pX|A1(x)+ ··· + P(An) pX|An(x) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) A1 ∩ {X = x} A2 ∩ {X = x} A3 ∩ {X = x} pX(x)= P(A1) pX|A1(x)+ ··· + P(An) pX|An(x) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] Total expectation theorem • Partition of sample space into disjoint events A1,A2,... ,An A1 B A2 A3 P(B) = P(A1) P(B | A1) + ··· + P(An) P(B | An) A1 ∩ {X = x} A2 ∩ {X = x} A3 ∩ {X = x} pX(x)= P(A1) pX|A1(x)+ ··· + P(An) pX|An(x) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p E[X | A2] E[X | A3] E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p E[X | A2] E[X | A3] E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p E[X | A2] E[X | A3] P(A1) P(A2) P(A3) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p E[X | A2] E[X | A3] P(A1) P(A2) P(A3) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p E[X | A2] E[X | A3] P(A1) P(A2) P(A3) E[X]= P(A1) E[X | A1]+ ··· + P(An) E[X | An] • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Total expectation example Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX (x) = P(X = x) = P￿{ω ∈ Ω s.t.X (ω)= x}￿ • Properties: pX (x) ≥ 0 ￿ x pX (x)=1 • Example: X =number of coin tosses until ﬁrst head – assume independent tosses, P(H )= p> 0 pX (k) = P(X = k) = P(TT ··· TH ) = (1 − p)k−1p, k =1, 2,... –geometric PMF Probability mass function (PMF) of a discrete r.v. X • It is the “probability law” or “probability distribution” of X • If we ﬁx some x, then “X = x” is an event • Notation: pX(x) = P(X = x) = P￿{ω ∈ Ω s.t.X(ω)= x}￿ • Properties: pX(x) ≥ 0 ￿ x pX(x)=1 • X ab cd pX(x) 345 Variance of the uniform n 1 n+1 01 Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Variance of the uniform n 1 n+1 01 Random speed • Traverse a 200 mile distance at constant but random speed V 1 200 1/2 v pV (v ) 1/2 • d = 200, T = t(V ) = 200/V • E[V ]= • var(V )= • σV = Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Total expectation example 1/92/92 6 7 8 Memorylessness: Number of remaining coin tosses, conditioned on Tails in the ﬁrst toss, is Geometric, with parameter p Conditioned on X> n, X − n is geometric with parameter p Conditioned on X> 1, X − 1 is geometric with parameter p Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head pX(k)=(1 − p)k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p)k−1p • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF 1 k pX (k) ... p(1-p)2 k pX |X>2(k) ... 3 p p k pX- 2|X>2(k) ... 1 p Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX (k)= k P(no Heads ever) 01 2 pX (x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n 123456789 pX(2) = Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head; P(H)= p pX(k)=(1 − p) k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p) k−1p • Memorylessness: Given that X> k, the r.v. X − k has the same geometric PMF • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF 1 k pX (k) ... p(1-p)2 k pX |X>2(k) ... 3 p p k pX- 2|X>2(k) ... 1 p Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head; P(H)= p pX(k)=(1 − p) k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p) k−1p • Memorylessness: Given that X> 1, the r.v. X − 1 has the same geometric PMF • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF Memorylessness: Number of remaining coin tosses, conditioned on Tails in the ﬁrst toss, is Geometric, with parameter p Conditioned on X> n, X − n is geometric with parameter p Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head pX(k)=(1 − p)k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p)k−1p • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF 1 k pX (k) ... p(1-p)2 k pX |X>2(k) ... 3 p p k pX- 2|X>2(k) ... 1 p Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX (k)= k P(no Heads ever) 01 2 pX (x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n 123456789 pX(2) = Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head; P(H)= p pX(k)=(1 − p) k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p) k−1p • Memorylessness: Given that X> k, the r.v. X − k has the same geometric PMF • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF 1 k pX (k) ... p(1-p)2 k pX |X>2(k) ... 3 p p k pX- 2|X>2(k) ... 1 p Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head; P(H)= p pX(k)=(1 − p) k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p) k−1p • Memorylessness: Given that X> 1, the r.v. X − 1 has the same geometric PMF • Memoryless property: Given that X> 2, the r.v. X − 2 has same geometric PMF Memorylessness: Number of remaining coin tosses, conditioned on Tails in the ﬁrst toss, is Geometric, with parameter p Conditioned on X> n, X − n is geometric with parameter p Memorylessness: Number of remaining coin tosses, conditioned on Tails in the ﬁrst toss, is Geometric, with parameter p Conditioned on X> n, X − n is geometric with parameter p The mean of the geometric • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]=1/p Conditioning a geometric random variable • X: number of independent coin tosses until ﬁrst head; P(H)= p pX(k)=(1 − p)k−1p, k =1, 2,... E[X]= ∞￿ k=1 kpX(k)= ∞￿ k=1 k(1 − p)k−1p pX−1|X>1(k) Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= k P(no Heads ever) 01 2 pX(x)= ￿n k ￿pk(1 − p)n−k, for k =0, 1,... ,n 123456789 pX(2) = Geometric random variable; parameter p:0 <p ≤ 1 • Experiment: inﬁnitely many independent tosses of a coin; P(Heads) = p • Sample space: Set of inﬁnite sequences of H and T • Random variable X: number of tosses until the ﬁrst Heads • Model of: waiting times; number of trials until a success pX(k)= P(no Heads ever) 01 2 pX(x)= ￿n k ￿ p k(1 − p) n−k, for k =0, 1,... ,n The mean of the geometric • Geometric example: A1 : {X =1}, A2 : {X> 1} E[X]= P(X = 1)E[X | X = 1] +P(X> 1)E[X | X> 1] • Solve to get E[X]= 1 p Multiple random variables and joint PMFs • pX,Y (x, y)= P(X = x and Y = y) 12 3 4 4 3 2 1 x y 1/20 4/20 3/20 2/20 2/20 2/201/20 1/20 1/20 1/20 2/20 • ￿ x ￿ y pX,Y (x, y)= • pX(x)= ￿ y pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) • ￿ x pX|Y (x | y)= Multiple random variables and joint PMFs X : pX Y : pY • pX,Y (x, y)= P(X = x and Y = y) 12 3 4 4 3 2 1 x y 1/20 4/20 3/20 2/20 2/20 2/201/20 1/20 1/20 1/20 2/20 • ￿ x ￿ y pX,Y (x, y)= • pX(x)= ￿ y pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) • ￿ x pX|Y (x | y)= Multiple random variables and joint PMFs X : pX Y : pY P(X = Y )=? • Joint PMF: pX,Y (x, y)= P(X = x and Y = y) 12 3 4 4 3 2 1 x y 1/20 4/20 3/20 2/20 2/20 2/201/20 1/20 1/20 1/20 2/20 • ￿ x ￿ y pX,Y (x, y)= • pX(x)= ￿ y pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) Multiple random variables and joint PMFs X : pX Y : pY P(X = Y )=? • Joint PMF: pX,Y (x, y)= P(X = x and Y = y) 12 3 4 4 3 2 1 x y 1/20 4/20 3/20 2/20 2/20 2/201/20 1/20 1/20 1/20 2/20 • ￿ x ￿ y pX,Y (x, y)= • pX(x)= ￿ y pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) • ￿ x ￿ y pX,Y (x, y)=1 • pX(x)= ￿ y pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) • ￿ x pX|Y (x | y)= • ￿ x ￿ y pX,Y (x, y)=1 • pX(x)= ￿ y pX,Y (x, y) pY (y)= ￿ x pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) • ￿ x pX|Y (x | y)= • ￿ x ￿ y pX,Y (x, y)=1 • pX(x)= ￿ y pX,Y (x, y) pY (y)= ￿ x pX,Y (x, y) • pX|Y (x | y)= P(X = x | Y = y)= pX,Y (x, y) pY (y) • ￿ x pX|Y (x | y)= Multiple random variables and joint PMFs X : pX Y : pY P(X = Y )= • Joint PMF: pX,Y (x, y)= P(X = x and Y = y) 12 3 4 4 3 2 1 x y 1/20 4/20 3/20 2/20 2/20 2/201/20 1/20 1/20 1/20 2/20 More than two random variables pX,Y,Z(x, y, z)= P(X = x and Y = y and Z = z) ￿ x ￿ y ￿ z pX,Y,Z(x, y, z)=1 pX(x)= ￿ y ￿ z pX,Y,Z(x, y, z) pX,Y (x, y)= ￿ z pX,Y,Z(x, y, z) More than two random variables pX,Y,Z(x, y, z)= P(X = x and Y = y and Z = z) ￿ x ￿ y ￿ z pX,Y,Z(x, y, z)=1 pX(x)= ￿ y ￿ z pX,Y,Z(x, y, z) pX,Y (x, y)= ￿ z pX,Y,Z(x, y, z) More than two random variables pX,Y,Z(x, y, z)= P(X = x and Y = y and Z = z) ￿ x ￿ y ￿ z pX,Y,Z(x, y, z)=1 pX(x)= ￿ y ￿ z pX,Y,Z(x, y, z) pX,Y (x, y)= ￿ z pX,Y,Z(x, y, z) More than two random variables pX,Y,Z(x, y, z)= P(X = x and Y = y and Z = z) ￿ x ￿ y ￿ z pX,Y,Z(x, y, z)=1 pX(x)= ￿ y ￿ z pX,Y,Z(x, y, z) pX,Y (x, y)= ￿ z pX,Y,Z(x, y, z) More than two random variables pX,Y,Z(x, y, z)= P(X = x and Y = y and Z = z) ￿ x ￿ y ￿ z pX,Y,Z(x, y, z)=1 pX(x)= ￿ y ￿ z pX,Y,Z(x, y, z) pX,Y (x, y)= ￿ z pX,Y,Z(x, y, z) More than two random variables pX,Y,Z(x, y, z)= P(X = x and Y = y and Z = z) ￿ x ￿ y ￿ z pX,Y,Z(x, y, z)=1 pX(x)= ￿ y ￿ z pX,Y,Z(x, y, z) pX,Y (x, y)= ￿ z pX,Y,Z(x, y, z) Functions of multiple random variablesFunctions of multiple random variables Z = g(X, Y ) pZ(z)= P(Z = z)= P￿g(X, Y )= z￿ Functions of multiple random variables Z = g(X, Y ) pZ(z)= P(Z = z)= P￿ g(X, Y )= z￿ Expected value rule: E[g(X, Y )] = ￿ x ￿ y g(x, y) pX,Y (x, y) Functions of multiple random variables Z = g(X, Y ) PMF: pZ(z)= P(Z = z)= P ￿ g(X, Y )= z￿ Expected value rule: E[g(X, Y )] = ￿ x ￿ y g(x, y) pX,Y (x, y) Linearity of expectations E[aX + b]= aE[X]+ b E[X + Y ]= E[X]+ E[Y ] E[X1 + ··· + Xn]= E[X1]+ ··· + E[Xn] E[2X +3Y − Z]= Linearity of expectations E[aX + b]= aE[X]+ b E[X + Y ]= E[X]+ E[Y ] E[X1 + ··· + Xn]= E[X1]+ ··· + E[Xn] E[2X +3Y − Z]= Linearity of expectations E[aX + b]= aE[X]+ b E[X + Y ]= E[X]+ E[Y ] E[X1 + ··· + Xn]= E[X1]+ ··· + E[Xn] E[2X +3Y − Z]= Linearity of expectations E[aX + b]= aE[X]+ b E[X + Y ]= E[X]+ E[Y ] E[X1 + ··· + Xn]= E[X1]+ ··· + E[Xn] E[2X +3Y − Z]= Linearity of expectations E[aX + b]= aE[X]+ b E[X + Y ]= E[X]+ E[Y ] E[X1 + ··· + Xn]= E[X1]+ ··· + E[Xn] E[2X +3Y − Z]= The mean of the binomial X: binomial with parameter p number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np The mean of the binomial • X: binomial with parameters n, p – number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np The mean of the binomial • X: binomial with parameters n, p – number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np The mean of the binomial • X: binomial with parameters n, p – number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np The mean of the binomial • X: binomial with parameters n, p – number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np The mean of the binomial • X: binomial with parameters n, p – number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np The mean of the binomial • X: binomial with parameters n, p – number of successes in n independent trials Xi =1 if ith trial is a success; Xi = 0 otherwise (indicator variable) X = X1 + ··· + Xn E[X]= np E[X]= n￿ k=0 k￿n k ￿pk(1 − p)n−k","libVersion":"0.3.2","langs":""}
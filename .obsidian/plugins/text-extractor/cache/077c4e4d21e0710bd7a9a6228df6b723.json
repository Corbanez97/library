{"path":"Books and Papers/Masters Points of Interest/Phase Transitions in Neural Networks - Liam Carroll.pdf","text":"Phase Transitions in Neural Networks Liam Carroll Supervised by Dr. Daniel Murfet Thesis for the Masters of Science October 2021 School of Mathematics and Statistics The University of Melbourne Acknowledgements Completed across the course of the COVID-19 pandemic through 250 days in lockdown in Mel- bourne, the existence of this thesis owes to the abundant love and support I have received from countless friends, family and mentors. In particular, I would like to share my sincere gratitude towards: my non-mathematics friends Charlie, Oscar, Nicola, Gioele and Rachel for countless phone calls oﬀering moral support; my mathematics friends Luke, Ben, Caleb, Spencer and other members of the Melbourne Deep Learning Group for oﬀering plenty of useful advice and answer- ing umpteen questions along the journey; and to my mentors Prof. Arun Ram and Dr. Thomas Quella for their words of wisdom and encouragement, particularly when the going got tough. I feel very lucky that so many people were willing to read drafts and oﬀer guidance from such diﬀerent perspectives. A student is only as good as their teacher helps them to be, and for this I am indebted to my supervisor, Dr. Daniel Murfet, for his enduring passion and care in inspiring me to be the best mathematician I can be. Finally, to my family, Lynette, James and Hannah, thank you for your unconditional love and support throughout this process, despite not quite understanding what I have been working on. I hope I have done you all proud. 1 Contents 1 Introduction 4 2 Preliminaries 6 2.1 Feedforward ReLU Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 The Objects of Statistical Learning Theory . . . . . . . . . . . . . . . . . . . . . . 9 2.2.1 Bayesian statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.2.2 The Kullback-Leibler divergence K(w) . . . . . . . . . . . . . . . . . . . . . 11 2.2.3 Empirical estimators of loss and error . . . . . . . . . . . . . . . . . . . . . 13 2.2.4 Tempered posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3 Deep Learning as a Gibbs Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3 Singular Learning Theory 18 3.1 Singular Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2 Free Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.3 Asymptotics of the Free Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4 Symmetries of W0 28 4.1 Topology of Two-Layer Feedforward ReLU Networks . . . . . . . . . . . . . . . . . 28 4.2 Classiﬁcation of W0 for m = d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.2.1 Deﬁnitions and hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.2.2 Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.2.3 Main theorem for m = d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.3 Classiﬁcation of W0 for m < d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.3.1 Hypotheses, deﬁnitions and lemmas . . . . . . . . . . . . . . . . . . . . . . 36 4.3.2 Main theorem for m < d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.4 Arbitrary Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.5 Example - m-symmetric Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5 Phase Transitions in ReLU Neural Networks 44 5.1 Phases and Phase Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.2 Experimental Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.2.1 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.2.2 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 5.2.3 Machine epsilon and practical limits . . . . . . . . . . . . . . . . . . . . . . 51 5.2.4 Visualising the posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 5.3 Phase Transition 1: Deforming to Degeneracy . . . . . . . . . . . . . . . . . . . . . 53 5.3.1 Deﬁning the order parameter . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.3.2 Results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5.4 Phase Transition 2: Orientation Reversing Symmetry . . . . . . . . . . . . . . . . . 57 5.4.1 Deﬁning the order parameter . . . . . . . . . . . . . . . . . . . . . . . . . . 57 5.4.2 Results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.4.3 An instructive calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.4.4 Phase transition 3: Equal Weight-Annihilation . . . . . . . . . . . . . . . . 61 6 Conclusion 63 A Appendix 65 Bibliography 69 2 Notation Notation Meaning Reference N Natural numbers {1, 2, . . . }. ReLU(x) The ReLU function max(0, x). Eq. (2.1) w ∈ W A parameter in parameter space W deﬁning a model, where W is a compact subset of RD. Deﬁnition 2.1 [d] The set of nodes of a network {1, . . . , d}. Deﬁnition 2.1 f (x, w), fw(x) A feedforward ReLU neural network. Deﬁnition 2.1 f0(x) A feedforward ReLU network deﬁning a true distribu- tion. Hypothesis 4.1 (wi, bi), (qi, c) The weights and biases of the ﬁrst and second layer re- spectively, of a two layer network for a node i ∈ [d]. Eq. (2.2) (p(y|x, w), q(y|x), ϕ(w)) The (model, truth, prior) triple. Hypothesis 2.1 q(x) The known distribution of inputs. Hypothesis 2.1 p β(w|Dn), Z β n The tempered posterior distribution and its partition function at inverse temperature β. Deﬁnition 2.11 EX , E β W Expectation with respect to q(y, x) and the truncated posterior respectively. Deﬁnition 2.3 Deﬁnition 3.5 Ln(w), Sn The negative log likelihood of a model, and the entropy of the true distribution. Deﬁnition 2.9 K(w) The Kullback-Leibler divergence from the model to the truth. Deﬁnition 2.7 W0 The set of true parameters {w ∈ W | K(w) = 0}. Deﬁnition 2.8 F β n , F β n (W) The free energy, and the free energy of a compact set W ⊆ W . Deﬁnition 3.6 λ The RLCT of a (model, truth, prior) triple. Deﬁnition 3.7 3 Chapter 1 Introduction In this thesis we provide accessible examples of singular statistical models in the form of simple feedforward ReLU neural networks, in order to illustrate the central message of Sumio Watanabe’s Singular Learning Theory: singularities lie at the heart of statistical learning [Wat09]. In doing so, we demonstrate why the theory of deep learning should shift from analysing points in the space of parameters W to considering singularities of the Kullback-Leibler divergence K(w). Deep learning is a part of Artiﬁcial Intelligence (AI) that uses statistical models called neural networks to model tasks such as computer vision, voice recognition, machine translation and many more [LBH15]. These models have recently been shown to be highly eﬀective [Bro+20] [Nak+19]. The number of parameters D in modern deep learning models is typically orders of magnitude more than the number of datapoints n that they are trained on [Zha+16]. Because of this, standard results for regular statistical models predict that neural networks should overﬁt the training data and thus have high generalisation error. Understanding why this is not the case, and understanding other observed phenomena such as scaling laws [Kap+20], are important open problems. We begin by casting deep learning as a Bayesian statistical learning model in Chapter 2. Here the Kullback-Leibler divergence K(w) from a model to the truth is presented as the fundamental object of study, alongside the set of true parameters W0 = {w ∈ W | K(w) = 0}. We then explain how one can draw an analogy between neural networks as Bayesian models and the Gibbs ensemble of statistical physics, hinting at objects and phenomena that arise naturally such as the free energy and phase transitions. Based on the work of [Wat07] and [Mur+20] we then show that feedforward ReLU neural networks are not regular but rather singular models, which is to say, have degenerate Fisher infor- mation matrices. Thus points on W0 are singularities of K in the sense of algebraic geometry. It is then argued that minimising the free energy is the central goal of statistical learning since it mea- sures the posterior density associated to diﬀerent regions of W , and is related to the generalisation error. The key result of Singular Learning Theory is then stated, that being the correct asymptotic expansion of the free energy for singular models. By desingularising K(w) using Hironaka’s Reso- lution of Singularities, Watanabe shows that the correct measure of model complexity in singular models is the RLCT λ ≤ D 2 which represents the eﬀective number of parameters associated to a singularity. We interpret this result in the context of Occam’s Razor, in line with the approach of [Bal97] which considers only regular models. Thus the model selection process is cast as a trade-oﬀ between accuracy and complexity. Perhaps Watanabe’s most profound realisation is that “knowledge to be discovered corresponds to a singularity,” [Wat09]. Put diﬀerently, “if a statistical model is devised so that it extracts hidden structure from a random phenomenon, then it naturally becomes singular,” [Wat13]. This oﬀers a groundbreaking shift in perspective of statistical thought from points to singularities. However, 4 this message has been underappreciated by practitioners of deep learning, perhaps owing to the heavy algebraic geometry machinery at the heart of the theory, as well as the lack of accessible mental models that exhibit the phenomena it describes. We aim to provide such models. To this end, in Chapter 4 we set about classifying the symmetries of W0 for two layer ReLU networks of arbitrary width. We consider the realisable case where the true distribution is deﬁned by such a network, for which the problem then becomes classifying functional equivalence of ReLU networks. This is done in two stages. We ﬁrst analyse the case where the truth and model have the same number of nodes, m = d, where we establish that W0 exhibits scaling symmetry, permutation symmetry and orientation reversing symmetry. The latter is non-generic, occurring only under the particular condition that weight vectors of the true network sum to zero (weight annihilation). This is then generalised to the case where the model is overparamaterised compared to the truth, m < d. Here we prove that each of the d − m excess nodes is either degenerate, meaning it is never meaningfully active, or has the same activation boundary as another model node. It is then shown that suitably adjusted scaling, permutation and orientation reversing symmetries must also hold. A more general result from [PL19] that considers networks of arbitrary depth under particular conditions is then discussed. We conclude the chapter by introducing a class of networks, m-symmetric networks, whose associated W0 exhibit non-generic symmetries. Armed with a full classiﬁcation of all points on W0 in these models, we then set about analysing these points as singularities of K in Chapter 5. The primary goal here is to show that not all points on W0 are equally good minimisers of the free energy. In line with the statistical physics analogy, we argue that a phase of a neural network corresponds to a small compact subset of W containing a particular singularity of interest. Phase transitions thus arise naturally from variations in the true distribution due to symmetry breaking of W0, which causes a meaningful change in the free energy. We demonstrate the existence and diﬀerences of such phases in two layer ReLU networks through a posterior estimation procedure using Markov Chain Monte Carlo methods. The key result of the work is showing that a point not on W0 can nonetheless be preferred by the posterior. We ﬁrst show experimentally that the complexity of a degenerate-node phase is lower than that of a non- degenerate node phase, and demonstrate both ﬁrst and second order phase transitions associated to this due to changes in the accuracy of both phases. We then show that the complexity of non- weight-annihilation phases is lower than that of weight-annihilation phases, and again demonstrate a second order phase transition accordingly. 5 Chapter 2 Preliminaries 2.1 Feedforward ReLU Neural Networks Artiﬁcial neural networks, which we will refer to simply as neural networks or networks, are the fundamental mathematical objects of deep learning. They consist of an input layer, a number of hidden layers, and an output layer. Each layer consists of a ﬁnite number of nodes. We call the number of layers the depth of the network, and the number of nodes in a given layer the width of the layer. In general, the architecture of the network is the data consisting of: • The depth L of the network, implying there are L − 1 hidden layers. 1 • The widths of each layer (d0, . . . , dL) ∈ NL+1. • The graph describing the connectivity of the layers. Each layer l connects to layer l + 1. In the simplest case the collection of neurons form a directed acyclic graph where the subgraph generated by successive layers is fully connected, that is, there exist edges connecting each node from layer l to every node of layer l + 1. Such a network is called a feedforward network. A typical schematic of a feedforward neural network is seen in Fig. 2.1. Architectures with diﬀerent graph structures have been recently used with great success, includ- ing graphs with the presence of loops (e.g. recurrent neural networks), or layers that are not fully connected (e.g. convolutional neural networks). We refer the reader to [GBC16] for elaboration on such architectures. In this thesis our study will be restricted to feedforward neural networks. To each edge between a node i ∈ [dl−1] := {1, . . . , dl−1} in layer l − 1 to a node j ∈ [dl] in layer l is a weight wl i,j ∈ R, and to each node j there is a bias bl j ∈ R. This gives rise to an aﬃne function Al : Rdl−1 → Rdl , A l(x) = (wl)T x + bl, where the ﬁrst term is matrix multiplication. Neurons are then “activated” via composition of the aﬃne function with a vectorised activation function σ(x), thus the output al from each layer 1 ≤ l ∈ L − 1 can be expressed recursively as a l = σ(wla l−1 + bl) . Interestingly, there is no widely accepted deﬁnition of an activation function, and indeed when one examines the plethora of such functions that are used in practice it is clear that there are no common traits other than the fact that they are non-zero somewhere. In the early literature the activation function was typically the step function σH (x) =   1 if x ≥ 0 0 if x < 0 , 1In a quirk of terminology, the depth ignores the input layer. For example, a two-layer network has an input layer, one hidden layer and output layer. 6 x1 x2 x3 x4 Input layer Hidden layer 1 Hidden layer 2 Hidden layer 3 Hidden layer 4 y1 y2 y3 y4 Output layer Figure 2.1: Five layer feedforward neural network with widths (N, d1, d2, d3, d4, M ) = (4, 5, 4, 5, 3, 4). which thus elucidates the use of the term “activation” [Ros62]. Neural networks with σH (x) as the activation function are called perceptron networks. Other common activation functions include the sigmoid function σs(x) = 1 1+e−x and hyperbolic tangent σt(x) = tanh(x), but the one we will almost exclusively discuss throughout this thesis is the Rectiﬁed Linear Unit (ReLU) deﬁned by σR(x) = ReLU(x) = max{0, x} =   x if x ≥ 0 0 if x < 0 . (2.1) Note that for λ ∈ R, ReLU(λx) = λ ReLU(x), however ReLU(−x) ̸= − ReLU(x). We may extend the deﬁnition of any of these activation functions to be vectorised by writing σ : Rn → Rn for some n such that σ(x1, . . . , xn) = (σ(x1), . . . , σ(xn)). From here on we assume that any activation function mentioned has naturally been vectorised. Remark 2.1. The ReLU function is not analytic at x = 0, which we will see is problematic when discussing such networks in the context of Singular Learning Theory in Chapter 3. An analytic alternative to ReLU is the swish function given by σγ(x) = x 1 + e−γx for some γ ∈ R, which satisﬁes limγ→∞ σγ(x) = ReLU(x) (see [RZL17]). We now have all of the pieces to deﬁne the neural networks we will examine in this thesis. Deﬁnition 2.1. Let W ⊆ RD denote the weight space, where D is the total number of weights and biases. A feedforward ReLU neural network of depth L with widths (d0, . . . , dL) ∈ NL+1 such that there are d0 = N inputs and dL = M outputs, is a feedforward neural network with activation function σ(x) = ReLU(x). That is, it is a function f : RN × W −→ RM f (x, w) = (AL ◦ ReLU ◦ AL−1 ◦ ReLU ◦ · · · ◦ ReLU ◦ A1)(x) such that for each layer 1 ≤ l ≤ L there is an aﬃne function Al : Rdl−1 → Rdl , parameterised by weights wl ∈ Rdl−1×dl (R) and biases b l ∈ Rdl×1(R), given by Al(x) = (wl) T x + b l . In the case where wl is a column vector we may write this as Al(x) = ⟨wl, x⟩ + bl where ⟨ , ⟩ denotes the dot product. When w is assumed ﬁxed we will often denote fw := f (·, w) : RN → RM . 7 −4 −2 2 4 1 2 3 ReLU(x) σγ(x) Figure 2.2: ReLU(x) versus σγ(x) for γ = 1. Feedforward ReLU networks are, by the deﬁnition of ReLU, piecewise aﬃne functions, mean- ing networks of relatively low depth and widths are quite simple functions. However, this class of networks has been shown to be arbitrarily expressive in the sense that, under suitable conditions, they are universal approximators of arbitrary Lebesgue integrable functions. The following Uni- versal Approximation Theorem of [Lu+17], which we shall not prove here but is included here for completeness, describes this: Theorem. Let g : RN → R be a Lebesgue-integrable function and let ε > 0 be arbitrary. Then there exists a feedforward ReLU neural network fw : RN → R of some depth L with bounded widths dl ≤ N + 4 such that ∫ RN |g(x) − f (x, w)|dx < ε . This theorem thus demonstrates the potential of feedforward ReLU neural networks in modern deep learning: they are simple functions to compute, yet they are able to express complicated functions to arbitrary precision. The popularity of such networks lies in this expressivity, together with the fact that in practice good approximations can be found on modern hardware for large datasets via stochastic gradient descent (SGD). Nearly all networks we will consider in this thesis will have two layers and one output. For readability we make a slight notational adjustment. Let fw : RN → R be such a feedforward ReLU neural network with d hidden nodes. For each i ∈ [d] we let wi ∈ RN and bi ∈ R denote the weights and biases associated to the ﬁrst layer, and let qi, c ∈ R be the weights and bias of the second layer. Thus fw has the form fw(x) = (A2 ◦ ReLU ◦ A1)(x) (2.2) = 〈 w2, ReLU((w1) T x + b 1) 〉 + b 2 = c + d∑ i=1 qiReLU(⟨wi, x⟩ + bi) where w = (w1, . . . , wd, b1, . . . , bd, q1, . . . , qd, c) ∈ W ⊆ R4d+1. We will return to topological properties of two-layer networks in Section 4.1, but for now we present a single example of such a function for ease of understanding. Example 2.1. Let fw : R2 → R1 be a two-layer neural network with two inputs, one output and d = 4 hidden nodes, so fw(x) = q1 ReLU(w1,1x1 + w2,1x2 + b1) + q2 ReLU(w1,2x1 + w2,2x2 + b2) + q3 ReLU(w1,3x1 + w2,3x2 + b3) + q4 ReLU(w1,4x1 + w2,4x2 + b4) + c , whose network architecture is seen in Fig. 2.3a. Consider w ∈ W such that fw(x) = ReLU(x1 − 1) + ReLU(x2 − 1) + ReLU(−x1 − 1) + ReLU(−x2 − 1) . 8 (a) Neural network graph of fw(x) (b) Contour plot of fw(x) and its activation bound- aries. Figure 2.3: The data of fw(x) in Example 2.1. A depiction of fw is seen in Fig. 2.3b, alongside the activation boundaries ⟨wi, x⟩ + bi = 0 that show where each node is activated. Note that the corresponding weight vector wi, which is normal to its respective activation boundary, points towards the regions of activation when anchored on the activation boundary it deﬁnes. 2.2 The Objects of Statistical Learning Theory Given a dataset Dn of inputs x ∈ RN and outputs y ∈ RM drawn from some true distribution q(y, x), the objective of statistical learning is to train a model (or learning machine) p(y|x, w) to predict an output for a given input from the true distribution. This amounts to estimating parameters w that minimise a loss metric K(w). Due to computational beneﬁts such as training parallelisation and scalability, it is standard practice within modern deep learning to view this estimation procedure within a frequentist frame- work, where w is viewed as being unknown yet ﬁxed [CB02]. Training is then performed using the Stochastic Gradient Descent (SGD) algorithm, which is achieved at scale via the famous back- propagation algorithm [GBC16]. However, as in Algebraic Geometry and Statistical Learning Theory [Wat09], our view of the learning procedure will be within the Bayesian framework, whereby the model parameters w ∈ W are assumed to be drawn from a probability distribution, and the learning goal thus becomes estimating the posterior distribution p(w|Dn). Remark 2.2. An assumption within the deep learning literature is that training via SGD is approximately equivalent to sampling from a Bayesian posterior, with evidence mounting that this is indeed the case (see [Min+20] and [MHB18]). If true, this justiﬁes our use of Bayesian statistics in drawing conclusions about deep learning, but keep in mind that this connection is not yet rigorous. The following exposition of Bayesian statistics and related deﬁnitions is largely drawn from [Wat18; Wat09] and [CB02]. 2.2.1 Bayesian statistics Let (Ω, B, P) be a probability space and (X, Y ) : Ω → RN × RM a jointly random variable subject to the probability density q(y, x) = q(y|x)q(x), where X is the input to the model and Y is the output. Recall that the objective of statistical learning is to estimate the true distribution q(y, x) 9 given a collection of random samples Dn of the form Dn = {(X1, Y1), . . . , (Xn, Yn)} where each (Xi, Yi) is independent and identically distributed, thus leading to a probability density of the form q((x1, y1), . . . , (xn, yn)) = q(x1, y1) · · · q(xn, yn) . We then assume that the data is drawn according to a joint probability distribution p(y, x|w) = p(y|x, w)q(x) which we call the model, paramaterised by some parameter w ∈ W . Since the samples are independent and identically distributed, we may deﬁne the likelihood function as l(w|x, y) := p(y1, . . . , yn, x1, . . . , xn|w) = n∏ i=1 p(yi, xi|w) = n∏ i=1 p(yi|xi, w)q(xi) . (2.3) The statistical learning goal is thus to estimate the posterior density p(w|x, y) subject to a dataset Dn. Let ϕ(w) denote the prior probability density of w ∈ W , which is a “subjective distribution based on the experimenter’s belief and is formulated before the data is seen” [CB02]. By Bayes’ rule, the posterior probability density is given by p(w|Dn) := p(Dn|w)ϕ(w) p(Dn) = 1 p(Dn) ϕ(w) n∏ i=1 p(yi, xi|w) = 1 p(Dn) ϕ(w) n∏ i=1 p(yi|xi, w)q(xi) , where the evidence p(Dn) (also called the marginal likelihood ) is given by p(Dn) = ∫ W p(Dn|w)ϕ(w)dw = ∫ W n∏ i=1 q(xi)p(yi|xi, w)ϕ(w)dw , which ensures the posterior is normalised and thus a well deﬁned probability density. But since ∏n i=1 q(xi), which is independent of w, is a factor of both p(Dn|w) and p(Dn), we may simplify this to give a more concise deﬁnition: Deﬁnition 2.2. The posterior probability density p(w|Dn) is given by p(w|Dn) = 1 Zn ϕ(w) n∏ i=1 p(yi|xi, w) , where Zn = ∫ W ϕ(w) n∏ i=1 p(yi|xi, w)dw . (2.4) We call Zn the partition function. Remark 2.3. Clearly the partition function and evidence are related via p(Dn) = Zn ∏n i=1 q(xi). Within our setup we are considering the random variable Dn (associated to the random inputs Xi and random outputs Yi), and the random variable w (which we do not denote with a capital for notational clarity). As such, we deﬁne the following expectations: Deﬁnition 2.3. Let g(X, Y ) be a function of one sample (X, Y ) drawn from the true distribution. Then we write EX [g(X, Y )] = ∫ ∫ RN +M g(x, y)q(y, x)dxdy . In the case where we have a dataset Dn of samples drawn from the true distribution, supposing g(Dn) = g((X1, Y1), . . . , (Xn, Yn)), we write EDn[g(Dn)] = ∫ ∫ Rn(N +M ) g(Dn) n∏ i=1 q(xi, yi)dxidyi . 10 Let f (w) be a function of the random weights, then the posterior expectation is given by Ew[f (w)] = ∫ W f (w)p(w|Dn)dw . Note that due to its dependence on the random variable Dn, Ew[f (w)] itself is a random variable. For any of these expectations the variance is deﬁned in the usual way, V(f (x)) = E[f (x)2] − E[f (x)] 2 . For this thesis we will restrict our attention to the following setup: Hypothesis 2.1. We consider a (model, truth, prior) triple (p(y|x, w), q(y|x), ϕ(w)) associated to the class of feedforward ReLU neural networks f : RN × W → RM . We assume that: • The true conditional distribution q(y|x) is unknown and to be modelled. • The distribution of inputs, q(x), is known (i.e. not modelled or estimated). • The prior on parameters, ϕ(w), is a known distribution on a compact space W ⊆ RD that contains the origin. • The model is a standard regression model on f ; that is, p is multivariate normally dis- tributed of dimension M with mean f (x, w) and identity covariance matrix, so p(y|x, w) ∼ N (f (x, w), 1 M ) with model density given by p(y|x, w) = 1 (2π) M 2 exp (− 1 2 ∥y − f (x, w)∥2) , where ∥.∥2 is the standard Euclidean norm on the output space RM . Thus we can express the joint densities in terms of conditional densities, q(y, x) = q(y|x)q(x) , and p(y, x|w) = p(y|x, w)q(x) . Remark 2.4. The case in which q(x) is to be modelled is of great interest in many real world settings such as natural language processing or image generation. In such situations, generative models are used, where the objective is to train a network to generate data similar to its inputs (see [Ope16] for more examples and explanation). Our hypothesis on q(x) is valid for the purposes of this thesis due to the nature of our experiments in Chapter 5. In Bayesian statistics there are two main ways of estimating a distribution on outputs y given an input x to the learning machine. Deﬁnition 2.4. The Bayes predictive distribution is given by p(y|x, Dn) = Ew[p(y|x, w)] = ∫ W p(y|x, w)p(w|Dn)dw . Deﬁnition 2.5. A Gibbs estimator is the model p(y|x, w) evaluated for a single sample drawn from the posterior, w ∼ p(w|Dn). 2.2.2 The Kullback-Leibler divergence K(w) The starting point of all supervised 2 statistical learning is to train a given model to minimum loss. In the Bayesian setting it is standard practice to take this loss function to be the Kullback- Leibler divergence K(w). Watanabe shows that the geometry of K(w) strongly aﬀects the learning process, thus it is a central object of our study. 2A statistical learning setting is supervised if the predictive model is trained via knowledge of the labels (outputs) for each input. This is true of our setting. 11 Deﬁnition 2.6. The entropy S of the true conditional distribution q(y|x) is S = EX [− log q(y|x)] = − ∫ ∫ RN +M q(y, x) log q(y|x)dxdy . The negative log loss (or negative log likelihood ) L(w) of a model for a given w ∈ W is L(w) = EX [− log p(y|x, w)] = − ∫ ∫ RN +M q(y, x) log p(y|x, w)dxdy . Remark 2.5. We can also deﬁne the joint entropy SJ = − ∫∫ q(y, x) log q(y, x)dydx and the input entropy Sx = − ∫∫ q(y, x) log q(x)dxdy, thus SJ = S + Sx. But since q(x) is assumed known and does not depend on w we are really only interested in the quantity S. Given arbitrary probability distributions p(z) and q(z), one typically deﬁnes the Kullback- Leibler divergence, or relative entropy, from p(z) to q(z) to be K(q||p) = ∫ q(z) log q(z) p(z) dz . Since our q(y, x) and p(y, x|w) both have the known q(x) as factors, we may reﬁne this deﬁnition for our purposes as follows: Deﬁnition 2.7. The Kullback-Leibler (KL) divergence of the true distribution q(y, x) from the model p(y, x|w) is a function K : W → R deﬁned by K(w) := EX [ log q(y|x) p(y|x, w) ] = ∫ ∫ RN +M q(y|x)q(x) log q(y|x) p(y|x, w) dxdy . (2.5) It satisﬁes K(w) = L(w) − S. Though it is often thought of as being a distance, K(w) is not a true metric as it is not symmetric in p and q, nor does it satisfy the triangle inequality. It is, however, a loss metric, as the next lemma shows. Lemma 2.1. Let q(y, x) and p(y, x|w) > 0 be continuous probability density functions. Then: • K(w) ≥ 0 for all w ∈ W . • K(w) = 0 if and only if p(y|x, w) = q(y|x) for almost all x ∈ RN , y ∈ RM . Proof. See Lemma A.1. Our statistical learning objective to minimise K(w) thus becomes ﬁnding the zero-sets: Deﬁnition 2.8. The set of true parameters is deﬁned as W0 := {w ∈ W | K(w) = 0} = { w ∈ W | p(y|x, w) = q(y|x) } , (2.6) where the second equality follows from Lemma 2.1. We say that the true distribution q(y|x) is realisable by the model p(y|x, w) if W0 is non-empty. That is, there exists a w ∈ W such that q(y|x) = p(y|x, w). When q(y|x) is realisable by a true network f0(x) = f (x, w(0)) for some w(0) ∈ W we will write W0(f0). The deﬁnition of realisability should be interpreted as saying that the chosen model is suﬃ- ciently expressive to perfectly capture the true distribution in question. This is, of course, unlikely to occur in real world distributions (especially at the scale of datasets at which most deep learning occurs), but the non-realisable case is signiﬁcantly more technically challenging to deal with, and many of the key results of Singular Learning Theory do not hold under this hypothesis. Under Hypothesis 2.1 we see that when q(y|x) is realisable, K(w) is just the mean squared error weighted by the prior on inputs q(x): 12 Lemma 2.2. Let q(y|x) = p(y|x, w(0)) be realisable, deﬁned by a parameter w(0) ∈ W . Then K(w) = 1 2 ∫ RN ∥f (x, w) − f (x, w(0))∥2q(x) dx . (2.7) Proof. See Lemma A.2. In general, for ReLU neural networks, K(w) is not analytic. For the minimal counterexample consider f (x, w) = ReLU(x − b) and truth f (x, w0) = ReLU(x) with input distribution q(x) the uniform distribution on [−a, a] for some a > 0. Then K(b) = ∫ a −a ( ReLU(x − b) − ReLU(x))2 dx =   − 1 3a b 3 + 1 2 b2 b ≥ 0 − 1 6a b 3 + 1 2 b2 b < 0 , which shows that K(w) is C 2 but is not C 3, let alone analytic. However, we can rectify this by instead considering the swish function σγ in Remark 2.1 as an approximation for ReLU, which gives an analytic K(w). When reading the remainder of this thesis, one should keep this correspondence in mind, as it allows us to exploit the properties of either function whenever convenient throughout our analysis. 2.2.3 Empirical estimators of loss and error In practice, we may only interact with the true distribution q(y|x) by drawing a set of samples Dn = {(Xi, Yi)} n i=1 and calculating an estimator of K(w) based on the observed samples. For notational aesthetics, we let (xi, yi) denote the random variables (Xi, Yi) drawn from q(y|x). Deﬁnition 2.9. Let Dn = {(xi, yi)}n i=1 be a dataset of inputs and outputs drawn from the true distribution q(y|x) with associated model p(y|x, w). We deﬁne the empirical entropy Sn of the true distribution to be Sn := − 1 n n∑ i=1 log q(yi|xi) , the empirical negative log likelihood Ln(w) (or empirical negative log loss) to be Ln(w) := − 1 n n∑ i=1 log p(yi|xi, w) , and the empirical Kullback-Leibler divergence to be Kn(w) := 1 n n∑ i=1 log q(yi|xi) p(yi|xi, w) = Ln(w) − Sn . (2.8) The negative log likelihood is so-called due to its relation to the likelihood function (Eq. (2.3)) e−nLn(w) = n∏ i=1 p(yi|xi, w) = l(w|x, y) ∏n i=1 q(xi) . (2.9) Using this, we can redeﬁne our posterior distribution to be p(w|Dn) = 1 Zn ϕ(w)e−nLn(w) , where Zn = ∫ W ϕ(w)e−nLn(w) . This is the form that we will use for the remainder of the thesis, and provides a clear link to the Gibbs distribution of statistical physics in Section 2.3. Lemma 2.3. Under Hypothesis 2.1, Ln(w) has the form Ln(w) = M 2 log 2π + 1 n n∑ i=1 1 2 ∥yi − f (xi, w)∥ 2 , which satisﬁes Ln(w) ≥ 0, and is a continuous function of w. 13 Proof. The ﬁrst claim follows from a trivial calculation recalling the deﬁnition p(yi|xi, w) = 1 (2π) M 2 exp (− 1 2 ∥yi − f (xi, w)∥ 2). Both terms are clearly positive by deﬁnition of the Euclidean norm. For any ReLU neural network the map w ↦→ f (x, w) is continuous for a ﬁxed x since it is simply a composition of continuous ReLU functions. Thus, since ∥.∥ 2 is continuous, Ln(w) is continuous. Lemma 2.3 means that under Hypothesis 2.1 we can interpret the negative log likelihood as simply being the mean-squared error plus a constant that only depends on M . It is important to note that, as stated at the start of the section, we never have access to q(y|x). Thus even though Kn(w) is an empirical estimator, we are prohibited from actually estimating it since we can only ever evaluate p(y|x, w). However, the reason we continue to discuss this quantity is because in the limit as n → ∞, Sn can be viewed as a constant that depends on neither the model nor the prior. Lemma 2.4. The empirical estimators satisfy EX [Kn(w)] = K(w) , and EX [Sn] = S , and EX [Ln(w)] = L(w) . If K(w), S, L(w) < ∞ then as n → ∞ we have almost sure convergence Kn(w) a.s. −→ K(w) , and Sn a.s. −→ S , and Ln(w) a.s. −→ L(w) . Proof. We will only calculate Kn(w) as the others are identical. Let w ∈ W be ﬁxed, then EX [Kn(w)] = EX   1 n n∑ i=1 log q(yi|xi) p(yi|xi, w)   = 1 n n∑ i=1 EX [ log q(yi|xi) p(yi|xi, w) ] = 1 n n∑ i=1 ∫ RN +M q(x, y) log q(y|x) p(y|x, w) dxdy = 1 n n∑ i=1 K(w) = K(w) . The second statement is a simple corollary of the above calculation by Kolmogorov’s Law of Large Numbers [Res99, §7]. Remark 2.6. One can deﬁne the normalised posterior and normalised partition function to be p(w|Dn) = 1 Z 0 n ϕ(w)e−nKn(w) , where Z 0 n = ∫ W ϕ(w)e−nKn(w)dw , where Z 0 n = eSn Zn. Which version you prefer to think of is simply a matter of taste - the inaccessible Sn term is ultimately irrelevant to the learning process in the limit n → ∞. Given a model deﬁned by some parameters achieves a low loss, it is then natural to ask how well it generalises beyond its training data. Deﬁnition 2.10. Let Dn be a dataset of inputs and outputs drawn from the true distribution q(y|x) with associated model p(y|x, w). The Bayesian training loss is given, in terms of the predictive distribution, by Tn = − 1 n n∑ i=1 log p(yi|xi, Dn) , and the Bayesian generalisation loss is given by Gn = EX [− log p(y|x, Dn)] = − ∫ ∫ RN +M q(y, x) log p(y|x, Dn)dxdy , which satisﬁes EX [Tn] = Gn. We will typically drop the “Bayesian” preﬁx. 14 The Gibbs training loss is given by Gt = Ew[Ln(w)] , and the Gibbs generalisation loss is given by Gg = Ew[L(w)] , which satisﬁes Gg = Ew[EX [Ln(w)]] = EX [Ew[Ln(w)]] = Ex[Gt] by Fubini’s theorem. Using these deﬁnitions we can deﬁne theoretical error functions TEn = Tn − Sn , and GEn = Gn − S . In particular we have GEn = K(q(y, x)||p(y|x, Dn)) and a similar result for the training error involving the empirical KL divergence. However, since Sn and S do not depend on the model or prior, minimising loss functions is equivalent to minimising the error. Thus, almost all of our discussions are based on these loss functions and their convergence properties. 2.2.4 Tempered posterior In accordance with the statistical physics analogy we will present in Section 2.3, we introduce a generalised version of the Bayesian posterior: Deﬁnition 2.11. The tempered posterior is deﬁned as p β(w|Dn) := 1 Z β n ϕ(w)e−nβLn(w) , where Z β n = ∫ W ϕ(w)e −nβLn(w)dw . We call β the inverse temperature. We denote the expectation of some function f (w) with respect to the tempered posterior by E β w[f (w)] := 1 Z β n ∫ W f (w)ϕ(w)e −nβLn(w)dw . Clearly when β = 1 we have pβ(w|Dn) = p(w|Dn), the standard posterior, and Ew = E 1 w. As such, we will mainly refer to the tempered posterior for the remainder of the thesis, where β = 1 can be viewed as a special case. Any other variable that we refer to with the β subscript is assumed to be in reference to the tempered posterior, for example, G β t = E β X [Ln(w)]. Remark 2.7. For practical purposes, when p is normally distributed as in Hypothesis 2.1, β manifests itself as the inverse variance of the regression model. The tempered posterior is well motivated in purely mathematical terms too. In computational Bayesian statistics, the presence of β can be used as a tunable hyperparameter. Furthermore, it arises naturally as the object which minimises information complexity under suitable constraints [Zha06], as the next lemma shows. Lemma 2.5. Let ϕ(w) > 0 be a prior on W . Suppose P (w) is the unique maximiser of the relative entropy K(P ||ϕ(w)) subject to the constraint Ew∼P [nLn(w)] = µβ for some ﬁxed µβ ∈ R. Then P (w) = pβ(w|Dn) for some β > 0 that depends on µβ. Proof. See Lemma A.3. 3 Remark 2.8. When β → ∞, the posterior is inﬁnitely concentrated at the maximum likelihood estimators ˆw. That is, limβ→∞ p(w|Dn) = δ(w − ˆw), where δ is the Dirac delta function [Wat09, §1.3]. 3My thanks to Matt Farrugia-Roberts for sharing this proof with me. 15 2.3 Deep Learning as a Gibbs Ensemble The use of terms from physics such as free energy, entropy and partition function can be justifed by formalising the setting of previous sections into a statistical physics context. The goal of this section is to explain an interpretation of our statistical learning setup as a physical canonical ensemble, shedding light on how it can be viewed as a complex system (see [THK18] for a formal deﬁnition of this). This analogy is by no means complete, but we hope it may serve as a useful conceptual framework for physical intuition, and also as a direct mathematical analogy to the widely developed literature of statistical physics. Thermodynamics is the study of macroscopic observables, such as energy, volume, pressure and mole numbers, associated to equilibrium states. Its central problem is to predict the equilibrium state which eventually results after a change to the total system, for example, after a constraint is removed from a system. We refer the reader to [Cal85] for an extensive overview of this ﬁeld. We formulate the learning machine as a Gibbs ensemble, in which we imagine the learning process as a physical system in contact with a thermal reservoir. This contact allows an exchange in energy, which corresponds to the ability to cause statistical ﬂuctuations of the system. In physics literature, the Gibbs ensemble, also known as the canonical ensemble, is deﬁned as follows: Deﬁnition 2.12. Consider a system with d particles, in a box of volume V , weakly coupled to and in thermal equilibrium with an inﬁnitely large heat reservoir at absolute temperature T . The number of particles in the system is ﬁxed but heat is exchanged with the environment to maintain a temperature T . Let Γ ⊆ RD denote the conﬁguration space of the particles and their properties, where σ ∈ Γ is viewed as a microscopic state (i.e. conﬁguration) of the system of particles. To each microscopic state is associated an energy given by the Hamiltonian, denoted H(σ). The fundamental postulate of the ensemble is that the probability density of points in phase space ρ : Γ → R is given by the Gibbs (or Boltzmann) distribution, ρ(σ) = e−βH(σ) Z , where Z = ∫ Γ e−βH(σ)dΓ . (2.10) We can immediately make the identiﬁcation between the phase space Γ of microscopic states and our weight space W , whereby a microstate conﬁguration σ ∈ Γ is identiﬁed4 with a weight w ∈ W (and we assume that the measure on Γ assigns an identical probability to each conﬁguration, thus dΓ = dσ = dw). Comparing the deﬁnition of ρ(σ) in (2.10) to pβ(w|Dn) in Deﬁnition 2.11, we note that to identify the Hamiltonian of the system we should rewrite pβ(w|Dn) = 1 Z 0,β n exp ( − β(nKn(w) − 1 β log ϕ(w) ) with Z 0,β n = ∫ W exp ( − β(nKn(w) − 1 β log ϕ(w))dw , which thus leads us to deﬁne: Deﬁnition 2.13. Given a dataset Dn, the (random) Hamiltonian of our learning machine is Hn(w) := nKn(w) − 1 β log ϕ(w) , (2.11) where β = 1 T is the inverse temperature of the ensemble. The Hamiltonian thus measures the violation of two diﬀerent penalty terms and can be thought of as the cost function that we wish to minimise. The ﬁrst violation is the loss of the model compared to the true distribution, nKn(w), and the second violation is imposed by the prior term − 1 β log ϕ(w). Notice that if ϕ(w) vanishes (or is negligibly small) then Hn(w) will be very large, meaning that we can interpret ϕ(w) as analogous to walls containing a gas. The posterior will 4One may choose to view the d particles of the ensemble as nodes of the neural network with associated conﬁg- urations given by incoming weights and biases. 16 thus be concentrated in those regions of W with low Hamiltonian values, which is where nKn(w) is small and ϕ(w) is large. The partition function can be viewed as the sum of Boltzmann weights over all possible con- ﬁgurations, meaning one might hope to express an “eﬀective” Hamiltonian F such that e−βF ≈ ∑ W e −βH(w)dw . This F is known as the free energy. This will be a crucial part of our analysis, and we will deﬁne it formally in Deﬁnition 3.4. Within physics, the free energy as a function of thermodynamic quantities is of fundamental importance, since expectation values of various functions often arise as derivatives of the free energy. For example, consider the canonical ensemble with the number of particles N and the temperature T ﬁxed. Then important physical quantities include: the entropy, S = − ∂F ∂T ; the speciﬁc heat capacity (at constant volume) CV = −T ∂2F ∂T 2 , and the pressure P = − ∂F ∂V . Using this setup it is thus possible to deﬁne macroscopic thermodynamic parameters implicit in statistical learning theory, such as the average energy U = E β w[Hn(w)] or the entropy of the Boltzmann distribution S = E β w[− log p β(w|Dn)]. Determining which such quantities are important to statistical learning remains an open problem. 17 Chapter 3 Singular Learning Theory In this chapter we outline some of the key concepts and results of Watanabe’s Singular Learning Theory as described in [Wat07; Wat09; Wat13; Wat18]. The basic observation of the theory is that many statistical models, including neural networks, are strictly singular, which implies that points on the set of true parameters W0 are degenerate singularities of K(w). This observation draws the link between statistical learning theory and the rich ﬁeld of singularity theory from algebraic geometry. For such models, regular free energy asymptotic results do not hold. By performing a desingularisation process of K(w) using Hironaka’s Resolution of Singularities, Watanabe derives the correct asymptotic forms of the free energy for singular models. In doing so, Watanabe arrives at a remarkable conjecture: complicated singularities correspond to simpler functions with lower generalisation error. Because of this, he says, singular models are naturally able to infer hidden structure from data. This is a profound statement with far reaching statistical consequences. The main purpose of the chapter is to provide a short summary of Singular Learning Theory for the uninitiated reader and show that feedforward ReLU neural networks are singular models. The proofs of the free energy asymptotic expansion is beyond the scope of this thesis, but we interpret the result in line with Occam’s Razor. We shall provide a mental framework that elucidates the mathematics underpinning the success of deep learning, and further informs our experiments demonstrating phase transitions in Chapter 5. 3.1 Singular Models Let us begin by outlining what deﬁnes a singular model and how the geometry of K(w) associated to such models is fundamentally diﬀerent to regular models. Deﬁnition 3.1. Let W ⊆ RD. The elements of the Fisher information matrix I(w) = {Ij,k(w)} D j,k=1 for a given statistical model p(y|x, w) are given by Ij,k(w) = ∫ ∫ RN +M ( ∂ ∂wj log p(y|x, w) ) ( ∂ ∂wk log p(y|x, w) ) p(y|x, w)q(x)dxdy , where the derivatives are evaluated at w. We assume q(x) is such that these integrals exist. Deﬁnition 3.2. A statistical model p(y|x, w) is identiﬁable if the map w ↦→ p(y|x, w) is injective for all x, y, and non-identiﬁable otherwise. Recall that I(w) is positive deﬁnite if for all x ∈ RD\\{0} and all w ∈ W , I satisﬁes xT I(w)x > 0. This is equivalent to I(w) having no zero eigenvalues for any w, thus det I(w) ̸= 0 for all w. Strictly singular models thus correspond to those models where det I(w) = 0 for some w ∈ W . Deﬁnition 3.3. A statistical model p(y|x, w) is regular if it is both identiﬁable and has positive deﬁnite I(w). It is called strictly singular if it is not regular. 18 The distinction between regular and singular models has profound consequences for the geom- etry of K(w), and therefore the learning process 1. The main reason for this is that when a model is regular, in a neighbourhood of a true parameter w(0) ∈ W0, K(w) can be approximated by a quadratic form K(w) ≈ 1 2 (w − w(0)) T I(w(0))(w − w(0)) , for which usual convex optimisation applies. However, if I(w(0)) is singular then this breaks down. Said diﬀerently, regular models obey asymptotic normality: p(w|Dn) d −→ N ( w(0), 1 n I(w(0)) −1) , which is known as the Bernstein-von Moses Theorem [Vaa07, §7]. If I(w(0)) is singular then this cannot hold as the inverse of I(w(0)) will not exist. Furthermore, the famed Bayesian information criterion, BIC = Ln(w0) + D 2 log n , Ln(w0) = min w0∈W Ln(w) used as a tool for comparison between two Bayesian models, is derived by performing a Laplace approximation of L(w) which depends on regularity of I(w(0)) for the second order term to exist [KK08]. All of this is to say, regular statistical results of model complexity are inadequate to describe singular models. The remainder of the section is dedicated to proving the following theorem. For simplicity of the proofs, we restrict our attention to two-layer networks deﬁned in Eq. (2.2), but the proof in full generality can be found in [Wat07] (and see [Mur+20]). Theorem 3.1. Let f : RN × W → RM be a ReLU neural network as deﬁned in Deﬁnition 2.1, and suppose we have a (model, truth, prior) triple as in Hypothesis 2.1. Then the associated Fisher information matrix I(w) is singular, thus feedforward ReLU neural network models are strictly singular models. Lemma 3.2. Under Hypothesis 2.1, the Fisher information is I(w)j,k = ∫ RN 〈 ∂ ∂wj f (x, w), ∂ ∂wk f (x, w) 〉 M q(x)dx . (3.1) Proof. Let M = 1 for simplicity, but note that the proof is easily generalised to higher dimensions using similar Gaussian arguments as in Lemma A.2. We have ∂ ∂wj log p(y|x, w) = ∂ ∂wj ( − M 2 log 2π − 1 2 (y − f (x, w)) 2) = − ( ∂ ∂wj f (x, w) ) (y − f (x, w)) which implies Ij,k(w) = ∫ RN ( ∂ ∂wj f (x, w) ) ( ∂ ∂wk f (x, w) ) q(x) (∫ R 1 √2π (y − f (x, w))2e− 1 2 (y−f (x,w))2dy) dx , and since the second central moment of a Gaussian with σ = 1 is 1, we get the result. The Fisher information matrix is related to the Hessian of K(w), HK(w) = { ∂2K(w) ∂wj ∂wk } D j,k=1, in a fundamental way: 1Rather loftily, Watanabe states that “almost all statistical models are singular” [Wat07], a statement to be taken with great seriousness, but in mathematical jest. 19 Lemma 3.3. Under Hypothesis 2.1, and assuming q(y|x) = p(y|x, w(0)) is realisable deﬁned by some ﬁxed w(0) ∈ W , the entries of the Hessian of K(w) satisfy ∂2 ∂wj∂wk K(w) = Ij,k(w) + ∫ RN 〈 f (x, w) − f (x, w(0)), ∂2 ∂wjwk f (x, w) 〉 M q(x)dx . In particular, HK(w(0)) = I(w(0)). Proof. The key property is the product rule applied to an inner product. Let g, h : W → RM be two functions, then writing ∂j = ∂ ∂wj we have ∂j⟨g(w), h(w)⟩ = ⟨∂jg(w), h(w)⟩ + ⟨g(w), ∂jh(w)⟩ . This gives ∂jK = ∫ RN ⟨∂jf (x, w), f (x, w) − f (x, w(0)⟩dx . The remaining details are left to the reader. To show that I(w) is singular we will show that its rows are linearly dependent. Lemma 3.4. Consider a given two-layer ReLU network fw : RN → R as deﬁned in Eq. (2.2) with d hidden nodes and a ﬁxed w ∈ W . Given a ﬁxed node i ∈ [d], fw satisﬁes the following diﬀerential equation on the open domains for which fw is diﬀerentiable (see Deﬁnition 4.2):    N∑ k=1 wik ∂ ∂wik + bi ∂ ∂bi − qi ∂ ∂qi    f = 0 . (3.2) Proof. Let ai = ⟨wi, x⟩ + bi. Straight calculations show ∂f ∂wi,k = qixk1 (ai > 0) , ∂f ∂bi = qi1 (ai > 0) , ∂f ∂qi = ReLU(ai) . Recalling that ReLU(ai) = ai1 (ai > 0), we have    N∑ k=1 wik ∂ ∂wik + bi ∂ ∂bi − qi ∂ ∂qi    f = qi   N∑ k=1 wi,kxk + bi   1 (ai > 0) − qi ReLU(ai) = 0 . We note that the scaling symmetry that shall be exhibited in Chapter 4 is responsible for this result. Let us now prove the main theorem. Proof of Theorem 3.1. Observing the form of Eq. (3.1), we ﬁrst show that I(w) is degenerate if and only if the set { ∂ ∂wj f (x, w) }D j=1 is linearly dependent. Note that here wj refers to the jth component of w ∈ W ⊆ RD, not to be confused with the speciﬁc weight vectors deﬁned in two-layer networks. To see this, ﬁrst suppose the set is linearly dependent, meaning there is some sequence rj ∈ R such that D∑ j=1 rj ∂ ∂wj f (x, w) = 0 . 20 Then for any ﬁxed k ∈ [d] we have D∑ j=1 rjIj,k(w) = D∑ j=1 rj ∫ ∫ RN +M 〈 ∂ ∂wj f (x, w), ∂ ∂wk f (x, w) 〉 q(x)dxdy = ∫ ∫ RN +M 〈 D∑ j=1 rj ∂ ∂wj f (x, w), ∂ ∂wk f (x, w) 〉 q(x)dxdy = 0 . In particular, letting I j(w) = [Ij,k(w)] D k=1 denote each row of I(w) we thus have D∑ j=1 rjI j(w) =   D∑ j=1 rjIj,k(w)   D k=1 = 0 , thus showing the rows of I(w) are linearly dependent and so I(w) is singular. For the reverse implication, if I(w) is singular then its rows must be dependent by the invertible matrix theorem. We leave the remaining details as an exercise for the reader. In particular, the diﬀerential equation in Eq. (3.2) implies that for any w ∈ W we have a lin- ear dependence relation for each node. The Fisher information restricted to each node is thus singular, and arranging I(w) into block diagonal form where the blocks correspond to each node shows that I(w) itself is singular, proving the claim. Connection to algebraic geometry This series of results provides the key link between statistical learning theory and algebraic geom- etry, as we now explain. Given an analytic function K : W → R, x ∈ W is a critical point of K if ∇K(x) = 0, and if it further satisﬁes K(x) = 0 then it is a singularity of K [Har10, §1.5]. In fact, any true parameter w(0) ∈ W0 is a singularity of K since K(w(0)) = 0, and ∇K(w(0)) = 0 because K(w) ≥ 0. However, what we are interested in are degenerate singularities. A singularity x ∈ W ⊆ RD of K is non-degenerate if in a neighbourhood of x one can write K(x) = x2 1 + · · · + x2 D for some set of local coordinates x1, . . . , xD. Otherwise, x is degenerate. By the Morse lemma [Gil93], if the Hessian of K at a singularity x is non-degenerate, then that singularity is non- degenerate, which corresponds to non-degenerate Fisher information matrix by Lemma 3.3. Then by Theorem 3.1, for feedforward ReLU neural networks, every point on W0 is a degenerate singu- larity of K. From the point of view of algebraic geometry, non-degenerate singularities of K are uninterest- ing. Even if a statistical model is non-identiﬁable (meaning true parameters are simply isolated minima of K), regular asymptotic results hold in a local sense, as discussed in [Bal97]. The strength of Singular Learning Theory’s results are only necessary for dealing with the case where W0 con- tains degenerate singularities, where it is shown that the nature of these singularities from an algebraic geometric perspective strongly aﬀect the statistical learning process. It is this realisation that makes Watanabe’s change in perspective truly groundbreaking for statistical learning. For intuition let us now examine how W0 typically diﬀers between regular and singular models. Example 3.1. Let W ⊆ R2 and denote w = (w, q) ∈ W . Consider a one-node two-layer ReLU network f : R × W → R. Deﬁne a model and an underlying truth by f (x, w) = q ReLU(wx) , f (x, w(0)) = θ ReLU(x) . 21 (a) Contour plots of KS(w, q) and KR(w, q). (b) Normalised posterior (see Remark 2.6) of singular and regular models, taking Kn(w) ≈ K(w). Figure 3.1: The diﬀerence between singular and regular models for θ = 1 5 and ϑ = 1 2 respectively, where ϕ(w) is uniform on [0, 1] 2 and n = 100. Assume that w, q, θ ≥ 0 where θ is ﬁxed, and assume that q(x) is the uniform distribution on [−√3, √3]. Then by Lemma 2.2, KS(w, q) = (qw − θ) 2 , so W S 0 = {(w, q) ∈ W | qw = θ} and thus every point is a degenerate singularity. We observe in Fig. 3.1a that this gives a kind of hyperbolic valley, where W0 corresponds to the ﬂoor of the valley. The degeneracy here comes from the fact that at every point w(0) ∈ W0, there is a tangent direction in which K(w) is still minimised. In contrast, consider a constant model and truth g : R × W → R2 (i.e. a feedforward ReLU network with only the last layer biases) such that g(x, w) = (w, q) , g(x, w(0)) = (ϑ, ϑ) , where we again assume w, q, ϑ ≥ 0 and ϑ is ﬁxed. Taking q(x) uniform on [0, 2] for ease, this gives KR(w, q) = (w − ϑ) 2 + (q − ϑ) 2 , and so W R 0 = (ϑ, ϑ). Fig. 3.1a shows that this induces a bowl-like paraboloid with a single minimiser at (w, q) = (ϑ, ϑ). For completeness, Fig. 3.1b shows the respective posteriors of the singular and regular model (up to a scale factor). 22 3.2 Free Energy As alluded to in Section 2.3, the free energy is a fundamental object of study in both physics and Bayesian statistics. Primarily, one can think of the free energy as being a measure of posterior density associated to a particular region of W . In physical terms, these regions of W are sets of microscopic states associated to particular macroscopic states. Such a view implies that optimising the free energy is perhaps the fundamental objective of statistical learning. Indeed many of the results of Singular Learning Theory suggest that this is the correct approach to understanding how neural networks generalise so eﬀectively, as well as being the key to understanding how neural networks undergo phase transitions. Deﬁnition 3.4. Given a dataset Dn, we deﬁne the total empirical free energy F β n ∈ R as F β n = − log Z β n = − log (∫ W ϕ(w)e−nβLn(w)dw) . We will typically just refer to F β n as the free energy. Let us inspect this deﬁnition a bit more closely. The free energy F β n depends on the choice of model p and prior ϕ, but more importantly it is inherently a random variable that depends on the random dataset Dn. To investigate the posterior landscape of a given true network in the search for phase transitions we will want to make statements independent of Dn, hence we may instead deﬁne the total free energy as a function of EX [nLn(w)] = nL(w), F β n = − log (∫ W ϕ(w)e −nβL(w)dw) . Note that F β n still depends on n even though the randomness in Dn has been marginalised out. Indeed, it is stated in [Wat18, §9.4] that F β n and F β n are asymptotically equivalent up to constant order, meaning we may interchange statements about either. Remark 3.1. In physics, F β n is known as the annealed average, whereas E[− log Z β n ] is known as the quenched average. This is a subtle but important distinction, which is explored in a statistical mechanics setting in [SST92] by appealing to the replica method. What makes the free energy more informative than K(w) is that it encodes both regions of maximum likelihood, which correspond to minimisers of K(w), as well as the generalisation of a region of parameters, as the next lemma shows. Lemma 3.5. Let Fn denote the free energy when β = 1. The generalisation loss is the average increase in free energy, Gn = EXn+1[Fn+1] − Fn . (3.3) In particular, the average free energy is the sum of the generalisation loss, EDn [Fn] = n−1∑ i=1 EDi[Gi] + ED1[F1] . Proof. The proof hinges on the fact that we may write Zn+1 Zn = ∫ W p(yn+1|xn+1, w)ϕ(w)e−nβLn(w)dw ∫ W ϕ(w)e−nβLn(w)dw = Ew[p(yn+1|xn+1, w)] = p(y|x, Dn) which implies Fn+1 − Fn = − log p(y|x, Dn) . The remaining details can be found in Lemma A.5. 23 In the process of model selection, which we will outline formally in Section 5.1, we will be interested in comparing the free energy associated to compact sets. Deﬁnition 3.5. Let W ⊆ W be a compact subset. We deﬁne the truncated posterior p β W (w|Dn) to be the posterior restricted to W, that is, p β W (w|Dn) = ϕ(w)e−nβLn(w)1 (w ∈ W) Z β n (W) , where Z β n (W) = ∫ W ϕ(w)e−nβLn(w)dw . We let E β W denote expectation with respect to pβ W (w|Dn). Deﬁnition 3.6. For a given β and n, the empirical free energy associated to a compact region W ⊆ W is F β n (W) = − log Z β n (W) = − log (∫ W ϕ(w)e−nβLn(w)dw) . The total free energy F β n(W) is deﬁned in the obvious way. Remark 3.2. Lemma 3.5 is easily generalised to Fn(W) by considering Zn+1(W) Zn(W) . Suppose W1, W2 ⊆ W are two compact sets, and suppose for simplicity that they have the same measure in W . If the posterior is more densely concentrated in W1 than W2, then ∫ W1 p(w|Dn)dw > ∫ W1 p(w|Dn)dw, implying F β n (W1) < F β n (W2). Combining this with its relation to generalisation from Lemma 3.5 shows why minimising the free energy should be our primary statistical goal. As in statistical physics, derivatives of the free energy with respect to intensive parameters often equate to expectation values and variances of quantities of interest. This is easy to see given the following lemma. Lemma 3.6. Let W ⊆ W be compact. The free energy of W satisﬁes ∂F β n (W) ∂β = E β W [nLn(w)] = nG β t (W) , and ∂2F β n (W) ∂β2 = −E β W [(nLn(w))2] + E β W [nLn(w)] 2 = −Vβ W [nLn(w)] . Proof. For the ﬁrst result, by straight calculation we have ∂F β n (W) ∂β = − 1 Z β n (W) ∂Z β n (W) ∂β = − 1 Z β n (W) ∂ ∂β (∫ W ϕ(w)e−nβLn(w)dw) = − 1 Z β n (W) ∫ W ϕ(w) ∂ ∂β e−nβLn(w)dw = 1 Z β n (W) ∫ W nLn(w)ϕ(w)e−nβLn(w)dw = E β W [nLn(w)] , where we may take the derivative inside the integral by similar arguments to those in Lemma 3.7. The second equality here follows from the deﬁnition Gβ t and Ln(w) = Kn(w) + Sn. We leave the second derivative to Lemma A.4. Whilst an important quantity, the free energy is both analytically and computationally in- tractable for most non-trivial models, meaning our model selection process hinges on asymptotic results instead. Deriving these asymptotics for singular models is the main result of Singular Learning Theory, which we will see in Section 3.3. The start of this proof begins with the following simple result. Lemma 3.7. Assume that Ln(w) is not a constant in w. Denote F β n (W) = F W n (β) to indicate W ⊆ W is ﬁxed and β is the function variable. Then 24 1. E β W [nLn(w)] is a decreasing function of β. 2. F W n (β) is continuous in β. 3. There exists a unique β∗ ∈ (0, 1) satisfying F W n (1) = E β∗ W [nLn(w)] . Proof. From Lemma 3.6, since ∂ ∂β E β W [nLn(w)] = −Vβ W [nLn(w)], and the variance is always posi- tive by the Cauchy-Schwarz inequality (and Ln(w) is non-constant), we see that ∂ ∂β E β W [nLn(w)] < 0 showing the ﬁrst claim. F W n (β) is continuous via a simple application of the Lebesgue dominated convergence theorem assuming Hypothesis 2.1, for which ϕ(w) and Ln(w) are continuous in w and positive. Merely consider a sequence βj → β and deﬁne a sequence of continuous functions fj(w) = ϕ(w)e−nβj Ln(w). Then fj(w) → f (w) = ϕ(w)e−nβLn(w), and |f (w)| is bounded since W is compact and f is continuous, meaning F W n (βj) = ∫ W fj(w)dw −→ ∫ W f (w)dw = F W n (β) by the dominated convergence theorem [SS05, §2] and so F W n (β) is continuous. For the this ﬁnal claim, ﬁrst note that by deﬁnition F W n (0) = 0, hence F W n (1) = ∫ 1 0 ∂F W n ∂β (β)dβ . Since Fn(β) is continuous, by the mean value theorem there exists a unique β∗ ∈ (0, 1) such that ∂F W n ∂β (β∗) = E β∗ W [nLn(w)] = F W n (1) − F W n (0) 1 − 0 = F W n (1) . Finding the optimal inverse temperature β∗ is where the heavy lifting of the resolution of singularities comes into play, which we shall now discuss. 3.3 Asymptotics of the Free Energy One of the key observations of Watanabe is that the real log canonical threshold λ is the funda- mental quantity determining the geometry of K(w), at least as it relates to statistical learning theory. Suppose ϕ(w) > 0 on all of W and K(w) is a real analytic function. The zeta function ζ(z) of a statistical model, where z ∈ C, is deﬁned as ζ(z) = ∫ W K(w)zϕ(w)dw . It is standard (see [Wat09, §4]) that ζ(z) can be analytically continued to a meromorphic function on the whole complex plane with Laurent expansion ζ(z) = ζ0(z) + ∞∑ k=1 mk∑ m=1 ckm (z + λk)m , (3.4) where ζ0(z) is holomorphic and ckm ∈ C are coeﬃcients, each λk ∈ Q>0 satisﬁes 0 < λ1 < λ2 < . . . and mk is the largest order of the pole λk. Deﬁnition 3.7. The real log canonical threshold (RLCT) of the (model, truth, prior) triple is λ = λ1 with multiplicity m = m1 of Eq. (3.4). Remark 3.3. The subsequent results hold on any compact set W , so in particular one may consider local RLCTs associated to some compact subset W ⊆ W by taking the integral over this domain in the deﬁnition of ζ(z). 25 Remark 3.4. Recall from Section 2.2.2 that K(w) is not in general analytic for ReLU networks, thus one instead pretends that we are referring to the swish function for the remainder of this discussion. As shown in [Wat09, Theorem 7.1], the RLCT can be understood as a volume co-dimension, that is, the number of “eﬀective parameters” near the most singular point of W0 ⊆ RD. In essence, λ = D′ 2 where D′ ≤ D is such that for every point w0 ∈ W0, K(w) has an expression in local coordinates of the form K(w) = D′ ∑ i=1 ciw2 i for some constants ci > 0 that may depend on w0. With this viewpoint, it becomes clear that the RLCT should be the quantity that describes the geometric behaviour of K(w) near true parameters, and thus the free energy. In fact, in regular models, the RLCT is precisely equal to D/2. To analyse the asymptotic expansion of the free energy, Watanabe’s idea is to desingularise K(w) by employing Hironaka’s Resolution of Singularities [Hir64], one of the fundamental results of algebraic geometry. Let us state the main theorem of [Wat13]. The fundamental conditions can be found in [Wat09, §6] and further summarised in [Wat13, §3]. In short, they ensure that W is a compact set whose boundary is deﬁned by several analytic functions, the prior is analytic, the model is suﬃciently integrable in the L s-space for s ≥ 6, and a local ﬁnite-variance property of K(w). Theorem 3.8. Consider a (model, truth, prior) triple satisfying the fundamental conditions. Let β = β0 log n for some constant β0 > 0, and let Ln(w0) = minw∈W Ln(w) (in the realisable case Sn = Ln(w0)). Then there exists a random variable Un such that E β w[nLn(w)] = nLn(w0) + λ β + Un √ λ log n 2β0 + Op(1) , (3.5) where λ is the RLCT of the triple and {Un} is a sequence of random variables which satisfy E[Un] = 0 and converges in distribution to a Gaussian random variable as n → ∞. Remark 3.5. Recall that op(1) denotes a sequence of random vectors that converge to zero in probability, and Op(1) denotes a sequence that is bounded in probability [Vaa07, §2.2]. Remark 3.6. So long as ϕ(w) > 0 on all of W Theorem 3.8 holds independent of the choice of ϕ(w). Combining this with Lemma 3.7, Watanabe shows: Theorem 3.9. The free energy satisifes Fn = E β∗ w [nLn(w)] at the optimal inverse temperature β∗ = 1 log n ( 1 + Un√2λ log n + op ( 1 √log n )) . The quantity E β∗ w [nLn(w)] is called the Widely Applicable Bayesian Information Criterion (WBIC). Remark 3.7. The WBIC result still holds for a truncated posterior on a compact W ⊆ W , that is, Fn(W) = E β∗ W [nLn(w)] for the same optimal inverse temperature. This allows us to discuss local free energy associated to diﬀerent regions of W . 26 Remark 3.8. These theorems have important consequences for practical computations, too. Firstly, Theorem 3.9 gives us a direct way of estimating the free energy of a given triple. We can estimate the posterior via a sampling procedure at inverse temperature β∗ = 1 log n , and then calculate the average log loss over these samples. Secondly, Theorem 3.8 shows we may estimate the RLCT via a simple linear regression on {(xj, yj)} for a sequence of points yj = E βj w [nLn(w)], xj = 1 βj = log n βj 0 , where ˆλ is the gradient of this line [Wat13, §6.2]. The WBIC is so called because it is the precise generalisation of the BIC for singular models: Corollary 3.10. If a model with W ⊆ RD is regular, then λ = D 2 and Un = 0, so WBIC = nLn(w0) + D 2 log n = BIC . (3.6) It is in this sense that deep learning is “unreasonably eﬀective” as described by Yann Lecun [LeC14]. According to the BIC, if D is very large (for example, D ∼ 1011 in the state of the art GPT-3 [Bro+20]) one should never use large neural networks as the dimensionality is massively penalised. But as Watanabe shows, it is not D that we care about in model selection, but λ. Let us explore this. Occam’s Razor We can interpret the asymptotic relationship in Eq. (3.5) as a competition between “energy and entropy”, or equivalently, “accuracy and complexity”. The above theorems show that in singular models, for any compact W ⊆ W we may write Fn(W) ≈ nLn(ω0) + λ log n β0 , (3.7) where Ln(ω0) = minω∈W Ln(ω), β0 = β∗ log n and λ is the local RLCT associated to the most singular point on W. For the rest of this discussion we assume we are in the realisable case. In [Bal97] the BIC in Eq. (3.6) is analysed for regular models. The nLn(w0) term is called the accuracy of the model, which is to say, the smallest loss that one can hope to attain for the model p(y|x, ω) evaluated at some parameter ω ∈ W. The D 2 log n term is a measure of complexity, where models with large numbers of parameters are penalised. This, he states, gives a mathematical realisation of Occam’s Razor: “plurality should not be posited without necessity,” [Bri] or typically known as “the simplest explanation is usually the right one”. This embodiment is even more apparent in the singular setting. Recall that λ can be thought of as measuring the eﬀective number of dimensions associated to a singularity on W0. The accuracy term in Eq. (3.5) is the same as in the BIC, but the complexity term is now dependent on λ. Let W, W ′ ⊆ W be two suﬃciently small compact sets and W0 = W ∩ W0, W ′ 0 = W ′ ∩ W0, which we assume are both non-trivial. Let ω0 ∈ W0 and ω′ 0 ∈ W ′. Since they are both true parameters their accuracies are equal, Ln(ω0) = Ln(ω′ 0) = Sn. Suppose, however, that the local RLCTs λ, λ ′ of W, W ′ respectively satisfy λ < λ ′. Then Fn(W) < Fn(W ′) meaning we should prefer models from W0 over W ′ 0 since they have lower model complexity as measured by the RLCT. Recalling the relationship between free energy and generalisation from Lemma 3.5, this illustrates Watanabe’s equivalence [Wat09, §7.6]: smaller λ ⇐⇒ more complicated singularity ⇐⇒ lower free energy ⇐⇒ better generalisation. This forms the motivation for our experiments in Chapter 5 where we will demonstrate that diﬀerent singularities can have diﬀerent free energies with simple examples. First we need to classify the points of W0 in some simple but nontrivial examples. This is the subject of the next chapter. 27 Chapter 4 Symmetries of W0 In order to demonstrate diﬀerences in the free energy of regions of W , we must ﬁrst begin by understanding what points are on W0. In this chapter we will fully charactersie the symmetries of the set of true parameters W0 when the true distribution is deﬁned by an activation-distinguished minimal two-layer feedforward ReLU network with two inputs and one output with m hidden nodes, and the model is deﬁned similarly but with d hidden nodes. This classiﬁcation procedure is equivalent to classifying functional equivalence of these networks, with the key insight being that their activation boundaries must be the same. We begin with the case where m = d and ﬁnd that W0 generically exhibits scaling, permutation, and under particular conditions, orientation reversing symmetry. This is then generalised to m < d where it is found that each of the d − m excess nodes is either degenerate or shares an activation boundary with another node. We then state the main theorem of [PL19] which proves similar results for networks of arbitrary depth. The section concludes with an example of networks with non-generic symmetries called m-symmetric networks. 4.1 Topology of Two-Layer Feedforward ReLU Networks Let fw : RN → R be a two-layer feedforward ReLU neural network with one input, two outputs and d hidden nodes for some ﬁxed w ∈ W . For convenience, recall Eq. (2.2): fw(x) = c + d∑ i=1 qiReLU(⟨wi, x⟩ + bi) (4.1) where w = (w1, . . . , wd, b1, . . . , bd, q1, . . . , qd, c) ∈ W ⊆ R4d+1 , where for each node i ∈ [d] := {1, . . . , d} we have wi ∈ R2, and bi, qi, c ∈ R. In this section a network will always refer to a network of this form. Deﬁnition 4.1. We say a node i ∈ [d] is degenerate in a network fw if either qi = 0 or wi = 0, thus meaning there is no meaningful contribution from i to fw(x). Remark 4.1. In the case where wi = 0 but qi ̸= 0 and bi ̸= 0 we may simply redeﬁne the total bias to be c ′ = c + qibi, thus meaning degeneracy need only be deﬁned in terms of the weights qi and wi. Without loss of generality, we exclude this case from all subsequent results. ReLU neural nets are piecewise aﬃne functions, thus determine a ﬁnite collection of regions with constant gradient deﬁned by activation boundaries, which we now formalise. Deﬁnition 4.2. Let α ∈ Λ where Λ is an index set. A linear domain U α ⊆ RN of fw is a connected open set such that: 1. fw is a simple plane with constant gradient and bias when fw is restricted to U α, that is, fw|U α (x) = ⟨wα, x⟩ + bα 28 for some wα ∈ RN and bα ∈ R, and; 2. U α is the maximal such set for which this plane is deﬁned. Since any network only contains ﬁnitely many nodes, |Λ| is necessarily ﬁnite. Remark 4.2. Note that wα and b α are the sum of the weights and biases that are active in the region U α, and also absorb the gradients qi and bias c. Given some network fw, the precise size of Λ is non-trivial in general. 1 Deﬁnition 4.3. Let i ∈ [d] be a non-degenerate node of fw(x) and let U α ∈ R2 be a linear domain. We say node i is active in U α if ⟨wi, x⟩ + bi ≥ 0 for all x ∈ U α. The activation boundary associated to i is the hyperplane Hi = {x ∈ RN | ⟨wi, x⟩ + bi = 0} . (4.2) When N = 2, Hi is merely a line in R2. We say Hi is degenerate if the corresponding node i is degenerate, meaning Hi is either empty or all of RN . Remark 4.3. Since each wi vector is normal to the activation boundary it deﬁnes, recall from Fig. 2.3b that these vectors “point” to the regions in which they are active when anchored on the activation boundary they deﬁne. Following from [PL19], we can make sense of these activation boundaries in the context of foldsets: Deﬁnition 4.4. Let Z ⊆ RN be open and g : Z → R a continuous piecewise linear function. The foldset of g is F(g) = {x ∈ Z ∣ ∣ ∣ g is not diﬀerentiable at x} . Lemma 4.1. Let fw : RN → R be a two-layer feedforward ReLU neural network as in (2.2). Then fw is continuous, and F(fw) = d⋃ i=1 {Hi | i is non-degenerate} . (4.3) Proof. Since ReLU(x) is continuous and fw is a sum of ReLU’s composed with aﬃne functions, continuity is clear. Note that ReLU(x) is non-diﬀerentiable at x = 0 since for x ̸= 0 we have d dx ReLU(x) =   1 x > 0 0 x < 0 which is clearly discontinuous at x = 0. Any degenerate nodes will have at most constant contribu- tion to fw(x), so these nodes won’t contribute to F(fw). For any non-degenerate node i, observing the form of fw(x) in Eq. (2.2) shows that fw is non-diﬀerentiable when ⟨wi, x⟩ + bi = 0, which is the activation boundary Hi, giving the result. By deﬁnition, the linear domains and foldsets are related by ⋃ α∈Λ U α = R2 \\ (F(fw)) . 1The reader is referred to [Iva10] for an interesting discussion when these are lines in the plane for N = 2. 29 4.2 Classiﬁcation of W0 for m = d Recall from Deﬁnition 2.8 that the set of true parameters is deﬁned by W0 = {w ∈ W | p(y|x, w) = q(y|x)} . In classifying W0 we are obviously only interested in the case that it is non-empty, hence we may assume that q(y|x) = p(y|x, w(0)), where we take w(0) ∈ W to be a ﬁxed parameter deﬁning a two-layer feedforward ReLU neural network f0(x) := f (x, w(0)) with two inputs and one output. Since probability distributions are uniquely deﬁned (up to a set of measure zero), this condition implies that p(y|x, w) and p(y|x, w(0)), under Hypothesis 2.1, must have the same mean. Thus W0 = {w ∈ W | f (x, w) = f0(x)} , and so the task of classifying W0 becomes classifying functional equivalence of this class of net- works. We begin in the simplest case where f0 is activation-distinguished and minimal, and the model and truth have the same number of parameters. This section is dedicated to proving the following classiﬁcation theorem. Suppose the model and truth have the same number of hidden nodes, and the true network is minimal and activation- distinguished. Then we will show in Theorem 4.7 that W0 exhibits three kinds of symmetry: • Scaling symmetry of the incoming and outgoing weights to any node. • Permutation symmetry of the hidden nodes. • Orientation reversing symmetry of the weights, only allowed for collections of weights which sum to zero. The key observation that guides this proof is that the foldsets of the model and truth must be equal. Once this is accounted for, the rest is merely a matter of good bookkeeping. 4.2.1 Deﬁnitions and hypotheses Inspired by deﬁnitions in [Sus92] which deals with tanh neural networks, we begin with some simplifying deﬁnitions. Deﬁnition 4.5. Let Z ⊆ R2 be an open set. Given a two-layer network fw : Z → R with d hidden nodes, fw is minimal if: given another two-layer network f ′ w′ : Z → R with d′ ≤ d hidden nodes such that fw(x) = f ′ w′(x) for all x ∈ Z, then the number of hidden nodes are necessarily equal, d = d ′. We say fw is activation-distinguished if each non-degenerate Hi is unique, that is, Hi ̸= Hj for each i ̸= j ∈ [d]. Remark 4.4. Each node of a minimal network is necessarily non-degenerate. To see why we impose the activation-distinguished condition, observe that if f (x, w) has d unique activation boundaries then it is necessarily minimal, but the converse is not necessarily true: Example 4.1. Consider w = (w1, w2, b1, b2, q1, q2, c) = ((1, 1), (−1, −1), 0, 0, 1, 1, 0), so f (x, w) = ReLU(x1 + x2) + ReLU(−x1 − x2) . Then H1 is the line x1 + x2 = 0 and H2 is the line −x1 − x2 = 0, and so clearly H1 = H2 as subsets of R2, thus there is only one unique foldset. However f (x, w) is minimal: suppose there was a one- node neural network f (x, wr) = c + qReLU(⟨x, r⟩ + b) which produced the same input output map. Then this network necessarily has a region of inactivation by the deﬁnition of ReLU, ⟨x, r⟩ + b < 0, and f (x, wr) has zero gradient in this region. But f (x, w) has non-zero gradient in both regions {(x1, x2) | x2 ≥ x1} and {(x1, x2) | x2 ≤ x1}, thus we could not have f (x, wr) = f (x, w). 30 Remark 4.5. It is possible to show that n non-parallel lines divide the plane into Ln = n(n+1) 2 + 1 diﬀerent regions. As such, in principle one can check for the minimality of a given network f by counting |Λ| and comparing this to Ln. For the remainder of this section we will enforce some simplifying hypotheses: Hypothesis 4.1. Let q(y|x) be a realisable true distribution deﬁned by a two-layer feedforward ReLU neural network f0 := f (· , w(0)) : R2 → R1 with m hidden nodes for some ﬁxed parameter w(0) ∈ W . We call f0 the true network. Let p(y|x, w) be the model as in Hypothesis 2.1 deﬁned by a two-layer feedforward ReLU neural network f : R2 × W → R1 with d hidden nodes. We further impose: • f0(x) is minimal and distinguished, so all activation boundaries Hi are non-degenerate and unique. • The width of the true network and the model are equal, so m = d. • w(0) is deﬁned by w(0) = (w(0) 1 , . . . , w(0) m , b (0) 1 , . . . , b(0) m , q(0) 1 , . . . , q(0) m , c (0)) , where w(0) i ∈ R2\\{(0, 0)}, b (0) i ∈ R, q(0) i ∈ R\\{0} and c(0) ∈ R for each i ∈ [d], since every node is non-degenerate. To characterise W0, we thus set f (x, w) = f0(x) and describe the possible values of w. Remark 4.6. Clearly the model f (x, w) must also be minimal and activation-distinguished. Armed with these formulations we are thus ready to begin investigating the symmetries of W0. 4.2.2 Lemmas The results of Lemma 4.2, Lemma 4.4, Lemma 4.5, Lemma 4.6 and Theorem 4.7 all assume the conditions of Hypothesis 4.1. The fundamental observation that guides this classiﬁcation is that the foldsets of a network are the pivotal piece of data. Given that the foldsets of the model and truth must be equal, this restricts the classiﬁcation process to observing equivalence of lines in the plane, and then ensuring the gradients and biases are equivalent. Beginning with this observation, the ﬁrst form of symmetry is a simple permutation of nodes. Lemma 4.2. Let Hi denote the activation boundaries associated to f (x, w) as in (4.2) and let H (0) i denote the activation boundaries associated to f0(x). Then there exists some permutation σ ∈ Sm (the symmetric group of order m) such that Hi = H (0) σ(i) for each i ∈ [d]. Proof. Since f (x, w) = f0(x) for all x ∈ R2, and f (x, w) must also be minimal, they must also be non-diﬀerentiable at the same points, so F(f (x, w)) = d⋃ i=1 Hi = m⋃ j=1 H (0) j = F(f0(x)) . Each Hi and H (0) j are distinct by hypothesis, which we will show implies there is a unique j ∈ [m] such that Hi = H (0) j . First observe that there can only be ﬁnitely many intersection points of the lines {Hi}i∈[d] (and similarly for {H (0) j }j∈[m]), thus let x ∈ F(f (x, w)) be a non-intersection point uniquely associated to a line Hi for some i ∈ [d], so x ∈ Hi \\ (⋃ j̸=i Hj). Then since the foldsets are equal, x ∈ F(f0(x)), and since the points of intersection of the sets {Hi}i∈[d] coincide with those of {H (0) j }j∈[m], x is a non-intersection point of the true foldset lines and thus is uniquely associated to a line H (0) j . 31 To see that this implies Hi = H (0) j for all x ∈ Hi, suppose we pick d + 1 non-intersection points on Hi. Then since m = d, by the pigeonhole principle there must be at least two points associated to some H (0) j . But since lines are uniquely determined by two points, this implies Hi = H (0) j for all non-intersection points x ∈ Hi. By continuity this also applies to intersection points, thus there is a unique j ∈ [m] such that Hi = H (0) j . Finally, since m = d, our previous statement simply says that there is a unique bijection σ : [d] → [m] associating lines in F(f0(x)) to lines in F(f (x, w)), thus giving the desired σ ∈ Sm. Lemma 4.2 induces a scaling symmetry in the weights and biases. Before demonstrating this, we begin with a more general result. Lemma 4.3. Let w, w′ ∈ R2 \\ {0} and b, b ′ ∈ R be given and let H = {x ∈ R2 | ⟨w, x⟩ + b = 0}, and H ′ = {x ∈ R2 | ⟨w′, x⟩ + b′ = 0} . Then H = H ′ if and only if there exists some scalar λ ∈ R \\ {0} such that w = λw′ and b = λb ′. Proof. See Lemma A.6. Lemma 4.4. Given f (x, w) = f0(x), there exists a unique σ ∈ Sm, and for each i ∈ [d] there exists an ϵi ∈ Z2 and λi ∈ R>0, such that wi = (−1) ϵiλiw(0) σ(i) , and bi = (−1) ϵiλib(0) σ(i) , where λi = q(0) σ(i) qi , meaning qi and q(0) σ(i) necessarily have the same sign. Proof. Lemma 4.2 gives the permutation σ ∈ Sm relating the activation boundaries of f (x, w) and f0(x). For each i ∈ [d] Lemma 4.3 gives us a µi ∈ R\\{0}, which we can decompose into µ = (−1)ϵiλi for some ϵi ∈ Z2 and λi ∈ R>0 such that we can initially write wi = (−1)ϵiλiw(0) σ(i) , and bi = (−1)ϵiλib (0) σ(i) . (4.4) Let i ∈ [d] be ﬁxed and ﬁx a non-intersection point x ∈ Hi \\ (⋃ j̸=i Hj). Let U ∋ x be a suﬃciently small open ball around x such that U ∩ (⋃ j̸=i Hj) = ∅, which exists since there are only ﬁnitely many (and thus isolated) points of intersection. Recall that by hypothesis Hi ̸= Hj for any other j ̸= i, thus excluding the possibility of any other node being activated across the boundary Hi. Then the vector wi emanating from x points towards a unique linear domain U α for which x is on the closure of U α, and node i is active in U α. Similarly, −wi points towards a diﬀerent unique linear domain U β satisfying the same closure property, but for which node i is inactive in U β. Thus we can ﬁnd a unique decomposition U = U − ∪ U + where U − = U ∩ U β , and U + = U ∩ U α Then we have f |U + (x, w) = f |U − (x, w) + qi(⟨wi, x⟩ + bi) . Similarly, consider the same set up for the line H (0) σ(i) = Hi associated to f0(x), where U0 = U is the same suﬃciently small neighbourhood and U − 0 and U + 0 are the regions of inactivation and activation (referring to f0) respectively. Note that the orientation of wi will determine whether U + = U + 0 or U + = U − 0 , and similarly for U −. Explicitly, we then have f0|U + (x) = f0|U − (x) + q(0) σ(i) (⟨w(0) σ(i), x⟩ + b(0) σ(i)) . (4.5) 32 First suppose ϵi = 0, so wi and w(0) σ(i) are oriented in the same direction, hence U + = U + 0 and U − = U − 0 . Since f (x, w) = f0(x) we have qi(⟨wi, x⟩ + bi) = f |U + (x, w) − f |U − (x, w) = f0|U + (x) − f0|U − (x) = q(0) σ(i) (⟨w(0) σ(i), x⟩ + b (0) σ(i)) , and so by comparing polynomial coeﬃcients we must have wi = q(0) σ(i) qi w(0) σ(i) , and bi = q(0) σ(i) qi b (0) σ(i) . (4.6) If ϵ1 = 1 then wi and w(0) σ(i) are oriented in diﬀerent directions, thus U + = U − 0 and U − = U + 0 so we have qi (⟨wi, x⟩ + bi) = f |U + (x, w) − f |U − (x, w) = f0|U − (x) − f0|U + (x) = −q(0) σ(i) (⟨w(0) σ(i), x⟩ + b(0) σ(i)) and so again comparing coeﬃcients we have wi = − q(0) σ(i) qi w(0) σ(i) , and bi = − q(0) σ(i) qi w(0) σ(i) . (4.7) Combining (4.6) with (4.7) we can write wi = (−1)ϵi q(0) σ(i) qi w(0) σ(i) , and bi = (−1) ϵi q(0) σ(i) qi w(0) σ(i) , thus giving λi = q(0) σ(i) qi as advertised. Since λi must be positive by (4.4), we see that qi necessarily has the same sign as q(0) σ(i). At ﬁrst glance, one may assume that all orientations must be preserved, that is, all ϵi = 0, to yield functional equivalence. But as the next example shows, this is not necessarily the case. Example 4.2. Consider a simple one dimensional ReLU neural network f : R × W → R with d = 2 hidden nodes. Deﬁning w(0), w ∈ W such that f0(x) = ReLU(x + 1) + ReLU(−x + 1) , and f (x, w) = 2 + ReLU(−x − 1) + ReLU(x − 1) , we see that f (x, w) = f0(x) and so w = (−1, 1, −1, −1, 1, 1, 2) is also a true parameter for w(0) = (1, −1, 1, 1, 1, 1, 0). Notice that here we have w1 + b1 = −(w(0) 1 + b (0) 1 ) and similarly w2 + b2 = −(w(0) 2 + b (0) 2 ), meaning ϵ1 = ϵ2 = 1. But this works because the weights of both networks sum to zero. We use this example as the inspiration for the orientation reversing symmetry. We let Gα(f (x, w)) ∈ R2 denote the gradient computed by f (x, w) in the linear domain U α, and similarly Bα(f (x, w)) ∈ R the bias. Lemma 4.5. Let E = {i ∈ [d] | ϵi = 1}. Given σ ∈ Sm, λi ∈ R\\{0} and ϵi ∈ Z2 from Lemma 4.4, the weights of the true network necessarily satisfy ∑ i∈E q(0) σ(i)w(0) σ(i) = 0. Proof. Let U α be a ﬁxed domain associated to f (x, w). For notational convenience, deﬁne δα j :=    1 if ⟨w(0) j , x⟩ + b(0) j ≥ 0 0 otherwise , and δj α := 1 − δα j , (4.8) 33 thus δα j indicates whether node j ∈ [m] of f0(x) is active in the region U α, and δα j indicates the converse. Let i ∈ [d] be a node of f (x, w) and σ(i) the corresponding node of f0(x) such that Hi = H (0) σ(i). Following the result of Lemma 4.4 we can calculate f (x, w) = c + d∑ i=1 qiReLU (⟨wi, x⟩ + bi) = c + d∑ i=1 qiλiReLU ( (−1)ϵi (⟨w(0) σ(i), x⟩ + b (0) σ(i))) , where λi = q(0) i qi > 0. In particular, the single node map ReLU(⟨wi, x⟩ + bi) ↦−→ λi ReLU ((−1) ϵi (⟨w(0) σ(i), x⟩ + b(0) σ(i))) shows that if node σ(i) is active in U α, then we are in one of two situations: either ϵi = 0 and δα σ(i) = 1, or ϵi = 1 and δα σ(i) = 1. We can then equate gradients Gα(f (x, w)) = Gα(f0(x)), and recall q(0) σ(i)w(0) σ(i) = (−1)ϵi qiwi from Lemma 4.4, to calculate ∑ i∈[d] δα σ(i)q(0) σ(i)w(0) σ(i) = ∑ i /∈E δα σ(i)qiwi + ∑ i∈E δα σ(i)qiwi = ∑ i /∈E δα σ(i)q(0) σ(i)w(0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)(−w(0) σ(i)) . But by deﬁnition we have ∑ i∈[d] δα σ(i)q(0) σ(i)w(0) σ(i) = ∑ i /∈E δα σ(i)q(0) σ(i)w(0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)w(0) σ(i) , so subtracting one from the other shows that ∑ i∈E (δα σ(i) + δα σ(i)) q(0) σ(i)w(0) σ(i) = ∑ i∈E q(0) σ(i)w(0) σ(i) = 0 . Example 4.2 shows us that we can expect a similar result for the biases. Lemma 4.6. With E as in Lemma 4.5, given σ ∈ Sm and ϵi ∈ Z2 from Lemma 4.4, the biases of the true network satisfy c0 + ∑ i∈E q(0) σ(i)b (0) σ(i) = c . Proof. Let U α be a ﬁxed linear domain, so the same arguments from Lemma 4.5 regarding active nodes on U α apply. The bias c is active on every domain, so equating B(f (x, w)) = B(f0(x)) gives c0 + ∑ i∈[d] δα σ(i)q(0) σ(i)b(0) σ(i) = c + ∑ i /∈E δα σ(i)qibi + ∑ i∈E δα σ(i)qibi = c + ∑ i /∈E δα σ(i)q(0) σ(i)b (0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)(−b (0) σ(i)) , but again recalling ∑ i∈[d] δα σ(i)q(0) σ(i)b(0) σ(i) = ∑ i /∈E δα σ(i)q(0) σ(i)b(0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)b(0) σ(i) , we thus have c0 + ∑ i∈E δα σ(i)q(0) σ(i)b (0) σ(i) = c − ∑ i∈E δα σ(i)q(0) σ(i)b (0) σ(i) , and so c0 + ∑ i∈E q(0) σ(i)b (0) σ(i) = c. Remark 4.7. Our convention is to take the empty sum to be zero, so if E is empty then we have c = c0. 34 4.2.3 Main theorem for m = d We have thus arrived at the main theorem of this section having classiﬁed all of the symmetries of W0 for m = d. We ﬁrst introduce some notation. Let σ ∈ Sm be a ﬁxed permutation and deﬁne Xi := { (λi, qi) ∈ R>0 × R ∣ ∣ ∣ λiqi = q(0) σ(i)} , and Υ := { ϵ : [d] → Z2 ∣ ∣ ∣ ∣ ∣ ∑ i∈E q(0) σ(i)w(0) σ(i) = 0 , and c0 + ∑ i∈E q(0) σ(i)b(0) σ(i) = c } , where E = {i ∈ [d] | ϵi = 1}. Theorem 4.7. Let f0 : R2 → R1 , f0(x) := f (x, w(0)) be an activation distinguished minimal feedforward ReLU neural network with two-layers, d hidden nodes deﬁned by some ﬁxed parameter w(0) ∈ W . Then there is a bijection Ψ : m∏ i=1 Xi × Sm × Υ ∼= −→ W0 ((λi, qi)m i=1 , σ, ϵ ) ↦−→   ((−1)ϵiλiw(0) σ(i))d i=1 , ((−1)ϵiλib(0) σ(i))d i=1 , (qi) d i=1, c0 + ∑ i∈E q(0) σ(i)b(0) σ(i)   . We refer to ∏m i=1 Xi as scaling symmetry, Sm as permutation symmetry and Υ as orientation reversing symmetry. Proof. We ﬁrst verify that Ψ is well deﬁned, that is, f (x, Ψ(θ)) = f0(x) as functions. We compute for a ﬁxed θ = ((λi, qi) m i=1, σ, ϵ) ∈ ∏m i=1 Xi × Sm × Υ f (x, Ψ(θ)) = c0 + ∑ i∈E q(0) σ(i)b (0) σ(i) + d∑ i=1 qiReLU (〈 (−1)ϵiλiw(0) σ(i), x 〉 + (−1)ϵi λib(0) σ(i) ) = c0 + ∑ i∈E q(0) σ(i)b (0) σ(i) + d∑ i=1 qiλiReLU ( (−1) ϵi (〈 w(0) σ(i), x 〉 + b (0) σ(i) )) = c0 + ∑ i∈E q(0) σ(i)b (0) σ(i) + d∑ i=1 q(0) σ(i)ReLU ( (−1) ϵi (〈 w(0) σ(i), x 〉 + b (0) σ(i) )) . Thus we see that f (x, Ψ(θ)) has the same foldsets as f0(x). It remains to check the gradients and biases in any domain U α agree. Let δα σ(i) be as in Lemma 4.5 so it refers to active nodes of f0(x). Then for any domain U α the gradient computed by f0(x) is Gα(f0(x)) = ∑ i∈[d] δα σ(i)q(0) σ(i)w(0) σ(i) , whereas the gradient computed by f (x, Ψ(θ)) is Gα(f (x, Ψ(θ))) = ∑ i /∈E δα σ(i)q(0) σ(i)w(0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)(−w(0) σ(i)) = ∑ i /∈E δα σ(i)q(0) σ(i)w(0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)w(0) σ(i) = ∑ i δα σ(i)q(0) σ(i)w(0) σ(i) = Gα(f0(x)) , where the second equality followed from our hypothesis on ϵ ∈ Υ. Similarly, the bias computed by f0(x) is Bα(f0(x)) = c0 + ∑ i∈[d] δα σ(i)q(0) σ(i)b(0) σ(i) 35 whereas for f (x, Ψ(θ)) it is Bα(f (x, Ψ(x))) = c0 + ∑ i∈E q(0) σ(i)b(0) σ(i) + ∑ i /∈E δα σ(i)q(0) σ(i)b(0) σ(i) + ∑ i∈E δα σ(i)q(0) σ(i)(−b(0) σ(i)) = c0 + ∑ i∈E q(0) σ(i)b(0) σ(i) + ∑ i∈[d] δα σ(i)q(0) σ(i)b (0) σ(i) − ∑ i∈E q(0) σ(i)b(0) σ(i) = c0 + ∑ i∈[d] δα σ(i)q(0) σ(i)b(0) σ(i) = Bα(f0(x)) , where the second equality follows from δα σ(i) = 1 − δα σ(i). Therefore we have f (x, Ψ(θ)) = f0(x) and so Ψ is well deﬁned. For injectivity, if Ψ(θ) = Ψ(θ′) for suitable θ, θ′ ∈ ∏m i=1 Xi × Sm × Υ then we can read oﬀ qi = q′ i, hence λi = λ′ i for all i. We may then compare foldsets of f (x, Ψ(θ)) and f (x, Ψ(θ′)) which are identically labelled to recover σ = σ′. Finally we may read oﬀ each ϵi and ϵ′ i from each weight entry, thus ϵ = ϵ′ and so θ = θ′. For surjectivity, if f (x, w) = f0(x) then they must have identical foldsets as in Lemma 4.2, where the weights and biases must be equal up to scaling from Lemma 4.3, and ϵ can only be non-zero under the hypothesis on Υ shown in Lemma 4.5. Thus Ψ is a bijection. Remark 4.8. We now see that the activation-distinguished condition allows us to uniquely identify the permutation σ relating activation boundaries, and ensures only one node changes across each boundary. 4.3 Classiﬁcation of W0 for m < d The key assumption of the previous section was that the number of parameters in the model d and (minimal) truth m were equal. Let us now weaken this condition and examine the case m < d. In this section we will show that of the d nodes associated to the model, the symmetries associated to m of them are the same as in Theorem 4.7 - without loss of generality let these nodes be [m]. By contrast, each “excess” node i ∈ {m + 1, . . . , d} will either be • Degenerate, so qi = 0 or wi = 0, or; • Have the same activation boundary as a node in [m]. Thus in this case W0 has degenerate-node symmetries, suitably adjusted scaling symmetries, suit- ably adjusted permutation symmetries and almost identical orientation reversing symmetry. 4.3.1 Hypotheses, deﬁnitions and lemmas Hypothesis 4.2. We assume the same conditions as in Hypothesis 2.1, but with the number of hidden nodes m in the true network f0(x) strictly less than those d in the model f (x, w), so m < d. In particular, we assume that f0(x) is still minimal and activation-distinguished. We assume the conditions of Hypothesis 4.2 for Lemma 4.8, Lemma 4.9, Lemma 4.10 and Theorem 4.11. Lemma 4.8. Let Hi denote the activation boundaries associated to the model for i ∈ [d], and H (0) j those to the truth for j ∈ [m]. Let K = d − m > 0 denote the number of excess parameters in the model. Then: 1. There exists a 0 ≤ k ≤ K such that k nodes of f (x, w) (and thus their corresponding activation boundaries) are degenerate. 36 2. Consider the remaining d ′ = K − k ≥ m nodes of the model. Without loss of generality we may write this set as {1, . . . , d′}. Then there is a surjective ﬁnite set map π : {1, . . . , m, m + 1, . . . d ′} −→ {1, . . . , m} such that Hi = H (0) π(i) for each i ∈ [d ′]. If d′ = m then π is a bijection. Proof. Once again, the key observation is that since f (x, w) = f0(x), their foldsets must also be equal, thus F(f0(x)) = m⋃ j=1 H (0) j = d⋃ i=1 {Hi | i is non-degenerate} = F(f (x, w)) . Since f0 is minimal and activation-distinguished, so ⋃m j=1 H (0) j comprises m distinct lines in the plane, the model requires at least m non-degenerate activation boundaries for these foldsets to be equal. Without loss of generality, suppose the non-degenerate nodes of the model are {1, . . . , m}. Thus, there exists a permutation σ ∈ Sm such that Hi = H (0) σ(i) for i ∈ [m]. For clarity, let us write F(f (x, w)) =   m⋃ i=1 Hi   ⋃   d⋃ i=m+1{Hi | i is non-degenerate}   . Since ⋃m i=1 Hi = ⋃m j=1 H (0) j , we see that d⋃ i=m+1{Hi | i is non-degenerate} ⊆ m⋃ j=1 H (0) j . Let Hi be one of the remaining activation boundaries for m + 1 ≤ i ≤ d. Then using the same arguments associating unions of lines to one another as in Lemma 4.2, there are two choices: 1. {Hi | i is non-degenerate} is empty, thus Hi is degenerate, or; 2. Hi = H (0) j for some j ∈ [m]. Enumerating these choices over each m + 1 ≤ i ≤ d, the model thus has 0 ≤ k ≤ d − m degenerate nodes, and d ′ = d − k ≥ m non-degenerate nodes. Let υ : {m + 1, . . . , d′} → {1, . . . , m} denote the map given as a result of the second choices above. Combining σ and υ gives a map π : {1, . . . , d′} → {1, . . . , m} which acts as σ on {1, . . . , m}, and as υ on {m + 1, . . . , d′}. The map π inherits its surjectivity from σ, and if d ′ = m then π = σ and thus is a bijection. Remark 4.9. If d ′ > m then π is clearly non-injective, meaning multiple nodes of the model will share the same activation boundaries. In light of Lemma 4.8, we can introduce a new kind of network that simpliﬁes the degeneracy property of f (x, w). Deﬁnition 4.6. Let w ∈ W ⊆ R4d+1, and f : R2 × W → R be a two-layer feedforward ReLU network with d hidden nodes. Suppose f (x, w) is an activation-distinguished network such that 0 ≤ k ≤ d nodes are degenerate, and let d ′ = d − k. Then by Lemma 4.8 there exists an • activation-distinguished two-layer ReLU network g : R2 × W ′ → R with d ′ hidden nodes such that W ′ ⊆ R4d ′+1 and W ′ ⊆ W , and; • a parameter w′ ∈ W ′ with d ′ non-degenerate nodes equal to the non-degenerate nodes of w; such that f (x, w) = g(x, w′) for all x ∈ R2. We call g(x, w′) the degenerate reduced network of f (x, w). 37 Since the networks in Deﬁnition 4.6 are activation distinguished, this deﬁnition uses the ﬁrst form of symmetry in Lemma 4.8 to remove degenerate nodes from both the function and the vector. Assuming f (x, w) to be in its degenerate reduced form with d′ hidden nodes, we can now classify the excess nodes m + 1 ≤ i ≤ d′ by appealing to the same scaling symmetry as seen in Lemma 4.3. Lemma 4.9. Assume we have the data of Lemma 4.8, where f (x, w) is in its degenerate reduced form with d ′ hidden nodes. Then for each i ∈ [d ′], there exists an ϵi ∈ Z2 and λi ∈ R>0 such that wi = (−1) ϵiλiw(0) π(i) , bi = (−1) ϵiλib(0) π(i) . (4.9) Moreover, let Mj = {i ∈ [d′] | π(i) = j} for j ∈ [m]. Then the λi are constrained such that ∑ i∈Mj qiλi = q(0) j . Proof. The proof is nearly identical to Lemma 4.4. Lemma 4.3 gives Eq. (4.9) since Hi = H (0) π(i) for each i ∈ [d ′]. As in Lemma 4.4, we analyse a small neighbourhood U centred at an isolated non-intersection point x ∈ Hi\\ (⋃ j̸=i Hj), and let U + be the region where the truth is active and U − where it is inactive. Let χ± i indicate when node i of the model is active in the regions U ±. Then f |U ± (x, w) = c + ∑ i∈[d′] χ± i qi(⟨wi, x⟩ + bi) . In particular, letting Ej = {i ∈ [d ′] | π(i) = j , ϵi = 1} we have f |U + (x, w) = c + ∑ i /∈Mj χ + i qi(⟨wi, x⟩ + bi) + ∑ i /∈Ej qiλi (⟨w(0) π(i), x⟩ + b(0) π(i)) , and f |U − (x, w) = c + ∑ i /∈Mj χ − i qi(⟨wi, x⟩ + bi) − ∑ i∈Ej qiλi (⟨w(0) π(i), x⟩ + b (0) π(i)) . The key insight is that the only nodes i that ﬂip activation on this boundary are those with π(i) = j. That is, for i /∈ Mj, χ + i = χ− i . Thus f |U + (x, w) − f |U − (x, w) = ∑ i /∈Ej qiλi (⟨w(0) π(i), x⟩ + b(0) π(i)) + ∑ i∈Ej qiλi (⟨w(0) π(i), x⟩ + b(0) π(i)) =   ∑ i /∈Ej qiλi + ∑ i∈Ej qiλi   (⟨w(0) π(i), x⟩ + b (0) π(i)) =   ∑ i∈Mj qiλi   (⟨w(0) π(i), x⟩ + b(0) π(i)) . But since the model and truth are functionally equivalent, recalling Eq. (4.5) we have f |U + (x, w) − f |U − (x, w) =   ∑ i∈Mj qiλi   (⟨w(0) π(i), x⟩ + b(0) π(i)) = q(0) π(i) (⟨w(0) π(i), x⟩ + b(0) π(i)) = f0|U + (x, w) − f0|U − (x, w) and so by comparing coeﬃcients of the polynomials we get the claim. Remark 4.10. It is easy to see that Lemma 4.4 is recovered in this more general result. If m = d then π is a bijection so |Mj| = 1 for all j ∈ [m], thus qiλi = q(0) π(i). It turns out that the ϵi are constrained in eﬀectively the same way as in the minimal activation- distinguished case. 38 Lemma 4.10. Given the data of Lemma 4.8, where E = {i ∈ [d ′] | ϵi = 1}, we have ∑ i∈E qiλiw(0) π(i) = 0 , and c0 + ∑ i∈E qiλib (0) j = c . Proof. We perform the same bookkeeping and notation as in Lemma 4.5, where δα j indicates whether node j of the truth is active in the linear domain U α. We have from Lemma 4.9 ∑ j∈[m] δα j q(0) j w(0) j = ∑ j∈[m]   ∑ i∈Mj qiλi   δα j w(0) j = ∑ j∈[m]   ∑ i /∈Ej δα π(i)qiλi + ∑ i∈Ej δα π(i)qiλi   w(0) j . (4.10) Making the same observations as in Lemma 4.5, we also have ∑ j∈[m] δα j q(0) j w(0) j = ∑ i /∈E δα π(i)qiwi + ∑ i∈E δα π(i)qiwi = ∑ i /∈E δα π(i)(−1) ϵiqiλiw(0) π(i) + ∑ i∈E δα π(i)(−1)ϵiqiλiw(0) π(i) = ∑ i /∈E δα π(i)qiλiw(0) π(i) − ∑ i∈E δα π(i)qiλiw(0) π(i) = ∑ j∈[m]   ∑ i /∈Ej δα π(i)qiλi − ∑ i∈Ej δα π(i)qiλi   w(0) j . (4.11) Equating (4.10) and (4.11) thus gives ∑ j∈[m] ∑ i∈Ej ( δα π(i) + δα π(i)) qiλiw(0) j = ∑ i∈E qiλiw(0) π(i) = 0 . The proof of the biases is identical. Remark 4.11. Using ∑ i∈Mj qiλi = ( ∑ i /∈Ej + ∑ i∈Ej )qiλi we can alternatively express this as ∑ j∈[m] q(0) j w(0) j = ∑ i /∈Ej qiλiw(0) π(i) . Again we note that the m = d case is a corollary of this lemma, which can be seen by substituting in qiλi = q(0) π(i). 4.3.2 Main theorem for m < d We are once again in a position to fully characterise W0. Deﬁne Sd′ m := { π : [d ′] → [m] ∣ ∣ ∣ π is surjective } . Let π ∈ Sd′ m be a ﬁxed surjection. For each i ∈ [d′] we write π(i) = j and deﬁne Mj := {i ∈ [d ′] | π(i) = j} , E := {i ∈ [d ′] | ϵi = 1} , Then for each j ∈ [m] we can deﬁne Yj :=   (λi, qi)i∈Mj ∈ (R>0 × R) |Mj | ∣ ∣ ∣ ∑ i∈Mj λiqi = q(0) j    , and Υ ′ :=   ϵ : [d′] → Z2 ∣ ∣ ∣ ∑ i∈E qiλiw(0) π(i) = 0 , and c0 + ∑ i∈E qiλib (0) j = c    . 39 Theorem 4.11. Let f : R2 × W → R be a two-layer feedforward ReLU neural network function and let w ∈ W ⊆ R4d+1 deﬁne a ﬁxed f (x, w) with d hidden nodes. Let the true network f0(x) be an activation-distinguished, minimal, two-layer network with m < d hidden nodes deﬁned by a true parameter w(0) ∈ W (0) ⊆ R4m+1, where W (0) ⊆ W . Suppose f (x, w) = f0(x) for all x ∈ R2. Then 1. Let 0 ≤ k ≤ d−m be the number of degenerate nodes of w. Then there is a degenerate reduced form g(x, w′) of f (x, w) (Deﬁnition 4.6), where g : R2 × W ′ → R is a two-layer network with d ′ ≥ m hidden nodes, and w′ ∈ W ′ ⊆ R4d ′+1 is a parameter whose non-degenerate nodes are equal to those of w, where W ′ ⊆ W . 2. Let W ′ 0 = {w′ ∈ W ′ | g(x, w′) = f0(x)}. Then there is a bijection Ψ : m∏ j=1 Yj × Sd′ m × Υ′ ∼= −→ W ′ 0 (((λi, qi)i∈Mj )m j=1 , π, ϵ) ↦−→   ((−1)ϵiλiw(0) π(i))d ′ i=1 , ((−1)ϵiλib (0) π(i))d ′ i=1 , (qi) d′ i=1, c0 + ∑ i∈E qiλib(0) i   . Proof. One simply needs to perform identical calculations to those in Theorem 4.7. The same justiﬁcations about bijectivity apply too, using Lemma 4.8, Lemma 4.9 and Lemma 4.10. As we have remarked above, the result in Theorem 4.7 is a corollary of Theorem 4.11 when m = d. Remark 4.12. In both Theorem 4.7 and Theorem 4.11 we have considered symmetries of W0 when the domain of the model and truth is all of R2, which allowed us to make conclusions about comparing polynomial coeﬃcients. In the case where f (x, w) and f0(x) are restricted to some open bounded domain Z ⊆ RN , there could in principle be more degeneracies and symmetries of W0, since the functional equivalence need only be on Z. To see this more explicitly, consider a true network f0(x) = 0 with single node model f (x, w) = q ReLU(⟨w, x⟩ + b) deﬁned on Z = (−a, a) 2 for some a > 0, where H is the single activation boundary. Then so long as H ∩ Z = ∅ and w points away from the origin, there will be func- tional equivalence f (x, w) = f0(x). This observation is important to keep in mind because our experiments in Chapter 5 involve such a situation. 4.4 Arbitrary Depth In Section 4.2 and Section 4.3 we have considered networks with two layers. In [PL19], Phuong and Lampert were able to show that this can be generalised to networks of arbitrary depth and arbitrary input dimension for networks with non-increasing widths. They show that, other than set of mea- sure zero, there are “no other function preserving parameter transformations besides permutation and scaling”. We state the main result with the key assumptions here for completeness, but the proof of this theorem is beyond the scope of this thesis, for which the reader is referred to the paper. Referring back to Deﬁnition 2.1, let f : RN × W → R1 be a neural network of depth L with non-increasing widths N = d0 ≥ d2 ≥ · · · ≥ dL ≥ dL = 1. Letting 1 ≤ l ≤ k ≤ L be two layers, we introduce notation for truncated networks Al:k := Ak ◦ ReLU ◦ Ak−1 ◦ · · · ◦ ReLU ◦ Al , where Al:k i denotes the ith component (i.e. the output of node i ∈ [dk+1]). Deﬁnition 4.7. A general feedforward ReLU network is one that satisﬁes the following three conditions: 40 1. For any node i ∈ [dl+1] of a layer 1 ≤ l ≤ L, the local optima of A1:l i do not have value exactly zero. 2. For all 1 ≤ k ≤ l ≤ L and diagonal indicator matrices (Ik, . . . , Il) with entries in {0, 1}, rank(IlwlIl−1 . . . Ikwk) = min{dk−1, rank(Ik), . . . , rank(Il−1), rank(Il)} . 3. Let i ∈ [dl], j ∈ [dk] be nodes of layers 1 ≤ l, k ≤ L respectively. Let U α be a linear domain of A1:l i , and U β be a linear domain of A1:k j . Then A1:l i ∣ ∣U α and A1:k j ∣ ∣ ∣U β are not multiples of one another. Relating to Theorem 4.7 and Theorem 4.11, the conditions of general networks ensure that there is no weight-cancellation (and thus no orientation-reversing symmetries), and that each node is non-degenerate. Thus Theorem 4.13 excludes these symmetries. The following lemma proven in Theorem 4.13 justiﬁes the study of general networks: Lemma 4.12. Almost all feedforward ReLU networks with this architecture are general. As Phuong and Lampert put it, “a suﬃcient condition for a network to be general with prob- ability one is that the weights are sampled from a distribution with a density.” We now state the main theorem of [PL19]. Theorem 4.13. Let Z ⊆ RN be a bounded non-empty connected open set, and let f : RN ×W → R1 be a neural network of depth L with non-increasing widths N = d1 ≥ d2 ≥ · · · ≥ dL ≥ dL+1 = 1. Let f0(x) = f (x, w(0)) be the true network deﬁned by a parameter w(0) ∈ W , which we assume is general, and suppose the model f (x, w) is functionally equivalent, so f (x, w) = f0(x) for all x ∈ Z. Then there exist permutations σ1, . . . , σL−1 ∈ Sm, and positive diagonal matrices M1, . . . , ML−1 such that w1 = M1σ1w1 (0) , b 1 = M1σ1b1 (0) , wl = Mlσlwl (0)σ−1 l−1M −1 l−1 , b l = Mlσlb l (0)σ−1 l−1M −1 l−1 , wL = wL (0)σ−1 L−1M −1 L−1 , bL = b L (0) , where l ∈ {2, . . . , L − 1}. In essence, this result shows that the only generic symmetries of W0 for ReLU networks with non-increasing widths and one output are scaling and permutation symmetries. Though degenerate and orientation-reversing symmetries are non-generic as points on W0, we will show in Section 5.1 that such points can nonetheless have lower free energies and thus be preferred by the posterior - in other words, be better model choices. In this way, we believe the theory of deep learning should shift its focus from points to singularities. 4.5 Example - m-symmetric Networks Let us consider a particular class of networks that gives rise to both degenerate-node and orientation reversing symmetries. In particular, the latter implies that Υ (in the notation of Section 4.2.3) is non-trivial. Deﬁnition 4.8. We deﬁne an m-symmetric network to be fm : R2 → R such that • fm is a two-layer feedforward ReLU neural network with two inputs and one output with d hidden nodes as deﬁned in (4.1); • Let m be an integer such that 2 ≤ m ≤ d and let sm ∈ W be the parameter deﬁning fm(x) = f (x, sm). Let g ∈ SO(2) denote the rotation matrix by 2π m and let w0 ̸= (0, 0) be some ﬁxed initial vector, e.g. w0 = ( 1 0 ). Then for each i ∈ [m], the weights are successively rotated by g, hence set wi = gi−1w0. 41 Figure 4.1: fm(x) for m = 3, 4, 5 with suitably deﬁned b and w0. • For i = 1, . . . , m we let qi = 1, c = 0, and bi = −b for some ﬁxed b > 0. • For each i = m + 1, . . . , d, node i is degenerate, so qi = 0. We can observe contour plots of m-symmetric networks (with no degeneracies) in Fig. 4.1. Visually, it is clear that these weight vectors always sum to zero, which will induce orientation reversing symmetry. Let’s prove it. Lemma 4.14. For any 2 ≤ m ≤ d, fm is a degenerate reducible network. The degenerate reduced form of fm is minimal distinguished. Proof. The ﬁrst claim follows immediately from the deﬁnition of fm, so assume without loss of generality that fm is degenerate reduced. For each node i ∈ [m] the corresponding activation boundary is Hi = {x ∈ R2 | ⟨gi−1w0, x⟩ + bi = 0} . If Hk = Hj for some k < j ∈ [m] (without loss of generality), then by Lemma 4.3 we would have for some λ ∈ R\\{0} gj−1w0 = λgk−1w0 , so gj−kw0 = λw0 , and bj = λbk . This would imply that w0 was an eigenvector of gj−k with eigenvalue λ. The eigenvalues of gj−k are λ± = e ±i 2π(j−k) m , which are only real-valued when j − k = 0 (λ = 1) or j − k = 2 m (λ = −1). If λ = −1 then we would have bj = −bk. But all biases are equal by deﬁnition (and by design), thus we see that j = k, showing that each Hj is unique and thus fm is distinguished. Since it is distinguished, it is necessarily minimal. In particular, m-symmetric networks satisfy the weight cancellation that induces orientation reversing symmetry in W0 (Theorem 4.7). Lemma 4.15. Let fm be an m-symmetric network for some ﬁxed integer m as above. Then the weights associated to fm satisfy d∑ i=1 qiwi = 0 . Furthermore, suppose m is prime and let I ⊆ [m]. Then ∑i∈I qiwi = 0 if and only if I is empty or I = [m]. Proof. By deﬁnition, for weights m < i ≤ d we have wi = 0, so we only need to consider the indices 1 ≤ i ≤ m. Further, recall that each qi = 1. Since each wi is a unit vector and g are rotation matrices, it is easier to reformulate this in terms of complex roots of unity under the isomorphism U (1) ∼= SO(2), so let g = e 2π m i ∈ C, which satisﬁes gm − 1 = 0. The key to the proof rests on the factorisation gm − 1 = (g − 1)(1 + g + g2 + · · · + gm−1) . (4.12) 42 We know g ̸= 1 since m ≥ 2, so g − 1 is invertible, hence m∑ i=1 wi = (1 + g + g2 + · · · + gm−1)w0 = (g − 1)−1(gm − 1)w0 = 0 , showing the ﬁrst claim. For the second claim, if m is prime then it is well known that 1 + X + X 2 + · · · + X m−1 is the minimal polynomial of the algebraic number g [Lan02], which is the unique irreducible polynomial of minimal degree such that g is a root. Suppose I is a non-empty subset of [m] such that ∑ i∈I wi = 0, and let J = {i − 1 | i ∈ I}. This would imply ∑ j∈J gj = 0, that is, g is a root of the polynomial ∑ j∈J X j = 0. But since ∑m−1 j=0 X j is the minimal polynomial, this implies J = {0, . . . , m − 1} or J is empty (since we take the empty sum to be zero), showing the claim. Corollary 4.16. Let fm be an m-symmetric true network in degenerate reduced form for some prime m, and let f (x, w) be the model with d > m hidden nodes such that f (x, w) = fm(x). Then fm(x) is a degenerate reduced form of f (x, w). For ease, now suppose f (x, w) is in its degenerate reduced form with d = m nodes as per Deﬁnition 4.6 and deﬁne W0 = {w ∈ W | f (x, w) = fm(x)}. Then W0 ∼= ∏m i=1 Xi × Sm × Υ where Υ = { (0) m i=1, (1) m i=1} , meaning W0(fm) = {( (wi) d i=1, (bi) d i=1, (qi) d i=1, c ) ∣ ∣ ∣ ϵ ∈ {0, −1} , qi ∈ R>0 , σ ∈ Sm } , where wi = ( (−1)ϵ qi gσ(i)−1w0 )d i=1 , bi = ( (−1)ϵ+1 qi b )d i=1 , c = ϵmb . Proof. By Lemma 4.14 we may reduce fm to its degenerate reduced network which is then dis- tinguished, meaning we can apply Theorem 4.7. By Lemma 4.15, the only subsets I for which ∑ i∈I qiwi = 0 is the empty set or [m]. Thus either no weights are reversed, ϵ = (0)m i=1, or all weights are reversed, ϵ = (1) m i=1. So by Theorem 4.7, Υ has the stated form. Remark 4.13. If m is not prime then Υ will have a more complicated form. In particular, not all ϵ ∈ Υ will have the same entries for i ∈ [m]. We leave it as an exercise to the reader to think about such cases, for example in the case of m = 4. 43 Chapter 5 Phase Transitions in ReLU Neural Networks In Chapter 4 we classiﬁed the symmetries of the set of true parameters W0 in order to determine all points that minimise K(w). In this chapter we will show that not all points on W0 are equally good minimisers of the free energy, and moreover, that parameters not in W0 can nonetheless be preferred by the posterior due to having lower model complexity. Inspired by statistical physics, we identify phases of Fn as compact sets containing a particular singularity of interest. Phase transitions arise from changes in the accuracy versus complexity trade-oﬀ of diﬀerent phases of W , which we induce by changing the symmetries of the underlying W0. In particular, we use m-symmetric networks and Markov Chain Monte Carlo methods to analyse the non-generic orientation-reversing and degenerate-node phases found in Chapter 4. 5.1 Phases and Phase Transitions Physicists typically think of a phase as an aggregate state of a system of many particles with complicated interactions. For example, the solid, liquid or gaseous states of H2O are all examples of phases. Phases are distinguished by their physical properties, such as the molar volume in the case of the phases of water. A phase transition, then, is a sudden change in such a property as a function of some order parameter θ. Mathematically speaking, this is a non-analyticity of the free energy, which corresponds to a change in the conﬁguration of phases resulting in one being newly preferred over another. Empirical measures of neural network performance, such as scaling laws [Kap+20] and gen- eralisation error learning curves [Nak+19] oﬀer evidence of phase transitions in neural networks. Furthermore, [SST92] provides a theoretical-physics treatment of phase transitions associated to the generalisation error, where the order parameter is n D . However, as in most statistical literature, incorrect assumptions of regularity of the neural network models are relied upon. In the context of Bayesian statistics, Watanabe simply deﬁnes a phase transition as a “drastic change in the posterior,” [Wat18], and in [Wat20] he outlines a useful conceptual framework for phase transitions in neural networks. Let us attempt to add a modicum more rigour to this statement in line with the interpretation of phase transitions in [Cal85] and [Gil93]. For simplicity of interpretation, we suppose the free energy is a function of a level set of W , as projected by a function V . We may think of this V as a macroscopic observable (e.g. volume) which thus extracts information about conﬁgurations in W . Deﬁnition 5.1. Let n and β be ﬁxed. Let V : W → R be an analytic function, and let V = Image(V ), which is compact since W is, and deﬁne Wv = {w ∈ W | V (w) = v} . 44 Figure 5.1: A depiction of two phases of Fθ(v) = 1 4 v4 + θ 2 v2 over the interval I = [−3, −1]. Suppose the posterior depends on a non-stochastic order parameter θ ∈ Θ ⊆ R. In particular, we will assume Ln(w) = Ln(w, θ), but this could also be true of the prior ϕ(w). Let the free energy be deﬁned by F : V × Θ −→ R , F (v, θ) = − log (∫ Wv ϕ(w)e−nβLn(w,θ)dw ) . We deﬁne the critical points of F to be CritF =   (v, θ) ∈ V × Θ ∣ ∣ ∣ ∣ ∣ ∂F ∂v (v, θ) = 0    , and we let Critmin F ⊆ CritF denote those critical points that are minima of F . Consider an interval I = [θ1, θ2] ⊆ Θ. A phase over I is a continuous map γ : I → Θ such that the following diagram commutes Crit min F V × Θ I Θ ι π2 γ ι where ι denotes the respective inclusions and π2 is the projection of the second component. In words, a phase is a minimum of the free energy that remains a minimum with small pertur- bations in θ, and as such can be viewed as a path in Critmin F . The precise value of the free energy is irrelevant to the phase structure, but the ordering of the free energy of phases is the substance of phase transitions, whereby the conﬁguration of the phases changes. The following deﬁnition is based on [Gil93, §10], where we adopt the Maxwell convention outlined in [Gil93, §8.2]. Deﬁnition 5.2. Let Γ = {γg} G g=1 be the set of phases of some F on a ﬁxed interval I, and assume G ≥ 2. Let Ic = [θ0, θ1] = [θc − ε, θc + ε] ⊆ I denote a small critical interval for some critical 45 Figure 5.2: A ﬁrst order phase transition (left), a second order merge phase transition (centre) and a second order creation phase transition (right). All curves are generated by Fθ(v) = 1 6 x6 + 1 8 c1x4 + 1 2 c2x2 + c3x + c4 for parameters ci that depend on θ as in [Gil93, §10.8]. parameter θc ∈ I. Without loss of generality, assume that Γ is ordered by the corresponding free energy of each phase at the left endpoint of the interval. That is, for each 1 ≤ g < h ≤ G, F (γg(θ0)) ≤ F (γh(θ0)) , where F (γ1(θ0)) thus denotes the global minimum. A ﬁrst order phase transition at θc is a reordering of Γ that exchanges a local and global minima. That is, for some g > 1, varying θ ∈ Ic gives    F (γ1(θ)) < F (γg(θ)) θ < θc F (γ1(θ)) = F (γg(θ)) θ = θc F (γ1(θ)) > F (γg(θ)) θ > θc . A second order phase transition at θc can be one of two things. A merge transition occurs when, given two phases γ1 and γg for some g > 1, varying θ ∈ Ic gives   γ1(θ) ̸= γg(θ) θ < θc γ1(θ) = γg(θ) θ ≥ θc , such that γ1, γg are global minima of F for θ ≥ θc. A creation transition occurs when, given any phase γg, varying θ ∈ Ic gives   γg is not a phase θ < θc γg is a phase θ ≥ θc . The direction of θ may be altered in any of these deﬁnitions. If the direction of the creation transition is reversed we refer to this as a destruction transition. The diﬀerent types of phase transitions can be seen in Fig. 5.2. We do not claim to have given a full classiﬁcation of phase transitions in these deﬁnitions. To do so generically requires one to pay careful attention to the diﬀerential geometry of F , and the possible types of catastrophes that can occur. For further discussion of this, see [Gil93]. Remark 5.1. In principle, one may also care to deﬁne phase transitions that exchange or merge local minima which are not global. We have restricted our attention to only consider global minima due to the fact that, in a physical sense, these are the only ones that have a meaningful impact on the state of a system. 46 Phases as singularities Let us now explore how phases can be associated to compact sets containing particular singularities of K, thus providing a diﬀerent kind of candidate solution to the problem of model selection. For this discussion, let V : W → R be a ﬁxed analytic function with level sets Wv as in Deﬁnition 5.1. The reader may wish to review the discussion of singularities in Chapter 3. For any compact set Wv ⊆ W there is a local RLCT λv as per Remark 3.3 which extracts the eﬀective dimension- ality of “the most singular” point in Wv. Let ωv 0 ∈ Wv be deﬁned by Ln(ωv 0 ) = minω∈Wv Ln(ω), thus has the best accuracy (meaning the lowest loss) of any parameter in Wv. Then according to [Wat09, §7.6] we have that Fn(Wv) ≈ nLn(ωv 0 ) + λv log n β0 . Assume for simplicity that n and β0 are both ﬁxed. This relation shows that a minimum of Fn will correspond to a compact set Wv with low Ln(ωv 0 ) and low λv compared to nearby v. Thus, a phase of F corresponds to a compact set Wv that contains a particular singularity of interest. Comparing the free energy of these diﬀerent Wv thus becomes the basis of the model selection process, where each phase can be thought of as a diﬀerent set from which to draw candidate solutions. Phase transitions, then, correspond to a change in the structure of K(w), which induces a change in either the accuracy or RLCT of the regions {Wv}v as a function of θ which causes a new phase to be preferred. In simple terms, it is a change in the accuracy versus complexity trade- oﬀ between candidate solutions. As the true distribution is varied, so is the geometry of K(w). We thus expect to see phase transitions occur near points where the underlying symmetry of W0 changes (for example, where a node is newly degenerate). We call such changes symmetry breaking of W0. Naively we may expect the critical point θc to occur precisely where the symmetry breaking takes place. But we will show in Section 5.3 that in fact due to the accuracy versus complexity trade-oﬀ, these critical values can occur before the point of symmetry breaking. Remark 5.2. In general, it is not physically or computationally reasonable to measure these level sets Wv. Instead we must rely on coarse graining. Let V = [v1, vH ], which we can evenly partition into intervals v1 < v2 < · · · < vH−1 < vH for some H ≥ 2. Then we may deﬁne compact sets for each 1 ≤ h ≤ H − 1 Wh = V −1([vh, vh+1]) = {w ∈ W | V (w) = [vh, vh+1]} . Since V is an analytic function deﬁned on the compact W , the Wh are disjoint and compact, meaning we can write Zn = H−1∑ h=1 Zn(Wh) . Then the model selection process becomes ﬁnding min h=1,...,H−1 Fn(Wh) . This is the scenario we will consider in our experiments. More precisely, we will consider compact sets that are unions of Wh, for which all of the above setup still applies. Before we move on to our experimental ﬁndings, it is useful to have an intuitive understanding of what we mean by the “nature of a singularity”. In algebraic geometry, typical examples of singularities are lines that self-intersect (perhaps more than once), cusps, and tacnodes. As an example, a curve that intersects itself three times at the same singularity will be more complicated than one that intersects itself only once. Let us give an example relevant to our context. 47 Figure 5.3: W θ 0 of Kθ(w, q) coloured by diﬀerent values of θ. Figure 5.4: ϕ(w)e−nKθ(w) for n = 50 over diﬀerent θ values. Notice the posterior concentration of the more singular point, (w, q) = (0, 0), when θ = 0. Example 5.1. Consider K(w, q) from Example 3.1, namely Kθ(w, q) = (wq − θ) 2 , this time for w, q ∈ [−1, 1]2 and some θ ≥ 0. Then it is visually clear from Fig. 5.3 that, the geometry of W0 is diﬀerent for θ = 0 compared to θ > 0, meaning we should think of θ > 0 as one phase and θ = 0 as another phase. This implies that the RLCT (and/or its multiplicity) is diﬀerent between these phases. To prove this, however, one must perform a resolution of singularities (an algorithm known as a “blow-up”) of the former case in order to calculate ζ(z). In Fig. 5.4 we observe the posterior (up to a scale factor) for uniform ϕ(w) as θ varies. Notice that the posterior concentration around the point (w, q) = (0, 0) for θ = 0 is diﬀerent to that around wq = θ for θ > 0, giving a clear visual picture of why one might expect “more singular points” to have lower free energy. We now turn our attention to studying phases and phase transitions of ReLU neural networks from an experimental perspective. 48 5.2 Experimental Methodology The following section outlines our methodology for estimating the posterior of ReLU neural net- works. MCMC methods are introduced for the unfamiliar reader and we outline the details of the experimental procedure. Finally, we explain how to interpret the density plots demonstrating the phase transitions, describing how we quotient out the scaling and permutation symmetries of the networks. The methodology used closely follows the work of [Mur+20]. 5.2.1 Markov Chain Monte Carlo In order to study phases in neural networks, we must ﬁrst begin with estimating the posterior density p β(w|Dn). To see why such a procedure is non-trivial, recall that whilst the numerator of the posterior ϕ(w)e−nβLn(w) is easily calculated for any given w, the partition function Zn is intractable even in simple settings. This problem has given rise to the ﬁeld of computational Bayesian statistics, whose primary focus is to develop algorithmic methods for estimating probability densities (see [RC04] for an introduction). Mathematically, the goal is to generate a set of samples {w(k)} K k=1 such that for any arbitrary function f (w) we have ∫ W f (w)p β(w|Dn)dw ≈ 1 K K∑ k=1 f (w(k)) as K → ∞ [Wat18, §7]. In particular we can retrieve the posterior on any open set A ⊆ W by simply taking f (w) = 1 (w ∈ A). The centrepiece of computational Bayesian statistics is a class of algorithms called Markov Chain Monte Carlo (MCMC). Recall that a Markov chain is a random walk on a space W such that the next step only depends on the present step and is independent of the previous history. The main idea of MCMC is to simulate a Markov chain such that the equilibrium distribution of the chain is equal to the probability density to be estimated. This is achieved in two key steps: 1. Selecting a candidate step from a distribution based on the initial position (Markov step). 2. Choosing to accept the candidate with a probability proportional to ϕ(w)e −nβLn(w) (Monte Carlo step). The simplest version of this algorithm is known as the Metropolis algorithm, but it suﬀers from some important shortcomings. Namely, it can struggle to suﬃciently explore the space of parame- ters due to potential and entropy barriers in ϕ(w)e−nβLn(w). Further discussions of these problems can be found in [Bro11] and [Wat18, §7]. Instead, let us turn our attention to the MCMC variant used in this thesis, Hamiltonian Monte Carlo, which aims to avoid these barrier problems by replacing the Markov step with a particle simulation through phase space according to Hamilton’s equations of motion. Concretely, deﬁne the Hamiltonian of our model-prior system by H(w) = βLn(w) − 1 β log ϕ(w) such that p β(w|Dn) ∝ e−H(w). Suppose v ∈ Rd is some randomly generated initial velocity, usually from a standard normal for simplicity. Then we may deﬁne the total Hamiltonian by H(w, v) = 1 2 ∥v∥ 2 + H(w) . Note that sampling (w, v) from e−H(w,v) still ensures that w is still subject to the equilibrium distribution e−H(w). After initialising some random starting point w(1) for k = 1, the algorithm thus runs as follows: 49 1. Hamiltonian step: Sample v0 ∼ N (0, 1 d). Let τ denote the time variable. Simulate a trajectory through phase space W according to the diﬀerential equation dw dτ = v , dv dτ = −∇H(w) , s.t. (w, v) = (w(k), v0) at time τ = 0 . Run the simulation up to some time τstop and let (w′, v′) = (w(τstop), v(τstop)) be the candi- date parameter. 2. Monte Carlo step: Deﬁne ∆H = H(w′, v′) − H(w(k), v0) and P = min{1, exp(−∆H)}. Then assign the next step w(k+1) according to the rule: w(k+1) =   w′ with probability P w(k) with probability 1 − P . (5.1) Notice that the candidate is accepted with probability 1 if H(w′, v′) < H(w(k), v0), thus ensuring that the sampler moves towards regions of lower energy. If the inequality is reversed but the candidate has relatively similar energy, then the candidate is still accepted with high probability. If the candidate energy is much higher than that of the current position, it is rejected with high probability. The Hamiltonian/Markov-chain step is thus the key to ensuring MCMC is not just a gloriﬁed gradient descent algorithm, but rather that it eﬀectively explores the space. In practice there is an invisible ﬁrst step called the “burn-in” period, which discards some number of initial samples to allow the sampler to approach the equilibrium distribution before it accepts samples. Remark 5.3. A useful conceptual framework for Hamiltonian Monte Carlo, as elaborated in great detail in [McE], is to imagine the posterior, or rather the reciprocal of the posterior, as a skate park. Peaks of pβ(w|Dn) correspond to the troughs of the bowls in the rink, and zero regions of p β(w|Dn) correspond to inﬁnitely high walls. Then HMC takes a ball (particle) w(k) at some starting point in W , generates a random initial velocity v0, and kicks the ball with that initial velocity. Hamilton’s equations are used to simulate the trajectory across the skate park. After some stopping time τ ′, we stop the ball, record its position, and restart the process from the chosen position. The experiments performed in this thesis use a variant on the Hamiltonian Monte Carlo method called the No U-Turn Sampler (NUTS), which improves the dynamical simulations by ensuring there are “no U-turns”, thus dynamically choosing τstop to avoid returning to a similar position as was started at. More details can again be found in [Bro11]. For the uninitiated but interested reader, a great introduction to HMC is found in [Bet18]. In this thesis, MCMC methods were implemented using the Python packages PyTorch [Pas+19] and Pyro [Bin+19], with experiments run on Spartan High Performance Computing of the Uni- versity of Melbourne [Laf17]. 5.2.2 Experimental setup We shall consider the realisable case under Hypothesis 2.1 where q(y|x) = p(y|x, w(0)) is deﬁned by a two-layer feedforward ReLU network with two inputs, one output and m hidden nodes as discussed in Eq. (2.2), and the model has the same architecture with d = m hidden nodes. Since the model p(y|x, w) is a normal distribution as in Hypothesis 2.1, the true distribution is also a normal distribution. In particular, the true network will depend on some order parameter θ ∈ Θ which we denote by f0(x, θ). Thus for any dataset of samples Dn = {(xi, yi)}n i=1 drawn from the true distribution (xi, yi) ∼ q(y, x|θ) we write Dn = Dn(θ), meaning the posterior depends on the order parameter through its variation in the true distribution, and thus the dataset. The exact nature of the order 50 parameter will diﬀer between experiments. The prior on inputs will be uniform on a square for some a > 0, q(x) = 1 4a2 1 ((x1, x2) ∈ [−a, a]2) , and the prior on parameters will be the standard normal with ﬁxed variance σ2 ϕ, ϕ(w) = 1 (2πσ2 ϕ) 4d+1 2 e− 1 2σ2 ϕ ∥w∥2 . An experiment will refer to a ﬁxed vector (θ, a, σϕ). In order to account for the randomness in Dn and make statements about the posterior independent of Dn, we will run T repeat trials of the same experiment and average the posterior estimates over the results following a validation procedure (see below). Typically T = 8 or T = 4. For a given trial, we begin by generating a dataset Dn for ﬁxed n = 10000, and then use HMC (NUTS) to generate a set of samples {w(k)} K k=1 from the tempered posterior w(k) ∼ p β∗ (w|Dn(θ, a)) with burn-in period K 20 . Here we take β∗ = 1 log n as per Theorem 3.9. Typically K = 20000. MCMC is quite a delicate algorithm and can occasionally produce results contrary to what is anticipated - for example, the sampler can get stuck in particular regions of space of much higher free energy than the true global minimum. Our ﬁrst form of validation is using standard MCMC chain divergence criterion, where trials with more than K 10 chain divergences discarded. We then perform a simple statistical validation process, discarding any outlier trials as measured by the average mean square error across all samples, which in our language is the empirical Gibbs training loss G β t for a trial t, G β t (t) ≈ 1 K K∑ k=1 Ln(w(k)) . Using the central limit theorem, G β t ≈ N (µT , s 2 T ) where µT and s 2 T are the sample mean and sample variance respectively. We discard any trials t such that 1 sT |G β t (t) − µT | > κ for some outlier threshold κ, which we usually set as κ = 1.5. 5.2.3 Machine epsilon and practical limits Strictly speaking, our setup violates two key assumptions of Singular Learning Theory, namely that W is compact and that K(w) is analytic. The former is violated by our assumption that ϕ(w) is normally distributed, and thus a density on all of R4d+1. The latter is violated due to the non-analyticity of f (x, w) for ReLU neural networks. In both cases we may appeal to machine epsilon εm, which is the ﬁnite ﬂoating point precision of any given computer. For the ﬁrst point, ϕ(w) is computationally zero outside of some (very large) neighbourhood of the origin. Thus we can simply consider W to be this very large compact neighbourhood, for which the experimental results do not meaningfully change. For the second point, we can use Remark 2.1 to ﬁnd some γ such that ReLU is approximated by swish, |σγ(x) − ReLU(x)| < εm. Then K(w) will be analytic if we take σγ(x) to be the activation function. But, importantly, for this γ, σγ and ReLU are indistinguishable from the point of view of the computer, meaning the experimental results also do not change. 5.2.4 Visualising the posterior Our main method of studying phases of neural networks will be to examine the posterior of a given neural network, where “drastic changes” in these posteriors as a function of some order parameter will correspond to phase transitions. Since W ⊆ R4d+1 for some d ≥ 2, we clearly cannot visualise the posterior directly. Nor are we interested in doing so: our experiments only consider variations in the weights, not the biases, so we only need to observe the posterior of the weights. 51 Figure 5.5: Scatterplot of raw (left) versus eﬀective (right) samples {{( ˆw(k) i,1 , ˆw(k) i,2 )}i∈[d]}K k=1 for K = 4000, d = 3, labelled according to their index i, for one trial. Recall from Theorem 4.7 and Theorem 4.11 that W0 generically admits scaling and permutation symmetry, which is also a property of more general networks as discussed in Theorem 4.13. With this in mind, these are uninteresting symmetries to analyse as singularities. Instead, we shall focus on the non-generic degenerate-node and orientation-reversing phases and measure the free energy of these as we vary the true distribution. Deﬁnition 5.3. Given a ﬁxed sample w(k) ∼ p β(w|Dn) we deﬁne the eﬀective parameter ˆw(k) = ({ ˆw(k) i } d i=1, {ˆb (k) i } d i=1, {1} d i=1, c), where for each node i ∈ [d] we deﬁne ˆw(k) i = |q(k) i |w(k) i , and ˆb(k) i = |q(k) i |bk i . By Theorem 4.7 we have f (x, w(k)) = f (x, ˆw(k)) , but now the eﬀective parameter has less degrees of freedom since we have taken the quotient of the scaling symmetry. Remark 5.4. In Deﬁnition 4.1 we deﬁned a degenerate node i to be one such that qi = 0 or wi = 0. In the language of eﬀective parameters, this is equivalent to ˆwi = 0. The posterior is invariant under a permutation of nodes, which implies that for each i ∈ [d] = {1, . . . , d}, wi is identically distributed (though there is a distributional dependence between each weight). This allows us to project each ˆw(k) i on to the same ( ˆwi,1, ˆwi,2) plane. Thus each sample w(k) is represented d times on each posterior plot by the points {( ˆw(k) i,1 , ˆw(k) i,2 )}i∈[d]. We will mostly present density estimates of the posterior, but for clarity, Fig. 5.5 shows a typical example of a scatterplot of points { {( ˆw(k) i,1 , ˆw(k) i,2 )}i∈[d]}K k=1 coloured according to the node index i and demonstrating the diﬀerence between the eﬀective and non-eﬀective estimates of the weights. On all plots the red dots indicate the true parameters used to generate Dn. All experiment plots have been generated using Seaborn [Was21] and Matplotlib [Hun07]. As we argued in Section 3.2, the free energy of a compact set W ⊆ W is a measure of posterior density associated to W. It was found that approximate values of the free energy using the WBIC was volatile. Instead, our inference about phases and phase transitions will use the following correspondence: phase ⇐⇒ minimum of the free energy ⇐⇒ concentrated region of posterior , where we compare minima associated to regions by comparing their respective posterior concen- trations. 52 5.3 Phase Transition 1: Deforming to Degeneracy Having taken the quotient of scaling and permutation symmetry, in this section we will explore phases associated to degenerate-node and non-degenerate-node phases. Recall that a node i ∈ [d] is degenerate if either wi = 0 or qi = 0, meaning ˆwi = 0. Figure 5.6: f2(x, θ) for θ = 0 (left), θ = π 4 (middle), θ = π 2 (right). 5.3.1 Deﬁning the order parameter Let f2 : R2 × Θ → R be an m-symmetric network as in Deﬁnition 4.8, with m = 2, w0 = (1, 0)T and b = − 1 3 , which we take to be the true network. We deﬁne an order parameter θ ∈ Θ = [0, π 2 ] that rotates the two weights toward one another, that is, the true weights are w(0) 1 = gθw0 = (cos θ, sin θ) , w(0) 2 = gπ−θw0 = (− cos θ, sin θ) , where gθ denotes rotation by θ. Explicitly, the truth is deﬁned by f2(x, θ) = ReLU (cos(θ)x1 + sin(θ)x2 − 1 3 ) + ReLU ( − cos(θ)x1 + sin(θ)x2 − 1 3 ) . We can see how the foldsets of f2(x, θ) change with θ in Fig. 5.6. Using f2(x, θ) we can thus induce symmetry breaking of W0 which occurs when node-degeneracy becomes one of the symmetries. Recalling that the biases of both nodes are equal, we see that node-degeneracy symmetry of W0 occurs when w(0) 1 = w(0) 2 , which is only at θ = π 2 . By contrast, for θ ∈ (0, π 2 ), W0 only exhibits the standard scaling and permutation symmetry. At θ = π 2 the model nodes ˆw1, ˆw2 ∈ W0(f2(x, π 2 )) can have two possible conﬁgurations: • Both non-degenerate: ˆw1, ˆw2 ̸= 0 such that ˆw1 + ˆw2 = (0, 2), • One degenerate, one non-degenerate: either ˆw1 = (0, 0) and ˆw2 = (0, 2), or ˆw1 = (0, 2) and ˆw2 = (0, 0). We thus identify neighbourhoods of these singularities as phases to compare. Accordingly, let us deﬁne compact subsets of W in order to compare their free energies. Let the analytic projection of Section 5.1 be V (wi) = ∥wi∥, and deﬁne an annulus in the plane as A(r, ε) = { ˆw ∈ R2 | r − ε ≤ ∥ ˆw∥ ≤ r + ε} . Then we deﬁne the two phases containing the singularities of interest to be ANonDegen = A(1, ε) × A(1, ε) = { ( ˆw1, ˆw2) | ˆw1 ∈ A(1, ε) and ˆw2 ∈ A(1, ε) } ADegen = (A(0, ε) × A(2, ε)) ∪ (A(2, ε) × A(0, ε) ) = { ( ˆw1, ˆw2) | ˆw1 ∈ A(0, ε) and ˆw2 ∈ A(2, ε) , or ˆw1 ∈ A(2, ε) and ˆw2 ∈ A(0, ε) } . For notational ease we then let A c = W \\(ANonDegen ∪ ADegen). We take {θj}J j=1 to be a sequence of angles (in radians) such that θ1 = 1.00c and θJ = π 2 and observe changes in the posterior over θ. In the following experiments there were K = 20, 000 samples taken over T = 8 trials with a = 2 and σϕ = 1. 53 5.3.2 Results and discussion Fig. 5.7 demonstrates the phase transitions in the posterior as we range over these θj values. This is further highlighted by Fig. 5.8a where we take ε = 0.3 as the annuli width. For θ < 1.16c the only phase detected by the posterior is ANonDegen. At θ = 1.16c we see a second order creation phase transition, where ADegen has suddenly emerged as a region of concen- tration, and thus a phase - though the ANonDegen phase is still the global minima. As θ → 1.26 c, the phase structure remains the same, though the free energy of ANonDegen is increasing, whilst for ADegen it is decreasing. Observing Fig. 5.8a, at the critical value θc = 1.26c we notice a ﬁrst order phase transition where the free energies of both phases are equal and switch roles as local and global minima. Once this transition has occurred and ADegen has become the global minima of F , the gap between the respective free energies continues to widen. Let us inspect the θ = π 2 ﬁgure in Fig. 5.7 more closely. Although at this θ the true network is deﬁned by two non-degenerate nodes ˆw(0) 1 = (0, 1) and ˆw(0) = (0, 1) in ANonDegen, the phase ADegen is nonetheless preferred. This is our ﬁrst clear example of the fact that, although all points on W0 minimise K(w), it is their structure as singularities of K(w) that determines which has lower model complexity, and thus lower free energy. Recall from Section 5.1 that for any phase A ⊆ W Fn(A) ≈ nLn(ω0) + λ log n β0 , (5.2) where Ln(ω0) = minω∈A Ln(ω) is the accuracy and λ log n β0 is the complexity. Since both phases are on W0, and thus have the same accuracy, these ﬁndings suggest that the RLCT λ of ADegen is lower than that of ANonDegen. 1 The crucial result is this: for 1.26 < θ < π 2 the degenerate phase ADegen does not contain a point on W0 by Theorem 4.11, thus has worse accuracy, yet it nonetheless has lower free energy. We suggest that this is due to the complexity term out-competing the accuracy term in this in- terval, with the ﬁrst order phase transition occurring when the accuracy of both phases becomes comparable. Observing Fig. 5.8 we see that for θ < 1.3 the accuracy of ADegen is worse than that of ANonDegen (meaning Ln(ω0) is higher), but then for θ ≥ 1.3 the accuracy of the two phases is approximately equal. The preference of ADegen over ANonDegen in θ ∈ (1.26, π 2 ) suggests that RLCT of each phase is approximately constant in θ, implying that the ﬁrst order phase transition is a result of a change in the accuracy of a phase. Extending this analysis further, we conjecture that as n increases, the critical value θc = 1.26 will move closer to π 2 , since the accuracy term is O(n) whereas the complexity is O(log n). It would be interesting to analyse this in future studies. As a ﬁnal remark, as per our discussion in Section 4.4 recall that [PL19] stated that “almost all symmetries of W0 are scaling and permutation”, or alternatively, that degenerate-nodes occur with probability zero in arbitrary-depth ReLU networks. These experiments suggest that while this view is correct, it is incomplete from the perspective of statistical learning. The singularity that determines the phase ADegen is non-generic, and yet we have shown that, for these particular networks, it is nonetheless preferred by the posterior even for θ < π 2 . This implies non-generic points in the space of parameters W can determine the shape of the posterior, and thus inﬂuence estimation procedures such as MCMC or Stochastic Gradient Descent. Accordingly, we believe that in order to understand the success of deep learning, the theory should shift perspective from considering points of W to considering singularities of K(w). 1In order to add weight to this claim we should attempt to estimate λ, and vary the experiments over n and β0. We leave this to future work. 54 Figure 5.7: Posterior densities of p β∗ (w|Dn(θ)), where each sample w(k) is represented by two points, (w(k) 1,1 , w(k) 1,2 ) and (w(k) 2,1 , w(k) 2,2 ). The red dots indicate w(0) 1 and w(0) 2 which are rotated by θ. The ADegen phase undergoes a second order creation transition at θ ≈ 1.16 c. There is a ﬁrst order phase transition at θ ≈ 1.26 c as the ADegen phase becomes the global minima. 55 (a) Density (relative frequency) of each phase. Notice how the preferred phase switches at θ = 1.26c, thus indicating a ﬁrst order phase transition. (b) Accuracy Ln(ω0) of each phase. Notice that variations in Ln(ω0) are closely correlated the accuracy Ln(w(0)) = Sn of the underlying true distribution. These variations in Sn are random (i.e. not constant) since each y ∼ q(y|x) which is a normal distribution as outlined in Section 5.2.2. Figure 5.8: Trajectory of diﬀerent phases for each θ. 56 5.4 Phase Transition 2: Orientation Reversing Symmetry Figure 5.9: Non-weight-annihilation with ϵ = (0, 0, 0) (left) versus weight-annihilation with ϵ = (1, 1, 1) (right). Figure 5.10: f3(x, ϑ) for ϑ = 1 (left), ϑ = 1.5 (middle) and ϑ = 2 (right). 5.4.1 Deﬁning the order parameter Let us now analyse the singularity associated to orientation reversing symmetry of Theorem 4.7, which is present when eﬀective weight vectors in the network sum to zero. Since the true parameter deﬁning W0 is not unique, we will need to be more careful in distinguishing between the two singularities. To this end, when we refer to weight annihilation symmetry, we mean a conﬁguration of weights such that multiple nodes are active in a linear domain, but cancel to give an eﬀective weight of zero. This is shown in Fig. 5.9. We set the true network to be an m-symmetric network f3(x, ϑ) : R2 × Θ → R with m = 3 hidden nodes and set w0 = g π 3 (1, 0) T and b = − 1 3 . We deﬁne an order parameter ϑ ∈ Θ = [1, 2.25] such that q2 = ϑ, so f3(x, θ) = ReLU ( cos ( π 3 ) x1 + sin ( π 3 ) x2 − 1 3 ) + ϑ ReLU (−x1 − 1 3 ) (5.3) + ReLU ( cos ( 5π 3 ) x1 + sin ( 5π 3 ) x2 − 1 3 ) . In essence ϑ merely scales the eﬀective weight ˆw2(ϑ), which can be seen in Fig. 5.10. Then according to Corollary 4.16 we have the usual scaling and permutation symmetry of W0, but weight-annihilation is dependent on ϑ: Υ =    { (0) 3 i=1} , if ϑ ̸= 1 { (0) 3 i=1, (1) 3 i=1} if ϑ = 1 . 57 Let R(θ) = (cos θ, sin θ) and B(x, ε) be the closed ball centred at x ∈ R2 of radius ε. As regions in the (wi,1, wi,2) plane, out setup in Eq. (5.3) dictates that ENonAnn = ⋃ σ∈S3 2∏ k=0 B ( R ( π 3 + 2σ(k)π 3 ) , ε ) EAnn = ⋃ σ∈S3 2∏ k=0 B ( R ( 2σ(k)π 3 ) , ε ) are the two phases that we are interested in. In these experiments there were K = 10, 000 samples over T = 4 trials and a = 1, σϕ = 1. 5.4.2 Results and discussion The posterior estimates ranging over ϑ ∈ [1, 2.25] can be seen in Fig. 5.11. As anticipated we see that both ENonAnn and EAnn are minima of the free energy. We see that ENonAnn is the global minimum, implying the non-annihilation phase has lower model complexity since they have the same accuracy as true parameters. As ϑ increases there is symmetry breaking of W0 where the singularity deﬁning EAnn is no longer on W0, meaning Ln(ω0) of this phase, and therefore the free energy, should increase. Our experiments agree with this conjecture. The free energy of ENonAnn increases as ϑ increases until we see a second order destruction transition at ϑ ≈ 2 where ENonAnn ceases to be a minimum of the free energy. This behaviour agrees with our analysis in Section 5.3.2 and reinforces the fact that singularities on W0 can have diﬀerent free energies. 5.4.3 An instructive calculation To illustrate why ENonAnn may have a lower free energy, let us consider a perturbation analysis of K(w) centred at singularities corresponding to weight-annihilation and non-weight-annihilation in a simple two-layer ReLU network with one input and one output. We will be interested in measuring the curvature of K(w) at these two points. Since this loosely corresponds to local density of the normalised posterior ϕ(w)e−nK(w), our results in Section 5.4.2 suggest that ENonAnn should have the lower curvature of the two phases. In principle, such a calculation potentially lacks meaning - after all, we have argued that it is not the Hessian at a point that aﬀects the free energy, but the singularity structure. Nevertheless, by removing the singularity in a very simple setting, we can gain some intuition into why it may be that the complexity of ENonAnn is less than EAnn. Consider a two-layer, one input, one output ReLU network f (x, w) : R × W → R with d = 2 hidden nodes as the model. The true network f0(x) has the same architecture and is deﬁned by f0(x) = ReLU(−x − 1) + ReLU(x − 1) + 2 . Let us deﬁne two parameters wNA, wAnn ∈ W which depend on a small δ, fwNA(x, δ) = ReLU((−1 + δ)x − 1) + ReLU(x − 1) + 2 fwAnn (x, δ) = ReLU((1 + δ)x + 1) + ReLU(−x + 1) , where fwNA is the non-weight-annihilation conﬁguration and fwAnn is weight-annihilation. Note that both wNA, wAnn ∈ W0 for δ = 0. Let us then deﬁne the KL divergence between from each network to f0(x) as a function of δ, for q(x) uniform on [−a, a] for some a > 0, KNA(δ) = ∫ a −a (fwNA(x, δ) − f0(x))2 dx KAnn(δ) = ∫ a −a (fwAnn (x, δ) − f0(x) )2 dx . 58 Figure 5.11: Posterior densities of p β∗ (w|Dn(ϑ)) for Eq. (5.3). The red dots indicate ˆw(0) 1 , ˆw(0) 2 and ˆw(0) 3 , where ˆw(0) 2 is scaled by ϑ. The EAnn phase always has higher free energy, and undergoes a second order destruction transition somewhere between ϑ ∈ (1.7, 2). 59 Figure 5.12: The diﬀerent perturbed networks compared to the truth. Notice the additional area between fwAnn and f0 in the region (−1, 1). Through a careful calculation performing the integrals over domains deﬁned by when each node is active, one ﬁnds that KNA(δ) =    δ2(a3 + δ − 1 ) 12a if δ < 0 δ2(a3(δ − 1) 3 − (δ − 1)) 12a(δ − 1)3 if 0 < δ < 1 , KAnn(δ) =    δ2(a3δ + a 3 + 1) 12a(δ + 1) if − 1 < δ < 0 δ2(a3(δ + 1)3 + δ + 1) 12a(δ + 1)3 if δ > 0 . In particular we have K ′′(0) = a 3 − 1 6a < a 3 + 1 6a = K ′′ ϵ (0) , which implies that for any δ ∈ (−1, 1)\\{0}, KNA(δ) < KAnn(δ) . Observing Fig. 5.12 gives some insight into the geometry at play here. Small perturbations in fwAnn lead to a meaningful change in the decision boundaries, which in turn results in additional contributions to the KL divergence from the region (−1, 1). On the other hand, fwNA still retains a constant region for small perturbations in δ. Furthermore, notice that lim a→∞ |K ′′ Ann(0) − K ′′ NA(0)| = lim a→∞ 1 3a = 0 , which implies that the free energy of both phases may become equal as a → ∞. This hints at the existence of another ﬁrst order phase transition. Let us investigate whether EAnn becomes a global minmia as we vary a. 60 Figure 5.13: The true distribution q(y, x) = 1 a2 p(y|x, w3)1 ((x1, x2) ∈ [−a, a] 2) for a = 1 (left), a = 1.5 (middle) and a = 2 (right). 5.4.4 Phase transition 3: Equal Weight-Annihilation Inspired by the results in Section 5.4.3, let us keep our same network f3(x) deﬁned in Section 5.4.1, but this time we shall ﬁx q2 = ϑ = 1 and make the order parameter a ∈ Θ = [1, 2] which deﬁnes the uniform square of q(x). How a changes the distribution is seen in Fig. 5.13. According to Theorem 4.7, there are no changes to the symmetries of W0 as we vary a. However, it is worth bearing in mind Remark 4.12, where we observed that our analysis of W0 in Chapter 4 did not adequately take into account how the form of q(x) aﬀected W0. In these experiments there were K = 1000 samples over T = 32 trials and σϕ = 1. The reason for more trials is because potential barriers were found, meaning the ﬁnal distribution of any trial chain was highly dependent on its initial value. Thus these experiments were averaged over random initial values for 32 trials. The results can be seen in Fig. 5.14. The regions of posterior concentration are not as distinct as in previous experiments, but the results nonetheless agree with what Section 5.4.3 suggested: the free energy of EAnn decreases as a increases. This shows how the geometry of K(w) is not just aﬀected by the nature of the ReLU network deﬁning the true distribution, but also the speciﬁcations of q(x). 61 Figure 5.14: Posterior densities of pβ∗ (w|Dn(a)) for f3(x) where q(x) is uniform on [−a, a] 2. Notice that the free energies of E0 and E1 become comparable as a increases. 62 Chapter 6 Conclusion This thesis aimed to provide accessible examples of singular models in the form of small ReLU networks in order to elucidate the key messages of Sumio Watanabe’s Singular Learning Theory and illuminate the change in statistical perspective from points to singularities. We started by casting deep learning as a Bayesian statistical learning model in Chapter 2. Here the Kullback-Leibler divergence K(w) between a model and truth was revealed as the fundamental object of study, alongside the set of true parameters W0 ⊆ W . We then explained how one can draw an analogy between neural networks as Bayesian models and the Gibbs ensemble of statistical physics, hinting at objects and phenomena that arise naturally such as the free energy and phase transitions. An exposition on Singular Learning Theory was then provided in Chapter 3. We began by demonstrating that neural networks have degenerate Fisher information matrices and are therefore singular models, thus showing that singularity theory lies at the heart of statistical learning theory of neural networks. We then explored the free energy Fn(W) associated to compact subsets of W , in particular its relation to the generalisation of a model, and why it is the main quantity of comparison between model. Watanabe’s groundbreaking formula for the asymptotics of the free energy in singular models was then explained, where we discussed how the RLCT λ is the correct measure of complexity in singular models and illustrated its interpretation in terms of Occam’s Razor. We then set about establishing the symmetries of W0 for two layer ReLU networks in the real- isable case in Chapter 4, which was equivalent to establishing for which parameters give functional equivalence between a model and a truth network. This was done in two stages. In the ﬁrst case where the model and truth networks had the same number of nodes, m = d, it was found that W0 exhibited scaling, permutation and orientation reversing symmetry, the latter only occurring when the weight vectors summed to zero. In the second case where the model was had more hidden nodes than the truth, m < d, it was proven that the excess nodes were either degenerate or had the same activation boundaries as some other node in the model. We then examined a more general result from the literature for networks of arbitrary depth, before ﬁnally providing the example of m-symmetric networks which exhibited interesting symmetries of W0. In Chapter 5 we endeavoured to show that not all points on W0 were equally good minimisers of the free energy due to their diﬀerence as singularities. Initially we explained a correspondence between phases and singularities of K(w). This naturally led to the notion of phase transitions, which we argued occurred as a result of a substantial change in the accuracy or RLCT of a compact subset of W , thus being associated to symmetry breaking of W0. We were able to show that points on W0 could indeed have diﬀerent free energies due to having diﬀerent model complexity. The key ﬁnding was that points on W \\W0 could still be favoured by the posterior despite not being minimisers of K(w). We ﬁrst showed that the complexity of degenerate-node singularities was less than that of non-degenerate node singularities. Moreover, we demonstrated a phase transition in these networks corresponding to a change in the accuracy of the degenerate-node phase. Finally we showed that weight-annihilation singularities had greater free energy than non-weight-annihilation 63 singularities and provided intuition for why this may be the case. Given the infancy of Singular Learning Theory there remain many key questions that should be examined in future studies. Relating to this thesis, we think it would be interesting to explore: • Numerical approximations of the RLCT of our established phases. • Theoretical values of the RLCT for these phases using the swish approximation to ReLU. • Generalisations of the proofs in Chapter 4 to examine W0 of networks with arbitrary input dimension, output dimension, depth, and sequences of hidden layer widths. • Scaling laws near critical values of phase transitions, particularly how these scaling exponents may arise theoretically, and how the RLCT is related. In summary, not all points on W0 are equally good. Thus, we believe it is time to evolve statistical analysis of deep learning from considering points to investigating singularities. 64 Appendix A Appendix Lemma A.1. Let q(y, x) and p(y, x|w) > 0 be continuous probability density functions. Then K(w) ≥ 0 for all w ∈ W , and K(w) = 0 if and only if p(y|x, w) = q(y|x) for almost all x ∈ RN , y ∈ RM . Proof. First note that if q(y, x) = 0 on some open set A ⊆ RN +M , since limx→0 x log x = 0 we may deﬁne in good conscience q(y, x) log q(y, x) − q(y, x) log p(y, x|w) := 0. Thus there will be no contribution to K(w) from the region A, so we can assume without loss of generality that q(y, x) > 0 on the region of integration. Consider the real-valued function S(t) = − log t + t − 1 for t ∈ (0, ∞) which is well deﬁned, continuous and diﬀerentiable everywhere on this domain. Then clearly S(1) = 0, and indeed we can show that t = 1 is the only root. Since S′(t) = − 1 t + 1, S(t) has a stationary point at t = 1, is strictly decreasing on (0, 1) and strictly increasing on (1, ∞), thus by continuity we see that t = 1 is the only root. Then since S′′(t) = 1 t2 , so S′′(1) = 1 > 0, we see that S is concave up at t = 1, thus showing S(t) ≥ 0 for all t ∈ (0, ∞) and S(t) = 0 if and only if t = 1. But then since p and q are probability distributions, hence ∫∫ RN +M p(y, x|w)dxdy = 1 and∫∫ RN +M q(y, x)dxdy = 1, we have ∫ ∫ RN +M q(y, x)S ( p(y, x|w) q(y|x) ) dxdy = ∫ ∫ RN +M q(y, x) log ( q(y, x) p(y, x|w) ) dxdy + ∫ ∫ RN +M q(y, x) p(y, x|w) q(y|x) dxdy − ∫ ∫ RN +M q(y, x)dxdy = K(w) . Since q(y, x), p(y, x|w) > 0 we have 0 < p(y,x|w) q(y,x) < ∞, hence the integrand in the ﬁrst integral is non-negative, thus the integral itself is non-negative, so K(w) ≥ 0. We have shown that if p(y, x|w) = q(y, x) then K(w) = 0, so suppose K(w) = 0. Since S(t) ≥ 0 and q(y, x) > 0 are continuous and non-negative on RN +M , by standard real analysis results we must have S ( p(y,x|w) q(y,x) ) = 0 for almost all (x, y) ∈ RN +M , hence p(y,x|w) q(y,x) = 1 as stated. Lemma A.2. Let q(y|x) = p(y|x, w0) be realisable, deﬁned by a parameter w0 ∈ W . Then K(w) = 1 2 ∫ RN ∥f (x, w) − f (x, w0)∥ 2q(x) dx . (A.1) 65 Proof. We calculate K(w) to be ∫ ∫ RN +M q(x) (2π) M 2 exp ( − 1 2 ∥y − f (x, w0)∥2) log    1 (2π) M 2 exp (− 1 2 ∥y − f (x, w0)∥ 2) 1 (2π) M 2 exp (− 1 2 ∥y − f (x, w)∥2)    dxdy . = 1 2(2π) M 2 ∫ ∫ RN +M q(x) exp (− 1 2 ∥y − f (x, w0)∥ 2) (∥y − f (x, w)∥ 2 − ∥y − f (x, w0)∥ 2) dxdy . Let u = y − f (x, w0), so du = dy, and let a = f (x, w) − f (x, w0) ∈ RM which is ﬁxed, then y − f (x, w) = u − a and so K(w) = 1 2(2π) M 2 ∫ RN q(x)K(w, w0, x)dx , (A.2) where for a ﬁxed x ∈ RN we deﬁne K(w, w0, x) = ∫ RM e− 1 2 ∥u∥2 (∥u − a∥ 2 − ∥u∥ 2) du = ∫ RM e− 1 2 ∥u∥2 (−2a · u + ∥a∥2) du . (A.3) Recall the standard identity ∫ RM e − 1 2 ∥x∥2dx = (2π) M 2 . For the dot product term we can show that this contribution is zero by induction on the dimension M . The base case for M = 1 is simply∫ ∞ −∞ a1u1e− 1 2 u2 1du = 0 since it is an odd integrand over a symmetric domain. For the inductive step, denote a = (a1, . . . , aM ) and u = (u1, . . . , uM ) and suppose ∫ RM (a · u)e− 1 2 ∥u∥2du = 0. Then ∫ RM ∫ ∞ −∞(a · u + aM +1uM +1)e− 1 2 (∥u∥2+u2 M +1)du duM +1 = ∫ ∞ −∞ e− 1 2 u 2 M +1duM +1 ∫ RM (a · u)e− 1 2 ∥u∥2du + ∫ ∞ −∞ aM +1uM +1e− 1 2 u 2 M +1duM +1 ∫ RM e− 1 2 ∥u∥2 du = 0 , where the ﬁrst integral vanishes by the inductive hypothesis and the second due to the odd integral over a symmetric domain. Substituting this into (A.3) gives K(w, w0, x) = ∥a∥ 2 ∫ RM e − 1 2 ∥u∥2du = (2π) M 2 ∥a∥2 , and so recalling the deﬁnition of a and substituting into (A.2) yields the result. Lemma A.3. Let ϕ(w) > 0 be a prior on W . Suppose P (w) is the unique maximiser of the relative entropy K(P ||ϕ(w)) subject to the constraint Ew∼P [nLn(w)] = µβ for some ﬁxed µβ ∈ R. Then P (w) = p β(w|Dn) for some β > 0 that depends on µβ Proof. Given the relative entropy functional K(P ||ϕ) = ∫ W P (w) log P (w) ϕ(w) dw , we want to solve for the probability distribution P (w) that maximises K(P ||ϕ) subject to the following constraints: − n∑ i=1 ∫ W P (w) log p(yi|xi, w)dw = µβ , and ∫ W P (w)dw = 1 , and ∫ W ϕ(w)dw = 1 , 66 where µβ ∈ R is assumed ﬁxed and given. Let k(w, P ) denote the integrand in K(P ||ϕ) and let g1(w, P ), g2(w, P ) and g3(w, P ) respectively denote the integrands in the constraints above and let λ1, λ2 and λ3 denote respective Lagrange multipliers. Then we wish to freely optimise the functional F[{λi}, P (w)] = ∫ W  k(w, P ) − 3∑ j=1 λjgj(w, P )   dw + λ1µβ + λ2 + λ3 . We can appeal to the Euler-Lagrange equation which states that F is extremised at the function P such that d dw ( ∂k ∂P ′ ) − ∂k ∂P − 3∑ j=1 λj [ d dw ( ∂gj ∂P ′ ) − ∂gj ∂P ] = 0 subject to the same constraints as above. This then evaluates to log P (w) ϕ(w) + 1 − λ1 n∑ i=1 log p(yi|xi, w) + λ2 = 0 , so P (w) = e−(1+λ2)ϕ(w) n∏ i=1 p λ1(yi|xi, w) . Then λ1 and λ2 can be solved by applying the ﬁrst two constraints, giving P (w) = p β(w|Dn) for β = λ1. Lemma A.4. Let W ⊆ W be compact. The free energy of W satisﬁes ∂F β n (W) ∂β = E β W [nLn(w)] = nG β t (W) , and ∂2F β n (W) ∂β2 = −E β W [(nLn(w))2] + E β W [nLn(w)] 2 = −Vβ W [nLn(w)] . Proof. The ﬁrst proof was provided in the main body of the text. For the second derivative we have ∂2F β n (W) ∂β2 = − ∂ ∂β ( 1 Z β n (W) ∂Z β n (W) ∂β ) = − ( ∂ ∂β 1 Z β n (W) ) ∂Z β n (W) ∂β − 1 Z β n (W) ∂2Z β n (W) ∂β2 = ( 1 Z β n (W) ∂Z β n (W) ∂β )2 − 1 Z β n (W) ∫ W (nLn(w))2ϕ(w)e −nβLn(w)dw = E β W [nLn(w)] 2 − E β W [(nLn(w))2] . Lemma A.5. Let Fn denote the free energy when β = 1. The generalisation loss is the average increase in free energy, Gn = EXn+1[Fn+1] − Fn . (A.4) In particular, the average free energy is the sum of the generalisation loss, EDn [Fn] = n−1∑ i=1 EDi[Gi] + ED1[F1] . Proof. The proof hinges on the fact that we may write Zn+1 Zn = ∫ W p(yn+1|xn+1, w)ϕ(w)e−nβLn(w)dw ∫ W ϕ(w)e−nβLn(w)dw = Ew[p(yn+1|xn+1, w)] = p(y|x, Dn) 67 which implies Fn+1 − Fn = − log p(y|x, Dn) . Since Fn does not depend on (Xn+1, Yn+1), taking EXn+1 of both sides gives the ﬁrst result. Taking expectation with respect to Dn of Eq. (A.4) gives EDn [Gn] = EDn+1 [Fn+1] − EDn [Fn] . Thus we have n−1∑ i=1 EDi[Gi] + ED1 [F1] = (EDn [Fn] − EDn−1[Fn−1]) + (EDn−1 [Fn−1] − EDn−2 [Fn−2]) + · · · + (ED2 [F2] − ED1[F1]) + ED1[F1] = EDn [Fn] . Lemma A.6. Let w, w′ ∈ R2 \\ {0} and b, b ′ ∈ R be given and let H = {x ∈ R2 | ⟨w, x⟩ + b = 0}, and H ′ = {x ∈ R2 | ⟨w′, x⟩ + b′ = 0} . Then H = H ′ if and only if there exists some scalar λ ∈ R \\ {0} such that w = λw′ and b = λb ′. Proof. The ﬁrst direction is simple: suppose λ ∈ R \\ {0} is such that w = λw′ and b = λb ′, then if x ∈ H ′ we have 0 = ⟨w′, x⟩ + b ′ = ⟨λw, x⟩ + λb = λ (⟨w, x⟩ + b) and so dividing by λ shows that x ∈ H, and by symmetry we clearly have H ′ ⊆ H too, so H = H ′. Now suppose H = H ′. Let t ∈ H be a scalar multiple of w, so t = µw for some µ ∈ R, then 0 = ⟨w, t⟩ + b = µ⟨w, w⟩ + b , so µ = − b ⟨w, w⟩ , and so t is the unique point such that b = −⟨w, t⟩. Similarly we have a unique t′ = µ ′w′, where µ ′ = − b ′ ⟨w′,w′⟩ , giving b ′ = −⟨w′, t ′⟩. Then saying x ∈ H is now equivalent to ⟨w, x − t⟩ = 0, but since x ∈ H ′ as well we also have ⟨w′, x − t′⟩ = 0. Taking x = t ′ in the ﬁrst case and x = t in the second case, noting ⟨w, t ′ − t⟩ = −⟨w, t − t′⟩, we have a system of equations Aw(t − t ′) := ( w1 w2 w′ 1 w′ 2 ) ( t1 − t′ 1 t2 − t′ 2 ) = 0 . Thus either t = t ′ or rank(Aw) = 1 (w and w′ are nonzero by hypothesis, excluding the possibility of rank(Aw) = 0). In the ﬁrst case we have t = µw = µ ′w′ = t ′, thus we can take λ = µ µ′ ∈ R to give w = λw′. In the second case, rank(Aw) = 1 implies w and w′ are linearly dependent, thus w = λw′ for some λ ∈ R. For such a λ we thus have b = −⟨w, t⟩ = −⟨w, t ′⟩ = −λ⟨w′, t ′⟩ = λb ′ , where the second equality follows from ⟨w, t − t ′⟩ = 0, thus proving the claim. 68 Bibliography [Bal97] Vijay Balasubramanian. “Statistical Inference, Occam’s Razor, and Statistical Mechan- ics on the Space of Probability Distributions”. In: Neural Computation 9.2 (Feb. 15, 1997), pp. 349–368. issn: 0899-7667. doi: 10.1162/neco.1997.9.2.349. url: https: //doi.org/10.1162/neco.1997.9.2.349 (visited on 08/06/2021). [Bet18] Michael Betancourt. “A Conceptual Introduction to Hamiltonian Monte Carlo”. In: arXiv:1701.02434 [stat] (July 15, 2018). arXiv: 1701 . 02434. url: http : / / arxiv . org/abs/1701.02434 (visited on 09/18/2021). [Bin+19] Eli Bingham et al. “Pyro: Deep Universal Probabilistic Programming”. In: J. Mach. Learn. Res. 20 (2019), 28:1–28:6. url: http://jmlr.org/papers/v20/18-403.html. [Bri] Occam’s razor. Encyclopedia Britannica. url: https://www.britannica.com/topic/ Occams-razor (visited on 09/18/2021). [Bro+20] Tom B. Brown et al. “Language Models are Few-Shot Learners”. In: arXiv:2005.14165 [cs] (July 22, 2020). arXiv: 2005.14165. url: http://arxiv.org/abs/2005.14165 (visited on 09/19/2021). [Bro11] Steve Brooks. Handbook of Markov Chain Monte Carlo. OCLC: 751677317. Hoboken: Chapman & Hall/CRC, 2011. isbn: 9781420079425. url: http://public.eblib.com/ choice/publicfullrecord.aspx?p=762505 (visited on 09/15/2021). [Cal85] Herbert B. Callen. Thermodynamics and an introduction to thermostatistics. 2nd ed. New York: Wiley, 1985. 493 pp. isbn: 9780471862567. [CB02] George Casella and Roger L. Berger. Statistical inference. 2nd ed. Australia ; Paciﬁc Grove, CA: Thomson Learning, 2002. 660 pp. isbn: 9780534243128. [GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. http://www. deeplearningbook.org. MIT Press, 2016. [Gil93] Robert Gilmore. Catastrophe theory for scientists and engineers. New York: Dover Publications, 1993. 666 pp. isbn: 9780486675398. [Har10] Robin Hartshorne. Algebraic geometry. Nachdr. Graduate texts in mathematics 52. New York, N.Y: Springer, 2010. 496 pp. isbn: 9781441928078. [Hir64] Heisuke Hironaka. “Resolution of Singularities of an Algebraic Variety Over a Field of Characteristic Zero: I”. In: Annals of Mathematics 79.1 (1964), pp. 109–203. issn: 0003-486X. doi: 10.2307/1970486. url: https://www.jstor.org/stable/1970486 (visited on 10/06/2021). [Hun07] J. D. Hunter. “Matplotlib: A 2D graphics environment”. In: Computing in Science & Engineering 9.3 (2007), pp. 90–95. doi: 10.1109/MCSE.2007.55. [Iva10] Ivanov, Oleg A. “On the Number of Regions into Which n Straight Lines Divide the Plane”. In: The American Mathematical Monthly 117.10 (2010), p. 881. issn: 00029890. doi: 10.4169/000298910x523362. url: https://www.tandfonline.com/doi/full/ 10.4169/000298910X523362 (visited on 08/28/2021). 69 [Kap+20] Jared Kaplan et al. “Scaling Laws for Neural Language Models”. In: arXiv:2001.08361 [cs, stat] (Jan. 22, 2020). arXiv: 2001.08361. url: http://arxiv.org/abs/2001. 08361 (visited on 09/19/2021). [KK08] Sadanori Konishi and G. Kitagawa. Information criteria and statistical modeling. Springer series in statistics. New York: Springer, 2008. 273 pp. isbn: 9780387718866 9780387718873. [Laf17] Lev Lafayette. Spartan HPC-Cloud Hybrid: Delivering Performance and Flexibility. ﬁgshare. Apr. 10, 2017. doi: 10.4225/49/58ead90dceaaa. url: https://melbourne. figshare.com/articles/online_resource/Spartan_HPC-Cloud_Hybrid_Delivering_ Performance_and_Flexibility/4768291/1 (visited on 10/05/2021). [Lan02] Serge Lang. Algebra. Rev. 3rd ed. Graduate texts in mathematics 211. New York: Springer, 2002. 914 pp. isbn: 9780387953854. [LBH15] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. “Deep learning”. In: Nature 521.7553 (May 28, 2015), pp. 436–444. issn: 0028-0836, 1476-4687. doi: 10.1038/nature14539. url: http://www.nature.com/articles/nature14539 (visited on 09/19/2021). [LeC14] Yan LeCun. “The Unreasonable Eﬀectiveness of Deep Learning”. Johns Hopkins Uni- versity, Center for Language and Speech Processing, Nov. 18, 2014. url: https:// www.ee.ucl.ac.uk/sahd2014/resources/LeCun.pdf. [Lu+17] Zhou Lu et al. “The Expressive Power of Neural Networks: A View from the Width”. In: arXiv:1709.02540 [cs] (Nov. 1, 2017). arXiv: 1709.02540. url: http://arxiv. org/abs/1709.02540 (visited on 08/12/2021). [McE] Richard McElreath. Markov Chains: Why Walk When You Can Flow? url: https: //elevanth.org/blog/2017/11/28/build-a-better-markov-chain/. [MHB18] Stephan Mandt, Matthew D. Hoﬀman, and David M. Blei. “Stochastic Gradient De- scent as Approximate Bayesian Inference”. In: arXiv:1704.04289 [cs, stat] (Jan. 19, 2018). arXiv: 1704.04289. url: http://arxiv.org/abs/1704.04289 (visited on 08/13/2021). [Min+20] Chris Mingard et al. “Is SGD a Bayesian sampler? Well, almost”. In: arXiv:2006.15191 [cs, stat] (Oct. 24, 2020). arXiv: 2006.15191. url: http://arxiv.org/abs/2006. 15191 (visited on 08/12/2021). [Mur+20] Daniel Murfet et al. “Deep Learning is Singular, and That’s Good”. In: arXiv:2010.11560 [cs] (Oct. 22, 2020). arXiv: 2010.11560. url: http://arxiv.org/abs/2010.11560 (visited on 09/05/2021). [Nak+19] Preetum Nakkiran et al. “Deep Double Descent: Where Bigger Models and More Data Hurt”. In: arXiv:1912.02292 [cs, stat] (Dec. 4, 2019). arXiv: 1912.02292. url: http: //arxiv.org/abs/1912.02292 (visited on 09/19/2021). [Ope16] Generative Models. OpenAI. June 16, 2016. url: https : / / openai . com / blog / generative-models/ (visited on 08/12/2021). [Pas+19] Adam Paszke et al. “PyTorch: An Imperative Style, High-Performance Deep Learning Library”. In: Advances in Neural Information Processing Systems 32. Ed. by H. Wallach et al. Curran Associates, Inc., 2019, pp. 8024–8035. url: http://papers.neurips.cc/ paper/9015-pytorch-an-imperative-style-high-performance-deep-learning- library.pdf. [PL19] Mary Phuong and Christoph H. Lampert. “Functional vs. parametric equivalence of ReLU networks”. In: International Conference on Learning Representations. Sept. 25, 2019. url: https://openreview.net/forum?id=Bylx-TNKvH (visited on 08/28/2021). [RC04] Christian P. Robert and George Casella. Monte Carlo statistical methods. 2nd ed. Springer texts in statistics. New York: Springer, 2004. 645 pp. isbn: 9780387212395. [Res99] Sidney I. Resnick. A probability path. Boston: Birkh¨auser, 1999. 453 pp. isbn: 9780817640552. 70 [Ros62] F. Rosenblatt. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Cornell Aeronautical Laboratory. Report no. VG-1196-G-8. Spartan Books, 1962. url: https://books.google.com.au/books?id=7FhRAAAAMAAJ. [RZL17] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. “Searching for Activation Func- tions”. In: arXiv:1710.05941 [cs] (Oct. 27, 2017). arXiv: 1710 . 05941. url: http : //arxiv.org/abs/1710.05941 (visited on 10/06/2021). [SS05] Elias M. Stein and Rami Shakarchi. Real analysis: measure theory, integration, and Hilbert spaces. Princeton lectures in analysis v. 3. Princeton, N.J: Princeton University Press, 2005. 402 pp. isbn: 9780691113869. [SST92] H. S. Seung, H. Sompolinsky, and N. Tishby. “Statistical mechanics of learning from examples”. In: Physical Review A 45.8 (Apr. 1, 1992), pp. 6056–6091. issn: 1050-2947, 1094-1622. doi: 10.1103/PhysRevA.45.6056. url: https://link.aps.org/doi/10. 1103/PhysRevA.45.6056 (visited on 09/29/2021). [Sus92] H´ector J. Sussmann. “Uniqueness of the weights for minimal feedforward nets with a given input-output map”. In: Neural Networks 5.4 (July 1, 1992), pp. 589–593. issn: 0893-6080. doi: 10 . 1016 / S0893 - 6080(05 ) 80037 - 1. url: https : / / www . sciencedirect.com/science/article/pii/S0893608005800371 (visited on 08/28/2021). [THK18] Stefan Thurner, R. A. Hanel, and Peter Klimek. Introduction to the theory of complex systems. OCLC: on1032587876. Oxford : New York: Oxford University Press, 2018. 431 pp. isbn: 9780198821939. [Vaa07] Aad W. van der Vaart. Asymptotic statistics. 1. paperback ed., 8. printing. Cambridge series in statistical and probabilistic mathematics. Cambridge: Cambridge Univ. Press, 2007. 443 pp. isbn: 9780521784504 9780521496032. [Was21] Michael L. Waskom. “seaborn: statistical data visualization”. In: Journal of Open Source Software 6.60 (2021), p. 3021. doi: 10 . 21105 / joss . 03021. url: https : //doi.org/10.21105/joss.03021. [Wat07] Sumio Watanabe. “Almost All Learning Machines are Singular”. In: 2007 IEEE Sym- posium on Foundations of Computational Intelligence. 2007 IEEE Symposium on Foun- dations of Computational Intelligence. Apr. 2007, pp. 383–388. doi: 10.1109/FOCI. 2007.371500. [Wat09] Sumio Watanabe. Algebraic geometry and statistical learning theory. OCLC: 521946407. Cambridge; New York: Cambridge University Press, 2009. isbn: 9780511603457 9780511800474 9780511651533. url: https : / / doi . org / 10 . 1017 / CBO9780511800474 (visited on 08/06/2021). [Wat13] Sumio Watanabe. “A Widely Applicable Bayesian Information Criterion”. In: Journal of Machine Learning Research 14 (Mar. 2013), 867897. issn: 1533-7928. url: https: //jmlr.csail.mit.edu/papers/v14/watanabe13a.html (visited on 08/06/2021). [Wat18] Sumio Watanabe. Mathematical theory of Bayesian statistics. Boca Raton: CRC Press, Taylor & Francis Group, 2018. isbn: 9781482238068. [Wat20] Sumio Watanabe. “Cross Validation, Information Criterion, and Phase Transitions”. 2020. url: http : / / watanabe - www . math . dis . titech . ac . jp / users / swatanab / slt202113.pdf. [Zha+16] Chiyuan Zhang et al. “Understanding deep learning requires rethinking generalization”. In: (Nov. 4, 2016). url: https://openreview.net/forum?id=Sy8gdB9xx (visited on 09/19/2021). [Zha06] Tong Zhang. “From -Entropy to KL-Entropy: Analysis of Minimum Information Com- plexity Density Estimation”. In: The Annals of Statistics 34.5 (2006), pp. 2180–2210. issn: 0090-5364. url: https : / / www . jstor . org / stable / 25463505 (visited on 08/12/2021). 71","libVersion":"0.3.2","langs":""}
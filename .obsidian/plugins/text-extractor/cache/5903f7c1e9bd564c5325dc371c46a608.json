{"path":"Books and Papers/Masters Points of Interest/Exact Solutions of a Deep Linear Network.pdf","text":"Exact Solutions of a Deep Linear Network Liu Ziyin 1, Botao Li 2, Xiangming Meng 3 1Department of Physics, The University of Tokyo 2Laboratoire de Physique de l’Ecole normale supérieure, ENS, Université PSL, CNRS, Sorbonne Université, Université de Paris Cité, Paris, France 3Institute for Physics of Intelligence, Graduate School of Science, The University of Tokyo Abstract This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that the origin is a special point in deep neural network loss landscape where highly nonlinear phenomenon emerges. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than 1 hidden layer, qualitatively different from a network with only 1 hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general. 1 Introduction Applications of neural networks have achieved great success in various fields. One central open question is why neural networks, being nonlinear and containing many saddle points and local minima, can sometimes be optimized easily (Choromanska et al., 2015a) while becoming difficult and requiring many tricks to train in some other scenarios (Glorot and Bengio, 2010; Gotmare et al., 2018). One established approach is to study the landscape of deep linear nets (Choromanska et al., 2015b), which are believed to approximate the landscape of a nonlinear net well. A series of works proved the famous results that for a deep linear net, all local minima are global (Kawaguchi, 2016; Lu and Kawaguchi, 2017; Laurent and Brecht, 2018), which is regarded to have successfully explained why deep neural networks are so easy to train because it implies that initialization in any attractive basin can reach the global minimum without much effort (Kawaguchi, 2016). However, the theoretical problem of when and why neural networks can be hard to train is understudied. In this work, we theoretically study a deep linear net with weight decay and stochastic neurons, whose loss function takes the following form in general: ExEϵ(1),ϵ(2),...,ϵ(D) ⎛ ⎝ d,d1,d2,...dD ∑ i,i1,i2,...,iD UiD ϵ(D) iD ...ϵ (2) i2 W (2) i2i1ϵ(1) i1 W (1) i1i xi − y⎞ ⎠ 2 ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ L0 + γu∣∣U ∣∣ 2 2 + D ∑ i=1 γi∣∣W (i)∣∣ 2 F ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ L2 reg. , (1) where Ex denotes the expectation over the training set, U and W (i) are the model parameters, D is the depth of the network,1 ϵ is the noise in the hidden layer (e.g., due to dropout), di is the width of the i-th layer, and γ is the strength of the weight decay. Previous works have studied special cases of this loss function. For example, Kawaguchi (2016) and Lu and Kawaguchi (2017) study the landscape of L0 when ϵ is a constant (namely, when there is no noise). Mehta et al. (2021) studies 36th Conference on Neural Information Processing Systems (NeurIPS 2022). 1In this work, we use “depth\" to refer to the number of hidden layers. For example, a linear regressor has depth 0.arXiv:2202.04777v7 [stat.ML] 13 Jun 2023 Figure 1: Left: A summary of the network landscape that is implied by the main results of this work when one increases the weight decay strength γ while fixing other terms. We show that the landscape of a depth-1 net can be precisely divided into two regimes, while, for D ≥ 2, there exists at least three regimes. The solid blue line indicates that the division of the regimes is precisely understood. The dashed lines indicate that the conditions we found are not tight and may be improved in the future. Right: ResNet18 on CIFAR10. The performance of a linear regressor never drops to that of a trivial model, whereas the performance of ResNet18 drops to the level of a trivial model, like a deep linear net with similar depth. L0 with (a more complicated type of) weight decay but without stochasticity and proved that all the stationary points are isolated. Another line of works studies L0 when the noise is caused by dropout (Mianjy and Arora, 2019; Cavazza et al., 2018). Our setting is more general than the previous works in two respects. First, apart from the mean square error (MSE) loss L0, an L2 regularization term (weight decay) with arbitrary strength is included; second, the noise ϵ is arbitrary. Thus, our setting is arguably closer to the actual deep learning practice, where the injection of noises to latent layers is common, and the use of weight decay is virtually ubiquitous (Krogh and Hertz, 1992; Loshchilov and Hutter, 2017). One major limitation of our work is that we assume the label y to be 1-dimensional, and it can be an important future problem to prove whether an exact solution exists or not when y is high-dimensional. Our foremost contribution is to prove that all the global minimum of an arbitrarily deep and wide linear net takes a simple analytical form. In other words, we identify in closed form the global minima of Eq. (1) up to a single scalar, whose analytical expression does not exist in general. We then show that it has nontrivial properties that can explain many phenomena in deep learning. In particular, the implications of our result include (but are not limited to): 1. Weight decay makes the landscape of neural nets more complicated; • we show that bad minima 2 emerge as weight decay is applied, whereas there is no bad minimum when there is no weight decay. This highlights the need to escape bad local minima in deep learning with weight decay. 2. Deeper nets are harder to optimize than shallower ones; • we show that a D ≥ 2 linear net contains a bad minimum at zero, whereas a D = 1 net does not. This partially explains why deep networks are much harder to optimize than shallower ones in deep learning practice. 3. Depending on the task, the common initialization methods (such as the Kaiming init.) can initialize a deep model in the basin of attraction of the bad minimum at zero; • common initialization methods initialize the models at a radius of roughly 1/ √width around the origin; however, we show that the width of the bad minimum is task- dependent and can be larger than the initialization radius for tasks with a small margin (∣∣E[xy]∣∣); 4. Thus, the use of (effective) weight decay is a major cause of various types of collapses in deep learning (for example, see Figure 1). Organization: In the next section, we discuss the related works. In Section 3, we derive the exact solution for a two-layer net. Section 4 extends the result to an arbitrary depth. In Section 5, we study and discuss the relevance of our results to many commonly encountered problems in deep learning. The last section concludes the work and discusses unresolved open problems. All proofs are delayed to Section B. Moreover, additional theoretical results on the effect of including a bias term is considered in Section D. 2Unless otherwise specified, we use the word “bad minimum\" to mean a local minimum that is not a global minimum. 2 Notation. For a matrix W , we use Wi∶ to denote the i-th row vector of W . ∣∣Z∣∣ denotes the L2 norm if Z is a vector and the Frobenius norm if Z is a matrix. The notation ∗ signals an optimized quantity. Additionally, we use the superscript ∗ and subscript ∗ interchangeably, whichever leads to a simpler expression. For example, b 2 ∗ and (b∗)2 denote the same quantity, while the former is “simpler.\" 2 Related Works In many ways, linear networks have been used to help understand nonlinear networks. For example, even at depth 0, where the linear net is just a linear regressor, linear nets are shown to be relevant for understanding the generalization behavior of modern overparametrized networks (Hastie et al., 2019). Saxe et al. (2013) studies the training dynamics of a depth-1 network and uses it to understand the dynamics of learning of nonlinear networks. These networks are the same as a linear regression model in terms of expressivity. However, the loss landscape is highly complicated due to the existence of more than one layer, and linear nets are widely believed to approximate the loss landscape of a nonlinear net (Kawaguchi, 2016; Hardt and Ma, 2016; Laurent and Brecht, 2018). In particular, the landscape of linear nets has been studied as early as 1989 in Baldi and Hornik (1989), which proposed the well-known conjecture that all local minima of a deep linear net are global. This conjecture is first proved in Kawaguchi (2016), and extended to other loss functions and deeper depths in Lu and Kawaguchi (2017) and Laurent and Brecht (2018). Many relevant contemporary deep learning problems can be understood with deep linear models. For example, two-layer linear VAE models are used to understand the cause of the posterior collapse problem (Lucas et al., 2019; Wang and Ziyin, 2022). Deep linear nets are also used to understand the neural collapse problem in contrastive learning (Tian, 2022). We also provide more empirical evidence in Section 5. 3 Two-layer Linear Net This section finds the global minima of a two-layer linear net. The data point is a d-dimensional vector x ∈ Rd drawn from an arbitrary distribution, and the labels are generated through an arbitrary function y = y(x) ∈ R. For generality, we let different layers have different strengths of weight decay even though they often take the same value in practice. We want to minimize the following objective: Ld,d1(U, W ) = ExEϵ ⎛ ⎝ d1 ∑ j Ujϵj d ∑ i Wjixi − y⎞ ⎠ 2 + γw∣∣W ∣∣ 2 + γu∣∣U ∣∣ 2, (2) where d1 is the width of the hidden layer and ϵi are independent random variables. γw > 0 and γu > 0 are the weight decay parameters. Here, we consider a general type of noise with E[ϵi] = 1 and E[ϵiϵj] = δijσ2 + 1 where δij is the Kronecker’s delta, and σ2 > 0.3 For shorthand, we use the notation A0 ∶= E[xxT ], and the largest and the smallest eigenvalues of A0 are denoted as amax and amin respectively. ai denotes the i-th eigenvalue of A0 viewed in any order. For now, it is sufficient for us to assume that the global minimum of Eq. (2) always exists. We will prove a more general result in Proposition 1, when we deal with multilayer nets. 3.1 Main Result We first present two lemmas showing that the global minimum can only lie on a rather restrictive subspace of all possible parameter settings due to invariances in the objective. Lemma 1. At the global minimum of Eq. (2), U 2 j = γw γu ∑i W 2 ji for all j. Proof Sketch. We use the fact that the first term of Eq. (2) is invariant to a simultaneous rescaling of rows of the weight matrix to find the optimal rescaling, which implies the lemma statement. ◻ This lemma implies that for all j, ∣Uj∣ must be proportional to the norm of its corresponding row vector in W . This lemma means that using weight decay makes all layers of a deep neural network balanced. This lemma has been referred to as the “weight balancing\" condition in recent works (Tanaka et al., 2020), and, in some sense, is a unique and potentially essential feature of neural networks that encourages a sparse solution (Ziyin and Wang, 2023). The following lemma further shows that, at the global minimum, all elements of U must be equal. 3While we formally require γ and σ to nonzero, one can show that the solutions we provided remain global minimizers in the zero limit by applying Theorem 2 from Ziyin and Ueda (2022). 3 Lemma 2. At the global minimum, for all i and j, we have { U 2 i = U 2 j ; UiWi∶ = UjWj∶. (3) Proof Sketch. We show that if the condition is not satisfied, then an “averaging\" transformation will strictly decrease the objective. ◻ This lemma can be seen as a formalization of the intuition suggested in the original dropout paper (Srivastava et al., 2014). Namely, using dropout encourages the neurons to be independent of one another and results in an averaging effect. The second lemma imposes strong conditions on the solution of the problem, and the essence of this lemma is the reduction of the original problem to a lower dimension. We are now ready to prove our first main result. Theorem 1. The global minimum U∗ and W∗ of Eq. (2) is U∗ = 0 and W∗ = 0 if and only if ∣∣E[xy]∣∣ 2 ≤ γuγw. (4) When ∣∣E[xy]∣∣ 2 > γuγw, the global minima are { U∗ = br; W∗ = rE[xy]T b [b 2 (σ2 + d1) A0 + γwI]−1 , (5) where r = (±1, ..., ±1) is an arbitrary vertex of a d1-dimensional hypercube, and b satisfies: ∣∣ [b2 (σ2 + d1) A0 + γwI]−1 E[xy]∣∣ 2 = γu γw . (6) Apparently, b = 0 is the trivial solution that has not learned any feature due to overregularization. Henceforth, we refer to this solution (and similar solutions for deeper nets) as the “trivial\" solution. We now analyze the properties of the nontrivial solution b∗ when it exists. The condition for the solution to become nontrivial is interesting: ∣∣E[xy]∣∣ 2 ≥ γuγw. The term ∣∣E[xy]∣∣ can be seen as the effective strength of the signal, and γuγw is the strength of regularization. This precise condition means that the learning of a two-layer can be divided into two qualitatively different regimes: an “overregularized regime\" where the global minimum is trivial, and a “feature learning regime\" where the global minimum involves actual learning. Lastly, note that our main result does not specify the exact value of b∗. This is because b∗ must satisfy the condition in Eq. (6), which is equivalent to a high-order polynomial in b with coefficients being general functions of the eigenvalues of A0, whose solutions are generally not analytical by Galois theory. One special case where an analytical formula exists for b is when A0 = σ2 xI. See Section C for more discussion. 3.2 Bounding the General Solution While the solution to b ∗ does not admit an analytical form for a general A0, one can find meaningful lower and upper bounds to b∗ such that we can perform an asymptotic analysis of b ∗. At the global minimum, the following inequality holds: ∣∣ [b 2 (σ2 + d1) amaxI + γwI]−1 E[xy]∣∣ 2 ≤ ∣∣[b2 (σ2 + d1) A0 + γwI]−1E[xy]∣∣ 2 ≤ ∣∣ [b 2 (σ2 + d1) aminI + γwI] −1 E[xy]∣∣ 2, (7) where amin and amax are the smallest and largest eigenvalue of A0, respectively. The middle term is equal to γu/γw by the global minimum condition in (33), and so, assuming amin > 0, this inequality is equivalent to the following inequality of b∗: √ γw γu ∣∣E[xy]∣∣ − γw (σ2 + d1)amax ≤ b2 ∗ ≤ √ γw γu ∣∣E[xy]∣∣ − γw (σ2 + d1)amin . (8) Namely, the general solution b∗ should scale similarly to the homogeneous solution in Eq. (104) if we treat the eigenvalues of A0 as constants. 4 4 Exact Solution for An Arbitrary-Depth Linear Net This section extends our result to multiple layers. We first derive the analytical formula for the global minimum of a general arbitrary-depth model. We then show that the landscape for a deeper network is highly nontrivial. 4.1 General Solution The loss function is ExEϵ(1),ϵ(2),...,ϵ(D) ⎛ ⎝ d,d1,d2,...dD ∑ i,i1,i2,...,iD UiD ϵ(D) iD ...ϵ(2) i2 W (2) i2i1 ϵ (1) i1 W (1) i1i xi − y⎞ ⎠ 2 + γu∣∣U ∣∣ 2 + D ∑ i=1 γi∣∣W (i)∣∣ 2, (9) where all the noises ϵ are independent, and for all i and j, E[ϵ(i) j ] = 1 and E[(ϵ(i) j )2] = σ2 i + 1 > 1. We first show that for general D, the global minimum exists for this objective. Proposition 1. For D ≥ 1 and strictly positive γu, γ1, ..., γD, the global minimum for Eq.(9) exists. Note that the positivity of the regularization strength is crucial. If one of the γi is zero, the global minimum may not exist. The following theorem is our second main result. Theorem 2. Any global minimum of Eq. (9) is of the form ⎧⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎩ U = burD; W (i) = birir T i−1; W (1) = r1E[xy]T (bu ∏D i=2 bi)µ [(bu ∏D i=2 bi) 2s 2 (σ2 + d1) A0 + γwI]−1 , (10) where µ = ∏ D i=2 di, s 2 = ∏ D i=2 di(σ2 + di), bu ≥ 0 and bi ≥ 0, and ri = (±1, ..., ±1) is an arbitrary vertex of a di-dimensional hypercube for all i. Furthermore, let b1 ∶= √∣∣Wi∶∣∣2/d and bD+1 ∶= bu, bi satisfies γk+1dk+1b 2 k+1 = γkdk−1b2 k. (11) Proof Sketch. We prove by induction on the depth D. The base case is proved in Theorem 1. We then show that for a general depth, the objective involves optimizing subproblems, one of which is a D − 1 layer problem that follows by the induction assumption, and the other is a two-layer problem that has been solved in Theorem 1. Putting these two subproblems together, one obtains Eq. (10). ◻ Remark. We deal with the technical case of having a bias term for each layer in Appendix D. For example, we will show that if one has preprocessed the data such that E[x] = 0 and E[y] = 0, our main results remain precisely unchanged. The condition in Eq. (11) shows that the scaling factor bi for all i is not independent of one another. This automatic balancing of the norm of all layers is a consequence of the rescaling invariance of the multilayer architecture and the use of weight decay. It is well-known that this rescaling invariance also exists in a neural network with the ReLU activation, and so this balancing condition is also directly relevant for ReLU networks. Condition (11) implies that all the bi can be written in terms of one of the bi: bu D ∏ i=2 bi = c0sgn (bu D ∏ i=2 bi) ∣b D 2 ∣ ∶= c0sgn (bu D ∏ i=2 bi) b D (12) where c0 = (γ2d2d1)D/2 √γu ∏ D i=2 γi ∏ D i=2 di√d1 and b ≥ 0. Consider the first layer (i = 1), Eq (11) shows that the global minimum must satisfy the following equation, which is equivalent to a high-order polynomial in b that does not have an analytical solution in general: ∣∣E[xy] T c0bDµ [c 2 0b2Ds 2 (σ2 + d1) A0 + γwI]−1 ∣∣ 2 = d2b 2. (13) Thus, this condition is an extension of the condition (6) for two-layer networks. At this point, it pays to clearly define the word “solution,\" especially given that it has a special meaning in this work because it now becomes highly nontrivial to differentiate between the two types of solutions. 5 Definition 1. We say that a non-negative real b is a solution if it satisfies Eq. (13). A solution is trivial if b = 0 and nontrivial otherwise. Namely, a global minimum must be a solution, but a solution is not necessarily a global minimum. We have seen that even in the two-layer case, the global minimum can be the trivial one when the strength of the signal is too weak or when the strength of regularization is too strong. It is thus natural to expect 0 to be the global minimum under a similar condition, and one is interested in whether the condition becomes stronger or weaker as the depth of the model is increased. However, it turns out this naive expectation is not true. In fact, when the depth of the model is larger than 2, the condition for the trivial global minimum becomes highly nontrivial. The following proposition shows why the problem becomes more complicated. In particular, we have seen that in the case of a two-layer net, some elementary argument has helped us show that the trivial solution b = 0 is either a saddle or the global minimum. However, the proposition below shows that with D ≥ 2, the landscape becomes more complicated in the sense that the trivial solution is always a local minimum, and it becomes difficult to compare the loss value of the trivial solution with the nontrivial solution because the value of b∗ is unknown in general. Proposition 2. Let D ≥ 2 in Eq. (9). Then, the solution U = 0, W (D) = 0, ..., W (1) = 0 is a local minimum with a diagonal positive-definite Hessian γI. Comparing the Hessian of D ≥ 2 and D = 1, one notices a qualitative difference: for D ≥ 2, the Hessian is always diagonal (at 0); for D = 1, in sharp contrast, the off-diagonal terms are nonzero in general, and it is these off-diagonal terms that can break the positive-definiteness of the Hessian. This offers a different perspective on why there is a qualitative difference between D = 1 and D = 2. Lastly, note that, unlike the depth-1 case, one can no longer find a precise condition such that a b ≠ 0 solution exists for a general A0. The reason is that the condition for the existence of the solution is now a high-order polynomial with quite arbitrary intermediate terms. The following proposition gives a sufficient but stronger-than-necessary condition for the existence of a nontrivial solution, when all the σi, intermediate width di and regularization strength γi are the same. 4 Proposition 3. Let σ2 i = σ2 > 0, di = d0 and γi = γ > 0 for all i. Assuming amin > 0, the only solution is trivial if D + 1 2D ∣∣E[xy]∣∣dD−1 0 ( (D − 1)∣∣E[xy]∣∣ 2Dd0(σ2 + d0)Damin ) D−1 D+1 < γ. (14) Nontrivial solutions exist if D + 1 2D ∣∣E[xy]∣∣dD−1 0 ( (D − 1)∣∣E[xy]∣∣ 2Dd0(σ2 + d0)Damax ) D−1 D+1 ≥ γ. (15) Moreover, the nontrivial solutions are both lower and upper-bounded: 5 1 d0 [ γ ∣∣E[xy]∣∣ ] 1 D−1 ≤ b ∗ ≤ [ ∣∣E[xy]∣∣ d0(σ2 + d0)Damax ] 1 D+1 . (16) Proof Sketch. The proof follows from the observation that the l.h.s. of Eq. (13) is a continuous function and must cross the r.h.s. under certain sufficient conditions. ◻ One should compare the general condition here with the special condition for D = 1. One sees that for D ≥ 2, many other factors (such as the width, the depth, and the spectrum of the data covariance A0) come into play to determine the existence of a solution apart from the signal strength E[xy] and the regularization strength γ. 4This is equivalent to setting c0 = √d0. The result is qualitatively similar but involves additional factors of c0 if σi, di, and γi all take different values. We thus only present the case when σi, di, and γi are the same for notational concision and for emphasizing the most relevant terms. Also, note that this proposition gives a sufficient and necessary condition if A0 = σ2 xI is proportional to the identity. 5For D = 1, we define the lower-bound as limη→0+ limD→1+ 1 d0 [ γ+η ∣∣E[xy]∣∣ ] 1 D−1 , which equal to zero if E[xy] ≥ γ, and ∞ if E[xy] < γ. With this definition, this proposition applies to a two-layer net as well. 6lossloss Figure 2: The training loss as a function of b for a D = 1 network with different activation functions in the hidden layer. For simplicity, dropout is not implemented. The non-linear activation functions we considered are ReLU, Tanh, and Swish. The left and right panels use different data. Left: X are Gaussian random vectors, and y = v ⋅ x is a linear function of x. Right: x are Gaussian random vectors, and y = v ⋅ tanh(x) are nonlinear functions of data; the weight v is obtained as a Gaussian random vector. 4.2 Which Solution is the Global Minimum? Again, we set γi = γ > 0, σ2 i = σ2 > 0 and di = d0 > 0 for all i for notational concision. Using this condition and applying Lemma 3 to Theorem 2, the solution now takes the following form, where b ≥ 0, ⎧⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎩ U = √d0brD; W (i) = brir T i−1; W (1) = r1E[xy] T d D− 1 2 0 b D [dD 0 (σ2 + d0)Db2DA0 + γ]−1 . (17) The following theorem gives a sufficient condition for the global minimum to be nontrivial. It also shows that the landscape of the linear net becomes complicated and can contain more than 1 local minimum. Theorem 3. Let σ2 i = σ2 > 0, di = d0 and γi = γ > 0 for all i and assuming amin > 0. Then, if ∣∣E[xy]∣∣ 2 ≥ γ D+1 D D2(σ2 + d0)D−1a D−1 D max dD−1 0 (D − 1) D−1 D (18) the global minimum of Eq. (9) is one of the nontrivial solutions. While there are various ways this bound can be improved, it is general enough for our purpose. In particular, one sees that, for a general depth, the condition for having a nontrivial global minimum depends not only on the E[xy] and γ but also on the model architecture in general. For a more general architecture with different widths etc., the architectural constant c0 from Eq. (13) will also enter the equation. In the limit of D → 1 +, relation (18) reduces to ∣∣E[xy]∣∣ 2 ≥ γ2, (19) which is the condition derived for the 2-layer case. 5 Implications Relevance to nonlinear models. We first caution the readers that the following discussion should be taken with a caveat and is based on the philosophy that deep linear nets can approximate the nonlinear ones. This approximation certainly holds for fully connected models with differentiable activation functions such as tanh or Swish because they are, up to first-order Taylor expansion, a deep linear net around zero, which is the region for which our theory is the most relevant. We empirically demonstrate that close to the origin, the landscape of linear nets can indeed approximate that of nonlinear nets quite well. To compare, we plug in the solution in Theorem 4 to both linear and nonlinear models of the same architecture and compare the loss values at different values of b around b = 0. For simplicity, we only consider the case D = 1. The activation functions we consider are ReLU, Tanh, and Swish (Ramachandran et al., 2017), a modern and differentiable variant of ReLU. See Fig. 2. The regressor x ∈ Rd is sampled as Gaussian random vectors. We consider two methods of generating y; the first one (left) is y = v ⋅ x. The second one (right) is y = v ⋅ tanh(x), where the weight v ∈ Rd is obtained as a Gaussian random vector. Fig. 2 shows that the landscape consisting of Tanh is always close to the linear landscape. Swish is not as good as Tanh, but the Swish landscape shows a similar tendency to the linear landscape. The ReLU landscape is not so close to the linear landscape 7 either for b > 0 or b < 0, but it agrees completely with the linear landscape on the other side, as expected. Besides the quantitative closeness, it is also important to note that all the landscapes agree qualitatively, containing the same number of local minima at similar values of b. Landscape of multi-layer neural networks. The combination of Theorem 3 and Proposition 2 shows that the landscape of a deep neural network can become highly nontrivial when there is a weight decay and when the depth of the model is larger than 2. This gives an incomplete but meaningful picture of a network’s complicated but interesting landscape beyond two layers (see Figure 1 for an incomplete summary of our results). In particular, even when the nontrivial solution is the global minimum, the trivial solution is still a local minimum that needs to be escaped. Our result suggests the previous understanding that all local minima of a deep linear net are global cannot generalize to many practical settings where deep learning is found to work well. For example, a series of works attribute the existence of bad (non-global) minima to the use of nonlinearities (Kawaguchi, 2016) or the use of a non-regular (non-differentiable) loss function (Laurent and Brecht, 2018). Our result, in contrast, shows that the use of a simple weight decay is sufficient to create a bad minimum.6 Moreover, the problem with such a minimum is two-fold: (1) (optimization) it is not global and so needs to be “overcome\" and (2) (generalization) it is a minimum that has not learned any feature at all because the model constantly outputs zero. To the best of our knowledge, previous to our work, there has not been any proof that a bad minimum can generically exist in a rather arbitrary network without any restriction on the data. 7 Thus, our result offers direct theoretical justification for the widely believed importance of escaping local minima in the field of deep learning (Kleinberg et al., 2018; Liu et al., 2021; Mori et al., 2022). In particular, previous works on escaping local minima often hypothesize landscapes that are of unknown relevance to an actual neural network. With our result, this line of research can now be established with respect to landscapes that are actually deep-learning-relevant. Previous works also argue that having a deeper depth does not create a bad minimum (Lu and Kawaguchi, 2017). While this remains true, its generality and applicability to practical settings now also seem low. Our result shows that as long as weight decay is used, and as long as D ≥ 2, there is indeed a bad local minimum at 0. In contrast, there is no bad minimum at 0 for a depth-2 network: the point b = 0 is either a saddle or the global minimum.8 Having a deeper depth thus alters the qualitative nature of the landscape, and our results agree better with the common observation that a deeper network is harder, if not impossible, to optimize. We note that our result can also be relevant for more modern architectures such as the ResNet. Using ResNet, one needs to change the dimension of the hidden layer after every bottleneck, and a learnable linear transformation is applied here. Thus, the “effective depth” of a ResNet would be roughly between the number of its bottlenecks and its total number of blocks. For example, a ResNet18 applied to CIFAR10 often has five bottlenecks and 18 layers in total. We thus expect it to have qualitatively similar behavior to a deep linear net with a depth in between. See Figure 1. The experimental details are given in Section A. Learnability of a neural network. Now we analyze the solution when D tends to infinity. We first note that the existence condition bound in (15) becomes exponentially harder to satisfy as D becomes large: ∣∣E[xy]∣∣ 2 ≥ 4d2 0amaxγe D log[(σ2+d0)/d0] + O(1). (20) When this bound is not satisfied, the given neural network cannot learn the data. Recall that for a two-layer net, the existence condition is nothing but ∣∣E[xy]∣∣ 2 > γ2, independent of the depth, width, or stochasticity in the model. For a deeper network, however, every factor comes into play, and the architecture of the model has a strong (and dominant) influence on the condition. In particular, a factor that increases polynomially in the model width and exponentially in the model depth appears. 6Some previous works do suggest the existence of bad minima when weight decay is present, but no direct proof exists yet. For example, Taghvaei et al. (2017) shows that when the model is approximated by a linear dynamical system, regularization can cause bad local minima. Mehta et al. (2021) shows the existence of bad local minima in deep linear networks with weight decay through numerical simulations. 7In the case of nonlinear networks without regularization, a few works proved the existence of bad minima. However, the previous results strongly depend on the data and are rather independent of architecture. For example, one major assumption is that the data cannot be perfected and fitted by a linear model (Yun et al., 2018; Liu, 2021; He et al., 2020). Some other works explicitly construct data distribution (Safran and Shamir, 2018; Venturi et al., 2019). Our result, in contrast, is independent of the data. 8Of course, in practice, the model trained with SGD can still converge to the trivial solution even if it is a saddle point (Ziyin et al., 2021) because minibatch SGD is, in general, not a good estimator of the local minima. 8 A practical implication is that the use of weight decay may be too strong for deep networks. If one increases the depth or width of the model, one should also roughly decrease γ according to Eq. (20). Figure 3: Training loss of D = 2 neu- ral networks with ReLU and Tanh activa- tions across synthetic tasks with different ∣∣E[xy]∣∣. We see that with the Kaiming initialization, both the Tanh net and the ReLU net are stuck at the trivial solution in expectation of our theory. In contrast, an optimized linear regressor (D = 0) is better than the trivial solution when ∣∣E[xy]∣∣ > 0. See Section A for experimental details. Insufficiency of the existing initialization schemes. We have shown that 0 is often a bad local minimum for deep learning. Our result further implies that escaping this local minimum can be highly practically relevant because standard initialization schemes are trapped in this local minimum for tasks where the signal E[xy] is weak. See Inequality (16): any nontrivial global minimum is lower-bounded by a factor proportional to (γ/∣∣E[xy]∣∣ 1/(D−1))/d0, which can be seen as an approximation of the radius of the local minimum at the origin. In comparison, standard deep learning initialization schemes such as Kaiming init. initialize at a radius roughly 1/ √d0. Thus, for tasks E[xy] ≪ γ/√ d0, these initialization methods are likely to initialize the model in the basin of attraction of the trivial regime, which can cause a serious failure in learning. To demonstrate, we perform a numerical simulation shown in the right panel of Figure 3, where we train D = 2 nonlinear networks with width 32 with SGD on tasks with varying ∣∣E[xy]∣∣. For sufficiently small ∣∣E[xy]∣∣, the model clearly is stuck at the origin. 9 In contrast, linear regression is never stuck at the origin. Our result thus suggests that it may be desirable to devise initialization methods that are functions of the data distribution. Prediction variance of stochastic nets. A major extension of the standard neural networks is to make them stochastic, namely, to make the output a random function of the input. In a broad sense, stochastic neural networks include neural networks trained with dropout (Srivastava et al., 2014; Gal and Ghahramani, 2016), Bayesian networks (Mackay, 1992), variational autoencoders (VAE) (Kingma and Welling, 2013), and generative adversarial networks (Goodfellow et al., 2014). Stochastic networks are thus of both practical and theoretical importance to study. Our result can also be used for studying the theoretical properties of stochastic neural networks. Here, we present a simple application of our general solution to analyze the properties of a stochastic net. The following theorem summarizes our technical results. Theorem 4. Let σ2 i = σ2 > 0, di = d0 and γi = γ > 0 for all i. Let A0 = σ2 xI. Then, at any global minimum of Eq. (9), holding other parameters fixed, 1. in the limit of large d0, Var[f (x)] = O (d−1 0 ) ; 2. in the limit of large σ2, Var[f (x)] = O ( 1 (σ2)D ); 3. In the limit of large D, Var[f (x)] = O (e−2D log[(σ2+d0)/d0]). Interestingly, the scaling of prediction variance in asymptotic σ2 is different for different widths. The third result shows that the prediction variance decreases exponentially fast in D. In particular, this result answers a question recently proposed in Ziyin et al. (2022): does a stochastic net trained on MSE have a prediction variance that scales towards 0? We improve on their result in the case of a deep linear net by (a) showing that the d −1 0 is tight in general, independent of the depth or other factors of the model, and (b) proving a bound showing that the variance also scales towards zero as depth increases, which is a novel result of our work. Our result also offers an important insight into the cause of the vanishing prediction variance. Previous works (Alemi et al., 2018) often attribute the cause to the fact that a wide neural network is too expressive. However, our result implies that this is not always the case because a linear network with limited expressivity can also have a vanishing variance as the model tends to an infinite width. Collapses in deep learning. Lastly, we comment briefly on the apparent similarity between different types of collapses that occur in deep learning. For neural collapse, our result agrees with the recent works that identify weight decay as a main cause (Rangamani and Banburski-Fahey, 2022). For Bayesian deep learning, Wang and Ziyin (2022) identified the cause of the posterior collapse in 9There are many natural problems where the signal is extremely weak. One well-known example is the problem of future price prediction in finance, where the fundamental theorem of finance forbids a large ∣∣E[xy]∣∣ (Fama, 1970). 9 a two-layer VAE structure to be that the regularization of the mean of the latent variable z is too strong. More recently, the origin and its stability have also been discussed as the dimensional collapse in self-supervised learning (Ziyin et al., 2023). Although appearing in different contexts of deep learning, the three types of collapses share the same phenomenology that the model converges to a “collapsed\" regime where the learned representation becomes low-rank or constant, which agrees with the behavior of the trivial regime we identified. We refer the readers to Ziyin and Ueda (2022) for a study of how the second-order phase transition framework of statistical physics can offer a possible unified explanation of these phenomena. 6 Conclusion In this work, we derived the exact solution of a deep linear net with arbitrary depth and width and with stochasticity. Our work sheds light on the highly complicated landscape of a deep neural network. Compared to the previous works that mostly focus on the qualitative understanding of the linear net, our result offers a more precise quantitative understanding of deep linear nets. Quantitative understanding is one major benefit of knowing the exact solution, whose usefulness we have also demonstrated with the various implications. The results, although derived for linear models, are also empirically shown to be relevant for networks with nonlinear activations. Lastly, our results strengthen the line of thought that analytical approaches to deep linear models can be used to understand deep neural networks, and it is the sincere hope of the authors to attract more attention to this promising field. Acknowledgement Ziyin is financially supported by the GSS scholarship of the University of Tokyo and the JSPS fellowship. Li is financially supported by CNRS. X. Meng is supported by JST CREST Grant Number JPMJCR1912, Japan. References Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R. A., and Murphy, K. (2018). Fixing a broken ELBO. In International Conference on Machine Learning, pages 159–168. PMLR. Baldi, P. and Hornik, K. (1989). Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53–58. Cavazza, J., Morerio, P., Haeffele, B., Lane, C., Murino, V., and Vidal, R. (2018). Dropout as a low-rank regularizer for matrix factorization. In International Conference on Artificial Intelligence and Statistics, pages 435–444. PMLR. Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2015a). The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192–204. Choromanska, A., LeCun, Y., and Arous, G. B. (2015b). Open problem: The landscape of the loss surfaces of multilayer networks. In Conference on Learning Theory, pages 1756–1760. PMLR. Fama, E. F. (1970). Efficient capital markets: A review of theory and empirical work. The journal of Finance, 25(2):383–417. Gal, Y. and Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050–1059. PMLR. Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27. 10 Gotmare, A., Keskar, N. S., Xiong, C., and Socher, R. (2018). A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243. Hardt, M. and Ma, T. (2016). Identity matters in deep learning. arXiv preprint arXiv:1611.04231. Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2019). Surprises in high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560. He, F., Wang, B., and Tao, D. (2020). Piecewise linear activations substantially shape the loss surfaces of neural networks. arXiv preprint arXiv:2003.12236. Kawaguchi, K. (2016). Deep learning without poor local minima. Advances in Neural Information Processing Systems, 29:586–594. Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Kleinberg, B., Li, Y., and Yuan, Y. (2018). An alternative view: When does sgd escape local minima? In International Conference on Machine Learning, pages 2698–2707. PMLR. Krogh, A. and Hertz, J. A. (1992). A simple weight decay can improve generalization. In Advances in neural information processing systems, pages 950–957. Laurent, T. and Brecht, J. (2018). Deep linear networks with arbitrary loss: All local minima are global. In International conference on machine learning, pages 2902–2907. PMLR. Liu, B. (2021). Spurious local minima are common for deep neural networks with piecewise linear activations. arXiv preprint arXiv:2102.13233. Liu, K., Ziyin, L., and Ueda, M. (2021). Noise and fluctuation of finite learning rate stochastic gradient descent. In International Conference on Machine Learning, pages 7045–7056. PMLR. Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Lu, H. and Kawaguchi, K. (2017). Depth creates no bad local minima. arXiv preprint arXiv:1702.08580. Lucas, J., Tucker, G., Grosse, R., and Norouzi, M. (2019). Don’t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse. Mackay, D. J. C. (1992). Bayesian methods for adaptive models. PhD thesis, California Institute of Technology. Mehta, D., Chen, T., Tang, T., and Hauenstein, J. (2021). The loss surface of deep linear networks viewed through the algebraic geometry lens. IEEE Transactions on Pattern Analysis and Machine Intelligence. Mianjy, P. and Arora, R. (2019). On dropout and nuclear norm regularization. In International Conference on Machine Learning, pages 4575–4584. PMLR. Mori, T., Ziyin, L., Liu, K., and Ueda, M. (2022). Power-law escape rate of sgd. In International Conference on Machine Learning, pages 15959–15975. PMLR. Ramachandran, P., Zoph, B., and Le, Q. V. (2017). Searching for activation functions. Rangamani, A. and Banburski-Fahey, A. (2022). Neural collapse in deep homogeneous classifiers and the role of weight decay. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4243–4247. IEEE. Safran, I. and Shamir, O. (2018). Spurious local minima are common in two-layer relu neural networks. In International conference on machine learning, pages 4433–4441. PMLR. Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120. 11 Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958. Taghvaei, A., Kim, J. W., and Mehta, P. (2017). How regularization affects the critical points in linear networks. Advances in neural information processing systems, 30. Tanaka, H., Kunin, D., Yamins, D. L., and Ganguli, S. (2020). Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in Neural Information Processing Systems, 33:6377–6389. Tian, Y. (2022). Deep contrastive learning is provably (almost) principal component analysis. arXiv preprint arXiv:2201.12680. Venturi, L., Bandeira, A. S., and Bruna, J. (2019). Spurious valleys in one-hidden-layer neural network optimization landscapes. Journal of Machine Learning Research, 20:133. Wang, Z. and Ziyin, L. (2022). Posterior collapse of a linear latent variable model. In Advances in Neural Information Processing Systems. Yun, C., Sra, S., and Jadbabaie, A. (2018). Small nonlinearities in activation functions create bad local minima in neural networks. arXiv preprint arXiv:1802.03487. Ziyin, L., Li, B., Simon, J. B., and Ueda, M. (2021). Sgd can converge to local maxima. In International Conference on Learning Representations. Ziyin, L., Lubana, E. S., Ueda, M., and Tanaka, H. (2023). What shapes the loss landscape of self supervised learning? In The Eleventh International Conference on Learning Representations. Ziyin, L. and Ueda, M. (2022). Exact phase transitions in deep learning. arXiv preprint arXiv:2205.12510. Ziyin, L. and Wang, Z. (2023). spred: Solving L1 Penalty with SGD. In International Conference on Machine Learning. Ziyin, L., Zhang, H., Meng, X., Lu, Y., Xing, E., and Ueda, M. (2022). Stochastic neural networks with infinite width are deterministic. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] See Appendix. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] The experiments are only for demonstration and are straightforward to reproduce following the theory. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [No] The fluctuations are visually negligible. 12 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] They are done on a single 3080Ti GPU. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 13 A Experimental Details For the experiment in Figure 3, the input data consists of 1000 data points sampled from a multivariate Gaussian distribution: x ∼ N (0, I5). The target is generated by a linear transformation y = v ⋅ x, where the norm of v is rescaled to obtain different values of ∣∣E[xy]∣∣ as the control parameter of the simulation. The models are with D = 2 neural networks with bias terms and with hidden width 32 for both hidden layers. The training proceeds with gradient descent with a learning rate of 0.1 for 104 iterations when the training loss has stopped decreasing for all the experiments. For the CIFAR10 experiments, we train a standard ResNet18 with roughly 107 parameters under the standard procedure, with a batch size of 256 for 100 epochs.10 For the linear models, we use a hidden width of 32 without any bias term. The training proceeds with SGD with batch size 256 for 100 epochs with a momentum of 0.9. The learning rate is 0.002, chosen as the best learning rate from a grid search over [0.001, 0.002, ..., 0.01]. B Proofs B.1 Proof of Lemma 1 Proof. Note that the first term in the loss function is invariant to the following rescaling for any a > 0: { Ui → aUi; Wij → Wij/a; (21) meanwhile, the L2 regularization term changes as a changes. Therefore, the global minimum must have a minimized a with respect to any U and W . One can easily find the solution: a ∗ = arg min a ⎛ ⎝γua 2U 2 i + γw ∑ j W 2 ij a2 ⎞ ⎠ = ( γw ∑j W 2 ij γuU 2 i ) 1/4 . (22) Therefore, at the global minimum, we must have γua 2U 2 i = γw ∑j W 2 ij a2 , so that (U ∗ i )2 = (a ∗Ui)2 = γw γu ∑ j (W ∗ ij)2, (23) which completes the proof. ◻ B.2 Proof of Lemma 2 Proof. By Lemma 1, we can write Ui as bi and Wi∶ as biwi where wi is a unit vector, and finding the global minimizer of Eq. (2) is equivalent to finding the minimizer of the following objective, Ex,ε ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎛ ⎝ ∑ i,j b2 i ϵiwijxj − y⎞ ⎠ 2⎤ ⎥ ⎥ ⎥ ⎥ ⎦ + (γu + γw)∣∣b∣∣ 2 2, (24) = Ex ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎛ ⎝∑ i,j b 2 i wijxj − y⎞ ⎠ 2⎤ ⎥ ⎥ ⎥ ⎥ ⎦ + σ2 ∑ ij b4 i (∑ k wikxk) 2 + (γu + γw)∣∣b∣∣ 2 2, (25) The lemma statement is equivalent to bi = bj for all i and j. We prove this by contradiction. Suppose there exist i and j such that bi ≠ bj, we can choose i to be the index of bi with maximum b 2 i , and let j be the index of bj with minimum b2 j . Now, we can construct a different solution by the following replacement of biwi∶ and bjwj∶: { b 2 i wi∶ → c2v; b 2 j wj∶ → c2v, (26) 10Specifically, we use the implementation and training procedure of https://github.com/ kuangliu/pytorch-cifar, with standard augmentations such as random crop, etc. 14 where c is a positive scalar and v is a unit vector such that 2c2v = b2 i wi∶ + b2 j wj∶. Note that, by the triangular inequality, 2c2 ≤ b2 i + b2 j . Meanwhile, all the other terms, bk for k ≠ i and k ≠ j, are left unchanged. This transformation leaves the first term in the loss function (25) unchanged, and we now show that it decreases the other terms. The change in the second term is (b2 i ∑ k wikxk) 2 + (b 2 j ∑ k wjkxk) 2 → 2 (c 2 ∑ k vkxk) 2 = 1 2 (b2 i ∑ k wikxk + b2 j ∑ k wjkxk) 2 . (27) By the inequality a2 + b 2 ≥ (a + b) 2/2, we see that the left-hand side is larger than the right-hand side. We now consider the L2 regularization term. The change is (γu + γw)(b2 i + b 2 j ) → 2(γu + γw)c 2, (28) and the left-hand side is again larger than the right-hand side by the inequality mentioned above: 2c2 ≤ b 2 i + b 2 j . Therefore, we have constructed a solution whose loss is strictly smaller than that of the global minimum: a contradiction. Thus, the global minimum must satisfy U 2 i = U 2 j (29) for all i and j. Likewise, we can show that UiWi∶ = UjWj∶ for all i and j. This is because the triangular inequality 2c2 ≤ b2 i + b 2 j is only an equality if UiWi∶ = UjWj∶. If UiWi∶ ≠ UjWj∶, following the same argument above, we arrive at another contradiction. ◻ B.3 Proof of Theorem 1 Proof. By Lemma 2, at any global minimum, we can write U∗ = br for some b ∈ R. We can also write W∗ = rvT for a general vector v ∈ Rd. Without loss of generality, we assume that b > 0 (because the sign of b can be absorbed into r). The original problem in Eq. (2) is now equivalently reduced following problem because r T r = d1: min b,v Ex ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎛ ⎝bd1 ∑ j vjxj − y⎞ ⎠ 2 + b 2d1σ2 (∑ k vkxk) 2⎤ ⎥ ⎥ ⎥ ⎥ ⎦ + γud1b2 + γwd1∣∣v∣∣ 2 2. (30) For any fixed b, the global minimum of v is well known:11 v = bE[xy]T [b2 (σ2 + d1) A0 + γwI]−1 . (31) By Lemma 1, at a global minimum, b also satisfies the following condition: b 2 = γw γu ∣∣v∣∣ 2, (32) One solution to this equation is b = 0, and we are interested in whether solutions with b ≠ 0 exist. If there is no other solution, then b = 0 must be the unique global minimum; otherwise, we need to identify which of the solutions are actual global minima. When b ≠ 0, ∣∣ [b2 (σ2 + d1) A0 + γwI]−1 E[xy]∣∣ 2 = γu γw . (33) Note that the left-hand side is monotonically decreasing in b2, and is equal to γ−2 w ∣∣E[xy]∣∣ 2 when b = 0. When b → ∞, the left-hand side tends to 0. Because the left-hand side is a continuous and monotonic function of b, a unique solution b∗ > 0 that satisfies Eq. (33) exists if and only if γ−2 w ∣∣E[xy]∣∣ 2 > γu/γw, or, ∣∣E[xy]∣∣ 2 > γuγw. (34) 11Namely, it is the solution of a ridge linear regression problem. 15 Therefore, at most, three candidates for global minima of the loss function exist: { b = 0, v = 0 if ∣∣E[xy]∣∣ 2 ≤ γuγw; b = ±b∗, v = b [b2 (σ2 + d1) A0 + γwI]−1 E[xy], if ∣∣E[xy]∣∣ 2 > γuγw, (35) where b∗ > 0. In the second case, one needs to discern the saddle points from the global minima. Using the expression of v, one finds the expression of the loss function as a function of b d1(d1 + σ2)b4 ∑ i E[x′y]2 i ai [b2(σ2 + d1)ai + γw]2 − 2b2d1 ∑ i E[x′y] 2 i b2(σ2 + d1)ai + γw + E[y2] + γud1b2 + γwd1 ∑ i E[x′y]2 i b2 [b2(σ2 + d1)ai + γw]2 , (36) where x′ = Rx such that RA0R−1 is a diagonal matrix. We now show that condition (34) is sufficient to guarantee that 0 is not the global minimum. At b = 0, the first nonvanishing derivative of b is the second-order derivative. The second order derivative at b = 0 is −2d1∣∣E[xy]∣∣ 2/γw + 2γud1, (37) which is negative if and only if ∣∣E[xy]∣∣2 > γuγw. If the second derivative at b = 0 is neg- ative, b = 0 cannot be a minimum. It then follows that for ∣∣E[xy]∣∣2 > γuγw, b = ±b ∗, v = b [b2 (σ2 + d1) A0 + γwI]−1 E[xy] are the two global minimum (because the loss is invari- ant to the sign flip of b). For the same reason, when ∣∣E[xy]∣∣ 2 < γuγw, b = 0 gives the unique global minimum. This finishes the proof. ◻ B.4 Proof of Proposition 1 Proof. We first show that there exists a constant r such that the global minimum must be confined within a (closed) r-Ball around the origin. The objective (9) can be upper-bounded by Eq. (9) ≥ γu∣∣U ∣∣ 2 + D ∑ i=1 γi∣∣W (i)∣∣ 2 ≥ γmin (∣∣U ∣∣ 2 + ∑ i ∣∣W (i)∣∣ 2) , (38) where γmin ∶= mini∈{u,1,2,...,D} > 0. Now, let w denote be the union of all the parameters (U, W (i)) and viewed as a vector. We see that the above inequality is equivalent to Eq. (9) ≥ γmin∣∣w∣∣ 2. (39) Now, note that the loss value at the origin is E[y2], which means that for any w, whose norm ∣∣w∣∣ 2 ≥ E[y2]/γmin, the loss value must be larger than the loss value of the origin. Therefore, let r = E[y2]/γmin, we have proved that the global minimum must lie in a closed r-Ball around the origin. As the last step, because the objective is a continuous function of w and the r-Ball is a compact set, the minimum of the objective in this r-Ball is achievable. This completes the proof. ◻ B.5 Proof of Theorem 2 We divide the proof into the proof of a proposition and a lemma, and combining the following proposition and lemma obtains the theorem statement. B.5.1 Proposition 4 Proposition 4. Any global minimum of Eq. (9) is of the form ⎧⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎩ U = burD; W (i) = birir T i−1; W (1) = r1E[xy]T (bu ∏D i=2 bi)µ [(bu ∏D i=2 bi) 2s 2 (σ2 + d1) A0 + γwI]−1 , (40) where µ = ∏ D i=2 di, s 2 = ∏ D i=2 di(σ2 + di), bu ≥ 0 and bi ≥ 0, and ri = (±1, ..., ±1) is an arbitrary vertex of a di-dimensional hypercube for all i. 16 Proof. Note that the trivial solution is also a special case of this solution with b = 0. We thus focus on deriving the form of the nontrivial solution. We prove by induction on D. The base case with depth 1 is proved in Theorem 1. We now assume that the same holds for depth D − 1 and prove that it also holds for depth D. For any fixed W (1), the loss function can be equivalently written as E˜xEϵ(2),...,ϵ(D) ⎛ ⎝ d1,d2,...dD ∑ i1,i2,...,iD UiD ϵ (D) iD ...ϵ (2) i2 W (2) i2i1 ˜xi1 − y⎞ ⎠ 2 + γu∣∣U ∣∣ 2 + D ∑ i=2 γi∣∣W (i)∣∣ 2 + const., (41) where ˜x = ϵ(1) i1 ∑i W (1) i1i xi. Namely, we have reduced the problem to a problem involving only a depth D − 1 linear net with a transformed input ˜x. By the induction assumption, the global minimum of this problem takes the form of Eq. (10), which means that the loss function can be written in the following form: E˜xEϵ(2),...,ϵ(D) ⎛ ⎝bubD...b3 d1,d2,...dD ∑ i1,i2,...,iD ϵ(D) iD ...ϵ (2) i2 vi1 ˜xi1 − y⎞ ⎠ 2 + L2 reg., (42) for an arbitrary optimizable vector vi1 . The term ∑d2,...dD i2,...,iD ϵ (D) iD ...ϵ (2) i2 ∶= η can now be regarded as a single random variable such that E[η] = ∏D i=2 di ∶= µ and E[η2] = ∏ D i=2 di(σ2 i +di) ∶= s 2. Computing the expectation over all the noises except for ϵ(1), one finds E˜x ⎛ ⎝bubD...b3s ∑ i1 vi1 ˜xi1 − µy s ⎞ ⎠ 2 + L2 reg. + const. (43) = Ex,ϵ(1) ⎛ ⎝bubD...b3s ∑ i,i1 vi1ϵ(1) i1 W (1) i1i xi − µy s ⎞ ⎠ 2 + L2 reg. + const., (44) where we have ignored the constant term because it does not affect the minimizer of the loss. Namely, we have reduced the original problem to a two-layer linear net problem where the label becomes effectively rescaled for a deep network. For any fixed bu, ..., b3, we can define ¯x ∶= bubD...b3sx, and obtain the following problem, whose global minimum we have already derived: E¯xEϵ2,...,ϵD ⎛ ⎝∑ i,i1 vi1W (1) i1i ¯xi − µy s ⎞ ⎠ 2 . (45) By Theorem 1, the global minimum is identically 0 if ∣∣E[µ¯xy/s]∣∣2 < d2γ2γ1, or, E[xy] ≤ γ2γ1 b2 3...b2 u(∏ D i=3 di) . When E[xy] > γ2γ1 b2 3...b2 u(∏ D i=3 di) , the solution can be non-trivial: { v∗ = b∗ 2r1; W∗ = r1E[xy]T µb ∗ 2b3...bu [(b∗ 2) 2d 2 3...d 2 Db 2 us 2 (σ2 + d1) A0 + γ1I] −1 , (46) for some b ∗ 2. This proves the theorem. ◻ B.6 Lemma 3 Lemma 3. At any global minimum of Eq. (9), let b1 ∶= √∣∣Wi∶∣∣2/d and bD+1 ∶= bu, γk+1dk+1b2 k+1 = γkdk−1b2 k. (47) Proof. It is sufficient to show that for all k and i, γk+1 ∑ ij (W k+1 ji )2 = γk ∑ ij (W k ij) 2. (48) 17 We prove by contradiction. Let U ∗, W ∗ be the global minimum of the loss function. Assuming that for an arbitrary k, γk+1 ∑ ij (W ∗,k+1 ji )2 ≠ γk ∑ ij (W ∗,k ij )2. (49) Now, we introduce W a such that W a,k+1 ji = aW ∗,k+1 ji and W a,k ji = W ∗,k ji /a. The loss without regularization is invariant under the transformation of W ∗ → W a, namely L0(W ∗) = L0(W a). (50) In the regularization, all the terms remain invariant except two terms: { γk+1 ∑ij(W ∗,k+1 ji )2 → γk+1 ∑ij(W a,k+1 ji )2 = a 2γk+1 ∑ij(W ∗,k+1 ji )2 γk ∑ij(W ∗,k ij ) 2 → γk ∑ij(W a,k ji )2 = a −2γk ∑ij(W ∗,k ji )2 (51) It could be shown that, the sum of a 2γk+1 ∑ij(W ∗,k+1 ji )2 and a −2γk ∑ij(W ∗,k ji )2 reaches its mini- mum when a 2 = √ γk ∑ij (W ∗,k ji )2 γk+1 ∑ij (W ∗,k+1 ji )2 . If γk+1 ∑ij(W ∗,k+1 ji )2 ≠ γk ∑ij(W ∗,k ij )2, one can choose a to minimize the regularization terms in the loss function such that L(W a) < L(W ∗), indicating W ∗ is not the global minimum. Thus, γk+1 ∑ij(W ∗,k+1 ji )2 ≠ γk ∑ij(W ∗,k ij ) 2 cannot be true. ◻ B.7 Proof of Proposition 2 Proof. Let L0 = E˜xEϵ2,...,ϵD ⎛ ⎝ d1,d2,...dD ∑ i1,i2,...,iD UiD ϵ(D) iD ...ϵ (1) i1 W (1) i1i xi − y⎞ ⎠ 2 . (52) L0 is a polynomial containing 2D + 2th order, D + 1th order, and 0th order terms in terms of parameters U and W . The second order derivative of L is thus a polynomial containing 2D-th order and (D − 1)-th order terms; however, other orders are not possible. For D ≥ 2, there are no constant terms in the Hessian of L, and there is at least a parameter in each of the terms. The Hessian of the full loss function with regularization is ∂2L ∂2UiUj = ∂2L0 ∂2UiUj + (1 − δij)2γu(Ui + Uj) + δij2γu; (53) ∂2L ∂2W (i) jk Ul = ∂2L0 ∂2W (i) jk Ul + 2(γwW (i) jk + γuUl); (54) ∂2L ∂2W (i) jk W (l) mn = ∂2L0 ∂2W (i) jk W (l) mn + (1 − δilδjmδkn)2γw(W (i) jk + W (l) mn) + δilδjmδkn2γw. (55) For U = 0, W = 0, the Hessian of L0 is 0, since each term in L0 contains at least a U or a W . The Hessian of L becomes ∂2L ∂2UiUj ∣ U,W =0 = δij2γu; (56) ∂2L ∂2W (i) jk Ul RRRRRRRRRRRRU,W =0 = 0; (57) ∂2L ∂2W (i) jk W (l) mn RRRRRRRRRRRRU,W =0 = δilδjmδkn2γw. (58) The Hessian of L is a positive-definite matrix. Thus, U = 0, W = 0 is always a local minimum of the loss function L. ◻ 18 B.8 Proof of Proposition 3 We first apply Lemma 3 to determine the condition for the nontrivial solution to exist. In particular, the Lemma must hold for W (2) and W (1), which leads to the following condition: ∣∣b D−1dD−1 0 [b2DdD 0 (σ2 + d0)DA0 + γ]−1E[xy]∣∣ 2 = 1. (59) Note that the left-hand side is a continuous function that tends to 0 as b → ∞. Therefore, it is sufficient to find the condition that guarantees that there exists b such that the l.h.s. is larger than 1. For any b, the l.h.s. is a monotonically decreasing function of any eigenvalue of A0, and so the following two inequalities hold: { ∣∣b D−1dD−1 0 (b2Dd D 0 (σ2 + d0)Dσ2 x + γ)−1E[xy]∣∣ ≤ ∣∣bD−1dD−1 0 (b 2Dd D 0 (σ2 + d0)Damin + γ)−1E[xy]∣∣ ∣∣b D−1dD−1 0 (b2Dd D 0 (σ2 + d0)Dσ2 x + γ)−1E[xy]∣∣ ≥ ∣∣bD−1dD−1 0 (b 2DdD 0 (σ2 + d0) Damax + γ)−1E[xy]∣∣. (60) The second inequality implies that if ∣∣bD−1d D−1 0 [b2Dd D 0 (σ2 + d0)Damax + γ] −1E[xy]∣∣ > 1, (61) a nontrivial solution must exist. This condition is equivalent to the existence of a b such that dD 0 (σ2 + d0)Damaxb2D − ∣∣E[xy]∣∣bD−1d D−1 0 < −γ, (62) which is a polynomial inequality that does not admit an explicit condition for b for a general D. Since the l.h.s is a continuous function that increases to infinity as b → ∞, one sufficient condition for (62) to hold is that the minimizer of the l.h.s. is smaller than γ. Note that the left-hand side of Eq. (62) diverges to ∞ as b → ±∞ and tends to zero as b → 0. Moreover, Eq. (62) is lower-bounded and must have a nontrivial minimizer for some b > 0 because the coefficient of the bD−1 term is strictly negative. One can thus find its minimizer by taking derivative. In particular, the left-hand side is minimized when b D+1 = (D − 1)∣∣E[xy]∣∣ 2Dd0(σ2 + d0)Damax , (63) and we can obtain the following sufficient condition for (62) to be satisfiable, which, in turn, implies that (59) is satisfiable: D + 1 2D ∣∣E[xy]∣∣dD−1 0 ( (D − 1)∣∣E[xy]∣∣ 2Dd0(σ2 + d0)Damax ) D−1 D+1 > γ, (64) which is identical to the proposition statement in (15). Now, we come back to condition (60) to derive a sufficient condition for the trivial solution to be the only solution. The first inequality in Condition (60) implies that if ∣∣b D−1dD−1 0 [b 2DdD 0 (σ2 + d0)Damin + γ] −1E[xy]∣∣ ≤ 1, (65) the only possible solution is the trivial one, and the condition for this to hold can be found using the same procedure as above to be D + 1 2D ∣∣E[xy]∣∣dD−1 0 ( (D − 1)∣∣E[xy]∣∣ 2Dd0(σ2 + d0)Damin ) D−1 D+1 ≤ γ, (66) which is identical to (14). We now prove the upper bound in (16). Because for any b, the first condition in (60) gives an upper bound, and so any b that makes the upper bound less than 1 cannot be a solution. This means that any b for which ∣∣bD−1d D−1 0 [b 2Dd D 0 (σ2 + d0)Damin + γ]−1E[xy]∣∣ ≤ 1 (67) cannot be a solution. This condition holds if and only if d D 0 (σ2 + d0)Daminb 2D − ∣∣E[xy]∣∣bD−1d D−1 0 > −γ. (68) Because γ > 0, one sufficient condition to ensure this is that there exists b such that d0(σ2 + d0)Daminb2D − ∣∣E[xy]∣∣bD−1 > 0, (69) 19 which is equivalent to b > [ ∣∣E[xy]∣∣ d0(σ2 + d0)Damin ] 1 D+1 . (70) Namely, any solution b∗ satisfies b∗ ≤ [ ∣∣E[xy]∣∣ d0(σ2 + d0)Damin ] 1 D+1 . (71) We can also find a lower bound for all possible solutions. When D > 1, another sufficient condition for Eq. (68) to hold is that there exists b such that ∣∣E[xy]∣∣d D−1 0 bD−1 < γ. (72) because the b2D term is always positive. This condition then implies that any solution must satisfy: b∗ ≥ 1 d0 [ γ ∣∣E[xy]∣∣ ] 1 D−1 . (73) For D = 1, we have by Theorem 1 that b∗ > 0 (74) if and only if E[xy] > γ. This means that b ∗ ≥ lim η→0+ lim D→1+ 1 d0 [ γ + η ∣∣E[xy]∣∣ ] 1 D−1 = { ∞ if E[xy] ≥ γ; 0 if E[xy] < γ. . (75) This finishes the proof. ◻ B.9 Proof of Theorem 3 Proof. When nontrivial solutions exist, we are interested in identifying when b = 0 is not the global minimum. To achieve this, we compare the loss of b = 0 with the other solutions. Plug the trivial solution into the loss function in Eq. (9), the loss is easily identified to be Ltrivial = E[y2]. For the nontrivial minimum, defining f to be the model, f (x) ∶= d,d1,d2,...dD ∑ i,i1,i2,...,iD UiD ϵ(D) iD ...ϵ (2) i2 W (2) i2i1ϵ(1) i1 W (1) i1i x (76) = ηd D 0 b 2DE[xy]T [b2Dd D 0 (σ2 + d0)DA0 + γI] −1x, (77) where, similar to the previous proof, we have defined ∑d1,...dD i1,...,iD ϵ(D) iD ...ϵ (1) i1 ∶= η such that E[η] = ∏ D i di = dD 0 and E[η2] = ∏ D i di(σ2 i + di) ∶= d D 0 (σ2 + d0)D. With this notation, The loss function becomes ExEη(f (x) − y)2 + L2 reg. (78) = Ex,η[f (x)2] − 2Ex,η[yf (x)] + Ex[y2] + L2 reg. (79) = ∑ i d 3D 0 (σ2 + d0)Db 4DaiE[x′y]2 i [dD 0 (σ2 + d0)Daib2D + γ]2 − 2 ∑ i d 2D 0 b2DE[x′y] 2 i dD 0 (σ2 + d0)Daib2D + γ + Ex[y2] + L2 reg. (80) The last equation is obtained by rotating x using a orthogonal matrix such that R−1A0R = diag(ai) and denoting the rotated x as x′ = Rx. With x′, The L2 reg term takes the form of L2 reg. = γDd2 0b2 + γ ∑ i d2D 0 b2D∣∣E[x′y]i∣∣ 2 (dD 0 (σ2 + d0)Db2Dai + γ)2 . (81) Combining the expressions of (81) and (80), we obtain that the difference between the loss at the non-trivial solution and the loss at 0 is − ∑ i d2D 0 b 2DE[x′y] 2 i [dD 0 (σ2 + d0)Daib2D + γ] + γDd2 0b2. (82) 20 Satisfaction of the following relation thus guarantees that the global minimum is nontrivial: ∑ i d 2D 0 b2DE[x′y]2 i [dD 0 (σ2 + d0)Daib2D + γ] ≥ γDd2 0b 2. (83) This relation is satisfied if d 2D 0 b2D∣∣E[xy]∣∣ 2 [dD 0 (σ2 + d0)Damaxb2D + γ] ≥ γDd2 0b2 (84) b2D−2 [dD 0 (σ2 + d0)Damaxb2D + γ] ≥ γD d2D−2 0 ∣∣E[xy]∣∣2 . (85) The derivative or l.h.s. with respect to b is b 2D−3[(2D − 2)γ − 2dD 0 (σ2 + d0) Damaxd2D] [dD 0 (σ2 + d0)Damaxb2D + γ]2 . (86) For b, γ ∈ (0, ∞), the derivative dives below 0, indicating the l.h.s. of (85) has a global maximum at a strictly positive b. The value of b is found when setting the derivative to 0, namely b2D−3[(2D − 2)γ − 2d D 0 (σ2 + d0)Damaxd 2D] [dD 0 (σ2 + d0)Damaxb2D + γ]2 = 0 (87) (2D − 2)γ − 2d D 0 (σ2 + d0)Damaxd 2D = 0 (88) b 2D = (D − 1)γ dD 0 (σ2 + d0)Damax . (89) The maximum value then takes the form (D − 1) D−1 D Dγ 1 D dD−1 0 (σ2 + d0)D−1a D−1 D max . (90) The following condition thus guarantees that the global minimum is non-trivial (D − 1) D−1 D Dγ 1 D dD−1 0 (σ2 + d0)D−1a D−1 D max ≥ γD d2D−2 0 ∣∣E[xy]∣∣2 (91) ∣∣E[xy]∣∣ 2 ≥ γ D+1 D D2(σ2 + d0)D−1a D−1 D max dD−1 0 (D − 1) D−1 D . (92) This finishes the proof. ◻ B.10 Proof of Theorem 4 Proof. The model prediction is: f (x) ∶= d,d1,d2,...dD ∑ i,i1,i2,...,iD UiD ϵ(D) iD ...ϵ (2) i2 W (2) i2i1ϵ(1) i1 W (1) i1i x (93) = ηd D 0 b 2DE[xy] T [b2Dd D 0 (σ2 + d0)Dσ2 xI + γI]−1x. (94) One can find the expectation value and variance of a model prediction: Eη[f (x)] = d2D 0 b 2DE[xy]T x b2DdD 0 (σ2 + d0)Dσ2 x + γ (95) For the trivial solution, the theorem is trivially true. We thus focus on the case when the global minimum is nontrivial. 21 The variance of the model is Var[f (x)] = E[f (x)2] − E[f (x)]2 (96) = (σ2 + d0)Dd 3D 0 b4D(E[xy]T x)2 [b2DdD 0 (σ2 + d0)Dσ2 x + γ]2 − d4D 0 b4D(E[xy] T x)2 [b2DdD 0 (σ2 + d0)D]2σ2 x + γ]2 (97) = d 3D 0 [(σ2 + d0)D − d D 0 ]b4D(E[xy] T x)2 [b2DdD 0 (σ2 + d0)Dσ2 x + γ]2 (98) = d 3D 0 [(σ2 + d0)D − d D 0 ]b2D+2(E[xy] T x)2 ∣∣E[xy]∣∣2 , (99) where the last equation follows from Eq. (13). The variance can be upper-bounded by applying (16), Var[f (x)] ≤ dD 0 [(σ2 + d0)D − d D 0 ](E[xy]T x)2 (σ2 + d0)2Dσ2 x ∝ dD 0 [(σ2 + d0)D − d D 0 ] (σ2 + d0)2D . (100) We first consider the limit d0 → ∞ with fixed σ2: Var[f (x)] ∝ Dd 2D−1 0 σ2 (d0 + σ2)2D = O ( 1 d0 ) . (101) For the limit σ2 → ∞ with d0 fixed, we have Var[f (x)] = O ( 1 (σ2)D ) . (102) Additionally, we can consider the limit when D → ∞ as we fix both σ2 and d0: Var[f (x)] = O (e−D2 log[(σ2+d0)/d0]) , (103) which is an exponential decay. ◻ C Exact Form of b∗ for D = 1 Note that our main result does not specify the exact value of b ∗. This is because b ∗ must satisfy the condition in Eq. (6), which is equivalent to a high-order polynomial in b with coefficients being general functions of the eigenvalues of A0, whose solutions are generally not analytical by Galois theory. One special case where an analytical formula exists for b is when A0 = σ2 xI. Practically, this can be achieved for any (full-rank) dataset if we disentangle and rescale the data by the whitening transformation: x → σx√A−1 0 x. In this case, we have b2 ∗ = √ γw γu ∣∣E[xy]∣∣ − γw (σ2 + d1)σ2 x , (104) and v = ± ¿ Á Á ÁÀ √ γu γw ∣∣E[xy]∣∣ − γu σ2 x(σ2 + d1) E[xy] ∣∣E[xy]∣∣ , (105) where v = Wi∶. D Effect of Bias This section studies a deep linear network with biases for every layer and compares it with the no-bias networks. We first study a general case when the data does not receive any preprocessing. We then show that the problem reduces to the setting we considered in the main text under the common data preprocessing schemes that centers the input and output data: E[x] = 0, and E[y] = 0. 22 D.1 Two-layer network The two-layer linear network with bias is defined as fb(x; U, W, βU , βW ) = ∑ i ϵiUi(Wi∶ ⋅ x + βW i ) + βU , (106) where βW ∈ Rd1 is the bias in the hidden layer, and βU ∈ R is the bias at the output layer. The loss function is Lb(U, W, βU , βW ) =Eϵ,x,y [(∑ i ϵiUi(Wi∶ ⋅ x + βW i ) + βU − y) 2] + L2 (107) =Ex,y [(U W x + U βW + βU − y)2 + σ2 ∑ i U 2 i (Wi∶ ⋅ x + βW i )2] + γu(∣∣U ∣∣ 2 + (βU )2) + γw(∣∣W ∣∣ 2 + ∣∣βW ∣∣ 2). (108) It is helpful to concatenate x and 1 into a single vector x′ ∶= (x, 1)T and concatenate W and βW into a single matrix W ′ such that W , βW , x, and W ′, x′ are related via the following equation W x + βW = W ′x′. (109) Using W ′ and x′, the model can be written as fb(x′, U, W ′, βU ) = ∑ i ϵiUiW ′ i∶ ⋅ x′ + βU . (110) The loss function simplifies to Lb(U, W ′, β) = Eϵ,x,y[(∑ i ϵiUiW ′ i∶ ⋅ x′ + βU − y)2] + γu(∣∣U ∣∣ 2 + (βU ) 2) + γw∣∣W ′∣∣ 2. (111) Note that (111) contains similar rescaling invariance between U and W ′ and the invariance of aligning W ′ i∶ and W ′ j∶. One can thus obtain the following two propositions that mirror Lemma 1 and 2. Proposition 5. At the global minimum of (107), U 2 j = γw γu (∑i W 2 ji + (βW j )2). Proposition 6. At the global minimum, for all i and j, we have ⎧⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎩ U 2 i = U 2 j ; UiWi∶ = UjWj∶; UiβW i = UjβW j . (112) The proofs are omitted because they are the same as those of Lemma 1 and 2, substituting W by W ′. Following a procedure similar to finding the solution for a no-bias network, one can prove the following theorem. Theorem 5. The global minimum of Eq. (107) is of the form ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ U = br; βU = d1b[(d1+σ2) γu γw b−1]vE[x]−(d1 γu γw b 2−1)E[y] (d1+σ2)d1 γ2 u γ2 w b4+(γu−2)d1 γu γw b2+γu+1 ; W = rb {E[x] [b γu γw (d1 + bσ2) − 1] βU + E[xy]} T [b2(d1 + σ2)A0 + γwI]−1; βW = −r γu γw bβU , (113) where b satisfies γub2 = b2 γw( E[y]S1S3 S4 E[x] − E[xy])(M −1)2( E[y]S1S3 S4 E[x] − E[xy])T + γ2 u γw ( S3 S4 E[y] − b S2 S4 E[x]M −1E[xy]) 2 (b S2S1 S4 E[x]M −1E[x]T − 1)2 , (114) where M, S1, S2, S3, S4 are functions of the model parameters and b, defined in Eq. (120). 23 Proof. First of all, we derive a handy relation satisfied by βU and βW at all the stationary points. The zero-gradient condition of the stationary points gives { Ex,y[2(U W x + U βW + βU − y)]U + 2γwβW = 0; Ex,y[2(U W x + U βW + βU − y)] + 2γuβU = 0, (115) leading to U γuβU + γwβW = 0 (116) βW i = − γu γw UiβU . (117) Proposition 5 and proposition 6 implies that we can define b ∶= ∣Ui∣ and bv ∶= UiWi∶. Consequently, UiβW i = − γu γw b 2βU , and the loss function can be written as Ex ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ⎛ ⎝ bd1 ∑ j vjxj − (d1 γu γw b2 − 1)βU − y⎞ ⎠ 2 + b 2d1σ2 (∑ k vkxk − γu γw bβU ) 2⎤ ⎥ ⎥ ⎥ ⎥ ⎦ + γud1b2 +γwd1∣∣v∣∣ 2 2 + γu ( b2d1γu γw + 1) (βU )2. (118) The respective zero-gradient condition for v and βU implies that for all stationary points, ⎧⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎩ v = [b2(d1 + σ2)A0 + γwI]−1b {E[x] [b γu γw (d1 + bσ2) − 1] βU + E[xy]} ; βU = d1b[(d1+σ2) γu γw b−1]vE[x]−(d1 γu γw b 2−1)E[y] (d1+σ2)d1 γ2 u γ2 w b4+(γu−2)d1 γu γw b2+γu+1 . (119) To shorten the expressions, we introduce ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ M = b 2(d1 + σ2)A0 + γwI; S1 = b γu γw (d1 + bσ2) − 1; S2 = d1b [(d1 + σ2) γu γw b − 1] ; S3 = d1 γu γw b 2 − 1; S4 = (d1 + σ2)d1 γ2 u γ2 w b 4 + (γu − 2)d1 γu γw b2 + γu + 1. (120) With M, S1, S2, S3, S4, we have { v = M −1b(E[x]S1βU + E[xy]); βU = S2vE[x]−S3E[y] S4 . (121) The inner product of v and E[x] can be solved as vE[x] = b S3 S4 E[x]M −1E[x]S1E[y] − E[x]M −1E[xy] b S2 S4 E[x]M −1E[x]S1 − 1 . (122) Inserting the expression of vE[x] into the expression of βU one obtains βU = S3E[y] − bS2E[x]M −1E[xy] bE[x]M −1E[x]S1S2 − S4 (123) The global minimum must thus satisfy γub 2 = γw∣∣v∣∣ 2 + γu b2d1γu γw (βU )2 (124) = b2 γw( E[y]S1S3 S4 E[x] − E[xy])(M −1)2( E[y]S1S3 S4 E[x] − E[xy])T + γ2 u γw ( S3 S4 E[y] − b S2 S4 E[x]M −1E[xy]) 2 (b S2S1 S4 E[x]M −1E[x]T − 1)2 . (125) This completes the proof. ◻ 24 Remark. As in the no-bias case, we have reduced the original problem to a one-dimensional problem. However, the condition for b becomes so complicated that it is almost impossible to understand. That being said, the numerical simulations we have done all carry the bias terms, suggesting that even with the bias term, the mechanisms are qualitatively similar, and so the approach in the main text is justified. When E[x] = 0, the solution can be simplified a little: ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ U = rb; βU = − d1 γu γw b2−1 (d1+σ2)d1 γ2 u γ2 w b4+(γu−2)d1 γu γw b2+γu+1 E[y]; W = rbE[xy]T [b2(d1 + σ2)A0 + γwI]−1; βW = r γu γw b d1 γu γw b2−1 (d1+σ2)d1 γ2 u γ2 w b4+(γu−2)d1 γu γw b2+γu+1 E[y], (126) where the value of b is either 0 or determined by γu = γw∣E[xy] T [b 2(d1+σ2)A0+γwI]−1∣ 2+ γ2 u γw E[y]2 ⎛ ⎜ ⎝ d1 γu γw b 2 − 1 (d1 + σ2)d1 γ2 u γ2 w b4 + (γu − 2)d1 γu γw b2 + γu + 1 ⎞ ⎟ ⎠ 2 . (127) In this case, the expression of W is identical to the no-bias model. The bias of both layers is proportional to E[y]. The equation determining the value of b is also similar to the no-bias case. The only difference is the term proportional to E[y]2. Lastly, the solution becomes significantly simplified when both E[x] = 0 and E[y] = 0. When this is the case, the solution reverts to the case when there is no bias. In practice, it is a common and usually recommended practice to subtract the average of x and y from the data and achieve precisely E[x] = 0 and E[y] = 0. We generalize this result to deeper networks in the next section. D.2 Deep linear network Let β be a (∑D i di + 1)-dimensional vector concatenating all β(1), β(2), ..., β(D), βU , and denoting the collection of all the weights U , W (D), ..., W (1) by w, the model of a deep linear network with bias is defined as fb(x, W (D), ..., W (1), U, β(D), ..., β(1), βU ) (128) =U (ϵ(D) ○ (W (D)(...(ϵ(2) ○ (W (2)(ϵ (1) ○ (W (1)x + β(1))) + β(2)))...) + βD)) + βU (129) =U (ϵ(D) ○ (W (D)(...(ϵ(2) ○ (W (2)(ϵ(1) ○ (W (1)x))))))) + U (ϵ(D) ○ (W (D)(...(ϵ(2) ○ (W (2)(ϵ (1) ○ β(1))))))) + U (ϵ(D) ○ (W (D)(...(ϵ(2) ○ β(2))))) + ... + U (ϵ(D) ○ β(D)) + βU (130) =U (ϵ (D) ○ (W (D)(...(ϵ(2) ○ (W (2)(ϵ(1) ○ (W (1)x))))))) + bias(w, β), (131) where bias(w, β) =U (ϵ (D) ○ (W (D)(...(ϵ(2) ○ (W (2)(ϵ(1) ○ β(1))))))) + U (ϵ(D) ○ (W (D)(...(ϵ(2) ○ β(2))))) + ... + U (ϵ(D) ○ β(D)) + βU , (132) and ○ denotes Hadamard product. The loss function is Lb(x, y, w, β) = Eϵ,x,y[(fb(x, w, β) − y)2] + L2(w, β). (133) Proposition 5 and Proposition 6 can be generated to deep linear network. Similar to the no-bias case, we can reduce the landscape to a 1-dimensional problem by performing induction on D and using the 2-dimensional case as the base step. However, we do not solve this case explicitly here because the involved expressions now become too long and complicated even to write down, nor can they directly offer too much insight. We thus only focus on the case when the data has been properly preprocessed. Namely, E[x] = 0 and E[y] = 0. For simplicity, we assume that the regularization strength for all the layers are identically γ. The following theorem shows that When E[x] = 0 and E[y] = 0, the biases vanish for an arbitrarily deep linear network: 25 Theorem 6. Let E[x] = 0 and E[y] = 0. The global minima of Eq. (133) have β(1) = 0, β(2) = 0, ..., β(D) = 0, βU = 0. Proof. At the global minimum, the gradient of the loss function vanishes. In particular, the derivatives with respect to β vanish: ∂Lb(x, y, w, β) ∂βi = 0; (134) Eϵ,x,y [ ∂fb(x, w, β) ∂βi (fb(x, w, β) − y)] + γβi = 0; (135) Eϵ,x,y [ ∂bias(w, β) ∂βi (fb(x, w, β) − y)] + γβi = 0; (136) Eϵ [ ∂bias(w, β) ∂βi (fb(E[x], w, β) − E[y])] + γβi = 0, (137) where βi is the ith element of β. The last equation is obtained since fb(x, w, β) is a linear function of x. Using the condition E[x] = 0 and E[y] = 0, Equation (137) becomes Eϵ [ ∂bias(w, β) ∂βi bias(w, β)] + γβi = 0. (138) bias(w, β) is a linear combination of βi. Consequently, ∂bias(w, β)/∂βi does not depend on β, and bias(w, β)∂bias(w, β)/∂βi is a linear combination of βi. Furthermore, the linearity of bias(w, β) indicates that β ⋅ ∂bias(w, β)/∂β = bias(w, β). The solution to the equation (138) also takes the form of β∗ = arg min β (∣∣bias(w, β)∣∣2 + γ∣∣β∣∣ 2). (139) The only choice of β that minimizes both the first term and the second term in (139) is, regardless of the value of w, β = 0. (140) This finishes the proof. ◻ Thus, for a deep linear network, a model without bias is good enough to describe data satisfying E[x] = 0 and E[y] = 0, which could be achieved by subtracting the mean of the data. 26","libVersion":"0.3.2","langs":""}
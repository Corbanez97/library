{"path":"Books and Papers/Eletrodynamics/Jakob Schwichtenberg - No-Nonsense Electrodynamics_ A Student Friendly Introduction-No-nonsense Books.pdf","text":"N AT U R E A L W AY S C R E AT E S T H E B E S T O F A L L O P T I O N S . A R I S T O T L E O N E S C I E N T I F I C E P O C H E N D E D A N D A N O T H E R B E G A N W I T H J A M E S C L E R K M A X W E L L . A L B E R T E I N S T E I N T H E T R U T H A L W AY S T U R N S O U T T O B E S I M P L E R T H A N Y O U T H O U G H T. R I C H A R D F E Y N M A N J A K O B S C H W I C H T E N B E R G N O - N O N S E N S E E L E C T R O D Y N A M I C S N O - N O N S E N S E B O O K S First printing, September 2020 Copyright © 2020 Jakob Schwichtenberg All rights reserved. No part of this publication may be reproduced, stored in, or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photo- copying, recording, or otherwise) without prior written permission. book edition: 1.6 Dedicated to my parents Preface There are already dozens of textbooks on electrodynamics. So why another one? First of all, almost all existing textbooks follow the same \"well- established route\" which is inspired by the historical develop- ments. The main idea is to start by discussing lots of experimen- tal facts and only afterwards put these puzzle pieces together. This way the reader is slowly guided towards Maxwell’s general theory of electrodynamics.1 1 In this sense, this is a bottom-up approach. However, I don’t think this approach is the most effective and certainly not the most entertaining one. Without seeing the bigger picture, discussing individual electric and magnetic phenomena can be quite confusing and makes electrodynamics appear more complicated than it really is. Moreover, readers are often left in the dark regarding which statements hold in general and which only in special situations. In addition, books which follow such a bottom-up approach usually spend most of the time at the \"bottom\" and only near the very end get to the \"top\". This often means that the general theory is not discussed in detail and many questions remain unanswered. In contrast, this book follows a top-down approach. We start by discussing the general theory and only afterwards discuss various special cases. All fundamental aspects of electrody- namics are described in the ﬁrst chapter and put into context. This allows the reader to see the bigger picture right away and 8 immediately makes clear which statements hold in general. In addition, there is no risk that students give up before they get to the most important stuff. Secondly, most other electrodynamics textbooks try to do a lot at once. For example, it’s not uncommon that in addition to Maxwell’s general theory, dozens of applications, edge cases, advanced topics, historical developments or even biographies of the most important contributors are discussed. I think this is problematic because, as the saying goes, if you try to be good at everything, you will not be great at anything. In contrast, this book focuses solely on the fundamental aspects of electrodynamics.2 This narrow focus allows us to discuss all 2 Applications are only discussed insofar as they help to deepen our understanding of the fundamental concepts and not as an end in themselves. In addition, there are already dozens of great books which discuss applications or other special topics in great detail. Some of the best ones are recommended in Chapter 9. the important concepts several times from various perspectives. A clear advantage of this approach is that the reader has mul- tiple chances to understand a given concept while in a normal textbook, the reader immediately has a problem when a passage is not understood perfectly.3 A second advantage of our narrow 3 In a normal textbook each topic is only introduced once. As a result, later chapters become harder and harder to understand without a full understanding of all previous chapters. Moreover, it’s easy to become discouraged when a few passages are not perfectly clear since you know that you need the knowledge to understand later chapters. focus is that it minimizes the risk of unnecessarily confusing the reader. Like all other fundamental theories, electrodynamics is, at its heart, quite simple. However, using it to describe com- plicated systems is far from easy and this is where most of the difﬁculties usually arise.4 Therefore, restricting ourselves to the 4 Most of the difﬁculties are really mathematics problems not physics problems anyway. For example, solving a difﬁcult integral or solv- ing a given differential equation. fundamentals allows us to introduce electrodynamics as gently as possible.5 5 While advanced applications are, of course, important, they are not essential to understand the fundamentals of electrodynamics. As already mentioned above, there are great books which focus on speciﬁc applications. After you’ve developed a solid understanding of the fundamentals, it’s far easier to learn more about those applications you’re interested in. While this alone may already justify the publication of another electrodynamics textbook, there are a few other things which make this book different: ◃ First of all, it wasn’t written by a professor. So this book is by no means an authoritative reference. It’s more like a casual conversation with a more experienced student who shares with you everything he wished he had known earlier. I’m convinced that someone who has just recently learned the topic can explain it much better than someone who learned it decades ago. Many textbooks are hard to understand, not because the subject is difﬁcult, but because the author can’t remember what it’s like to be a beginner. 9 ◃ Another aspect that makes this book unique is that it con- tains lots of idiosyncratic hand-drawn illustrations. Usually, textbooks include very few pictures since drawing them is either a lot of work or expensive. However, drawing ﬁgures is only a lot of work if you are a perfectionist. The images in this book are not as pretty as the pictures in a typical text- book since I ﬁrmly believe that lots of imperfect illustrations are much better than a few perfect ones. The goal of this book, after all, is that you’ll understand electrodynamics and not that I win prizes for my pretty illustrations. ◃ Moreover, my only goal with this book was to write the most student-friendly electrodynamics textbook and not, for ex- ample, to build my reputation. Too many books are unnec- essarily complicated because if a book is hard to understand it makes the author appear smarter.6 Nothing is assumed to 6 To quote C. Lanczos: \"Many of the scientiﬁc treatises of today are formulated in a half-mystical language, as though to impress the reader with the uncomfortable feeling that he is in the permanent presence of a superman.\" be \"obvious\" or \"easy to see\". Moreover, calculations are done step-by-step and annotated to help you understand faster. So, without any further ado, let’s begin. I hope you enjoy read- ing this book as much as I have enjoyed writing it. Karlsruhe, October 2018 Jakob Schwichtenberg PS: I update the book regularly based on reader feedback. So if you ﬁnd an error, I would appreciate a short email to errors@jakobschwichtenberg.com. Acknowledgments: Special thanks to Michael Havrilla, Tom Harper, Luke Durant, Steve Burk, Alex Huang, Jonathan Hob- son, Vicente Aboites, Xavier Constant, Fabian Waetermans, and Phil Connolly for reporting several typos. Moreover, many thanks are due to Jacob Ayres for his careful proofreading and countless invaluable suggestions. 10 Before we dive in, we need to talk about two things. The ﬁrst one is the following crucial question: Why should you care about Electrodynamics? First of all, electrodynamics correctly describes the behavior of one of only four known fundamental interactions.77 This is true at least on a macro- scopic level. Electromagnetic interactions of elementary par- ticles are described by quantum electrodynamics. ◃ At large (cosmological) scales, gravity is the most important of the four interactions. ◃ At tiny scales, strong and weak interactions are responsible for most of the interesting phenomena. For example, pro- tons are held together by the strong force and atoms decay through weak interactions. ◃ However, in between these two extremes it’s electromagnetic interactions which hold sway. In particular, electrodynamics not only allows us to understand how electricity and magnetism come about, but also what light is and how it travels. In addition, a solid understanding of electrodynamics is essen- tial to understand the theories describing all other fundamental interactions. Moreover, the equations describing electromagnetic interactions of macroscopic objects are exactly the same as the equations governing electromagnetic interactions of elementary particles.8 This means that the knowledge gathered by studying 8 The equations describing the other fundamental interactions are extremely similar and can be understood much easier once the equations of electrodynamics are understood. electromagnetic interactions on a macroscopic level helps us im- mediately to understand what happens on a more fundamental level. Moreover, electrodynamics is an ideal playground to under- stand several of the most important concepts underlying mod- ern physics in a simpliﬁed setup. In particular, in the context of electrodynamics we can understand: 11 ◃ What gauge symmetry is. ◃ What a gauge ﬁeld is. How we can interpret it geometri- cally and why gauge ﬁelds are responsible for fundamental interactions. ◃ What special relativity is all about. Lastly, electrodynamics is prototypical for what progress in physics means. Before it was developed, even a genius like Leonhard Euler stated: \"The subject I am going to recommend to your attention almost ter- riﬁes me. The variety it presents is immense, and the enumeration of facts serves to confound rather than to inform. The subject I mean is electricity.\" Nowadays, all we have to do to understand the multitude of electric and magnetic phenomena is to study ﬁve short equa- tions and you don’t have to be a genius to master electricity. Formulated differently, electrodynamics is certainly one of the most important scientiﬁc works of all time. This is especially true if we use David Hilbert’s criterion: \"One can measure the importance of a scientiﬁc work by the number of earlier publications rendered superﬂuous by it.\" Before Maxwell developed electrodynamics in its modern form, there were hundreds of publications, each describing a different electric and magnetic phenomena or, for example, properties of light. Nowadays, we only need Maxwell’s equations to describe all of this. The second thing that we need to talk about is the meaning of a few special symbols which we will use in the following chapters. 12 Notation ◃ Three dots in front of an equation ∴ mean \"therefore\", i.e., that this line follows directly from the previous one: ω = E ¯h ∴ E = ¯hω . This helps to make it clear that we are not dealing with a system of equations. ◃ Three horizontal lines ≡ indicate that we are dealing with a deﬁnition. ◃ The symbol ! = means \"has to be\", i.e. indicates that we are dealing with a condition. ◃ The most important equations, statements and results are highlighted like this: ∇2⃗E = µ0ϵ0 ∂2 ∂t2 ⃗E ◃ We often use the shorthand notation ∂i ≡ ∂ ∂i for the partial derivative where i ∈ {x, y, z}. ◃ We write ⃗x as the argument of a function f (⃗x) as a shorthand notation for f (⃗x) = f (x, y, z). Moreover, often the explicit argument is suppressed altogether to unclutter the notation. ◃ The symbol ∮ is used for integrals over closed surfaces and closed paths. ◃ We denote the components of a vector ⃗v by vi. ◃ Whenever the word \"charge\" is mentioned without further context, we always mean electric charge.99 In a more general context, it could also mean weak charge, color charge etc. ◃ If an index occurs twice, a sum is implicitly assumed: 3 ∑ i=1 aibi = aibi = a1b1 + a2b2 + a3b3, 13 but 3 ∑ i=1 aibj = a1bj + a2bj + a3bj ̸= aibj. This is known as Einstein’s summation convention. ◃ Greek indices like µ, ν or σ, are always summed from 0 to 3: xµyµ = 3 ∑ µ=0 xµyµ. ◃ In contrast, Roman indices like i, j, k are always summed from 1 to 3: xixi ≡ 3 ∑ i=1 xixi. ◃ Basis vectors are always denoted by ⃗ei. For example, we use ⃗ex,⃗ey,⃗ez for the basis vector in a Cartesian coordinate system and ⃗er,⃗eϕ,⃗eθ for the basis vectors in a spherical coordinate system. ◃ δij denotes the Kronecker delta, which is deﬁned as follows: δij =   1 if i = j 0 if i ̸= j ◃ ϵijk denotes the three dimensional Levi-Civita symbol: ϵijk =    1 if (i, j, k) = {(1, 2, 3), (2, 3, 1), (3, 1, 2)} 0 if i = j or j = k or k = i −1 if (i, j, k) = {(1, 3, 2), (3, 2, 1), (2, 1, 3)} That’s it. We are ready to dive in. (After a short look at the table of contents). Contents 1 Birds-Eye View of Electrodynamics 19 Part I What Everybody Ought to Know About Elec- trodynamics 2 Fundamental Concepts 31 2.1 Electric charge . . . . . . . . . . . . . . . . . . . . . . 32 2.2 Charge density . . . . . . . . . . . . . . . . . . . . . 35 2.3 The electric current . . . . . . . . . . . . . . . . . . . 37 2.4 The electromagnetic ﬁeld . . . . . . . . . . . . . . . 42 2.5 The electromagnetic potential . . . . . . . . . . . . . 48 2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 50 3 Fundamental Equations 53 3.1 The continuity equation . . . . . . . . . . . . . . . . 54 3.2 The Lorentz force law . . . . . . . . . . . . . . . . . 57 3.3 Gauss’s law for the electric ﬁeld . . . . . . . . . . . 61 3.4 Gauss’s law for the magnetic ﬁeld . . . . . . . . . . 69 3.5 Faraday’s law . . . . . . . . . . . . . . . . . . . . . . 74 3.6 The Ampere-Maxwell law . . . . . . . . . . . . . . . 79 3.7 The wave equations . . . . . . . . . . . . . . . . . . . 82 3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 84 Part II Essential Electrodynamical Systems and Tools 4 Electrostatics and Magnetostatics 95 4.1 Electrostatic Systems . . . . . . . . . . . . . . . . . . 101 16 4.1.1 Electric ﬁeld of a single point charge . . . . 101 4.1.2 Electric ﬁeld of a sphere . . . . . . . . . . . . 106 4.1.3 Electric ﬁeld of an electric dipole . . . . . . . 110 4.1.4 Electric ﬁeld of more complicated charge dis- tributions . . . . . . . . . . . . . . . . . . . . 111 4.1.5 Charged object in a static electric ﬁeld . . . . 114 4.1.6 Further Systems . . . . . . . . . . . . . . . . . 117 4.2 Magnetostatic Systems . . . . . . . . . . . . . . . . . 123 4.2.1 Magnetic ﬁeld of a wire . . . . . . . . . . . . 123 4.2.2 Charged object in a static magnetic ﬁeld . . 125 4.2.3 Further Systems . . . . . . . . . . . . . . . . . 130 5 Electrodynamics 133 5.1 An explicit solution of the wave equation . . . . . . 134 5.2 Corresponding solution of the magnetic wave equa- tion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 5.3 General solutions of the wave equations . . . . . . . 138 5.4 Basic properties of electromagnetic waves . . . . . . 143 5.5 Advanced properties of electromagnetic waves . . . 149 Part III Get an Understanding of Electrodynamics You Can Be Proud Of 6 Special Relativity 163 6.1 The origin of Maxwell’s equations . . . . . . . . . . 169 7 Gauge Symmetry 175 8 Electrodynamics as a Gauge Theory 181 8.1 Symmetries intuitively . . . . . . . . . . . . . . . . . 182 8.1.1 Global vs. local symmetries . . . . . . . . . . 183 8.2 A toy gauge theory . . . . . . . . . . . . . . . . . . . 184 8.2.1 Active vs. passive transformations . . . . . . 186 8.2.2 Symmetries vs. redundancies . . . . . . . . . 188 8.3 Gauge dynamics . . . . . . . . . . . . . . . . . . . . 190 8.3.1 Mathematical description of the toy model . 194 8.3.2 Gauge rules . . . . . . . . . . . . . . . . . . . 200 8.4 Gauge symmetry in physics . . . . . . . . . . . . . . 203 8.4.1 Gauge symmetry in Quantum Mechanics . . 203 8.4.2 Gauge Symmetry in Electrodynamics . . . . 206 17 8.4.3 Putting the puzzle pieces together . . . . . . 208 8.5 Gauge symmetries mathematically . . . . . . . . . . 210 8.5.1 Gauge connections in Quantum Mechanics and the toy model . . . . . . . . . . . . . . . . . . 216 9 Further Reading Recommendations 219 Part IV Appendices A Vector Calculus 227 A.1 Scalars, Vectors, Tensors . . . . . . . . . . . . . . . . 233 A.2 The dot product . . . . . . . . . . . . . . . . . . . . . 235 A.3 The cross product . . . . . . . . . . . . . . . . . . . . 238 A.4 Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 A.5 Line integral . . . . . . . . . . . . . . . . . . . . . . . 244 A.6 Path integral . . . . . . . . . . . . . . . . . . . . . . . 247 A.6.1 Tangent vector . . . . . . . . . . . . . . . . . 250 A.7 Circulation integral . . . . . . . . . . . . . . . . . . . 253 A.8 Surface integral . . . . . . . . . . . . . . . . . . . . . 254 A.8.1 Example: surface integral . . . . . . . . . . . 256 A.9 Flux Integral . . . . . . . . . . . . . . . . . . . . . . . 257 A.10 Gradient . . . . . . . . . . . . . . . . . . . . . . . . . 262 A.11 Divergence . . . . . . . . . . . . . . . . . . . . . . . . 264 A.12 Curl . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 A.13 The fundamental theorem for gradients . . . . . . . 276 A.14 The fundamental theorem for divergences a.k.a. Gauss’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 278 A.15 The fundamental theorem for curls a.k.a. Stokes’ the- orem . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 A.16 Vector identities . . . . . . . . . . . . . . . . . . . . . 284 A.16.1 The Bianchi identity . . . . . . . . . . . . . . 287 A.16.2 Summary of vector identities . . . . . . . . . 289 A.17 Index notation and Maxwell’s equations . . . . . . . 290 A.17.1 Electrodynamical Lagrangian . . . . . . . . . 294 B Taylor Expansion 297 C Delta Distribution 301 Bibliography 305 1 Birds-Eye View of Electro- dynamics As mentioned in the preface, electrodynamics is at its heart, like most other theories in physics, quite simple. However, certain applications can be extremely complicated. For this reason it’s easy to lose the forest for the trees. To prevent this, we start this book by talking about all fundamental notions and concepts and putting them into context. Afterwards, we will talk about the various concepts in more detail and gradually reﬁne our understanding until we are ready for real-world applications. There are three kinds of puzzle pieces that we need to master in order to get a proper understanding of electrodynamics: 1. Concepts (electric charge, electric current, charge density, current density, electric and magnetic ﬁeld, electromagnetic potential, electromagnetic waves) 2. Equations (continuity equation, Maxwell’s equations, Lorentz force law, wave equations) 20 no-nonsense electrodynamics 3. Tools (vector calculus, the delta distribution) In addition, there are lots of formulas which follow from these fundamental equations1 and several important advanced math- 1 For example, Coulomb’s Law and the Biot-Savart Law. In addition, take note that only Maxwell’s equations and the Lorentz force law are really fundamentals. The wave equations and the continuity equation can be derived using them. ematical tools2 and concepts3. These are important for speciﬁc 2 Greens functions, method of image charges, multipole expansion. 3 Polarization, Poynting vector, gauge invariance applications and will be discussed when they are needed, not in advance. Don’t worry if not everything is immediately clear in this chap- ter. Our goal is solely to get an overview and each idea dis- cussed in this chapter will be discussed later in more detail. First of all, what is our goal in electrodynamics? The short version is: We want to understand the interplay between electrically charged objects and the electromagnetic ﬁeld. This rather broad goal consists of four smaller goals: 1. We want to describe how charged objects behave in the pres- ence of the electromagnetic ﬁeld. 2. We want to describe how charged objects inﬂuence the elec- tromagnetic ﬁeld. 3. We want to describe how charged objects interact with each other. 4. We want to understand how the electromagnetic ﬁeld be- haves when there are no charged objects present.4 4 Typically, in this context we speak about electromagnetic radiation. The most famous example of electromagnetic radiation is light. All this is accomplished by Maxwell’s equations5 5 Be assured that these equations are not really as complicated as they may seem at ﬁrst glance. In the next chapter we will see that Maxwell’s equations are quite natural statements translated into a mathematical form. birds-eye view of electrodynamics 21 ∂µFµν = µ0 Jν ∂λFµν + ∂µFνλ + ∂νFλµ = 0 (1.1) plus the Lorentz force law dpµ dt = qFµν dxν dt , (1.2) where µ, ν, λ ∈ {0, 1, 2, 3} and µ0 and q are constants. Maxwell’s equations tell us how the electromagnetic ﬁeld reacts to the presence and ﬂow of electrically charged objects. We describe the electromagnetic ﬁeld using the electromagnetic ﬁeld tensor Fµν and electric charges using the current density Jµ.6 The electromagnetic ﬁeld tensor is deﬁned in terms of the 6 A tensor is an object with more than one index and in this sense is a generalization of vectors. It is also possible to call vectors 1−tensors since they have exactly one index. We talk about this in more detail in Appendix A.1. electromagnetic potential Aµ as follows: Fµν = ∂µ Aν − ∂ν Aµ . (1.3) In turn, the Lorentz force law tells us how charged objects are inﬂuenced by the electromagnetic ﬁeld. To summarize: electric charges Maxwell’s equations .. electromagnetic ﬁeld Lorentz force law nn There are different ways to write these equations. The form given here is especially useful since it is compact and useful for fundamental considerations. For many real world applications it’s better to \"unpack\" these equations by introducing the electric ﬁeld ⃗E and the magnetic ﬁeld ⃗B:7 7 What we do here is simply to give special names to certain components of the electromagnetic ﬁeld tensor Fµν. Since, µ, ν ∈ {0, 1, 2, 3} the ﬁeld tensor can be written as a (4 × 4) matrix: Fµν =     F00 F01 F02 F03 F10 F11 F12 F13 F20 F21 F22 F23 F30 F31 F32 F33     =     0 −E1 /c −E2 /c −E3 /c E1 /c 0 −B3 B2 E2 /c B3 0 −B1 E3 /c −B2 B1 0     We can write the ﬁeld tensor like this because it is antisymmetric: Fµν = −Fνµ (or in matrix form FT µν = −Fµν). We will derive later why this is the case. The antisymmetry implies that there must be zeroes on the diagonal since, for example, F11 = −F11, which is only true for F11 = 0. Moreover, we have, for example, F12 = −F21 and therefore there are in total only 6 independent components. Ei ≡ cFi0 Bi ≡ − 1 2 ϵijk Fjk , where i, j ∈ {1, 2, 3} and ϵijk denotes the Levi-Civita symbol. 22 no-nonsense electrodynamics Moreover, the zeroth component of the electromagnetic current Jµ gets a special name: J0 ≡ cρ, where c denotes the speed of light and ρ the charge density.88 We will discuss in Chapter 8 in detail why the charge density, in some sense, also can be thought of as some kind of current. The main idea will be that while the spatial components Ji describe how charge ﬂows between different locations, ρ describes the ﬂow between different points in time at a ﬁxed location. The remaining three components Ji with i ∈ {1, 2, 3} are then called the electric current density and usually written as an ordinary three-vector ⃗J.9 9 A \"three-vector\" is a vector with three components while a \"four- vector\" is a vector with, well, four components. Four vectors are what we usually use in special relativity because here, spatial i ∈ {1, 2, 3} components and temporal component 0 are mixed. This is also true here since a charge at rest (ρ ̸= 0, ⃗J = 0) is described by a different observer who moves relative to the ﬁrst observer as a moving charge (ρ ̸= 0, ⃗J ̸= 0) . This is discussed in more detail in Chapter 6. With these deﬁnitions, Maxwell’s equations read:10 10 This is shown explicitly in Ap- pendix A.16.2. But be warned that rewriting Maxwell’s equations in terms of ⃗B and ⃗E is far from trivial and it doesn’t make much sense to spend much thought on this derivation at this point. ∇ · ⃗E = ρ ϵ0 ∇ × ⃗B − µ0ϵ0 ∂⃗E ∂t = µ0⃗J ∇ · ⃗B = 0 ∇ × ⃗E + ∂⃗B ∂t = 0 (1.4) and the Lorentz force law reads: d⃗p dt = q ( ⃗E + d⃗x dt × ⃗B) . (1.5) Of course, it’s possible to unpack these equations even further, for example, by writing a separate equation for each compo- nent. This is, in fact, what Maxwell did historically. But this is not very useful and doesn’t lead to further insights. Now, with these equations at hand (in whatever form) the main task in electrodynamics is to solve them for different bound- ary conditions. As a result, we get formulas which describe the magnetic and electric ﬁeld conﬁguration in a given system. These formulas tell us that the electric and magnetic ﬁeld con- birds-eye view of electrodynamics 23 ﬁgurations look, for example, like this: Moreover, we can then use the Lorentz force law to derive how charged objects react to these ﬁeld conﬁgurations. One of the most important phenomena described by Maxwell’s equations are electromagnetic waves. An electromagnetic wave is a wave-like electric and magnetic ﬁeld conﬁguration which is able to travel through space. 24 no-nonsense electrodynamics This is possible because a changing magnetic ﬁeld leads to a changing electric ﬁeld and, in turn, a changing electric ﬁeld leads to a changing magnetic ﬁeld.1111 This fact is encoded in Maxwell’s equations. For example, the fourth line in Eq. 1.4: ∇ × ⃗E + ∂⃗B ∂t = 0 tells us whenever our magnetic ﬁeld is changing in time ( ∂⃗B ∂t ̸= 0), there will be a non-zero electric ﬁeld strength ⃗E. changing ⃗B // changing ⃗E // changing ⃗B // . . . To be a bit more precise: using Maxwell’s equations we can derive the so-called wave equations ∇2⃗E = µ0ϵ0 ∂2 ∂t2 ⃗E ∇2⃗B = µ0ϵ0 ∂2 ∂t2 ⃗B . Solutions of these equations describe wave-like electric and magnetic ﬁeld conﬁgurations (⃗E ∝ cos(ωt − kx)⃗ey).1212 Take note that it is possible to ﬁnd special wave-like solutions which do not travel anywhere. These are known as standing wave solutions and can be understood as linear combinations of the more fundamental plane wave solutions. We will discuss this in detail in Section 5. Also note that we could equally write here ⃗E ∝ cos(kx − ωt)⃗ey since the cosine is an even function and hence it makes no difference if we reverse its argument. After this certainly overwhelming ﬁrst glance at what electro- dynamics is all about, let’s take a step back and talk about the concepts introduced here in a bit more detail.13 13 As mentioned above, each concept will be discussed several times from multiple perspectives. birds-eye view of electrodynamics 25 Part I What Everybody Ought to Know About Electrodynamics \"Only connect! That was the whole of her sermon . . . Live in fragments no longer.\" E. M. Forster PS: You can discuss the content of Part I with other readers and give feedback at www.nononsensebooks.com/edyn/part1. birds-eye view of electrodynamics 29 In the following two chapters, we will focus solely on the funda- mental concepts and equations of electrodynamics. The mathe- matical tools needed to describe electrodynamics are discussed in the Appendices and it’s entirely up to you when you read them. This is really a matter of taste and you can read this book however you like. ◃ For example, you can start by reading the appendices ﬁrst to get a solid understanding of the required tools. ◃ Alternatively, you can read the corresponding appendix whenever a given tool shows up for the ﬁrst time in the text.14 14 The relevant appendix is always mentioned in a sidenote like this. ◃ But my personal recommendation would be that you start by reading Chapter 2 and Chapter 3 quickly without jumping around. During this ﬁrst run, you can simply write down a little comment or question mark in the margin every time a mathematical concept is used that you’re not familiar with.15 15 That’s why the margin is so big. Then, once you’ve ﬁnished reading these two chapters, you can go back to these question marks and read the corre- sponding appendices to deepen your understanding. 2 Fundamental Concepts In this chapter, we discuss the basic notions needed to describe electrodynamics. This establishes the language we will use in the rest of the book. We will start with the most basic notion: electric charge. Af- terwards, we will discuss how we can describe electric charges in the context of electrodynamics using the concepts: charge density, electric current and electric current density. While the charge density is what we use to describe the locations of elec- tric charges, the electric current and current density are used to describe how these charges move around. We will then talk about the various concepts we can use to describe how electric charges inﬂuence each other, including the electromagnetic ﬁeld, the electric ﬁeld, the magnetic ﬁeld and the electromagnetic potential. Each of these concepts have strengths and weaknesses depending on the task at hand. For example, the electric ﬁeld and the magnetic ﬁeld are helpful to analyze the electromagnetic ﬁeld in speciﬁc real-world sit- uations. However, the common origin of the electric ﬁeld and the magnetic ﬁeld becomes especially transparent when we introduce the electromagnetic potential.1 1 The electric and magnetic ﬁelds can be calculated directly once the electromagnetic potential is speciﬁed. 32 no-nonsense electrodynamics Now, as promised, let’s start with the simplest but arguably most fundamental notion in electrodynamics: Electric charge. 2.1 Electric charge Each elementary particle is characterized by certain labels like its mass. Since these labels determine how particles behave in experiments, we use them to deﬁne and distinguish different types of elementary particles. One of the most famous labels is what we call electric charge. But there are also others like the weak charge (\"isospin\"), the strong charge (\"color\") or the spin of the particle. For example, an electron is characterized by the labels: ◃ mass: 9, 109 · 10−31 kg, ◃ spin: 1 2 , ◃ electric charge: 1, 602 · 10−19 C, ◃ weak charge, called weak isospin: − 1 2 , ◃ strong charge, called color charge: 0. An important thing these labels have in common is that they determine how (if at all) a particle takes part in fundamental interactions.2 For example, a particle without electric charge 2 One of the most beautiful aspects of modern physics is how we can understand the origin of all these labels using symmetries. We will not discuss this in detail here but if you’re interested you may enjoy Jakob Schwichtenberg. Physics from Symmetry. Springer, Cham, Switzerland, 2018a. ISBN 978- 3319666303 does not take part in electromagnetic interactions. Analogously, only particles which carry a non-zero strong charge take part in strong interactions. The way a particle takes part in a given interaction depends on the actual value of the charge. For example, a particle with negative electric charge (e.g. an electron) behaves different than a particle with positive electric charge (e.g. a proton). Similarly, a particle with a large electric charge behaves differently than a particle with a smaller charge. A crucial point is that not only fundamental concepts 33 different values for the electric charge are possible, but also different signs. This means that the charges of different particles can cancel and even yield zero when an object carries equal amounts of positive and negative charge. Another important property is that like charges repel and unlike charges attract. Since our macroscopic everyday objects consist of elementary particles, we have to use the same labels to describe them. The electric charge of a macroscopic object is simply the sum of the charges of all elementary particles. An extremely surprising fact of nature is that the overall electric charge of almost all macro- scopic objects is exactly zero even though each of them consists of billions of charged elementary particles (electrons, protons).3 3 Take note that protons are not re- ally elementary particles since they consist of three more fundamental quarks. In other words, the charges of the billions of elementary parti- cles inside any macroscopic object almost always average out to exactly zero. This comes about since protons have exactly the same charge as an electron, only with a different sign.4 4 This is known as the quantization of electric charge and is a com- pletely non-trivial observation. So far, there is no generally accepted explanation for this curious fact of nature. In principle, the charges of protons and electrons could be com- pletely arbitrary numbers and there is no reason why they should have anything to do with each other. Another important aspect of electric charge is that it’s con- served. In other words, the total amount of electric charge in any closed system always stays the same.5 This means that if 5 This can also be understood using symmetry arguments and Noether’s theorem, c.f. [Schwichtenberg, 2018a]. However, at this point the best we can do is to collect these experimental facts and then see how we can build them into our theory. the charge of an object is changed, the charge of a different ob- ject must also change by an equal and opposite amount so that the total amount of charge remains constant.6 6 In the illustration below, the letter \"C\" stands for \"Coulomb\" which is the SI-unit of electric charge. Formulated differently, electric charge can never be produced or destroyed, only moved from one place to another. In macro- scopic applications, we usually move electrons around. If we want to generate a positive charge for some object, we need to remove electrons from it. Of course, the electrons then have to 34 no-nonsense electrodynamics go somewhere and, as a result, the place where they end up will be negatively charged. The conservation of electric charge holds even at the most fun- damental level. All interactions of fundamental particles which have been observed so far conserve electric charge. For example, while an electron can indeed appear seemingly out of nowhere, at the same time a positron (the electrons’ antiparticle) always appears which carries exactly the opposite charge. Therefore, the total electric charge before and after such a process remains exactly the same. This also implies that electric charge is conserved locally. What this means is that electric charge doesn’t vanish at one point and then magically appears at another place. This would be no problem if electric charge were only conserved globally. If only the total charge inside the whole universe would be conserved, there would be no problem if electric charge vanishes from a given small system since it could have materialized somewhere else. However, this is not the case. Electric charge is conserved no matter how small our system is, even for elementary particle processes.7 Formulated differently, no matter how small the 7 Take note that local conservation implies global conservation, but not vice versa. volume is we are considering, the total charge inside remains the same (if the system is closed). Now, in interesting systems we are usually dealing with more than one charged object and want to describe how these move. fundamental concepts 35 In particular, the total charge of any macroscopic charged object is the result of the individual charges of an incredibly large number of electrons and protons. To keep track of the locations and movements of a large number of charged objects, we need new tools. These tools are the topic of the following sections. 2.2 Charge density The ﬁrst extremely useful notion to describe systems with lots of charged objects is charge density.8 Charge density ρ(⃗x, t) 8 An alternative name for the charge density is charge distribution.is deﬁned as \"charge per unit volume\" and enables us to keep track of the locations of charges in our system. While electric charge is quantized and therefore only appears in discrete lumps, in macroscopic systems we are usually dealing with continuous charge densities. This is a valid approach since any macroscopic region contains an incredibly large number of fundamental charges and the discrete nature of the electric charge makes no difference. If we could zoom in until we \"see\" individual electrons, we would notice that the charge density varies wildly from point to point. However, in macroscopic applications, we are so far away and always consider systems which consist of lots of elementary charges such that the charge distribution appears smooth. The main idea behind the charge density is that we can describe this incredibly large number of fundamental charges sufﬁciently 36 no-nonsense electrodynamics accurately by taking the total charge within some manageable region and dividing it by the region’s volume. The total charge inside any volume V is then given by the inte- gral over the charge density: Q = ∫ V d3x ρ(⃗x, t) . (2.1) So when we integrate the charge density ρ(⃗x, t) over a speciﬁc region V of our system and end up with a large number Q, we know immediately that there is a large amount of electric charge concentrated in this region. In other words, the charge density encodes where electric charges are concentrated in our system. Usually, our charge density is a smooth function which we can think of as arising from averaging over many individual charges. For example, when we have a ball with total charge Q which is evenly dis- tributed through the ball, the charge density is constant: ρ = Q V , (2.2) where V is the volume of the ball. Outside the ball, the charge density is zero. Of course, we can also use the charge density if there is only one charged object in our system. In this case, the charge den- sity is zero everywhere except at one speciﬁc point. Any in- tegral over a region which contains the location of the object, yields simply the charge of the object. fundamental concepts 37 The correct mathematical tool to describe such a charge density is the delta distribution.9 Speciﬁcally, the charge density of a 9 If you are unfamiliar with the delta distribution, have a look at Appendix C. system with just one charged object located at ⃗x0 is ρ(⃗x) = qδ(⃗x − ⃗x0) , (2.3) where q is the charge of the object. Any integral over a region V0 which contains ⃗x0 yields exactly q, as it should be ∫ V0 d3x ρ(⃗x, t) = ∫ V0 d3x qδ(⃗x − ⃗x0)↷ = q ∫ V0 d3x δ(⃗x − ⃗x0)↷∫ dx δ(x) = 1 = q . (2.4) Usually, we not only want to describe the locations of charged objects, but also how they move. This is possible by using what we call current and current density. These concepts are what we will talk about next. 2.3 The electric current In general, a current describes how a given quantity (heat, elec- tric charge, etc.) moves through a system. There can be moving objects everywhere. Therefore, a current encodes information about the ﬂow of the quantity at each point in space. In other words, a current I(⃗x) is a function that eats a point in space ⃗x and spits out a number I(⃗x). In electrodynamics this number tells us the amount of charge that passes through the given location per unit time I = dQ dt . (2.5) 38 no-nonsense electrodynamics For simplicity, let’s assume we are dealing with a one-dimensional system, for example, an extremely thin wire. Moreover, let’s as- sume that the charge density in the wire is constant ρc and that our charges move with the constant velocity vc. During a given time interval ∆t, the distance our charges travel in a line segment is ∆L = vc∆t. The total charge contained in a line segment is ∆Q = ρc∆L. This means that the total charge contained in this line segment is ∆Q = ρcvc∆t . (2.6) A crucial observation is that the total amount of electric charge passing any speciﬁc point P during this time interval is equal to the amount of charge contained in this line segment: The electric current inside the wire is therefore I = ∆Q ∆t = ρcvc . (2.7) In general, of course, our charges can move in more than just one dimension. If this is the case, we need to keep track of fundamental concepts 39 the various directions in which our charges move. Mathemat- ically, this means that a simple number at each location is no longer sufﬁcient and we need vectors instead. The correct tool to describe the ﬂow of charges in three dimensions is called cur- rent density. A current density yields a vector at each point in space.10 The direction of the vector at a given point describes 10 Mathematically, we use a vector ﬁeld. We will talk more about ﬁelds in the next section and also in Appendix A.4. the direction of the ﬂow. The length of the vector describes how much is ﬂowing. Keeping track of charges in three dimensions can easily become quite messy. Therefore, we ﬁrst discuss the concept \"current density\" in the context of a simple special situation. Afterwards, we will discuss current densities in general. Analogous to what we did above, we consider again a constant charge density ρc and assume that all charges move with the velocity ⃗vc in a common direction.11 In one dimension, our 11 We can imagine, for example, that ⃗vc describes the average velocity.task was to describe how many charges pass any given point per given time interval. Now, in three dimensions our task is to describe how many charges pass any given frame per time interval.12 12 Take note that the frame is not necessarily at right angles to the direction of ﬂow of the charges. How many of the charges shown here will pass the frame dur- ing the interval ∆t? Analogous to how we were able to answer this question for 40 no-nonsense electrodynamics the one-dimensional problem using a line segment, we now consider a \"segment\" of our three-dimensional space. Here our \"segment\" is a prism with a base given by the frame and an edge of length |⃗vc|∆t, which is again the distance our charges travel during the interval ∆t.1313 Charges outside this prism are either too far away to reach the frame during the interval ∆t or miss the frame. The number of charges which pass our frame are exactly those contained in the prism: ∆Q = ρ0Vprism.14 As usual, the volume 14 This is completely analogous to what we discussed above for one- dimensional systems where exactly those charges contained in the line segment |⃗vc|∆t passed the point P. of the prism is given by the product of its base and height. The height of the prism is |⃗vc|∆t cos θ: fundamental concepts 41 Therefore, if we denote the surface area of our frame with A, we can write the volume of the prism as Vprism = A|⃗vc|∆t cos θ . (2.8) The total amount of electric charge passing through the frame is then ∆Q = ρ0Vprism = ρ0 A|⃗vc|∆t cos θ . (2.9) By introducing a vector ⃗n of unit length normal to the frame and recalling the deﬁnition of the scalar product, we can write this formula as ∆Q = ρ0 A∆t⃗vc · ⃗n , (2.10) since ⃗vc · ⃗n = |⃗vc||⃗n| cos θ = |⃗vc| cos θ, where θ is the angle between the two vectors.15 15 The scalar product is discussed in Appendix A.2. Using this equation, we can conclude that the amount of charge passing our frame per given time interval (= our electric cur- rent) is16 16 This equation is completely analogous to Eq. 2.7. I ≡ ∆Q ∆t = ρ0 A⃗vc · ⃗n . (2.11) An important point is that this formula yields a speciﬁc number (the electric current) for each speciﬁc frame A⃗n. Usually, it is more convenient to use a quantity which encodes information about the ﬂow for any given frame and not only one speciﬁc one. For this purpose, we deﬁne the electric current density as ⃗J ≡ ρ0⃗vc . (2.12) This current density assigns a vector to each point in space, which is exactly the idea discussed above. Moreover, to get the current ﬂowing through any speciﬁc frame, we simply have to multiply the current density by ⃗nA: ⃗J · ⃗nA = ρ0⃗vc · ⃗nA = I . (2.13) In general, the magnitude of the current density |⃗J| at one spe- ciﬁc point describes the amount of electric charge which passes per unit time through an inﬁnitesimal surface element which is 42 no-nonsense electrodynamics at right angles to the direction of the ﬂow. The direction of the charge density vector ⃗J encodes where the charges are ﬂowing. If we want to know how much electric charge is ﬂowing through a more complicated surface S, we have to calculate the contribu- tion from each inﬁnitesimal element ⃗dS that the surface consists of and then calculate the sum of these contributions: I = ∫ S ⃗J · ⃗dS . (2.14) The total amount of charge passing through the surface S dur- ing some time interval ∆t is then given by ∆Q = ∆t ∫ S ⃗J · ⃗dS . (2.15) Mathematically, we call ∫ S ⃗J · ⃗dS a surface integral and it de- scribes the ﬂux of electric charge through the surface S.1717 To learn more about surface inte- grals and ﬂux, read Appendix A.9. So far, we have only talked about how we can describe the locations and the ﬂow of charged objects. However, in electro- dynamics we are primarily interested in how charged objects inﬂuence each other. To describe this, we need further mathe- matical tools and concepts and this is what will we talk about next. 2.4 The electromagnetic ﬁeld The question we want to answer in this section is: Which math- ematical tool is the right one to describe electromagnetic interac- tions? A ﬁrst crucial hint is that electrically charged objects can inﬂu- ence each other even though they do not touch. fundamental concepts 43 This means that electromagnetic interactions have to be medi- ated by something. We call this something the electromagnetic ﬁeld and, as the name already indicates, mathematically we de- scribe it using a ﬁeld.18 A scalar ﬁeld is certainly not sufﬁcient 18 If you’re unfamiliar with \"ﬁelds\" and the different kinds of ﬁelds: scalar ﬁelds, vector ﬁelds, tensor ﬁelds, have a look at Appendix A.4. since charged particles get pushed through electromagnetic interactions in a certain direction, analogous to how a piece of paper gets pushed by wind.19 Directional information are 19 As mentioned in Appendix A.1, we describe wind using a vector ﬁeld. described using a vector at each point in space. An important feature of the electromagnetic ﬁeld is that even a vector ﬁeld is not sufﬁcient and we need instead a tensor ﬁeld. The electromagnetic ﬁeld tensor is an antisymmetric (4 × 4) matrix20 20 The antisymmetry follows from the deﬁnition of the ﬁeld tensor in terms of the electromagnetic potential Aµ, which is something we will discuss in the next section.Fµν(t,⃗x) =      0 F01(t,⃗x) F02(t,⃗x) F03(t,⃗x) −F01(t,⃗x) 0 F12(t,⃗x) F13(t,⃗x) −F02(t,⃗x) −F12(t,⃗x) 0 F23(t,⃗x) −F03(t,⃗x) −F13(t,⃗x) −F23(t,⃗x) 0      (2.16) This tensor ﬁeld, in some sense, assigns exactly two vectors to each point in space ⃗x at each point in time t. We need two 44 no-nonsense electrodynamics vectors at each point to describe the electromagnetic ﬁeld com- pletely. The vectors at each location represent the strength and direction of the electromagnetic ﬁeld.2121 This will make a lot more sense in a moment. The most famous way to visualize this is to use a magnet and lots of tiny pieces of metal. If we put these pieces of metal around the magnet, they arrange in a speciﬁc pattern, which makes the electromagnetic ﬁeld at least somewhat visible: It is conventional to introduce the electric vector ﬁeld ⃗E(t,⃗x) and the magnetic vector ﬁeld ⃗B(t,⃗x) and work with them, in- stead of with the electromagnetic tensor ﬁeld Fµν(t,⃗x).2222 Here c denotes the speed of light which is arguably the most impor- tant constant in electrodynamics. We will talk about it in detail in Chapter 6. Fµν(t,⃗x) =      0 F01(t,⃗x) F02(t,⃗x) F03(t,⃗x) −F01(t,⃗x) 0 F12(t,⃗x) F13(t,⃗x) −F02(t,⃗x) −F12(t,⃗x) 0 F23(t,⃗x) −F03(t,⃗x) −F13(t,⃗x) −F23(t,⃗x) 0      ≡      0 −E1(t,⃗x)/c −E2(t,⃗x)/c −E3(t,⃗x)/c E1(t,⃗x)/c 0 −B3(t,⃗x) B2(t,⃗x) E2(t,⃗x)/c B3(t,⃗x) 0 −B1(t,⃗x) E3(t,⃗x)/c −B2(t,⃗x) B1(t,⃗x) 0      . (2.17) Each of these two ﬁelds ⃗E(t,⃗x) = (E1(t,⃗x), E2(t,⃗x), E3(t,⃗x))T and ⃗B(t,⃗x) = (B1(t,⃗x), B2(t,⃗x), B3(t,⃗x))T assigns a vector to each point in space ⃗x at each point in time t.23 23 Writing out explicitly that our ﬁelds depend on the location ⃗x and the time t makes our equa- tions quickly overcluttered. Hence, usually we do not write these de- pendencies explicitly and only write ⃗E or ⃗B. However, it is always important to keep in mind that, in general, these ﬁelds assign a different vector to each point in space and time. Take note that the superscript T here denotes transpo- sition which is a transformation that turns a column vector into a row vector and vice versa. Here, I’ve used transposed vectors because column vectors don’t ﬁt nicely into a text paragraph. fundamental concepts 45 Now, what’s the meaning of these little vectors at each point in space? To understand this, let’s recall what similar vectors describe in physics.24 For example, for the vector ﬁeld describing air, 24 Again: If you’re unfamiliar with vector ﬁelds, have a look at Appendix A.4. the little arrows attached to each point in space represent the motion of the individual air molecules. In addition, the electric current density ⃗J(t,⃗x) also assigns a little vector to each point in space and is therefore also a vector ﬁeld. Here the little arrows tell us the speed of our charges and in which direction they are moving. Unfortunately, the interpretation of the electric ﬁeld ⃗E(t,⃗x) and the magnetic ﬁeld ⃗B(t,⃗x) is not so simple. For these ﬁelds the little arrows have a more abstract meaning since nothing is really ﬂowing around.25 Instead, the little arrows encode 25 Historically, physicists did imag- ine that the electromagnetic ﬁeld is a real substance like water and this substance called ether really does ﬂow around. However, this \"ether hypothesis\" has been abandoned in modern physics since experiments never found any of the predicted ether effects. information about the somewhat abstract \"physical ﬁeld\", which we can understand as follows. The direction of the vector at a given location encodes in which direction a test charge would move if it were placed here.26 The 26 A test charge is an object with a tiny electric charge which we use to gather information about the electromagnetic ﬁeld. Each charge has a direct inﬂuence on the elec- tromagnetic ﬁeld. But we want to use the test charge to really test what the electric ﬁeld would look like in the absence of our \"measur- ing device\" and therefore want to minimize the inﬂuence of the object we are using as much as possible. Formulated differently, by using a tiny test charge we minimize the modiﬁcation of the electric ﬁeld due to our measurement procedure. magnitude of the vector encodes how fast the test charge would accelerate as a result of, for example, the electric ﬁeld. In this sense, these more abstract ﬁelds encode how something would ﬂow if it were there. 46 no-nonsense electrodynamics In practice the electric ﬁeld strength at a point is measured by placing a small positive test charge at that point and measuring the force on it: ⃗E(⃗x) ≡ ⃗F(⃗x) q , (2.18) where q denotes the charge of the object we put at the location ⃗x.2727 Analogously, the magnetic ﬁeld at a point can be measured by using a moving test charge. Take note that we also use electric test charges to probe the magnetic ﬁeld since, so far, no magnetic monopole has ever been observed. We will talk more about this point below. This way we can deduce ﬁeld conﬁgurations which look, for example, like this: This whole physical ﬁeld idea is somewhat abstract and cer- tainly needs some time to get used to. However, the electric and the magnetic ﬁelds lose much of the mystery surrounding them as soon as we use them to describe speciﬁc situations. At this point, a possibly helpful point of view is that these ab- stract ﬁelds are simply useful mathematical devices to help us describe electromagnetic interactions. They assign little arrows to each point in space that encode how a charge would react if it were there, even if no charge is there. Maybe it helps that even Nobel laureate Richard Feynman had a somewhat blurry picture of the electromagnetic ﬁeld in mind:2828 [Feynman, 2011] I’ll tell you what I see. I see some kind of vague showy, wiggling lines - here and there an E and a B written on them somehow, and perhaps some of the lines have arrows on them — an arrow here or there which disappears when I look too closely at it. When I talk about the ﬁelds swishing through space, I have a terrible confusion between fundamental concepts 47 the symbols I use to describe the objects and the objects themselves. I cannot really make a picture that is even nearly like the true waves. So if you have difﬁculty making such a picture, you should not be worried that your difﬁculty is unusual. Before we move on, three short comments: ◃ Firstly, while thinking about the electromagnetic ﬁeld in terms of the electric ﬁeld and the magnetic ﬁeld can be use- ful, for fundamental consideration this splitting of the elec- tromagnetic ﬁeld Fµν into two seemingly separate parts (⃗E, ⃗B) can be quite confusing.29 Instead, the deep structure of elec- 29 An important point we will discuss later is, for example, that different observers do not necessar- ily agree on what they call electric and what they call magnetic ﬁeld. The electric ﬁeld of one observer may appear, at least in part, like a magnetic ﬁeld for another observer. This is discussed in Chapter 6. trodynamics becomes a lot more transparent as soon as we talk about the electromagnetic potential, which is the topic of the next section. ◃ Secondly, there is only one electromagnetic ﬁeld in the uni- verse and not many different ones. But it can be useful to imagine that each charge generates its own electromagnetic ﬁeld and the total electromagnetic ﬁeld is then the sum of all these hypothetical individual electromagnetic ﬁelds.30 Alter- 30 This is possible because the Maxwell equations are linear and we can therefore generate new solutions by using superpositions of known solutions. We will discuss this in detail in Chapter 4. natively, we can imagine that each charge generates a speciﬁc conﬁguration of the surrounding electromagnetic ﬁeld.31 31 In a similar spirit, the modern point of view in quantum ﬁeld theory is that, for example, there is only one electron ﬁeld in the whole universe. Each electron that we ob- serve is simply an excitation of this electron ﬁeld and this explains why every electron we have observed so far has exactly the same properties (mass, charge, etc.). ◃ Thirdly, it is important to keep in mind that usually things in nature are subject to constant change. This means that our electromagnetic ﬁeld, or analogously our electric and magnetic ﬁeld, vary over time. Therefore, the little vectors attached to each point in space change their direction and magnitude as time passes. 48 no-nonsense electrodynamics Next, as promised, let’s talk about the electromagnetic potential. 2.5 The electromagnetic potential Due to their appearance in Newtonian mechanics, potentials are a concept that should be much more familiar than ﬁelds. A potential tells us how much potential energy a given object has if we release it at a speciﬁc location. For example, if we re- lease an object of mass m in the gravitational potential φ(x) at the location x1, we know that it has potential energy mφ(x1). If we release the object it’s possible that it will start moving. This means that its kinetic energy changes from zero to a non-zero value and becomes larger. Since energy must be conserved, the kinetic energy must have come from somewhere. This some- where is the corresponding potential. Usually, in mathematical terms a potential is a scalar ﬁeld, in the sense that it eats a location and spits out a number.32 For- 32 We discuss scalar ﬁelds in Ap- pendix A.1. mulated differently, a potential usually assigns to each point in space a number and not an arrow (or tensor). Moreover, there is a close relationship between potentials and the forces that act on an object.33 For a potential φ, the corre- 33 However, take note that not every force is the result of a potential. sponding force ⃗F that acts on an object is34 34 The symbol ∇ is known as \"del\" or \"nabla\" and ∇φ is known as the gradient of the scalar ﬁeld φ. Gradients are discussed in detail in Appendix A.10. ⃗F = −⃗∇φ . (2.19) fundamental concepts 49 Therefore, given a potential, we can immediately calculate its effect on an object using Newton’s second law ⃗F = m⃗a. Completely analogous to the gravitational potential, we can also deﬁne an electromagnetic potential. This potential is a convenient tool to describe electromagnetic interactions since once the potential is deﬁned, we can use the relationship between forces and potentials to calculate how charges move in a non-zero potential.35 35 Take note that potentials can- not be directly measured. We can see this, because the relation- ship between a potential and the corresponding force (which is some- thing we can actually measure) is somewhat indirect (Eq. 2.19). In particular, take note that we can always add an arbitrary constant to the potential without changing the resulting force: φ → φ′ = φ + C ⇒ ⃗F → −∇φ′ = −∇(φ + C) = −∇φ − ∇C︸︷︷︸ =0 = −∇φ = ⃗F ✓ This freedom is usually called \"gauge freedom\" and we will talk more about it in Chapter 7. In contrast, we can actually measure the gravitational ﬁeld, the electric ﬁeld ⃗E and magnetic ﬁeld ⃗B since the relationship between the ﬁeld and the corresponding force is a lot more direct (Eq. 2.18). In other words, as mentioned above, we can measure the ﬁeld strength and direction at a particular location by observing the resulting force acting on a test charge. The crucial point is that in order to describe electromagnetic interactions, a scalar potential is not sufﬁcient. Instead, we need a four-vector potential Aµ(⃗t,⃗x), where µ = 0, 1, 2, 3. We will discuss the meaning of this vector potential in detail in Chapter 8. For the moment, we only note that the electromagnetic potential is characterized by 4 numbers at each point in space and time Aµ(t,⃗x) = (A0(t,⃗x), A1(t,⃗x), A2(t,⃗x), A3(t,⃗x))T and that the electric and magnetic ﬁelds can be calculated immediately once the electromagnetic potential is speciﬁed:36 36 As already mentioned above, the little superscript \"T\" denotes transposition, which means that we turn a row vector into a column vector and vice versa. This is useful since writing Aµ as a column vector would destroy the layout of the page completely. Ei = c(∂i A0 − ∂0 Ai) Bi = ϵijk∂j Ak , (2.20) where i, j, k ∈ {1, 2, 3}. We can also write this as a vector equa- tion37 37 It is often more convenient to write equations in index notation. The cross product ⃗A × ⃗B reads in index notation ϵijk Ai Bj, where ϵijk is the Levi-Civita symbol. ⃗E = c∇A0 − ∂t ⃗A ⃗B = ∇ × ⃗A . (2.21) We can see that the electric and magnetic ﬁelds have indeed a common origin, namely the electromagnetic potential Aµ. In this sense it makes sense to talk about the electromagnetic potential. Working with the electromagnetic potential is not only concep- tually attractive, but also more economical. The magnetic ﬁeld ⃗B and the electric ﬁeld ⃗E have 3 components each. This means 50 no-nonsense electrodynamics that if we describe electrodynamics in terms of ⃗B and ⃗E, we need to keep track of 6 functions (E1(t,⃗x), E2(t,⃗x), . . .). In contrast, if we work with Aµ, we only have to deal with 4 functions (A0(t,⃗x), A1(t,⃗x), A2(t,⃗x), A3(t,⃗x)).3838 Take note that this is still not the most economical choice possible. In fundamental terms electromagnetic interactions are mediated by parti- cles called photons. A photon has only 2 internal degrees of freedom and therefore, in principle, two functions are sufﬁcient to describe it. However, this is far beyond the scope of this book. To describe photons properly, we need to use quantum ﬁeld theory. Moreover, since the electric and magnetic ﬁelds appear as com- ponents of the electromagnetic ﬁeld tensor, we can also express the tensor ﬁeld itself using the electromagnetic potential3939 The electric and magnetic ﬁelds were deﬁned as components of the electromagnetic tensor in Eq. 2.17. The deﬁnition of the ﬁeld strength tensor in terms of the potential may seem rather strange and ad hoc. However, we will see in Chapter 8 that this equation has a really intuitive meaning. Fµν = ∂µ Aν − ∂ν Aµ (2.22) Before we move on and talk about the equations describing the interplay between the various concepts introduced in this chapter, let’s summarize what we have learned so far. 2.6 Summary There are different tools we can use to describe electromagnetic interactions. One economical possibility is the electromagnetic potential Aµ(t,⃗x), which assigns four numbers to each point in space.40 For many concrete applications it is conventional to 40 Aµ = (A0(t,⃗x), A1(t,⃗x), A2(t,⃗x), A3(t,⃗x))T fundamental concepts 51 use the electric ﬁeld ⃗E(t,⃗x) and magnetic ﬁeld ⃗B(t,⃗x) instead. Both can be calculated directly from the given electromagnetic potential (Eq. 2.20). A third possibility is to use the electromag- netic tensor Fµν(t,⃗x). This tensor is an antisymmetric (4 × 4) matrix which contains the components of the electric and mag- netic ﬁelds as entries (Eq. 2.17). Moreover, the electromagnetic ﬁeld tensor can be calculated directly using the electromagnetic potential (Eq. 2.22). electromagnetic potential Aµ Fµν≡∂µ Aν−∂ν Aµ \u000f\u000f Ei≡c(−∂i A0−∂0 Ai) \f\f Bi≡ϵijk∂j Ak \u0012\u0012 electromagnetic ﬁeld tensor Fµν Ei≡cF0i ww Bi≡− 1 2 ϵijk Fjk '' electric ﬁeld ⃗E magnetic ﬁeld ⃗B In addition to these concepts that deal with electromagnetic interactions directly, we also need notions to describe the ob- jects that are inﬂuenced by them. The most basic notion in this context is electric charge. Only objects with a non-zero elec- tric charge take part in electromagnetic interactions. Moreover, usually we are interested in systems that consist of lots of in- dividual charges. To describe the locations of charges in such systems, we use charge density ρ(t,⃗x) which encodes exactly where charges are concentrated within our system. In addition, to describe how charged objects move, we use electric current I and electric current density ⃗J(t,⃗x).41 41 Recall that the ﬂow of a quantity like electric charge is a directional quantity and hence, we need vectors to describe it completely.Now that we have established these fundamental physical con- cepts, we can discuss the fundamental equations of electrody- namics. 3 Fundamental Equations The most important equations of electrodynamics are Maxwell’s equations (Eq. 1.4) and the Lorentz force law (Eq. 1.5).1 The 1 Reminder: Eq. 1.4 reads ∇ · ⃗E = ρ ϵ0 ∇ × ⃗B − µ0ϵ0 ∂⃗E ∂t = µ0⃗J ∇ · ⃗B = 0 ∇ × ⃗E + ∂⃗B ∂t = 0 . and Eq. 1.5 d⃗p dt = q ( ⃗E + d⃗x dt × ⃗B) . (3.1) Lorentz force law describes how charged objects react to the presence of the electric and magnetic ﬁeld. Maxwell’s equations describe how non-zero electric and magnetic ﬁeld strengths are generated by charges and currents. Moreover, they also describe how the electric and magnetic ﬁelds inﬂuence each other. This interplay is the reason why electromagnetic waves exist which are described by the so-called wave equations. In addition, there is another incredibly important equation which encodes the fact that electric charge is conserved: the continuity equation.2 All 2 The wave equations and the continuity equation can both be derived using Maxwell’s equations.this is summarized by the following diagram. electric charge electric ﬁeld Gauss law oo continuity equation // electric current Ampere-Maxwell law \u0013\u0013 electric ﬁeld Lorentz force law @@ oo wave equations // Ampere-Maxwell law magnetic ﬁeld Lorentz force law `` Faraday’s law \\\\ no magnetic charge magnetic ﬁeld Gauss law OO 54 no-nonsense electrodynamics Whenever you feel lost in the following sections, come back here to see how what we are doing ﬁts into the bigger picture. But there is no reason to spend much time looking at this dia- gram if you’re just starting this chapter. Take note that we will not discuss explicit applications of these equations in the following sections since it would be too easy to get lost. Applications are discussed separately in Part II. With that said, let’s start discussing the fundamental equations of electrodynamics in detail. As usual, we start with the sim- plest one. 3.1 The continuity equation In plain language, the continuity equation says that when the amount of electric charge within some volume gets smaller, it must have been transported to a location somewhere outside the volume. Analogously, it says that when the electric charge gets larger within some volume, it must have come from somewhere outside the volume.33 Take note that, of course, it’s also possible to have a continuity equation for a quantity which is not conserved. In this case, there is an additional source term σ in the equation which describes how much of the quantity gets produced per unit time: ∂ρ ∂t = σ − ∇ · ⃗J. The equation then tells us that when the amount of the quantity we want to describe gets larger, it must either come from somewhere else or be produced inside the vol- ume. However, in electrodynamics, we want to describe electric charge, which is conserved and therefore there is no such source term. Using the concepts introduced in the previous sections, we can formulate this statement directly in mathematical terms. The amount of electric charge within some volume V is given fundamental equations 55 by4 4 As a reminder: we use the symbol ρ to describe the charge density. If we integrate the charge density over some volume, we get the total amount of charge within this volume. Q = ∫ V ρdV . (3.2) The change in the amount of electric charge within the volume during some time interval ∆t is given by5 5 Take note that we can’t simply state, for example, that a positive ∆Q means that charge has moved into the volume. While it is positive that ∆Q is positive because, for example, protons moved into the volume, it is also possible that electrons left the volume. Since electrons are negatively charged, if they leave the volume the total charge inside it becomes more positive. In practice, it’s almost always electrons which are moving around. ∆Q = −∆t ∂ ∂t ∫ V ρdV . (3.3) Here ∂ ∂t ∫ V ρdV is the rate at which the amount of electric charge changes and if we multiply it by an interval ∆t, we get the total change in the amount of electric charge. Now, the whole idea of the continuity equation is that if ∆Q is non-zero, electric charge must have moved into the volume or, alternatively, has been removed from the volume because it got pushed across the boundary of the volume. Information about the movement of electric charge is encoded in the current density ⃗J. In Section 2.3, we already discussed that the total amount of electric charge passing a given surface S during an interval ∆t is given by (Eq. 2.15)6 6 The symbol ∮ S indicates that we are integrating over a closed surface. Here, we have a closed surface since S is the surface of the volume V. The minus sign appears here as a result of the convention that we deﬁne the electric ﬁeld in terms of its force on a positive test charge (c.f. Eq. 2.18). Moreover, we use the convention that the vector that characterizes our surface, ⃗dS, is outward normal. Hence we consider an outward ﬂux of positive charge. Feel free to ignore details like this on a ﬁrst reading. ∆ ˜Q = −∆t ∮ S ⃗J · ⃗dS . (3.4) Since electric charge is conserved, we know that the change in the amount of electric charge inside the volume must be a result of electric charge passing the surface of the volume. Hence, charge conservation means that ∆Q = ∆ ˜Q and therefore we ﬁnd ∆Q = ∆ ˜Q ∴ ∆t ∂ ∂t ∫ V ρdV = −∆t ∮ S ⃗J · ⃗dS . (3.5) Since the interval ∆t appears on both sides it drops out from the equation and we can conclude. − ∂ ∂t ∫ V ρdV = ∮ S ⃗J · ⃗dS (3.6) This is the integral form of the continuity equation. It is im- portant to take note that this equation really describes the local 56 no-nonsense electrodynamics conservation of electric charge, since we can make the volume V as small as we please.77 The distinction between local and global conservation of electric charge was discussed in Section 2.1. In words, the continuity equations tells us: The rate at which the charge density changes in a given volume is exactly equal to the net amount of electric charge ﬂowing through the surface of the volume per unit time. An important observation is that we have on the left-hand side an integral over the volume, but on the right-hand side we have an integral over the surface of the volume. Luckily, there is a clever trick known as Gauss’s theorem that allows us to get a volume integral on the right-hand side too. This, in turn, al- lows us to write the equation without the integral. The resulting equation is known as the differential form of the continuity equa- tion. This differential form allows us to encode exactly the same idea in a more compact form.88 The differential form is also a lot more useful for many advanced applications. Here’s how this works. First of all, Gauss’s theorem is a fundamental theorem of vec- tor calculus which allows us to rewrite a surface integral as a volume integral:99 For a detailed discussion of Gauss’s theorem, see Ap- pendix A.13. An important point is that we get the volume V enclosed by the surface S on the right-hand side and not some arbitrary volume. ∮ S ⃗J · ⃗dS = ∫ V ∇ · ⃗JdV . (3.7) Here, ∇ · ⃗J denotes the divergence of ⃗J.10 Gauss’s theorem holds 10 The divergence of a vector ﬁeld is discussed in Appendix A.11. for any vector ﬁeld and therefore also for our current density ⃗J. Using Gauss’s theorem we can rewrite our continuity equation as follows − ∂ ∂t ∫ V ρdV = ∮ S ⃗J · ⃗dS this is Eq. 3.6↷Gauss’s theorem − ∂ ∂t ∫ V ρdV = ∫ V ∇ · ⃗JdV↷∫ V ( ∂ ∂t ρ + ∇ · ⃗J) dV = 0 . fundamental equations 57 We can now use that we made no assumptions about the vol- ume and therefore the only possibility to ensure that the inte- gral vanishes is if the integrand ( ∂ ∂t ρ + ∇ · ⃗J) is zero: ∂ ∂t ρ + ∇ · ⃗J = 0 (3.8) This is the differential form of the continuity equation.11 11 We will see in a moment that, completely analogously, there is an integral form and a differential form of Maxwell’s equations.Now, after this short discussion of the only fundamental equa- tion which deals exclusively with electric charges and their ﬂow, it’s time to move on to equations which describe the interplay between charges and the electromagnetic ﬁeld. As already men- tioned above, these equations are known as Maxwell’s equations and the Lorentz force law. In the following sections, we will discuss them one by one. 3.2 The Lorentz force law The Lorentz force law (Eq. 1.5) describes how electric charges react to the presence of a non-zero electric or magnetic ﬁeld strength. It consists of two parts. The ﬁrst part, tells us that the force resulting from the electric ﬁeld ⃗E is directly proportional to ⃗E: ⃗FE = q⃗E , (3.9) where q is the proportionality constant, called electric charge, that encodes how strongly our object reacts to ⃗E. In words, Eq. 3.9 tells us that a charged object gets pushed in the direction in which the electric ﬁeld points:12 12 Take note that a positively charged object gets pushed in the direction of the electric ﬁeld, while a negatively charged object gets pushed in the opposite direc- tion. We will discuss this further in Section 4.1.5. 58 no-nonsense electrodynamics The second part of the Lorentz force law is a bit more compli- cated. While the force resulting from the magnetic ﬁeld ⃗B is also directly proportional to ⃗B, we need to take into account that only moving charged objects react to the presence of the magnetic ﬁeld.1313 This can be understood using spe- cial relativity, which we will discuss in Chapter 6. From this perspective the statement here is a truism since \"magnetic ﬁeld\" is simply how we call the relevant component of the electromagnetic ﬁeld for an observer moving relative to the charge. The quantity which we use to describe whether or not an object is moving is the velocity ⃗v. In general, the velocity ⃗v is a vector and we therefore need to combine it somehow with the vector ﬁeld ⃗B to yield a force ⃗F. Since the force ⃗F is also a vector, we can’t use the scalar product ⃗v · ⃗B which yields simply a number and not a vector.14 14 As the name scalar product indicates, the result is a scalar, i.e. an ordinary number, not a vector. The scalar product is discussed in Appendix A.2. However, we can use the cross-product ⃗v × ⃗B since it allows us to combine two vectors in such a way that they yield another vector.15 This is indeed the correct idea and the second part of 15 The cross product is the topic of Appendix A.3. the Lorentz force law reads ⃗FB = q⃗v × ⃗B . (3.10) In words, this means that while the force ⃗FE resulting from the electric ﬁeld points in the same direction as ⃗E, the force resulting from the magnetic ﬁeld ⃗B is perpendicular to ⃗B and the velocity ⃗v.1616 As discussed in Appendix A.3, the deﬁnition of the cross-product is ⃗a ×⃗b = |⃗a||⃗b| sin θ⃗n, where ⃗n is the unit vector perpen- dicular to ⃗a and ⃗b and θ is the angle between ⃗a and ⃗b. We can use this to deﬁne the magnetic ﬁeld strength |⃗B| through the force experienced by a test charge: |⃗B| ≡ ∣ ∣ ∣⃗FB∣ ∣ ∣ q|⃗v| sin(θ) . (3.11) As a result the path of a charged object in a magnetic ﬁeld is helical:17 17 We will derive this explicitly in Section 4.2.2. fundamental equations 59 To be a bit more precise: we get such a helical path whenever the velocity ⃗v is not completely perpendicular to the magnetic ﬁeld ⃗B, since then there is a component of the velocity which moves the object further forward. In contrast, when the veloc- ity ⃗v is completely perpendicular to the magnetic ﬁeld ⃗B, the trajectory is simply a circle: Maybe a second perspective helps to understand this better. In the following picture the X denote magnetic ﬁeld vectors which point into the page you are looking at: 60 no-nonsense electrodynamics Let’s summarize: The total force on a moving charged object is the sum of the force resulting from the electric ﬁeld ⃗E and the magnetic ﬁeld ⃗B: ⃗Fres = ⃗FE + ⃗FB. The general Lorentz force law therefore reads ⃗F = q(⃗E + ⃗v × ⃗B) . (3.12) Now, a natural question is: Where do nontrivial electric and magnetic ﬁeld conﬁgurations come from? This is what Maxwell’s equations tell us. In the following section, we will discuss Maxwell’s equations in detail and start with the simplest one. fundamental equations 61 3.3 Gauss’s law for the electric ﬁeld One of the most basic ideas in electrodynamics is that electric charges inﬂuence each other by modifying the surrounding electromagnetic ﬁeld. Each electric charge generates a particular structure in the electromagnetic ﬁeld and this, in turn, has a direct impact on other charges.18 18 The impact of a non-zero electric ﬁeld strength on a given charge is described by the Lorentz force law (Eq. 1.5), which was the topic of the previous section. Gauss’s law for the electric ﬁeld puts this statement into a math- ematical form.19 However, it only describes how charges inﬂu- 19 Take note that Gauss’s Law ̸= Gauss’s Theorem. Moreover, two of the four Maxwell equations (Eq. 1.4) are called Gauss’s law. One describes the relationship between charges and the electric ﬁeld and the second one describes the relationship between ﬂowing charges and the magnetic ﬁeld. ence the electric part of the electromagnetic ﬁeld. If our charges are also moving, they additionally have a direct impact on the magnetic part of the electromagnetic ﬁeld. This is described by \"Gauss’s law for the magnetic ﬁeld\". We will discuss this second Gauss’s law in Section 3.4.20 20 We can already get a ﬁrst glance here that it can lead to problems when we separate the electric and magnetic ﬁeld. Namely, a charge which appears at rest for one observer, looks like a moving charge for a second observer who moves with some constant velocity relative to the charge. Hence, the ﬁrst observer will calculate a non- zero electric ﬁeld strength and a vanishing magnetic ﬁeld strength, while the second observer calculates that the electric ﬁeld vanishes but the magnetic ﬁeld is non-zero. Thought experiments like this are what we usually consider in the context of special relativity and we will talk a bit more about this in Chapter 6. In words, Gauss’s law for the electric ﬁeld describes how the electric ﬁeld reacts to the presence of electric charges. How can we write this in mathematical terms using the fundamental notions introduced in Chapter 2? Electric charge is described by the charge density ρ and the total charge inside some volume V is given by ∫ V ρdV. The basic statement described above means that if this total charge is non- zero, there will be a non-zero electric ﬁeld strength ⃗E. However, an equation like a⃗E = ∫ V ρdV , where a denotes a constant, makes no sense since ∫ V ρdV on the right-hand side is simply a number while ⃗E is a vector. If we have a simple number on one side of an equation, we also need a number on the other side of the equation. Otherwise we are comparing apples with oranges. Moreover, it seems reasonable that since the choice of the vol- ume V plays a role on the right-hand side, it must play a role on the left-hand side too. 62 no-nonsense electrodynamics We were in a similar situation when we wrote down the conti- nuity equation in Section 3.1. There we wanted to write down an equation which connects the ﬂow of electric charges (de- scribed by ⃗J) with changes in the total amount of electric charge within some volume ∆t ∂ ∂t ∫ V ρdV. The idea which allowed us to connect the vector ⃗J with the number ∆t ∂ ∂t ∫ V ρdV was to use the total ﬂux of electric charges through the surface of V (Eq. 2.14): ﬂux of electric charge ≡ ∫ S ⃗J · ⃗dS . This ﬂux represents the amount of electric charge which ﬂows through the surface per unit time. Hence, if we multiply it by some interval ∆t, we get the total amount of charge which passes through the surface during this interval. This amount is a simple number and depends directly on the choice of the volume V since it is an integral over the surface of the volume. Writing this on one side of an equation and ∆t ∂ ∂t ∫ V ρdV on the other side makes sense. The equation we get this way is exactly the continuity equation (Eq. 3.6) discussed in Section 3.1.21 The 21 Reminder: Eq. 3.6 reads − ∂ ∂t ∫ V ρdV = ∮ S ⃗J · ⃗dS . crucial idea is that we use exactly the same idea again. This means that we introduce the ﬂux of the electric ﬁeld through some surface S: ﬂux of the electric ﬁeld φ ≡ ∫ S ⃗E · ⃗dS . (3.13) This is simply a number and not a vector and therefore a good candidate for our left-hand side. Moreover, it also depends on the choice of the volume since S represents its surface.2222 The main idea behind a ﬂux integral like this is discussed in detail in Appendix A.9. With this in mind, let’s go back to what we discussed at the beginning of this section. Our goal is to ﬁnd an equation which describes that a non- zero charge within some volume has a direct impact on the structure of the electric ﬁeld. The charge within some volume V is given by ∫ V ρdV and we just argued that a reasonable object to represent the electric ﬁeld in such an equation is the ﬂux of the electric ﬁeld (Eq. 3.13). fundamental equations 63 Putting these puzzle pieces together yields23 23 Take note that we have ∮ since S is the boundary of the volume V and therefore a closed surface. Of course, the \"derivation\" here is by no means rigorous. As men- tioned above, our goal is only to get a rough feeling for what elec- trodynamics is all about. Questions like where this equation and all other Maxwell equations really come from will be discussed later in detail. Historically they were, of course, simply deduced exper- imentally like, for example, the conservation of electric charge too. However, nowadays we can also understand the origin of Maxwell’s equations and the conservation of electric charge from a more theoret- ical perspective. We will talk about this in Chapter 6.1. ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV , (3.14) where ϵ0 is a constant known as electric permittivity which describes how strongly the electric ﬁeld reacts to the presence of charges. This is the integral form of Gauss’s law for electric ﬁelds. In words, it tells us: The ﬂux of the electric ﬁeld passing through a closed surface is directly proportional to the amount of electric charge contained inside the volume. It is important to keep in mind that the word \"ﬂux\" is simply a statement about the length of the vectors at the boundary of our volume, i.e. on the surface. This means that if there is a lot of charge inside the surface, we get big vectors. If there is only a little charge inside, we get tiny vectors: Moreover, the sign of the ﬂux tells us in which direction the vectors are pointing. If the ﬂux is positive, the vectors point outwards while if the ﬂux is negative the vectors point inwards. 64 no-nonsense electrodynamics Take note that there is really a vector at each point of the sur- face, but there is no way to draw them all. In addition, take note that if there is no charge inside the surface S or if there is an equal amount of positive and negative charges inside the volume, the net ﬂux through the surface is zero. So, as promised above, Gauss’s law simply puts the statement: non-zero charge ↔ non-zero electric ﬁeld strength into a mathematical form.2424 The explicit mathematical form of Gauss’s law seems quite intimidat- ing at ﬁrst glance. However, it will become a lot clearer as soon as we use it below to calculate the electric ﬁeld strength that results from a given charge distribution. We will do this in detail in Part II. Before we move on, three short comments on what we just learned. ◃ An important difference between the ﬂux of electric charge (the electric current per unit time) and the ﬂux of the electric ﬁeld is that the latter does not really describe the movement of anything. This can be quite confusing and has to do with the two ways we can understand vector ﬁelds like ⃗J and ⃗E.25 Both, ⃗J and ⃗E assign a vector to each point in space. 25 We discussed this already near the end of Section 2.4. However, for ⃗J these vectors represent the real movement of our electric charges. For the more abstract electric ﬁeld ⃗E, the arrows only represent how a charge would move if it were there.2626 As already mentioned above, his- torically, physicists really thought that the arrows the electric ﬁeld assigns to each point represent the movement of the \"electric ﬁeld substance\". However, this idea has been experimentally falsiﬁed since such a real substance would lead to speciﬁc effects which were never observed. ◃ As mentioned above, there are two ways of thinking about Gauss’s law (and also all other Maxwell equations). Either, we say that it describes how the electromagnetic ﬁeld gets modiﬁed when there are electric charges present. In this view there is only one electromagnetic ﬁeld which expands over fundamental equations 65 all space but possibly has zero magnitude in some regions. Alternatively, we can say that it tells us what the electro- magnetic ﬁelds produced by charges look like. Formulated differently, in this second view we imagine that each charge produces its own electromagnetic ﬁeld. The second view is often more useful for concrete applications, but it is always important to keep in mind that there is really one electromag- netic ﬁeld. The individual ﬁelds \"produced\" by charges are only a helpful way to think about the total ﬁeld in simpler terms. The total ﬁeld is what we can really observe in nature and it is always given by the sum over all these individual ﬁelds. Now, before we discuss the second Maxwell equation we will rewrite Gauss’s law in a more compact form. The steps are com- pletely analogous to what we already discussed in Section 3.1 for the continuity equation. The result will be the differential form of Gauss’s law. We start with Gauss’s law in integral form (Eq. 3.14) and then use again Gauss’s theorem27 to get a volume integral on both 27 Gauss’s theorem is explained in Appendix A.13.sides: ∫ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV this is Eq. 3.14↷Gauss’s theorem ∴ ∫ V ∇ · ⃗EdV = 1 ϵ0 ∫ V ρdV↷ ∴ ∫ V (∇ · ⃗E − 1 ϵ0 ρ) dV = 0 . (3.15) We can therefore conclude28 28 Take note that this is the form of the ﬁrst Maxwell equation, as given in Eq. 1.4. ∇ · ⃗E = ρ ϵ0 . (3.16) This is the differential form of Gauss’s law for the electric ﬁeld. 66 no-nonsense electrodynamics What does this differential form tell us in plain language? Since there are no integrals left in the ﬁnal equation, the inter- pretation is a different one than for the integral form.29 On the 29 Reminder: the integral form of Gauss’s law tells us that the ﬂux of the electric ﬁeld through a surface is directly proportional to the electric charge contained in the volume. left-hand side we have the divergence of the electric ﬁeld ∇ · ⃗E. In general, the divergence of a vector ﬁeld gives us information about the tendency of the ﬁeld to ﬂow towards or away from a speciﬁc point.30 On the right-hand side, we have the charge 30 The divergence of a vector ﬁeld is discussed in detail in Eq. A.11. density ρ and the proportionality constant ϵ0.31 31 Reminder: the proportionality constant ϵ0 is known as the permit- tivity and encodes how strongly the electromagnetic ﬁeld reacts to the presence of charges. With this in mind, we can say that the differential form of Gauss’s law for the electric ﬁeld tells us: The structure of the electric ﬁeld generated by electric charges is such that it converges upon negative charges and diverges from positive charges. We can also recast this statement in pictorial form: Formulated differently, a positive charge implies a positive divergence of the electric ﬁeld. This, in turn, means that the electric ﬁeld vectors point away from the location of the positive charge. A negative charge implies a negative divergence of the electric ﬁeld. Hence, the electric ﬁeld vectors point towards the location of the negative charge. Another important aspect of the differential form of Gauss’s law is that it tells us that whenever the divergence of the electric ﬁeld is non-zero, there must necessarily be a non-zero electric charge. This means that as soon as we ﬁnd the particular struc- fundamental equations 67 ture in the electric ﬁeld associated with a non-zero divergence, we know immediately that there is non-zero electric charge. Figure 3.1: By gradually scanning the neighborhood of a given charge with different surfaces, we can deduce the electric ﬁeld conﬁgura- tion. An important observation is that we always get the same total ﬂux for all surfaces around a given charge distribution. This means that we have smaller arrows if we are farther away from the charges since the same amount of ﬂux must be distributed along a larger surface. With this in mind, we can say that the crucial difference be- tween the integral and the differential forms of our equations is that the latter make statements about the ﬁeld structure and charge distribution at individual points. In contrast, the integral form always makes statements about the structure of the electric ﬁeld on complete surfaces and on the total charge within some volume. However, take note that the interpretations of the two forms of Gauss’s law are, of course, not really independent. For the integral form, we can imagine that our volume gets smaller and smaller until it only represents the neighborhood of a single point. We can then make our volume gradually larger and scan the complete neighborhood of a given charge. At each step, we can use the integral form of Gauss’s law to calculate the ﬂux of the electric ﬁeld. For a positive charge we ﬁnd a positive ﬂux. For a negative charge, we ﬁnd a negative ﬂux. A positive ﬂux tells us that the vectors of the electric ﬁeld point away from the single point where we started. A negative ﬂux tells us that the vectors point towards this point. In this sense, the integral form also contains the statement that the ﬁeld structure converges upon negative charges and diverges from positive charges. But then why do we write our equations in two different ways? Well, both the integral form and the differential form have im- portant advantages depending on the problem at hand. The integral form is especially useful for macroscopic problems which posses a high degree of symmetry, e.g. when there is a rotationally symmetric charge distribution.32 The differential 32 We will see this explicitly in Chapter 4.form is useful, for example, when we want to talk about elec- tromagnetic waves.33 Moreover, the differential form is often 33 Electromagnetic waves are partic- ular structures in the electromag- netic ﬁeld which can travel without any charges nearby. We will talk about such waves in Chapter 5. more useful when we want to evaluate Maxwell’s equations numerically for a speciﬁc problem and also for fundamental considerations like in the context of special relativity. 68 no-nonsense electrodynamics Now, let’s move on to the second Maxwell equation which is really similar to the ﬁrst one. fundamental equations 69 3.4 Gauss’s law for the magnetic ﬁeld If Gauss’s law for the magnetic ﬁeld were completely analo- gous to Gauss’s electric law discussed in the previous section, it would read34 34 Reminder: Gauss’s law for the electric ﬁeld (Eq. 3.14) reads ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV . ∮ S ⃗B · ⃗dS = 1 ˜ϵ0 ∫ V ˜ρdV , (3.17) where ˜ϵ0 is some new proportionality constant and ˜ρ the mag- netic charge density. However, so far no non-zero magnetic charge density has ever been observed.35 Since no magnetic 35 Formulated differently, no mag- netic monopole has ever been observed. Magnetic monopoles would act as a source for magnetic ﬁeld, analogous to how electric charges generate non-zero electric ﬁeld strengths. Even today, quite a few physicist believe in the exis- tence of magnetic monopoles since they appear inevitably in so-called Grand Uniﬁed Theories and, in addition, would help to explain why electric charge is quantized. However, the modern point of view is that magnetic monopoles, if they exist, are not point charges like electric charges, but rather special extended conﬁgurations of the electromagnetic ﬁeld. These special conﬁgurations are stable thanks to their special (topological) structure and happen to act from a distance exactly like a magnetic monopole. charge has ever been observed, for all practical purposes we have ˜ρ = 0 and the integral form of Gauss’s law for the mag- netic ﬁeld reads ∮ S ⃗B · ⃗dS = 0 , (3.18) Completely analogous to what we discussed for Gauss’s law for the electric ﬁeld in the previous section, we have on the left-hand side the ﬂux of the magnetic ﬁeld through the closed surface S. So in words, it tells us The ﬂux of the magnetic ﬁeld passing through any closed surface is zero. The statement \"no magnetic charge has ever been observed\" can be quite confusing since where then does a non-zero magnetic ﬁeld strength come from? The short answer is that a non-zero magnetic ﬁeld strength can be generated by moving electric charges and also by a changing electric ﬁeld. We will discuss this in more detail in the following sections. If we could produce a non-zero magnetic charge density, we would have to replace Eq. 3.18 with Eq. 3.17, which in words 70 no-nonsense electrodynamics tells us: The ﬂux of the magnetic ﬁeld passing through a closed surface is directly proportional to the amount of magnetic charge contained inside the surface. Technically, this statement is true in general, since it also holds for a zero magnetic charge, which is exactly the statement from above. Formulated differently, if a non-zero magnetic charge density would exist, there would be a non-zero ﬂux of the magnetic ﬁeld through a closed surface. However, since no non-zero mag- netic charge density has ever been observed, the ﬂux through a closed surface happens to be always zero. Next, completely analogous to what we did in the previous two sections, we can derive the differential form of Gauss’s law for the magnetic ﬁeld. We start again with Gauss’s law in integral form (Eq. 3.18) and then use Gauss’s theorem36 to transform the surface integral 36 As already mentioned several times above, Gauss’s theorem is explained in Appendix A.13. into a volume integral:37 37 Take note that this trick is nec- essary since we cannot make a statement about the integrand without it. Especially take note that we cannot conclude ⃗B = 0 since we have in the integral form the projection of the magnetic ﬁeld onto the surface as the integrand. This is what the scalar product ⃗B · ⃗dS encodes. ∮ S ⃗B · ⃗dS = 0 this is Eq. 3.18↷Gauss’s theorem ∴ ∫ V ∇ · ⃗BdV = 0 . (3.19) We can therefore conclude38 38 Take note that this is the form of the third Maxwell equation, as given in Eq. 1.4. ∇ · ⃗B = 0 . (3.20) This is the differential form of Gauss’s law for magnetic ﬁeld. In words, this form of Gauss’s law tells us The divergence of the magnetic ﬁeld is always zero. Therefore, once more the differential form makes an explicit statement about conﬁgurations at speciﬁc locations while the in- tegral form makes a statement about conﬁgurations on surfaces. fundamental equations 71 We have on the left-hand side the divergence of the magnetic ﬁeld ∇ · ⃗B.39 The divergence of a vector ﬁeld describes how the 39 Reminder: the differential form of Gauss’s law for electric ﬁeld reads ∇ · ⃗E = ρ ϵ0 . ﬁeld ﬂows towards or away from a given point.40 40 This is discussed in detail in Appendix A.11. Since the right-hand side of our equation is zero, we can con- clude that the tendency of the magnetic ﬁeld to ﬂow towards a point P is always exactly equal to its tendency to ﬂow away from P. Any \"inﬂow\" of the magnetic ﬁeld is always accompa- nied by an \"outﬂow\" of exactly the same magnitude.41 41 Once more it is important to keep in mind that nothing is really ﬂowing when we speak about the ﬂow or ﬂux of the electric and magnetic ﬁeld. This language is only convenient to get some intuitive feeling for how the vectors that make up the vector ﬁeld are arranged. Here’s an example of a non-trivial ﬁeld conﬁguration where this is indeed the case:42 42 Of course, the statement \"in- ﬂow\"=\"outﬂow\" is trivially true when the vectors have zero length, i.e. for a vanishing magnetic ﬁeld strengths. A net non-zero ﬂow in either direction would only be possible with an isolated magnetic charge, analogous to how the diver- gence of an electric ﬁeld can be zero if there is an electric charge present. Formulated differently, since there are no sources or sinks for the magnetic ﬁeld (i.e. magnetic monopoles) at each point the 72 no-nonsense electrodynamics amount of magnetic ﬁeld ﬂux entering a point must be exactly equal to the amount of magnetic ﬁeld ﬂux leaving the point.4343 As we discuss in Appendix A.11, the divergence is a scalar ﬁeld which assigns a number to each point in space. A non-zero diver- gence always indicates that there is a source or sink for the corre- sponding vector ﬁeld. A vector ﬁeld with divergence zero is known as a solenoidal vector ﬁeld. Once more, before we move on, a few comments on what we just learned: ◃ You probably wonder why people talk about magnets but above, we argued that there is no magnetic charge. We can, of course, produce a non-zero magnetic ﬁeld strength in systems. Objects which generate a non-zero magnetic ﬁeld strength are what we call magnets. However, the structure of the magnetic ﬁeld generated this way is quite different from the structure that a real fundamental magnetic charge would produce. We will see in Section 3.6 that we can generate a non-zero magnetic ﬁeld strength using a varying electric ﬁeld or by using moving electric charges. The crucial difference between the structure of the ﬁeld generated by a fundamental charge and the structure generated by another ﬁeld or mov- ing electric charges is visible in the divergence of the vector ﬁeld. If the divergence is non-zero, the structure was cre- ated by a fundamental charge. If the divergence is zero, the structure was created by a different ﬁeld (or a current).44 44 Roughly think: \"circulating\" structure OO \u000f\u000f divergence zero OO \u000f\u000f produced by ﬁeld (or current) and \"radial\" structureOO \u000f\u000f divergence non-zero OO \u000f\u000f produced by charge. All this is described by the Ampere- Maxwell law, which we will discuss in Section 3.6. The non-zero magnetic ﬁeld strengths generated by conven- tional magnets are generated by the billions of individual electrons which exist inside any macroscopic object. Each electron orbits around a nucleus and possesses a non-zero in- ternal angular momentum.45 As a result, we have billions of 45 This internal angular momentum is known as spin. Naively, you can imagine that each electron rotates like a little spinning top and there is no way to make them stop. The spin of each elementary particle is a deﬁning and unchangeable feature of it, analogous to the labels: electric charge and mass. tiny currents inside any macroscopic object and each of these currents generates a tiny non-zero magnetic ﬁeld strength. But this happens in any object and for any material. The spe- cial thing about magnets is that in these objects the majority of electrons rotate around approximately the same axis. This way the hundreds of tiny magnetic ﬁeld strengths add up to a measurable macroscopic magnetic ﬁeld strength. fundamental equations 73 In contrast, in a normal object, the electrons circulate around random axes and the tiny magnetic ﬁeld strengths average out to zero.46 46 Take note that usually there is no non-zero macroscopic electric ﬁeld strength due to the tiny electric ﬁeld strengths of the electrons since they are canceled through the electric ﬁeld strengths of the protons. ◃ Take note that we can also achieve a non-zero electric ﬁeld with zero divergence. This is possible since a non-zero electric ﬁeld strength can not only be generated by elec- tric charges but also by a changing magnetic ﬁeld. This is described by Faraday’s law, which is the topic of the next section. The structure of the electric ﬁeld generated this way is characterized by a vanishing divergence. This means that the structure of the electric ﬁeld generated by a changing magnetic ﬁeld circulates back on itself, while the structure generated by an electric charge has a non-vanishing diver- gence and does not circulate back on itself. 74 no-nonsense electrodynamics With this in mind, let’s move on to the third Maxwell equa- tion which describes the interplay between the electric and the magnetic ﬁeld. 3.5 Faraday’s law As already mentioned in the previous section, non-zero ﬁeld strengths can not only be produced by charges but also by other ﬁelds. For example, a non-zero electric ﬁeld strength can be generated when there is a changing magnetic ﬁeld. How can we put the idea changing magnetic ﬁeld → nontrivial electric ﬁeld into a mathematical form? We want an equation with the changing magnetic ﬁeld on one side and the resulting electric ﬁeld conﬁguration on the other side. Therefore, ﬁrst of all we need to understand how we can en- code that a magnetic ﬁeld is changing. So far, when our ﬁelds appeared in the integral form of an equation, they appeared in the form of a surface integral (or volume integral after we used Gauss’s theorem). For example, in Gauss’s law for the magnetic ﬁeld we have ∮ S ⃗B · ⃗dS on the left-hand side.4747 Recall that such a surface integral describes the ﬂux of the corre- sponding vector ﬁeld through the surface S. Hence, a good ﬁrst guess is to describe our magnetic ﬁeld using a surface integral again. However, there are two things that we need to take care of. ◃ We want a description in which a non-zero electric ﬁeld strength shows up when the magnetic ﬁeld is changing. The correct mathematical notion for this task are time-derivatives. If the time-derivative of the magnetic ﬁeld is non-zero, this means that the magnetic ﬁeld is changing over time. Hence, our ﬁrst puzzle piece is ∮ S ∂ ∂t ⃗B · ⃗dS. fundamental equations 75 ◃ In the previous two sections our surface was always a closed surface since it represented the surface of a speciﬁc volume. However, the ﬂux of the magnetic ﬁeld through a closed surface is always zero and hence cannot be used here. This is what Gauss’s law for the magnetic ﬁeld tells us.48 But we 48 Just imagine that we put the time-derivative in front of the integral. Then we simply get the time-derivative of the ﬂux. But the ﬂux of the magnetic ﬁeld is zero for any closed surface. can try to write down an equation for the time-derivative of the magnetic ﬂux through an arbitrary (not necessarily closed) surface S: . . . = ∫ S ∂ ∂t ⃗B · ⃗dS . (3.21) There is no reason why this should vanish in general. The second puzzle piece that we need is something which de- scribes the structure of the electric ﬁeld that results from such a changing magnetic ﬁeld. A ﬁrst hint is that the left-hand side must depend on our choice of the surface S too. However, a volume integral over ⃗E is not a good idea since then S would represent the surface of the volume. As a result S would be a closed surface and, as men- tioned above, Gauss’s law for the magnetic ﬁeld tells us that the integral over the magnetic ﬁeld for any closed surface vanishes. But maybe this time we can use the boundary of the surface S instead? The boundary of a surface is simply its contour:49 49 It is conventional to denote the boundary of a given geometrical object G by δG. For example, the boundary of a volume V is δV and the boundary of some surface S is δS. With this in mind, we try the contour integral over the electric ﬁeld ∮ C ⃗E · ⃗dl on the left-hand side, where C denotes the contour 76 no-nonsense electrodynamics of the surface S: ∮ C ⃗E · ⃗dl = ∫ S ∂ ∂t ⃗B · ⃗dS . (3.22) This is almost correct. Only a minus sign is missing. Correctly, Faraday’s law in integral form reads ∮ C ⃗E · ⃗dl = − ∫ S ∂ ∂t ⃗B · ⃗dS . (3.23) The contour integral on the left-hand side describes the circula- tion of the electric ﬁeld.50 Therefore, the relative sign between 50 The circulation of a vector ﬁeld is discussed in more detail in Appendix A.7 and also below. the right-hand and left-hand side encodes in which direction the resulting electric ﬁeld circulates.51 The minus sign is an 51 Again, nothing is really circulat- ing. This is only a convenient way to talk about the structure of the vectors of the electric ﬁeld. How- ever, if there really are electrons present in the system, they will indeed start circulating. empirical result known as Lenz’s law. In some sense, Lenz’s law is not very surprising since it simply encodes the usual wisdom that Nature is lazy.52 We will see in 52 \"Nature is lazy\" is often a helpful guiding principles in physics. For example, it allows us to understand why the Lagrangian formalism works. See, for example, Jennifer Coopersmith. The lazy universe : an introduction to the principle of least action. Oxford University Press, Oxford New York, NY, 2017. ISBN 9780198743040 the next section that an electric current generates a magnetic ﬁeld. Thus, if there are electrons at the location of the contour they will start circulating and this, in turn, will generate a spe- ciﬁc structure in the magnetic ﬁeld. The direction in which they circulate determines if they make it easier for the magnetic ﬁeld to change or if they make it harder. The relative minus sign tells us that they make it harder, i.e. they generate a magnetic ﬁeld structure which tries to stop the original change in the magnetic ﬁeld. So when the magnetic ﬂux through the surface is increas- ing, the electrons circulate in such a way that they generate an additional magnetic ﬂux in the opposite direction of the original ﬂux. In other words, nature tries to make sure that everything stays as it is and actively works against changes e.g., in the magnetic ﬁeld.53 53 However, take note that the change in the magnetic ﬁeld is only made harder if there are really electrons. If no electrons are present, Lenz’s law only tells us that the direction in which the electric ﬁeld circulates is such that the corresponding change in the magnetic ﬂux would get harder if there were electrons. To summarize, Faraday’s law tells us A changing magnetic ﬁeld ﬂux generates a circulating structure in the electric ﬁeld. fundamental equations 77 In the previous sections we always dealt with surface inte- grals which could be interpreted as the ﬂux of our ﬁelds. Now, what’s the physical meaning of a contour integral like ∮ C ⃗E · ⃗dl?54 54 As mentioned above, in somewhat abstract terms such a contour integral describes the circulation of the ﬁeld. This point of view is discussed in Appendix A.7. We can understand this circulation of the electric ﬁeld by re- calling how the electric ﬁeld and the resulting force on a test charge are related. The connection between the two is given by the Lorentz force law (Eq. 3.12) ⃗F = q⃗E . (3.24) Using this, we can calculate ∮ C ⃗E · ⃗dl = ∮ C ⃗F q · ⃗dl this is Eq. 3.24 integrated↷the charge is a constant = 1 q ∮ C ⃗F · ⃗dl↷ ≡ W q , (3.25) where W denotes the work done by the electric ﬁeld if we move a test charge q along the path C. Therefore, we can conclude that the circulation of the electric ﬁeld represents the energy for each unit of charge moving along the contour C.55 55 An analogous statement is true for the circulation of the magnetic ﬁeld which will be important in the next section.Next, analogous to what we did in the previous sections, we will derive the differential form of Faraday’s law. This time, the crucial trick is not Gauss’s theorem but a closely related trick known as Stokes’ theorem. In the previous sections, we always had a surface integral which we wanted to turn into a volume integral and this is exactly what Gauss’s theorem allows us to do. Now, we have on the 78 no-nonsense electrodynamics left-hand side a contour integral and on the right-hand side a surface integral. Therefore, we need a trick that allows us to convert a contour integral into a surface integral and this is exactly what Stokes’ theorem allows us to do.56 Stokes’ theorem 56 As a reminder: we want the same kind of integral on both sides of the equation since this allows us to write the equation without any integral. tells us57 57 Stokes’ theorem is discussed in Appendix A.15. ∮ C ⃗E · ⃗dl = ∫ S ∇ × ⃗E · ⃗dS (3.26) for any vector ﬁeld ⃗E and therefore also for our electric ﬁeld. S denotes a surface for which the contour C is a boundary. Using this theorem, we can derive the differential form of Fara- day’s law: ∮ C ⃗E · ⃗dl = − ∫ S ∂ ∂t ⃗B · ⃗dS this is Faraday’s law, Eq. 3.23↷Stokes’ theorem ∴ ∫ S ∇ × ⃗E · ⃗dS = − ∫ S ∂ ∂t ⃗B · ⃗dS . Since this equation holds for any surface S, we can conclude5858 Take note that this is the form of the fourth Maxwell equation, as given in Eq. 1.4. ∇ × ⃗E = − ∂ ∂t ⃗B . (3.27) This is the differential form of Faraday’s law. The object ∇ × ⃗E appearing on the left-hand side is the curl of the electric ﬁeld. In general, a curl always tells us about the tendency of a vector ﬁeld to circulate around a given point.5959 The curl of vector ﬁelds is dis- cussed in general in Appendix A.12. Therefore, the differential form of Faraday’s law tells us in words6060 In contrast, the two Maxwell equations we discussed previously involved the divergence of the ﬁelds and describe how radial structures emerge in them. A changing magnetic ﬁeld generates a non-zero curl of the electric ﬁeld. Next, we are ﬁnally ready to discuss the ﬁnal Maxwell equation. fundamental equations 79 3.6 The Ampere-Maxwell law The Ampere-Maxwell law is quite similar to Faraday’s law but the roles of the electric and magnetic ﬁelds are switched. This means that the Ampere-Maxwell law describes how a changing electric ﬁeld generates a non-zero magnetic ﬁeld strength.61 If it 61 Reminder: Faraday’s law de- scribes how a changing magnetic ﬁeld generates a non-zero electric ﬁeld strength and reads (Eq. 3.23) ∮ C ⃗E · ⃗dl = − ∫ S ∂ ∂t ⃗B · ⃗dS . were completely analogous to Faraday’s law it would read ∮ C ⃗B · ⃗dl = −const. × ∫ S ∂ ∂t ⃗E · ⃗dS . (3.28) While this equation is correct for some systems, there is an additional effect that we need to take into account. In the previous sections, we have learned that a non-zero elec- tric ﬁeld strength can be generated through a non-zero charge density or a changing magnetic ﬁeld. Now, Eq. 3.28 tells us that a non-zero magnetic ﬁeld strength can be generated through a changing electric ﬁeld. Is there any other way to generate a non-zero magnetic ﬁeld strength? It turns out that we can also generate a non-zero magnetic ﬁeld strength using moving electric charges:62 62 In fact, the electric current I and the current density ⃗J are the only ingredients we haven’t used so far in Maxwell’s equations. Adding the current to Eq. 3.28 yields ∮ C ⃗B · ⃗dl = µ0 (Ienc + ϵ0 ∫ S ∂ ∂t ⃗E · ⃗dS) . (3.29) This is the integral form of the Ampere-Maxwell law. µ0 and ϵ0 are proportionality constants known as vacuum permeability 80 no-nonsense electrodynamics and vacuum permittivity.63 Moreover, Ienc denotes the electric 63 Take note that µ0ϵ0 = 1 c2 , where c denotes the speed of light. We will discuss this in detail in Appendix 6. current enclosed by the contour C. On the left-hand side we have a contour integral which de- scribes the circulation of the ﬁeld ⃗B along the contour. On the right-hand side, we have the current enclosed by the contour and the ﬂux of the electric ﬁeld through a surface S of which the contour C is the boundary. Therefore, in words the Ampere-Maxwell law tells us6464 Of course, there are systems where we have both a changing electric ﬁeld and a non-zero current. In this case, the effects of the two either cancel or amplify each other. A changing electric ﬁeld ﬂux or a non-zero electric current generates a circulating structure in the magnetic ﬁeld. So while the Ampere-Maxwell law may seem intimidating at ﬁrst glance, the basic message is quite simple. Most importantly, we can see that its structure is completely analogous to the structure of the other Maxwell equations. Finally, analogous to what we did in the previous sections, we can derive the differential form of the Ampere-Maxwell law. The crucial trick is exactly the same as in the previous section (Stokes’ theorem) since again, we want to transform a contour integral into a surface integral: ∮ C ⃗B · ⃗dl = ∫ S ∇ × ⃗B · ⃗dS . (3.30) With this trick in mind, we can derive the differential form of fundamental equations 81 the Ampere-Maxwell law immediately: ∮ C ⃗B · ⃗dl = µ0 (Ienc + ϵ0 ∫ S ∂ ∂t ⃗E · ⃗dS) this is Eq. 3.29↷Stokes’ theorem ∴ ∫ S ∇ × ⃗B · ⃗dS = µ0 (Ienc + ϵ0 ∫ S ∂ ∂t ⃗E · ⃗dS)↷def. of ⃗J in Eq. 2.14 ∴ ∫ S ∇ × ⃗B · ⃗dS = µ0 (∫ S ⃗J · ⃗dS + ϵ0 ∫ S ∂ ∂t ⃗E · ⃗dS)↷ ∴ ∫ S ∇ × ⃗B · ⃗dS = µ0 ∫ S ( ⃗J + ϵ0 ∂ ∂t ⃗E) · ⃗dS . (3.31) Again, this equation holds for any surface S and we can there- fore conclude ∇ × ⃗B = µ0 ( ⃗J + ϵ0 ∂ ∂t ⃗E) . (3.32) This is the differential form of the Ampere-Maxwell law. In words, it tells us65 65 Again, of course, there are sys- tems where we have both a chang- ing electric ﬁeld and a non-zero current density. In this case, the effects of the two either cancel or amplify each other. A changing electric ﬁeld or a non-zero current density generates a non-zero curl in the magnetic ﬁeld. Before we move on and discuss applications of Maxwell’s equa- tions, there are two additional equations we should talk about: the wave equations of electrodynamics. In some sense, they are not as fundamental as Maxwell’s equations since we can derive them by starting with Maxwell’s equations. However, they re- veal an important feature of electrodynamics which is otherwise somewhat hidden.66 66 The same is true for the continu- ity equation. We can derive it using Maxwell’s equations but it is still an important equation because it makes the conservation of electric charge completely transparent. The feature I’m talking about is that there can be electromag- netic waves. We have seen in the previous sections that a chang- ing magnetic ﬁeld can generate a non-zero electric ﬁeld strength and vice versa. The structure which is generated in the electric ﬁeld this way is itself changing over time and therefore inﬂu- ences the magnetic ﬁeld: changing ⃗B // changing ⃗E // changing ⃗B // . . . 82 no-nonsense electrodynamics This way, nontrivial electric and magnetic ﬁeld conﬁgurations can travel long distances and this is what we call an electromag- netic wave. No messenger, no medium, no electric charges are needed. An electromagnetic wave can propagate completely on its own, even through a vacuum. The most famous example of this phenomena is sunlight which travels from the sun to the earth. The equations which describe electromagnetic waves are the topic of our ﬁnal section in this chapter. 3.7 The wave equations So far, we have no equation that tells us how our electric and magnetic ﬁelds evolve as time passes. In other words, we have no equation of motion for our ﬁelds ⃗E and ⃗B. Such an equation of motion has a time derivative on one side and other terms, possibly with spatial derivatives, containing the same ﬁeld on the other side. In this section, we will combine Maxwell’s equation to derive exactly such equations.67 67 As already mentioned above, for ⃗E and ⃗B these equations of motion are known as wave equations. This name comes about since solutions of these equations look and behave like waves. We will talk more about this in Chapter 5. To derive the equation of motion for the electric ﬁeld ⃗E, we start by taking the curl on both sides of Faraday’s law6868 In the last step we use that partial derivatives are symmetric: ∂ ∂x ∂ ∂t = ∂ ∂t ∂ ∂x . This is known as Schwarz’s theorem and is one of the basic results in calculus. ∇ × ⃗E = − ∂ ∂t ⃗B Faraday’s law, Eq. 3.27↷taking the curl on both sides ∴ ∇ × ∇ × ⃗E = −∇ × ∂ ∂t ⃗B↷partial deriviatives commute ∴ ∇ × ∇ × ⃗E = − ∂ ∂t ∇ × ⃗B . (3.33) fundamental equations 83 Next, we can rewrite the left-hand side using the vector iden- tity69 69 This is discussed in Ap- pendix A.16. Here ∇2 is known as the Laplacian operator ∇2⃗E =      ( ∂2 ∂x2 + ∂2 ∂y2 + ∂2 ∂z2 ) Ex ( ∂2 ∂x2 + ∂2 ∂y2 + ∂2 ∂z2 ) Ey ( ∂2 ∂x2 + ∂2 ∂y2 + ∂2 ∂z2 ) Ez      . In contrast ∇(∇ · ⃗E) = ∇ ( ∂Ex ∂x + ∂Ey ∂y + ∂E ∂z ) =      ∂x ( ∂Ex ∂x + ∂Ey ∂y + ∂E ∂z ) ∂y ( ∂Ex ∂x + ∂Ey ∂y + ∂E ∂z ) ∂z ( ∂Ex ∂x + ∂Ey ∂y + ∂E ∂z )      ̸= ∇2⃗E . (3.34) ∇ × ∇ × ⃗E = ∇(∇ · ⃗E) − ∇2⃗E (3.35) which holds for any vector and therefore also for our electric vector ﬁeld ⃗E. In addition, on the right-hand side we can use the Ampere-Maxwell law70 to get an equation which only de- 70 Reminder: the Ampere-Maxwell law reads ∇ × ⃗B = µ0 ( ⃗J + ϵ0 ∂ ∂t ⃗E) pends on the electric ﬁeld: ∇ × ∇ × ⃗E = − ∂ ∂t ∇ × ⃗B this is Eq. 3.33↷Eq. 3.35 ∇(∇ · ⃗E) − ∇2⃗E = − ∂ ∂t ∇ × ⃗B↷Eq. 3.32 ∇(∇ · ⃗E) − ∇2⃗E = − ∂ ∂t (µ0 ( ⃗J + ϵ0 ∂ ∂t ⃗E)) . (3.36) We can simplify this further using Gauss’s law for the electric ﬁeld71 and by restricting ourselves to systems with no charge 71 Reminder: Gauss’s law for the electric ﬁeld reads ∇ · ⃗E = ρ ϵ0 . density and no current present (ρ = 0 and ⃗J = 0) ∇(∇ · ⃗E) − ∇2⃗E = − ∂ ∂t (µ0 ( ⃗J + ϵ0 ∂ ∂t ⃗E)) Eq. 3.36↷Eq. 3.16 ∇( ρ ϵ0 ) − ∇2⃗E = − ∂ ∂t (µ0 ( ⃗J + ϵ0 ∂ ∂t ⃗E))↷ρ = 0,⃗J = 0 −∇2⃗E = −µ0ϵ0 ∂2 ∂t2 ⃗E . (3.37) Therefore, we can conclude ∇2⃗E = µ0ϵ0 ∂2 ∂t2 ⃗E . (3.38) This is the wave equation for the electric ﬁeld. Starting with the Ampere-Maxwell law and then following completely analogous steps yields ∇2⃗B = µ0ϵ0 ∂2 ∂t2 ⃗B . (3.39) This is the wave equation for the magnetic ﬁeld. 84 no-nonsense electrodynamics Before we wrap up and summarize what we have learned in this chapter, two short comments on these wave equations. ◃ In Chapter 5, we will discuss solutions of these wave equa- tions explicitly. The main result will be that these solutions really look and behave like waves, which is why we call these equations wave equations. ◃ At ﬁrst it may seem strange that we get two separate equa- tions for the electric and the magnetic ﬁeld. This can be espe- cially confusing if you recall that at the end of the last section I argued that an electromagnetic wave propagates through the interplay of the electric and magnetic ﬁeld. However, the electric and magnetic ﬁelds always inﬂuence each other, even if we have here two seemingly independent equations since Maxwell’s equations are always valid. Hence, a speciﬁc so- lution of the wave equation for the electric ﬁeld automatically implies a corresponding solution of the wave equation for the magnetic ﬁeld. In this sense, the two equations only appear to be independent. But in fact, they are directly connected via Maxwell’s equations. We will discuss this explicitly in Section 5.2. Now it’s time to summarize what we have learned in this chap- ter. 3.8 Summary The ﬁrst equation we talked about is the continuity equation in integral form (Eq. 3.6) and differential form (Eq. 3.8) − ∂ ∂t ∫ V ρdV = ∫ S ⃗J · ⃗dS Gauss’s =⇒ theorem ∂ ∂t ρ + ∇ · ⃗J = 0 . This equation tells us that whenever the amount of electric charge in some region changes, exactly the missing or addi- fundamental equations 85 tional amount of electric charge must have passed the bound- ary of the region. Formulated differently, when the amount of electric charge in some region gets larger over time, the conti- nuity equation tells us that there must be a net inﬂow of electric charge. Conversely, when the amount of electric charge gets smaller, there must be a net outﬂow. This is necessarily the case because electric charge is conserved, i.e. cannot be produced or destroyed. Next, we talked about the Lorentz force law (Eq. 3.12) ⃗F = q(⃗E + ⃗v × ⃗B) . (3.40) This equation describes how electrically charged objects react to the presence of a non-zero electric or magnetic ﬁeld strength. It tells us that a positively charged object gets pushed in the direction of the electric ﬁeld. A negatively charged object gets pushed in the opposite direction. In addition, the Lorentz force law tells us that the magnetic ﬁeld only inﬂuences moving charged objects. The force resulting from 86 no-nonsense electrodynamics a non-zero magnetic ﬁeld strength is perpendicular to both the direction of the magnetic ﬁeld ⃗B and the direction in which the object moves ⃗v. Afterwards, we started talking about Maxwell’s equations. They describe how the electric and magnetic ﬁelds react to the presence of charged objects and how they inﬂuence each other. We started with Gauss’s law for the electric ﬁeld and talked about its integral form (Eq. 3.14) and its differential form (Eq. 3.16) ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV Gauss’s =⇒ theorem ∇ · ⃗E = ρ ϵ0 . This equation tells us that the electric ﬁeld strength around some region is directly proportional to the amount of charge contained within the region. Moreover, it tells us that the electric ﬁeld points radially away from any positively charged object and radially towards any negatively charged object.7272 This is non-trivial since, in princi- ple, it could also ﬂow, for example, circularly around any given charge. fundamental equations 87 The second Maxwell equation we talked about was Gauss’s law for the magnetic ﬁeld (Eq. 3.18, Eq. 3.20) ∮ S ⃗B · ⃗dS = 0 Gauss’s =⇒ theorem ∇ · ⃗B = 0 . In words, it tells us that the magnetic ﬁeld never points radially towards or away from a single point since there are no magnetic monopoles. Then we talked about the two most complicated Maxwell equa- tions which both describe how the electric and magnetic ﬁelds inﬂuence each other. In particular, Faraday’s law (Eq. 3.23, Eq. 3.27) ∮ C ⃗E · ⃗dl = − ∫ S ∂ ∂t ⃗B · ⃗dS . Stokes’ =⇒ theorem ∇ × ⃗E = − ∂ ∂t ⃗B tells us that a changing magnetic ﬁeld generates a circulating electric ﬁeld conﬁguration: 88 no-nonsense electrodynamics Analogously, the Ampere-Maxwell law (Eq. 3.29, Eq. 3.32) ∮ C ⃗B · ⃗dl = µ0 ∫ S ( ⃗J + ϵ0 ∂ ∂t ⃗E) · ⃗dS Stokes’ =⇒ theorem ∇ × ⃗B = µ0 ( ⃗J + ϵ0 ∂ ∂t ⃗E) tells us that a changing electric ﬁeld generates a circulating magnetic ﬁeld conﬁguration: In addition, it tells us that moving electric charges (a non-zero current density ⃗J) also generate a circulating magnetic ﬁeld: So in summary, Maxwell’s equations tell us that charged objects generate speciﬁc structures in the electric and magnetic ﬁeld:73 73 We are oversimplifying a bit here. As we will discuss in the next chapter, only steady currents lead to a static circulating magnetic ﬁeld. A single moving charged object leads to a time-dependent magnetic ﬁeld conﬁguration. fundamental equations 89 static charged object → static radial electric ﬁeld moving charged object → static circulating magnetic ﬁeld . Moreover, they tell us that we can also generate different struc- tures by using the ﬁelds themselves changing magnetic ﬁeld → changing circulating electric ﬁeld changing electric ﬁeld → changing circulating magnetic ﬁeld . The Lorentz force law then tells us how our charged objects react if such structures are present in our system. In addition, we have learned that the interplay between the elec- tric ﬁeld and the magnetic ﬁeld means that there can be electro- magnetic waves. These are described by the wave equation for the electric ﬁeld (Eq. 3.38) ∇2⃗E = µ0ϵ0 ∂2 ∂t2 ⃗E and the wave equation for the magnetic ﬁeld (Eq. 3.39) ∇2⃗B = µ0ϵ0 ∂2 ∂t2 ⃗B . Now it’s time to see how all this works in practice. In the fol- lowing sections, we will talk about the most important electro- dynamical systems and how we can describe them using the tools that we talked about in the previous two chapters. 90 no-nonsense electrodynamics Part II Essential Electrodynamical Systems and Tools \"It’s of no use whatsoever. This is just an experiment that proves Maestro Maxwell was right—we just have these mysterious electromagnetic waves that we cannot see with the naked eye. But they are there.\" Heinrich Hertz PS: You can discuss the content of Part II with other readers and give feedback at www.nononsensebooks.com/edyn/part2. fundamental equations 93 In the following sections, we’ll discuss how we can use Maxwell’s equations to describe concrete systems. An important observa- tion is that in many systems, the electric and magnetic ﬁelds are static. This means that they don’t change as time passes. Ev- erything is much easier when our ﬁelds are static and therefore we will start by discussing such electrostatic and magnetostatic systems. Afterwards, we will talk about what happens when our ﬁelds change dynamically. The most important new feature is that a changing magnetic ﬁeld leads to changing electric ﬁeld and vice versa. We will see that this directly implies there can be electromagnetic waves. An electromagnetic wave is a nontrivial structure in the electric and magnetic ﬁelds which is able to move forward as time passes. Before we start, it is instructive to talk about one general prop- erty of solutions of Maxwell’s equations and the wave equa- tions. An extremely important observation is that as soon as we have found the electric ﬁeld conﬁguration ⃗E1 around a speciﬁc charged object, we can calculate the total electric ﬁeld conﬁg- uration in more general systems in which our charged object appears by using ⃗Esup = a⃗E1 + b⃗E2 , (3.41) where ⃗E2 is the conﬁguration which all other objects in the systems would generate if they were alone. This is known as a superposition of solutions. In other words, the total ﬁeld conﬁgurations generated by various charged objects is simply the sum of the individual solutions. Why is a superposition of solutions also a solution? To see this, let’s assume we have found two solutions ⃗E1, ⃗E2 of the wave equation for the electric ﬁeld (Eq. 3.38).74 Mathemati- 74 The same reasoning is true for any linear equation, i.e. for Maxwell’s equations and the wave equation for the magnetic ﬁeld too. 94 no-nonsense electrodynamics cally, this means that ∇2⃗E1 = µ0ϵ0 ∂2 ∂t2 ⃗E1 ∇2⃗E2 = µ0ϵ0 ∂2 ∂t2 ⃗E2 . (3.42) Putting a superposition of these two solutions (Eq. 3.41) into the wave equation yields ∇2⃗Esup = µ0ϵ0 ∂2 ∂t2 ⃗Esup↷Eq. 3.41 ∴ ∇2(a⃗E1 + b⃗E2) = µ0ϵ0 ∂2 ∂t2 (a⃗E1 + b⃗E2)↷ ∴ a∇2⃗E1 + b∇2⃗E2 = µ0ϵ0a ∂2 ∂t2 ⃗E1 + µ0ϵ0b ∂2 ∂t2 ⃗E2↷Eq. 3.42 ∴ a(µ0ϵ0 ∂2 ∂t2 ⃗E1) + b(µ0ϵ0 ∂2 ∂t2 ⃗E2) = µ0ϵ0a ∂2 ∂t2 ⃗E1 + µ0ϵ0b ∂2 ∂t2 ⃗E2 ✓ So the linear combination ⃗Esup really is also a solution.7575 This only works because there is no term ⃗E2 or ⃗E3 in the wave equation. In other words, as already mentioned above, superpositions are automatically solutions only if the equation is linear. If you don’t believe this, try to do the same calculation with an additional term ⃗E2 or ⃗E3 in the wave equation. This observation is important because it allows us to investigate the implications of simple systems individually. Then we can use these results to get solutions for complicated systems by using appropriate sums of the simple solutions. In other words, we can act as if each object in the systems generates its own ﬁeld and the total observable ﬁeld is then simply the sum of these individual ﬁelds. Using superpositions of solutions always works when the equa- tion we want to solve is linear.76 Formulated differently, when- 76 Luckily, many important equa- tions in physics are indeed linear. A noteworthy exception is the Ein- stein equation in general relativity and the Yang-Mills equations in quantum ﬁeld theory. ever the equation is linear, the sum of two or more solutions yields another solution. With this in mind, we are ready to discuss electrostatics and magnetostatics. 4 Electrostatics and Magneto- statics As in the previous sections, we will start with a birds-eye overview and only afterwards dive into the details. In many important electromagnetic systems, the magnetic and electric ﬁeld do not change over time. This is the case if the charge density and the current density are time-independent. If the charge density is static (∂tρ = 0), the resulting electric ﬁeld conﬁguration is time-independent.1 Analogously, if our 1 Take note that ∂tρ = 0 does not mean that our charges aren’t mov- ing. Instead, it only means that if our charges are moving, the amount that ﬂows in is exactly equal to the amount that ﬂows out. For example, in an ordinary wire we can also have a static charge dis- tribution. In a (perfect) wire there is no net charge transport since all electrons move forward together such that each electron is immedi- ately replaced by another one as soon as it moves away. Otherwise the wire would run out of charges at some point. Formulated differ- ently, our individual charges can move around even though there is no net charge transport. However, take note that we have ρ = 0 in a wire since the positive charge of the protons cancels the charge of the electrons everywhere. Hence, a wire generates a nontrivial mag- netic ﬁeld conﬁguration but has no inﬂuence on the electric ﬁeld. current is steady (∂t⃗J = 0), the resulting magnetic ﬁeld conﬁgu- ration is time-independent. In such systems we can treat the electric ﬁeld and the magnetic ﬁeld independently because only if the electric ﬁeld is changing does it have an effect on the magnetic ﬁeld and vice versa. This is what the Ampere-Maxwell law (Eq. 3.23) and Faraday’s law (Eq. 3.23) tells us. Maxwell’s equations then become two sepa- rate sets of equations. One describes static electric ﬁelds while the second set describes static magnetic ﬁelds. 96 no-nonsense electrodynamics For the electric ﬁeld, we have ∇ · ⃗E = ρ ϵ0 (electric Gauss’ law, Eq. 3.16) ∇ × ⃗E = 0 (Faraday’s law, Eq. 3.27 with ∂t⃗B = 0) . (4.1) These are the fundamental equations of electrostatics. We can use these equations to calculate the electric ﬁeld conﬁguration ⃗E(⃗x) around a given static charge distribution ρ(⃗x). A famous example is the electric ﬁeld conﬁguration around a charged sphere.22 We will discuss the electric ﬁeld conﬁguration around a sphere in Section 4.1.2. Analogously, for the magnetic ﬁeld we have ∇ · ⃗B = 0 (magnetic Gauss’ law, Eq. 3.20) ∇ × ⃗B = µ0⃗J (Ampere-Maxwell law, Eq. 3.32, with ∂t⃗E = 0) , (4.2) which are known as the fundamental equations of magnetostat- ics. We can use these equations to calculate the magnetic ﬁeld conﬁguration ⃗B(⃗x) around steady electric current densities ⃗J(⃗x). A famous example, is the magnetic ﬁeld conﬁguration around a long wire carrying a steady electric current.33 We will discuss the magnetic ﬁeld conﬁguration around a wire in Section 4.2.1. The general solution of the electrostatic equations (Eq. 4.1) reads ⃗E(⃗r) = 1 4πϵ0 ∫ ρ(⃗r′)(⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′ , (4.3) which is known as Coulomb’s law. Here, ⃗r is a vector that points to the point we’re evaluating the ﬁeld at. Moreover, ⃗r′ points to the location of the source of the electric ﬁeld. In words, this solution tells us that the electric ﬁeld conﬁgura- tion generated by a general charge distribution is simply the sum over the ﬁeld conﬁgurations generated by the individual charges. We can see this by writing our charge distribution in terms of individual charges4 4 Reminder: the charge distribution for a single point charge is ρ(⃗x) = qδ(⃗x − ⃗x′) , where ⃗x′ is the location of the charge, q is its electric charge and δ(⃗x − ⃗x′) is the delta distribution. We talked about this in Section 2.2. In addition, the delta distribution is discussed in Appendix C. ρ(⃗r) = ∑ i qiδ(⃗r −⃗ri) . (4.4) electrostatics and magnetostatics 97 Putting this into Eq. 4.3 yields ⃗E(⃗r) = 1 4πϵ0 ∫ ρ(⃗r′)(⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′ this is Coulomb’s law, Eq. 4.3↷Eq. 4.4 = 1 4πϵ0 ∫ ( ∑i qiδ(⃗r′ −⃗ri))(⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′↷∫ δ(x − a) f (x)dx = f (a) = ∑ i qi 4πϵ0 ⃗r −⃗ri |⃗r −⃗ri|3 . (4.5) We can visualize this as follows The ﬁnal puzzle piece we need is that the ﬁeld conﬁguration generated by a single point charge q localized at ⃗ri is5 5 We will derive this in Section 4.1.1. ⃗E(⃗r) = q 4πϵ0 ⃗r −⃗ri |⃗r −⃗ri|3 . Therefore, by looking at Eq. 4.5 again, we can conclude that Coulomb’s law really tells us that the total ﬁeld conﬁguration ⃗E(⃗r) resulting from a general charge distribution ρ(⃗r) is simply given by the sum over the ﬁeld conﬁgurations of the individual charges. Of course, this is not a very surprising result because we know already that superpositions of solutions are solutions too. Therefore, the total electric ﬁeld in any given system can be understood as a sum over the ﬁeld conﬁgurations generated by the individual charges. 98 no-nonsense electrodynamics With this general solution at hand, we can calculate the ex- plicit form of the electric ﬁeld around speciﬁc time-independent charge distributions: system electric ﬁeld conﬁguration point charge q ⃗E = 1 4πε0 q r2⃗er charged sphere Q ⃗E = 1 4πε0 Q r2⃗er charged straight wire (inﬁnitely long with charge density λ) ⃗E = 1 2πε0 λ r⃗er charged plane (inﬁnitely large with charge density σ) ⃗E = σ 2ε0 ˆn dipole (distance d between q and −q) ⃗E(r) = q 4πϵ0|⃗r|2⃗er − q 4πϵ0|⃗r−⃗d|2⃗er This tells us that the electric ﬁeld conﬁgurations around these charges objects look as follows: Analogously, the general solution of the magnetostatics equa- tions (Eq. 4.2) reads ⃗B(⃗r) = µ0 4π ∫ ⃗J(⃗r′) × (⃗r −⃗r′) |⃗r −⃗r′|3 d3r′ , (4.6) electrostatics and magnetostatics 99 which is known as the Biot-Savart law. In words, it tells us that the magnetic ﬁeld conﬁguration gen- erated by a general current density is simply the sum over the ﬁeld conﬁgurations generated by the individual moving charges.6 We can see this by writing our current density in 6 This is completely analogous to what we discussed for Coulomb’s law. terms of the movement of the individual charges7 7 Here, we use the deﬁnition of the charge density (Eq. 2.12): ⃗J(⃗r) ≡ ρ(⃗r)⃗v , which for a single point charge q at ⃗ri reads ⃗J ≡ qδ(⃗r −⃗ri)⃗v . ⃗J(⃗r) = ∑ i qiδ(⃗r −⃗ri)⃗vi , (4.7) where ⃗vi denotes the velocities of the point charges. Putting this into Eq. 4.6 yields ⃗B(⃗r) = µ0 4π ∫ ⃗J(⃗r′) × (⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′ this is the Biot-Savart law, Eq. 4.6↷Eq. 4.7 = µ0 4π ∫ ( ∑i qiδ(⃗r′ −⃗ri)⃗vi) × (⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′↷∫ δ(x − a) f (x)dx = f (a) = ∑ i µ0qi 4π ⃗vi × ⃗r −⃗ri |⃗r −⃗ri|3 . (4.8) We can visualize this as follows The second puzzle piece we need is that the ﬁeld conﬁguration generated by a single moving point charge q at ⃗r′ moving with the velocity ⃗v is8 8 Take note that a single moving point charge is not a magneto- static system since it generates a time-dependent electric ﬁeld conﬁg- uration. However, multiple moving point charges together can yield a magnetostatic system. In other words, only lots of moving charges together can yield a steady cur- rent and therefore a magnetostatic system. 100 no-nonsense electrodynamics ⃗B(⃗r) = µ0q 4π ⃗v × ⃗r −⃗r′ |⃗r −⃗r′|3 . Therefore, by looking at Eq. 4.8 again we can see, as promised above, that the Biot-Savart law simply tells us that the total ﬁeld conﬁguration ⃗B(⃗r) resulting from a general current density ⃗J(⃗r) is given by the sum over the ﬁeld conﬁgurations generated by the movement of the individual charges. With this general solutions at hand, we can calculate the explicit form of the magnetic ﬁeld around speciﬁc time-independent electric currents: system magnetic ﬁeld conﬁguration straight wire (carrying current I) ⃗B = µ0 I 2πr⃗eϕ circular loop (in xy-plane, radius R, carrying current I) ⃗B = µ0 IR2 2(z2+R2)3/2⃗ez solenoid (N turns, length l, carrying current I) ⃗B = µ0 N I l ⃗ex (inside) torus (N turns, radius R, carrying current I) ⃗B = µ0 N I 2πr ⃗eϕ (inside) Therefore, the magnetic ﬁeld conﬁguration around these steady currents look as follows: electrostatics and magnetostatics 101 To summarize: Electrostatics static charge density Maxwell’s equations // ∇ · ⃗E = ρ ϵ0 and ∇ × ⃗E = 0 solved by \u000f\u000f constant electric ﬁeld Coulomb’s lawdescribesoo Magnetostatics steady current Maxwell’s equations // ∇ · ⃗B = 0 and ∇ × ⃗B = µ0⃗J solved by \u000f\u000f constant magnetic ﬁeld Biot-Savart lawdescribesoo With all this in mind, we are ready to dive in and can discuss concrete electrostatic and magnetostatic systems. 4.1 Electrostatic Systems We will start with the simplest system possible: a single point charge. 4.1.1 Electric ﬁeld of a single point charge In this example, we want to ﬁnd the structure of the electric ﬁeld ⃗E which is generated by a single point charge sitting at the origin of our coordinate system ρ(⃗r) = qδ(⃗r).9 Gauss’s law 9 A point charge is an object for which the entire electric charge is localized at a single point. (Eq. 3.14) tells us ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV . (4.9) For simplicity we choose a sphere with radius r around the point charge as our surface S: 102 no-nonsense electrodynamics Since we are considering a single point charge, the integral on the right-hand side simply yields q, the charge of the point charge. For Gauss’s law this means: ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V qδ(⃗r)dV↷∫ S ⃗E · ⃗dS = q ϵ0 ∫ V δ(⃗r)dV↷∫ V δ(⃗r)dV = 1 ∫ S ⃗E · ⃗dS = q ϵ0↷∫ 2π 0 ∫ π 0 ⃗E ·⃗err2 sin(θ) dθ dφ = q ϵ0 , (4.10) where in the last step we switched to spherical coordinates and ⃗er denotes the radial unit vector which is normal to the sphere. Our task is to solve this equation for ⃗E. This is not easy and we need some clever arguments to solve this problem. The crucial idea is that since our point charge is spherically symmetric, the structure of the electric ﬁeld around the point charge will be spherically symmetric too. This is necessarily the case because our point charge does not single out any speciﬁc direction and therefore our electric ﬁeld cannot point in any speciﬁc direction. In other words, since our point charge is spherically symmetric, rotating it cannot have any effect on the electric ﬁeld. If the electric ﬁeld around the point charge were to point in any speciﬁc direction, rotating the electrostatics and magnetostatics 103 point charge would have a measurable effect on the electric ﬁeld and this cannot be the case since the point charge is spherically symmetric.10 10 Take note that the same argument applies for any spherically symmet- ric charge distribution, not only a point charge.Spherical symmetry means that our electric ﬁeld only depends on r, but not on θ and φ. Any dependence on the angles θ and φ would mean immediately that a rotation makes a difference and therefore our electric ﬁeld wouldn’t be rotationally symmetric. So we have ⃗E(r, θ, φ) = E(r)⃗er. In words, this means that our electric ﬁeld vectors all point radially away or towards the point charge but do not, for example, circulate around the surface S. With this in mind, we can evaluate the integral in Eq. 4.10: ∫ 2π 0 ∫ π 0 ⃗E · ⃗nr2 sin(θ) dθ dφ = q ϵ0↷using the explicit structure of ⃗E = E⃗er∫ 2π 0 ∫ π 0 E⃗er ·⃗er︸ ︷︷ ︸ =1 r2 sin(θ) dθ dφ = q ϵ0↷since E does not depend on θ, φ Er2 ∫ 2π 0 ∫ π 0 sin(θ) dθ dφ = q ϵ0↷∫ 2π 0 ∫ π 0 sin(θ) dθ dφ = 4π Er24π = q ϵ0↷ E = q r24πϵ0 . (4.11) This is the electric ﬁeld strength at a distance r to the point charge q. The electric ﬁeld therefore reads ⃗E(r) = q r24πϵ0⃗er (4.12) and looks like this (depending on the sign of the charge q) 104 no-nonsense electrodynamics Now, we can ask: what’s the force that such a single point charge exerts on another charged object? To answer this question, we can use the Lorentz force law: ⃗F = Q⃗E Lorentz force law, Eq. 3.12 with ⃗B = 0↷Eq. 4.12 ⃗F = Q q r24πϵ0⃗er , (4.13) where Q denotes the charge of the second charged object. The ﬁnal equation ⃗FC = Qq r24πϵ0⃗er , (4.14) is known as Coulomb’s law. It describes the force which two charged objects with charges q and Q exert on each other. The force depends inversely on the distance r between the two ob- jects. This means that the force on two charged objects close to each other is much larger than the force on two far away objects.1111 Of course, according to Newton’s third law, there is an equal force on both charges. This means that in our example there is also a force ⃗FC pointing in the opposite direction on the larger object. electrostatics and magnetostatics 105 Moreover, it tells us that the force points along the axis between the two objects. The actual direction depends on the relative signs of the two charges (q and Q). If both have the same sign, the force is positive12 which means the two objects get pushed 12 qQ > 0 for either q > 0 and Q > 0, i.e. if both are positively charged or q < 0 and Q < 0, i.e. if both are negatively charged. away from each other. If the charges of the two objects have opposite signs, the force is negative and this means that the two objects attract each other. The corresponding electric potential13 13 We discussed the electric potential in Section 2.5. The potential here is the correct one which yields the force in Eq. 4.14 if we calculate the gradient −∇φ = ⃗FC, where φ = cA0 and c denotes the speed of light. φ = Qq r4πϵ0 , (4.15) is known as the Coulomb potential. It tells us the potential energy of an object with charge Q in the electric ﬁeld of an object with charge q: Take note that we can also calculate the correct electric ﬁeld conﬁguration by putting our charge distribution ρ = qδ(⃗r) 106 no-nonsense electrodynamics directly into Coulomb’s general law (Eq. 4.3): ⃗E(⃗r) = 1 4πϵ0 ∫ ρ(⃗r′)(⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′ this is Coulomb’s law, Eq. 4.3↷ρ(⃗r′) = qδ(⃗r′) = 1 4πϵ0 ∫ qδ(⃗r′)(⃗r −⃗r′) |⃗r −⃗r′|3 d3⃗r′↷∫ dx δ(x) f (x) = f (0) = 1 4πϵ0 q⃗r |⃗r|3↷⃗r = |⃗r|⃗er = q 4πϵ0 |⃗r|⃗er |⃗r|3↷\u0000\u0000|⃗r| = q 4πϵ0 ⃗er |⃗r|2 . (4.16) This is exactly the same result that we already calculated above (Eq. 4.12). Next, we calculate the electric ﬁeld conﬁguration around a charged sphere. This is, in some sense, the logical next step after a point charge. The charge distribution is again spherically symmetric but now no longer concentrated at a single point. 4.1.2 Electric ﬁeld of a sphere We assume that we have a uniformly charged sphere and we want to calculate the electric ﬁeld conﬁguration in the region surrounding the sphere. Our charge distribution reads ρ(⃗r − R) =   ρ0, on the sphere: |⃗r| = R 0, otherwise , (4.17) where ρ0 is a constant that denotes the charge density on the sphere. Gauss’s law (Eq. 3.14) tells us ∫ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV (4.18) and as our surface, we choose a sphere S around our charged sphere. electrostatics and magnetostatics 107 Since our sphere is uniformly charged, exactly the same ar- guments that we used in the previous section can be used again. Our electric ﬁeld has to be spherically symmetric and this means, following exactly the same steps as in Eq. 4.11, that the result is again14 14 This is exactly the same result that we calculated for a single point charge in Eq. 4.12.⃗E(⃗r) = q r24πϵ0⃗er , (4.19) where q is the total charge of the sphere. However, take note that we can also choose a surface S2 which lies inside the sphere. In this case the total charge contained within the surface is zero. Therefore Gauss’s law tells us the electric ﬁeld inside a charged sphere vanishes: ⃗E(⃗r) = 0 for |⃗r| < R (4.20) 108 no-nonsense electrodynamics As in the previous section, we can derive exactly the same result by using the general form of Coulomb’s law (Eq. 4.3): ⃗E(⃗r) = 1 4πϵ0 ∫ ρ(⃗r′)(⃗r −⃗r′) |⃗r −⃗r′|3 d3r′ this is Coulombs law, Eq. 4.3↷ρ(⃗r) in Eq. 4.17 = 1 4πϵ0 ∫ ρ0δ(|⃗r′| − R)(⃗r −⃗r′) |⃗r −⃗r′|3 d3r′↷ = ρ0 4πϵ0 ∫ δ(|⃗r′| − R)(⃗r −⃗r′) |⃗r −⃗r′|3 d3r′↷ = ρ0 4πϵ0 ∫ 2π 0 ∫ π 0 ∫ ∞ 0 δ(|⃗r′| − R)(⃗r −⃗r′) |⃗r −⃗r′|3 r′2dr′ sin(θ′)dθ′dφ′↷ = ρ0 4πϵ0 ∫ 2π 0 ∫ π 0 ∫ ∞ 0 δ(|⃗r′| − R)(⃗r −⃗r′) |⃗r −⃗r′|3 r′2dr′ sin(θ′)dθ′dφ′ . (4.21) A useful observation is that our system is spherically symmet- ric and hence we only need to calculate the electric ﬁeld for a radial line starting at the origin. The conﬁguration at all other points follow automatically because the electric ﬁeld is spher- ically symmetric, i.e. we can reach all other points by rotating our solution. Therefore, for simplicity we choose ⃗r = (0, 0, z)T.15 15 Note that, as before, the super- script T denotes the transpose operation, which turns a row vector into an ordinary column vector. Moreover, in general, a point in spherical coordinates is de- scribed by16 16⃗r′ denotes the location of each charge on the sphere and therefore, we need to take all charges into account and can’t restrict ourselves to a line. This is only possible for ⃗r which describes the location we want to evaluate the electric ﬁeld at. ⃗r′ = (R sin θ′ cos φ′, R sin θ′ sin φ′, R cos θ′)T . (4.22) Using this, we can calculate1717 We need this because this is the term which appears in the denominator of our formula. ∣ ∣⃗r −⃗r′∣ ∣ = √(⃗r −⃗r′)2 = √(⃗r)2 + (⃗r′)2 − 2⃗r ·⃗r′ = √ √ √ √ √ √   0 0 z    2 +   R sin θ′ cos φ′ R sin θ′ sin φ′ R cos θ′    2 − 2   0 0 z    ·   R sin θ′ cos φ′ R sin θ′ sin φ′ R cos θ′    = (R2 + z2 − 2Rz cos θ′)1/2 . (4.23) The electric ﬁeld conﬁguration therefore reads electrostatics and magnetostatics 109 ⃗E(⃗r) = ρ0 4πϵ0 ∫ 2π 0 ∫ π 0 ∫ R 0 δ(|⃗r′| − R)(⃗r −⃗r′) |⃗r −⃗r′|3 r′2dr′ sin(θ′)dφ′dθ′ this is Eq. 4.21↷Eq. 4.23 = ρ0R2 4πϵ0 ∫ π 0 sin θ′dθ′ ∫ 2π 0 dφ′ 1 (R2 + z2 − 2Rz cos θ′)3/2   −R sin θ′ cos φ′ −R sin θ′ sin φ′ z − R cos θ′   ↷ ∫ 2π 0 sin φ dφ = 0 ∫ 2π 0 cos φ dφ = 0 = q 8πϵ0 ∫ π 0 sin θ′dθ′ 1 (R2 + z2 − 2Rz cos θ′)3/2    0 0 z − R cos θ′   ↷t ≡ cos θ′ ⇒ dt dθ′ = − sin(θ′) = q 8πϵ0 ∫ 1 −1 z − Rt (R2 + z2 − 2Rzt)3/2 dt⃗ez↷ = q 8πϵ0 · (−1) ∂ ∂z ∫ 1 −1 dt (R2 + z2 − 2Rzt)1/2⃗ez↷ = q 8πϵ0 ∂ ∂z [ 1 Rz (R2 + z2 − 2Rzt)1/2∣ ∣ ∣ ∣ 1 −1 ] ⃗ez↷ = q 8πϵ0 ∂ ∂z [ 1 Rz (|R − z| − |R + z|)⃗ez ]↷ = q 4πϵ0 { ⃗ez/z2, |z| > R 0, |z| < R . (4.24) Now, using the fact that our system is spherically symmetric, we can conclude that outside of the sphere the electric ﬁeld is described by ⃗E(⃗r) = q 4πϵ0r2⃗er , (4.25) where ⃗er denotes the basis vector that points radially outward. This is exactly the same result that we derived by using Gauss’s law (Eq. 4.19). We can see that evaluating Coulomb’s law can be quite compli- cated, even for a relatively simple system like a charged sphere. 110 no-nonsense electrodynamics 4.1.3 Electric ﬁeld of an electric dipole An electric dipole is the logical next step after a single charged object and consists of two charges with opposite charge, i.e. q and −q. For simplicity, we assume that we are dealing with two point charges and choose our coordinate system such that one charge sits at the origin. The charge density then reads ρ(⃗x) = qδ(⃗x) − qδ(⃗x − ⃗d) (4.26) where ⃗d is a vector pointing from the charge at the origin to the second charge. We can now immediately write down the resulting electric ﬁeld conﬁguration because we know that we simply have to use a superposition of the individual conﬁgurations.18. Therefore, the 18 We discussed this at the begin- ning of this part. total electric ﬁeld conﬁguration resulting from a dipole is the sum of the conﬁgurations generated by the two point charges independently (Eq. 4.12): ⃗E(r) = ⃗Eq(r) + ⃗E−q(r) = q⃗r 4πϵ0|⃗r|3 − q(⃗r − ⃗d) 4πϵ0|⃗r − ⃗d|3 (4.27) The electric ﬁeld conﬁguration therefore looks like this: electrostatics and magnetostatics 111 As discussed before, the mathematical reason for this result is that Maxwell’s equations are linear.19 To understand this point, 19 This means that we don’t have any terms like ρ2 or ⃗E2 etc.we put our charge density into Gauss’s law ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV↷ = 1 ϵ0 ∫ V (qδ(⃗x) − qδ(⃗x − ⃗d))dV↷ = 1 ϵ0 ∫ V qδ(⃗x)dV − 1 ϵ0 ∫ V qδ(⃗x − ⃗d)dV . (4.28) Since there is no ρ2 term, we simply get a sum of integrals on the right-hand side. For each of these integrals we can apply the same reasoning which we used for a single point charge in Section 4.1.1 and the result is therefore simply the sum of two point charge solutions (Eq. 4.27). Now, after discussing systems consisting of one and two charged objects, it’s time to talk about how we can make sense of sys- tems consisting of many charged objects. 4.1.4 Electric ﬁeld of more complicated charge distri- butions An extremely important result is that the electric ﬁeld conﬁgu- ration resulting from any charge distribution which is localized 112 no-nonsense electrodynamics within some region, looks exactly like the charge distribution of a single point charge if we are sufﬁciently far away.2020 There is an important caveat which we will talk about in a moment. We can see this most easily by using the electric potential in- stead of the electric ﬁeld. The general formula for the electric potential is2121 Analogous to Coulomb’s law for the electric ﬁeld (Eq. 4.3), this for- mula tells us that the total potential is the sum over the potentials gen- erated by the individual charges. If we calculate −∇φ using this general form of the potential, we ﬁnd exactly Coulomb’s general law. We will discuss this general formula in more detail at the end of Section 4.1.6. φ(⃗r) = 1 4πϵ0 ∫ ρ(⃗r′) |⃗r −⃗r′| d3r′ . (4.29) We assume that the charge distribution ρ(⃗r′) is localized within some region V. In mathematical terms, being far away from our charge distribution then means |⃗r| ≫ |⃗r′| for all ⃗r′ in V. The main idea is that we can then use the Taylor expansion to simplify the general formula (Eq. 4.29).22 In particular, we need 22 The general idea behind the Taylor expansion is discussed in Appendix B. the expansion23 23 Here r ≡ |⃗r|. 1 |⃗r −⃗r′| = 1 r +⃗r′ · ∇ 1 r + . . .↷ = 1 r +⃗r′ · ⃗r r3 + . . . (4.30) electrostatics and magnetostatics 113 Putting this expansion into our general formula (Eq. 4.29) yields φ(⃗r) = 1 4πϵ0 ∫ ρ(⃗r′) 1 |⃗r −⃗r′| d3r′ general formula, Eq. 4.29↷Taylor expansion in Eq. 4.30 = 1 4πϵ0 ∫ ρ(⃗r′) ( 1 r +⃗r′ · ⃗r r3 + . . .) d3r′↷we integrate over r′ not r = 1 4πϵ0r ∫ ρ(⃗r′)d3r′↷ − 1 4πϵ0r3 ∫ ρ(⃗r′)⃗r′ ·⃗rd3r′ + . . . The integral in the ﬁrst term simply yields the total charge q contained in the volume. Therefore, if we are sufﬁciently far away, we can approximate our general formula as24 24 The idea behind the Taylor expansion is that the ﬁrst term in the expansion is dominant and all additional terms yield progressively small corrections. The largest correction comes from the second term etc. φ(⃗r) ≈ q 4πϵ0r , (4.31) which is exactly the potential we found for a single point charge (Eq. 4.15). In words, this means that if we are far away, we can’t distinguish between a complicated charge distribution and a simple point charge. However, if we look closely enough, we can observe deviations from this point charge ﬁeld which are described by the higher- order terms in the Taylor expansion (Eq. 4.30). For example, if we measure our potential a bit more closely, we can describe it using the formula φ(⃗r) ≈ q 4πϵ0r + 1 4πϵ0r3 ∫ ρ(⃗r′)⃗r′ ·⃗rd3r′ , (4.32) It is conventional to introduce the symbol ⃗p for the quantity that characterizes this second order term: ⃗p ≡ ∫ ρ(⃗r′)⃗r′d3r′ . (4.33) Usually, ⃗p is known as the dipole moment. Our formula in Eq. 4.32 then reads φ(⃗r) ≈ q 4πϵ0r + 1 4πϵ0r3 ⃗p ·⃗r . (4.34) The reason for the name dipole moment is that the total charge of a dipole is zero q − q = 0. This means that the otherwise 114 no-nonsense electrodynamics dominant term in Eq. 4.31 vanishes and the long distance be- havior is characterized by the second term in the Taylor expan- sion: φdipole(⃗r) ≈ 1 4πϵ0r3 ⃗p ·⃗r . (4.35) Analogously it’s possible to introduce additional quantities which characterize the higher order terms in the Taylor ex- pansion. For example, if we measure our potential extremely precisely we can use the formula φ(⃗r) ≈ 1 4πϵ0 ( q r + ⃗p ·⃗r r3 + ∑ ij Qijrirj 2r5 ) , (4.36) where2525 Take note that Qij has two indices and is therefore a matrix. Qij ≡ ∫ V ρ(⃗r′)(3r′ ir′ j − δijr′2)d3r′ (4.37) is known as the quadrupole moment.2626 The reason for this name is anal- ogous to the reason for the name \"dipole moment\". The total charge of an electric quadrupole (four charges, +q, +q, −q, −q arranged in a square) is zero and the dipole moment also vanishes. Hence the leading term in the Taylor ex- pansion is the third one which we therefore call the quadrupole moment. The main idea is that charge distributions which are localized within some region V, no matter how complicated, can be de- scribed using a small number of relatively simple quantities of decreasing importance: ◃ The most important feature if we look at the charge distribu- tion from far away is the total charge q. ◃ The second-most important quantity is the dipole moment ⃗p, which contains some rough information about how the charges are distributed. ◃ The third-most important quantity is the quadrupole moment Qij, which contains more detailed information about how the charges are distributed.2727 Of course, it is possible to in- troduce even more quantities. For example, the next term in the Taylor expansion is characterized by the so-called octopole moment. This method of collecting step-by-step information about com- plicated charge distributions is known as multipole expansion. 4.1.5 Charged object in a static electric ﬁeld Now that we’ve learned how to derive the speciﬁc form of the electric ﬁeld in various systems, it’s time to talk about how electrostatics and magnetostatics 115 electrically charged objects react to a non-zero electric ﬁeld strength. The Lorentz force law (Eq. 3.12) allows us to calculate for any given electric ﬁeld conﬁguration the resulting force ⃗F on a charged object.28 Then, with this force at hand, we can do what 28 Reminder: ⃗F = q(⃗E + ⃗v × ⃗B) .we always do in classical mechanics: put it into Newton’s sec- ond law ⃗F = m⃗a. Solving this equation yields the trajectory of our charged object. To summarize, the steps are: 1. Determine the electric ﬁeld conﬁguration ⃗E(⃗x) in the sys- tem.29 29 We discussed in the previous sections how this works in practice. 2. Use the Lorentz force law ⃗F = q⃗E to determine the resulting force on a charged object.30 30 This is the Lorentz force law (Eq. 3.12) for a vanishing magnetic ﬁeld strength ⃗B = 0. 3. Solve the differential equation given by Newton’s second law ⃗F = m⃗a to ﬁnd the trajectory of the charged object in the system. Let’s consider a simple example to see how this works. We assume that the electric ﬁeld in our system has the same strength and direction everywhere. This is, for example, the case between two charged plates: 116 no-nonsense electrodynamics Our goal is to calculate the trajectory of a charged object with charge q which moves with some initial velocity ⃗v(0) = v⃗ex perpendicular to the electric ﬁeld ⃗E = E⃗ey. Moreover, we use a coordinate system where the charged object is exactly at the origin at t = 0 exactly, i.e. ⃗x(0) = 0. The Lorentz force law tells us ⃗F = q⃗E = qE⃗ey. In words, this means that there is a constant force which pushes the charge in the y-direction. Putting this result into Newton’s second law yields m⃗a = ⃗F↷Lorentz force law m⃗a = qE⃗ey↷⃗a ≡ d2⃗x dt2 m d2⃗x dt2 = qE⃗ey . (4.38) This is a differential equation which we need to solve using the initial condition ⃗v(0) = d⃗x dt (0) = v⃗ex. The result is ⃗x(t) which describes the trajectory of the object. Since we have a constant on the right-hand side, we can integrate the equation directly m d2⃗x dt2 = qE⃗ey↷∫ dt ∫ dt m d2⃗x dt2 = ∫ dt qE⃗ey↷ m d⃗x dt = qEt⃗ey +⃗c1↷m d⃗x dt (0) = mv⃗ex ⇒ ⃗c1 = mv⃗ex m d⃗x dt = qEt⃗ey + mv⃗ex↷∫ dt ∫ dt m d⃗x dt = ∫ dt (qEt⃗ey + mv⃗ex)↷ m⃗x = 1 2 qEt2⃗ey + mv⃗ext +⃗c2↷⃗x(0) = 0 ⇒ ⃗c2 = 0 m⃗x = 1 2 qEt2⃗ey + mv⃗ext , where we determined the integration constants using the initial electrostatics and magnetostatics 117 conditions. In words, this result tells us that our charged object will continue to move forward in the x-direction with velocity v since there is no component of the force in the x-direction. At the same time, the object gets pushed in the y-direction as a result of the electric ﬁeld: 4.1.6 Further Systems It’s clear that there are an inﬁnite number of electrostatic sys- tems that we did not discuss here. In most of these systems nothing really new can be learned and for this reason we re- stricted ourselves to the fundamental systems discussed in the previous sections.31 Nevertheless, a few comments on more 31 Most textbooks discuss them solely to prepare students for exams. advanced systems are in order. ◃ First of all, there is a large class of electrostatic systems that we didn’t discuss at all.32 In this class of systems the posi- 32 If you’re interested in these ad- vanced applications, you can ﬁnd excellent discussions in the text- books recommended in Chapter 9. tions of the charges is not ﬁxed and known. This happens whenever there are conductors in the system where charges move around freely. Describing these kind of systems is a lot more difﬁcult since we need the locations of the charges to calculate the resulting electric ﬁeld conﬁguration but, in turn, we need to know the ﬁeld conﬁguration in order to calculate the locations of the charges. Formulated differently, the main difﬁculty is that the charge density redistributes itself as a result of the electric ﬁeld. The main idea is to use the so-called uniqueness theorem. 118 no-nonsense electrodynamics This theorem states that if the electric potential on the sur- faces of all conductors is known, the electric potential is ﬁxed everywhere uniquely. For this reason, ﬁnding a description of electrostatic systems where the charge distribution is not ﬁxed and known is usually called a boundary value prob- lem. Using the uniqueness theorem it is also possible to develop the method of image charges, which is a clever tool to derive the electric ﬁeld conﬁguration near conductors. The trick we used here is that if we are dealing with a complicated sys- tems, we can replace it with a simpler system with the same boundary conditions. Since the uniqueness theorem assures us that the solution of Poisson’s equation is unique once the boundary conditions are ﬁxed, we know that the simpler system and the more complicated system correspond to the same solution. The main idea is therefore to ﬁnd a system with the same boundary conditions as the system we want to describe. For many systems, the same boundary conditions can be fulﬁlled using a few so-called image charges. Then we solve the systems consisting of our original charge and the image charges. As usual, the solution for such a system is simply the sum over the solutions of the individual charges. Since the simple system consisting of the image charges and our charge fulﬁlls the same boundary conditions as our orig- inal system, we know immediately that the solution is also valid for our original system. For example, let’s imagine we want to calculate the electric ﬁeld conﬁguration in a system consisting of a single point charge q and an inﬁnitely large grounded plate (φ = 0 in- side the plate). The presence of the point charge pushes the charge in the plate around. We don’t know how exactly the resulting charge distribution on the plate looks and therefore this is quite a difﬁcult problem. But we can solve it by using the method of image charges. All we have to do is to ﬁnd a simpler electrostatic system with the same boundary conditions, i.e. that yields φ = 0 at electrostatics and magnetostatics 119 the location of the plate. An extremely simple system which fulﬁlls this condition consists of our original charge q and a second image charge −q symmetrically behind the axis deﬁned by the plate.33 33 Take note that the plate is not present in this alternative system. As mentioned above, we replace our system consisting of (charge) + (plate) with the system consisting of (charge) + (image charge) since it is far simpler to solve. Each of the two charges in our new system generate a partic- ular electric potential. But since we placed our image charge at a particular spot and gave it charge −q, the two potentials cancel exactly everywhere on the plane where the grounded plate is located in our original system. Therefore, the two systems (charge) + (plate) and (charge) + (image charge) fulﬁll the same boundary conditions. The uniqueness theorem tells us that both systems have the same solution and therefore, the extremely simple solution for the system (charge) + (image charge):34 34 For concreteness, we assume that the plate is in the yz-plane and our original charge is located at ⃗r′ = (d, 0, 0)T = d⃗ex. The correct location for our image charge is then ⃗r′ = (−d, 0, 0)T. Moreover, we use that the potential of a single point charge at ⃗r′ is q 4πϵ0|⃗r−⃗r′ | φ(⃗r) = q 4πϵ0|⃗r − d⃗ex| − q 4πϵ0|⃗r + d⃗ex| (4.39) is also a solution for the system (charge + plate). Maybe it’s not obvious that our potential fulﬁlls the boundary condition (φ = 0 in the yz-plane) and we therefore check it explicitly: φ(0, y, z) = q 4πϵ0|y⃗ey + z⃗ez − d⃗ex| − q 4πϵ0|y⃗ey + z⃗ez + d⃗ex|↷ = q 4πϵ0√y2 + z2 + d2 − q 4πϵ0√y2 + z2 + d2↷ = 0 ✓ We can then calculate the electric ﬁeld conﬁguration either by 120 no-nonsense electrodynamics using ⃗E = −∇φ or by writing down directly the superposi- tion of the well known point charge solutions. ⃗E(⃗r) = q(⃗r − d⃗ex) 4πϵ0|⃗r − d⃗ex|3 − q(⃗r + d⃗ex) 4πϵ0|⃗r + d⃗ex|3 (4.40) ◃ A common type of electrostatic problem discussed in text- books is the electric ﬁeld conﬁguration around constant charge distributions conﬁned to some special geometry. We already discussed three examples of this type of prob- lem: a point charge, a sphere and a dipole. In general, these problems can be classiﬁed according to the dimension of the geometry which contains the non-zero charge distribution. A zero-dimensional geometry is just a point which therefore corresponds to the point charge example we discussed in Section 4.1.1. A one-dimensional geometry is, for example, a line and these kinds of problems therefore involve line charges.35 A two-dimensional geometry is a surface and 35 Another popular example of this kind is a ring of charge. therefore contains a surface charge while three-dimensional charge distributions are known as volume charges.36 36 A popular two-dimensional example is a charged disk. The main task in all these problems is to solve the integral in Coulomb’s law (Eq. 4.3) and therefore, in some sense, this is really a mathematics problem, not a physics problem.3737 For many realistic charge distri- butions, Maxwell’s equations can only be solved numerically anyway. Probably all examples which can be solved nicely analytically can be found in some textbook. ◃ In the context of boundary value problems it is often more convenient to use the electric potential instead of the elec- tric ﬁeld. The correct electrostatic equation for the electric potential can be derived using Maxwell’s equations and the relationship between the potential φ and the electric ﬁeld ⃗E:3838 Take note that we use the con- ventional symbol φ for the electric potential and use that ∂t Ai = 0 since we are dealing with a static situation. electrostatics and magnetostatics 121 ∇ · ⃗E = ρ ϵ0 this is the electric Gauss’ law, Eq. 3.16↷⃗E = −∇φ , c.f. Eq. 2.20 with ∂t Ai = 0 −∇2φ = ρ ϵ0 . (4.41) This equation is known as the Poisson equation. If an ob- server is in a region external to a charge distribution where no charges are present, then ρ = 0 and the equation for the electric potential reads −∇2φ = 0 Eq. 4.41 with ρ = 0 , (4.42) which is known as the Laplace equation. Also take note that the second electrostatic equation (Eq. 4.1) ∇ × ⃗E = 0 this is Eq. 4.1↷⃗E = −∇φ , c.f. Eq. 2.20 with ∂t Ai = 0 −∇ × ∇φ = 0 (4.43) is trivially fulﬁlled by the potential since ∇ × ∇ f = 0 (\"a gradient cannot curl\") holds for any scalar ﬁeld f .39 39 See Appendix A.16. Take note that the Poisson equation is linear in φ and ρ, which means again that we can use superpositions of so- lutions as new solutions. ◃ There are clever methods for solving the Poisson equation. The main idea behind the most famous one is that we can calculate the solution for a general charge distribution by using the known solutions for individual charges. Hence, the basic building block for a general solution is the solution for a single point charge. The charge distribution of a single point charge at ⃗r′ is ρ(⃗r) ∼ δ(⃗r −⃗r′). Therefore, our main task in constructing a general solution is to solve the equation40 40 This is the Poisson equation (Eq. 4.41) with ρ(⃗r) ∼ δ(⃗r −⃗r′) without all constants. ∇2G(⃗r,⃗r′) = −δ(⃗r −⃗r′) . (4.44) The solution G(⃗r,⃗r′) is known as the Green’s function for the Laplacian operator ∇2 ≡ ∆.41 41 In general, a Green’s function characterizes the response of a system (characterized by a speciﬁc equation) to the presence of a point source. 122 no-nonsense electrodynamics The Green’s function for the Laplacian operator reads4242 We simply assume that someone somehow found this solution and we can verify that it indeed solves Eq. 4.44. The question of how to ﬁnd Green’s functions for a given differential operator is a difﬁcult one and not one that we will dive into. For our modest goals it is enough to recall that we already calculated the electric ﬁeld surrounding a point charge (Section 4.1.1) and can use this to derive the corresponding potential (⃗E = −∇φ). The potential we ﬁnd this way is exactly the Green’s function we are looking for. G(⃗r,⃗r′) = −1 4π 1 |⃗r −⃗r′| . (4.45) The main idea is that as soon as we have the solution G(⃗r,⃗r′) for a single point charge, we can calculate directly the solu- tion of the Poisson equation for a general charge distribution ρ(⃗r′) as follows: φsol(⃗r) = 1 ϵ0 ∫ ρ(⃗r′)G(⃗r,⃗r′)d3r′ . (4.46) We can check this explicitly43 43 The general idea used by the Green’s function method is to ﬁnd a function which yields the delta distribution if we act with the relevant differential operator (here ∇2) on it. Then we can immediately write down the general solution like this. ρ(⃗r) ϵ0 = −∇2φsol(⃗r) this is Eq. 4.41↷Eq. 4.46 = −∇2 1 ϵ0 ∫ ρ(⃗r′)G(⃗r,⃗r′)d3r′↷∇2 acts on ⃗r and not on ⃗r′ = − 1 ϵ0 ∫ ρ(⃗r′)∇2G(⃗r,⃗r′)d3r′↷∇2G(⃗r,⃗r′) = −δ(⃗r −⃗r′), Eq. 4.45 = 1 ϵ0 ∫ ρ(⃗r′)δ(⃗r −⃗r′)d3r′↷∫ dx f (x)δ(x − y) = f (y) = 1 ϵ0 ρ(⃗r) ✓ (4.47) Therefore, the general solution of the Poisson equation reads φsol(⃗r) = −1 4πϵ0 ∫ ρ(⃗r′) |⃗r −⃗r′| d3r′ (combining Eq. 4.45 and Eq. 4.46) . This is exactly the equation we used in Section 4.1.4 and yields Coulomb’s law for the electric ﬁeld (Eq. 4.3) if we use ⃗E = −∇φ. Now, let’s move on and talk about magnetostatic systems. electrostatics and magnetostatics 123 4.2 Magnetostatic Systems As already mentioned above, a single moving point charge is not a magnetostatic system since it generates a time-dependent electric ﬁeld conﬁguration. Hence, the simplest possible magne- tostatic system is instead a thin straight wire carrying a steady current. 4.2.1 Magnetic ﬁeld of a wire In this example, we want to ﬁnd the structure of the magnetic ﬁeld ⃗B which is generated around a wire carrying a steady current. We can calculate ⃗B by using the equations of magnetostatics (Eq. 4.2). In particular, the Ampere-Maxwell law with ∂tE = 0 (Eq. 3.29) tells us:44 44 Often, the Ampere-Maxwell law with ∂tE = 0 is called Ampere’s law. ∮ C ⃗B · ⃗dl = µ0 ∫ S ⃗J · ⃗dS . (4.48) To determine ⃗B, our task is to get it out of the integral. The trick we can use to do this is completely analogous to what we 124 no-nonsense electrodynamics already used in Section 4.1.1 to calculate the electric ﬁeld of a single point charge. The crucial idea is that the contour C is arbitrary, as long as it encloses the wire. Hence, we choose a circle centered around the wire. This means that our contour integral is simply an integral around a circle ∫ 2π 0 r⃗eϕ dϕ, where r denotes the radius of the circle and ⃗eϕ is a unit vector always pointing tangential when we move along the circle. Since we are dealing with a com- pletely structureless wire, there is no reason why our magnetic ﬁeld should take on different values on this circle. Moreover, from the discussion in the previous chapters, we know that our magnetic ﬁeld circles around currents. Mathematically, this means that ⃗B = |⃗B|⃗eϕ. These arguments allow us to get |⃗B| out of the integral ∮ C ⃗B · ⃗dl = ∫ 2π 0 |⃗B|⃗eϕ ·⃗eϕr dϕ↷ = |⃗B|r ∫ 2π 0 dϕ↷ = |⃗B|r2π . (4.49) On the right-hand side in Eq. 4.48, we simply get the total cur- rent through the wire I: µ0 ∫ S ⃗J · ⃗dS = µ0 Iwire . (4.50) electrostatics and magnetostatics 125 Putting these results for the left-hand and right-hand side to- gether yields ∮ C ⃗B · ⃗dl = µ0 ∫ S ⃗J · ⃗dS this is the Ampere-Maxwell law, Eq. 4.48↷Eq. 4.49 and Eq. 4.50 |⃗B|r2π = µ0 Iwire↷ |⃗B| = µ0 Iwire r2π . (4.51) The magnetic ﬁeld surrounding a wire is therefore45 45 The wire is inﬁnite, so the charge q on it is inﬁnite too. Thus one really needs to work with charge per unit length which we denote by ql. ⃗B(⃗r) = µ0ql|⃗v| 2πr ⃗eϕ . (4.52) In words, this means that the magnetic ﬁeld circles around the wire. The direction in which it circles can be determined by the so-called \"right hand rule\". If your thumb points in the direction of the current, the remaining ﬁngers curl in the direction of the magnetic ﬁeld. 4.2.2 Charged object in a static magnetic ﬁeld Completely analogous to what we discussed in Section 4.1.5, we can calculate how a charged object reacts to the presence of a non-zero magnetic ﬁeld strength. To understand how this works, we consider again a simple explicit example. 126 no-nonsense electrodynamics We assume that the magnetic ﬁeld in our system has the same strength and direction everywhere. This is, for example, the case inside a solenoid: Our goal is to calculate the trajectory of a charged object with charge q which moves with some initial velocity ⃗v(0) = v⃗ex perpendicular to the magnetic ﬁeld ⃗B = B⃗ez. Moreover, for simplicity we again use a coordinate system where the charged object is at the origin at t = 0, i.e. ⃗x(0) = 0. The Lorentz force law tells us4646 This for a vanishing electric ﬁeld strength. ⃗F = q⃗v × ⃗B this is the Lorentz force law (Eq. 3.12) with ⃗E = ⃗0↷ = q   vx vy vz    ×   0 0 B   ↷ = q   vyB − 0 0 − vxB 0 − 0   ↷ = qB    vy −vx 0    . (4.53) electrostatics and magnetostatics 127 Putting this result into Newton’s second law yields m⃗a = ⃗F↷⃗a ≡ d⃗v dt , ⃗F = q⃗v × ⃗B m d⃗v dt = q⃗v × ⃗B↷Eq. 4.53 m d dt   vx vy vz    = qB    vy −vx 0    . (4.54) This differential equation is quite complicated since the ve- locity in the y-direction directly inﬂuences the velocity in the x-direction. However, we can disentangle it as follows47 47 To unclutter the notation in the following calculations we set m = 1. d dt vx = qBvy this is the ﬁrst line in Eq. 4.54↷d dt d2 dt2 vx = qB d dt vy↷using the second line in Eq. 4.54 d2 dt2 vx = qB(−qBvx)↷ d2 dt2 vx = −q2B2vx . (4.55) This equation is solved by a function whose second derivative is the function itself with a minus sign. A function with exactly this property is sin(ct), where c is some appropriate constant. We can therefore conclude that our equation is solved by48 48 You can also check this explicitly by putting this solution into the differential equation. ⃗v(t) =    c1 cos(qBt) −c1 sin(qBt) c2    , (4.56) where c1, c2 are constants which we need to determine using the initial condition: ⃗v(0) =   v 0 0    . (4.57) 128 no-nonsense electrodynamics This yields ⃗v(0) =    c1 cos(qB0) −c1 sin(qB0) c2    ! =   v 0 0   ↷cos(0) = 1, sin(0) = 0  c1 0 c2    ! =   v 0 0    . (4.58) Our explicit solution therefore reads ⃗v(t) =    v cos(qBt) −v sin(qBt) 0    . (4.59) We can calculate the trajectory ⃗x(t) by integrating this solution ⃗x(t) = ∫ dt ⃗v(t) = ∫ dt    v cos(qBt) −v sin(qBt) 0   ↷ =    v qB sin(qBt) + c3 v qB cos(qBt) + c4 c5    (4.60) and using our initial condition ⃗x(0) = 0: ⃗x(0) =    v qB sin(qB0) + c3 v qB cos(qB0) + c4 c5    ! =   0 0 0   ↷cos(0) = 1, sin(0) = 0   0 + c3 v qB + c4 c5    ! =   0 0 0    . (4.61) We can therefore conclude that our charged object follows the trajectory ⃗x(t) =    v qB sin(qBt) v qB cos(qBt) − v qB 0    . (4.62) electrostatics and magnetostatics 129 In words, this means that our charged object follows a circular path Take note that if our object’s initial velocity has an additional non-zero component in the z-direction, ⃗v(0) =    v 0 v0    , (4.63) the ﬁnal trajectory is a helix ⃗x(t) =    v qB sin(qBt) v qB cos(qBt) − v qB v0t    . (4.64) Moreover, take note that if our object initially moves only in the 130 no-nonsense electrodynamics z-direction ⃗v(0) =    0 0 v0    , (4.65) it won’t be inﬂuenced by the magnetic ﬁeld at all because ⃗F(0) = q⃗v(0) × ⃗B = qv0⃗ez × B⃗ez = 0 (4.66) since ⃗ez ×⃗ez = 0, i.e. the Lorentz force is and stays zero. We can also see this because now, Eq. 4.58 reads ⃗v(0) =    c1 cos(qB0) −c1 sin(qB0) c2    ! =    0 0 v0   ↷cos(0) = 1, sin(0) = 0  c1 0 c2    ! =    0 0 v0    . (4.67) In words, this means that our object will simply continue to move in the z-direction without being inﬂuenced by the mag- netic ﬁeld. 4.2.3 Further Systems There are, of course, also an inﬁnite number of magnetostatic systems which we will not discuss here. However, a few com- ments are in order. ◃ Many magnetostatic systems discussed in textbooks are vari- ations of the wire system. For example, popular examples are two parallel wires and a circular loop. In addition, it is, of course, also possible to discuss the magnetic ﬁeld conﬁg- uration resulting from steady currents through more compli- cated geometries like a solenoid, a plane or a torus. The main task in these problems is always to solve the inte- gral appearing in the general Biot-Savart law (Eq. 4.6). electrostatics and magnetostatics 131 ◃ Since in magnetostatic systems we are usually dealing with thin wires, the one-dimensional analogue of the general Biot- Savart law (Eq. 4.6) is often introduced ⃗B(⃗r) = µ0 4π ∫ C I ⃗dx′ × (⃗r −⃗r′) |⃗r −⃗r′|3 , (4.68) where I is the current and C is the path traced out by the wire. In many textbooks, this one-dimensional version is called the Biot-Savart law. ◃ Completely analogous to what we discussed in Section 4.1.4, it is possible to introduce magnetic multipole moments to characterize the long-range magnetic ﬁeld conﬁguration generated by general current densities. ◃ There is also a Poisson equation for the magnetic potential49 49 Take note that for the magnetic ﬁeld ∇ · ⃗B = 0 and ∇ × ⃗B = µ0⃗J is the nontrivial equation. The Poisson equation for the vector potential ⃗A can then be derived by rewriting the magnetic ﬁeld in terms of the potential: ⃗B = ∇ × ⃗A (Eq. 2.20). This yields µ0⃗J = ∇ × ⃗B = ∇ × ∇ × ⃗A. Then, we can rewrite this using the identity ∇ × ∇ × ⃗A = ∇(∇ · ⃗A) − ∇2 ⃗A. (See Appendix A.16.) To simplify this further, we can use the obser- vation that potentials cannot be directly measured and only po- tential differences can. Hence, we can always add a constant to our potentials, and this so-called gauge freedom can be used to achieve ∇ · ⃗A = 0. If we use the gauge freedom like this, we get µ0⃗J = ∇ × ∇ × ⃗A = ∇(∇ · ⃗A︸ ︷︷ ︸ =0 ) − ∇2 ⃗A = −∇2 ⃗A , which is the Poisson equation for ⃗A. We will discuss gauge freedom in detail in Chapter 7. ∇2 ⃗A = −µ0⃗J . (4.69) We have here, in fact, a Poisson equation for each compo- nent Ai. The general solution can be derived again using the Green’s function method and reads50 50 We discussed the Green’s function method in Section 4.1.6. ⃗Asol(⃗r) = µ0 4π ∫ ⃗J(⃗r′) |⃗r −⃗r′| d3r′ . Then using the relationship ⃗B = ∇ × ⃗A between the vec- tor potential ⃗A and the magnetic ﬁeld ⃗B yields exactly the general Biot-Savart law (Eq. 4.6). Now, let’s move on and talk about electrodynamics. 5 Electrodynamics In the previous chapter we focused on systems in which the electric and magnetic ﬁelds are static. Now, we are interested in what happens when the ﬁelds change dynamically. Arguably the most important new effect is the appearance of electromag- netic waves.1 1 In fact, the role of waves in mod- ern physics cannot be overstated. In quantum mechanics we use waves to describe particles. In quantum ﬁeld theory, particles are waves. It therefore makes sense to study them in detail in a relatively simple context like electrodynamics. In Section 3.7, we already used Maxwell’s equations to derive the wave equations (Eq. 3.38 and Eq. 3.39) ∇2⃗E = µ0ϵ0 ∂2 ∂t2 ⃗E ∇2⃗B = µ0ϵ0 ∂2 ∂t2 ⃗B . (5.1) We will now talk about explicit solutions of these equations and discuss why they describe electromagnetic waves. The main idea is that a changing magnetic ﬁeld generates a changing electric ﬁeld and a changing electric ﬁeld generates a changing magnetic ﬁeld changing ⃗B // changing ⃗E // changing ⃗B // . . . This way, nontrivial conﬁgurations in the electric and magnetic ﬁelds can travel together, for example, from the sun to the earth 134 no-nonsense electrodynamics although there is a vacuum in-between, i.e. no medium which could help to transmit the wave.22 A nontrivial conﬁguration is often called a perturbation of the ﬁeld. The trivial or unperturbed conﬁg- uration is what we call the vacuum or ground state. In physical terms, the ground state corresponds to the conﬁguration with minimum en- ergy, i.e. zero electric and magnetic ﬁeld strengths. We can understand how and why this works in practice most easily by considering an explicit solution of the wave equations. 5.1 An explicit solution of the wave equation An explicit solution of the wave equation for the electric ﬁeld ⃗E is ⃗E = A   cos(kz − ωt) 0 0    , (5.2) where A, k and ω are constants. We can check this claim ∇2⃗E = µ0ϵ0 ∂2 ∂t2 ⃗E this is Eq. 3.38↷Eq. 5.2 ∇2 A   cos(kz − ωt) 0 0    = µ0ϵ0 ∂2 ∂t2 A   cos(kz − ωt) 0 0   ↷∂2 z cos az = −a2 cos az −k2 A   cos(kz − ωt) 0 0    = −µ0ϵ0ω2 A   cos(kz − ωt) 0 0    . (5.3) The right-hand side is equal to the left-hand side if k2 = µ0ϵ0ω2. Therefore, Eq. 5.2 is a solution of the wave equation as long as this condition is fulﬁlled by the constants k, ω.33 We will talk about the meaning of these constants and the condition in a moment. A solution of this form describes a wave which travels in the z-direction with an amplitude that points in the x-direction. electrodynamics 135 So far, we have only discussed a solution of the wave equation for the electric ﬁeld. However, an electromagnetic wave travels through the interplay of the electric ﬁeld and the magnetic ﬁeld. How this comes about is the topic of the next section. 136 no-nonsense electrodynamics 5.2 Corresponding solution of the magnetic wave equation As already mentioned at the end of Section 3.7, the wave equa- tion for the electric ﬁeld and the wave equation for the magnetic ﬁeld are only seemingly independent. Maxwell’s equations must always be fulﬁlled. Using this fact, we can directly de- rive a corresponding solution of the magnetic wave equation for each solution of the electric wave equation. Hence, the two equations are not really independent, since the magnetic and electric ﬁeld are still connected by Maxwell’s equations. Using our explicit solution from Section 5.1, we can understand how this works in practice. We start by recalling that Faraday’s law (Eq. 3.27) describes the interplay between the electric and the magnetic ﬁeld. Putting our explicit solution for the electric ﬁeld ⃗E (Eq. 5.2) into Faraday’s law yields ∇ × ⃗E = − ∂ ∂t ⃗B this is Faraday’s law, Eq. 3.27↷Eq. 5.2 ∇ × A   cos(kz − ωt) 0 0    = − ∂ ∂t ⃗B↷∇ × ⃗E =  ∂yEz − ∂zEy ∂zEx − ∂x Ez ∂x Ey − ∂yEx   A    0 ∂z cos(kz − ωt) 0    = − ∂ ∂t ⃗B↷∂z cos(kz) = −k sin(kz) A    0 −k sin(kz − ωt) 0    = − ∂ ∂t ⃗B . (5.4) We can conclude that the corresponding magnetic ﬁeld con- ﬁguration ⃗B has no component in the x-direction and no z- component: Bx = 0, Bz = 0. Moreover, we can calculate the y electrodynamics 137 component directly since Eq. 5.4 tells us ∂ ∂t By = Ak sin(kz − ωt) . (5.5) This equation is solved by By = A k ω cos(kz − ωt) , (5.6) because ∂ ∂t A k ω cos(kz − ωt) = Ak sin(kz − ωt) . (5.7) Therefore, if the electric ﬁeld is in the conﬁguration described by Eq. 5.2, we automatically get the magnetic ﬁeld conﬁgura- tion:4 4 Take note that the solution we found this way has exactly the same structure as our solution for ⃗E and is also a solution of the wave equation for the magnetic ﬁeld (Eq. 3.39). We can see this immediately by putting it into the wave equation for ⃗B since the wave equation for ⃗B is completely analogous to the wave equation for ⃗E. Therefore, a solution of the ⃗E wave equation automatically yields a solution for the ⃗B via Maxwell’s equations. ⃗B = A k ω    0 cos(kz − ωt) 0    . (5.8) This solution describes a wave which travels in the z-direction with its amplitude pointing in the y-direction. Therefore, we can conclude that an ⃗E wave which travels in the z-direction and is polarized in the x-direction (Eq. 5.2), is automatically accompanied by a ⃗B wave which also travels in the z-direction but which is polarized in the y-direction (Eq. 5.8): 138 no-nonsense electrodynamics In general, the electric ﬁeld and the magnetic ﬁeld oscillate in phase but in perpendicular directions. Next, after this explicit example, let’s talk about solutions of the wave equations in more general terms. 5.3 General solutions of the wave equations We will restrict our discussion to solutions of the wave equation for the electric ﬁeld (Eq. 3.38) since all statements for solutions of the wave equation for the magnetic ﬁeld (Eq. 3.39) follow automatically since the two equations are equivalent. The most basic solutions of the electric wave equations look like this55 Completely analogous to what we did in the previous sections, you can check that this is indeed a solution by putting it into the wave equation (Eq. 3.38). Moreover, take note that δ = π/2 means that our cosine function becomes a sine function. In other words, there is nothing special about our choice of the cosine function and we could equally use sin(⃗k ·⃗r ± ωt + δ). ⃗E = ⃗E0 cos(⃗k ·⃗r ± ωt + δ) , (5.9) where ⃗k is a vector that determines in which direction the wave is traveling and δ encodes possible phase shifts which can be important if we are dealing with superpositions of waves. Moreover, ⃗E0 is a vector whose magnitude | ⃗E0| describes the amplitude of the wave and its direction determines the direction in which the electric ﬁeld oscillates. The choice of the sign ± in the cosine function determines whether the wave travels in the positive direction or negative direction on the axis deﬁned by ⃗k, e.g. on the x-axis to the right or to the left. The constant ω describes the frequency at which the wave oscillates and ⃗k is directly related to the wavelength.66 Below, we will talk about these interpretations in more detail. electrodynamics 139 The solution discussed in the previous sections therefore corre- sponds to the special choice ⃗E0 = A⃗ex , ⃗k = k⃗ez , δ = 0 , ± → − , (5.10) since ⃗E = ⃗E0 cos(⃗k ·⃗r ± ωt + δ) this is Eq. 5.9↷Eq. 5.10) = A⃗ex cos(k⃗ez ·⃗r − ωt)↷ = A   1 0 0    cos      0 0 k    ·   x y z    − ωt   ↷ = A   cos(kz − ωt) 0 0    . (5.11) This is exactly our explicit solution in Eq. 5.2. Solutions of the form in Eq. 5.9 are known as plane wave solu- tions and are the basic building blocks for all possible solutions of the wave equations. In physical terms these solutions de- scribe monochromatic electromagnetic waves.7 7 For example, for ω = 60 × 1012 rad s we are dealing with what we usually call red light. ω is the angular frequency which is directly related to the more familiar ordinary frequency f : ω = 2π f . However, take note that pure plane waves do not exist in nature. A plane wave spreads out over all space with equal amplitude. This means that an inﬁnite amount of energy is required to create such a plane wave and is therefore impossible. Realistic solutions are wave packets, which are localized within some ﬁnite region and can be created with a ﬁnite amount of energy. \"Monochromatic\" light in experiments is never completely monochromatic but still contains some range of wavelengths. We will talk about wave packets below. The wave equations are, like Maxwell’s equations, linear in ⃗E and ⃗B. Therefore, we can use superpositions of known solutions as additional solutions.8 8 As mentioned before, this princi- ple of superposition is true for all linear equations. This means that we can use sums of the basic plane wave solu- tions to create more complicated solutions. A simple example is 140 no-nonsense electrodynamics a standing wave ⃗E = ⃗E0( cos(⃗k ·⃗r − ωt) + cos(⃗k ·⃗r + ωt)) . (5.12) A solution of this type describes a superposition of a wave with a second wave with equal amplitude which travels in the opposite direction. The resulting total wave is not traveling in any direction but still oscillating. To see this mathematically, we use a trigonometric identity ⃗E = ⃗E0( cos(⃗k ·⃗r − ωt) + cos(⃗k ·⃗r + ωt)) this is Eq. 5.12↷cos(a + b) + cos(a − b) = 2 cos(a) cos(b) = 2 ⃗E0( cos(⃗k ·⃗r) cos(ωt)) . (5.13) The crucial point is that the spatial dependence ⃗k ·⃗r of the wave is completely separated from the time dependence ωt. This means that the spatial shape of the wave is ﬁxed and does not change as time moves on. However, since we are still multiply- ing this ﬁxed form of the wave by cos(ωt), the amplitude of the wave at each point still varies (but not the wave form). electrodynamics 141 In general we can, of course, also construct arbitrarily compli- cated linear combinations of plane waves. A famous example is sunlight which consists of all kinds of different monochromatic light. We can see this by using a prism. Waves of different wave- length get diffracted differently and as a result the different monochromatic waves are spatially separated by the prism. In particular, the most general solution of the wave equation is a linear combination of all possible plane waves9 9 We use the notation ⃗k = (kx, ky, kz)T. Moreover, take note that solutions with a plus in the cosine function are also included since ⃗k also takes on negative values and we can use cos(−x) = cos(x).⃗E(⃗r, t) = ∞ ∑ kx=−∞ ∞ ∑ ky=−∞ ∞ ∑ kz=−∞ ⃗E⃗k cos ( ⃗k ·⃗r − ωt) . (5.14) Each possible wave appears in this general solution with a par- ticular amplitude ⃗E⃗k. In general, there is no reason why our waves should only appear with a discrete set of wavelengths and therefore, we have to replace the sum with an integral10 10 Now that our variables kx, ky, kz take on continuous values, our discrete set of amplitudes ⃗E⃗k be- comes a function ⃗E(⃗k). However, the meaning is still the same: each possible plane wave appears in this general solution with a particular amplitude. Moreover, (2π)3 is a conventional normalization factor. ⃗E(⃗r, t) = ∫ ∞ −∞ d3k (2π)3 ⃗E(⃗k) cos ( ⃗k ·⃗r − ωt) . (5.15) By using such a linear combination of plane wave solutions it’s possible to construct any waveform you can imagine. An extremely important example are wave packets. 142 no-nonsense electrodynamics While a plane wave spreads out all over space with equal ampli- tude, a wave packet is localized within some ﬁnite region and can be created using a ﬁnite amount of energy. Now, as promised above, we will discuss the properties of elec- tromagnetic waves in a bit more detail. electrodynamics 143 5.4 Basic properties of electromagnetic waves First of all, let’s talk about the various quantities (⃗k, ω, ±, ⃗E0, δ) which appear in solutions of the wave equation (Eq. 5.9) in a bit more detail.11 11 Reminder: Eq. 5.9 reads ⃗E = ⃗E0 cos(⃗k ·⃗r ± ωt + δ) , ◃ The argument ϕ ≡ (⃗k ·⃗r ± ωt) of our periodic function cos ϕ is called the phase of the wave. A phase of zero ϕ = 0 means we are at the top of our waveform since cos(0) = 1 If the phase is ϕ = π, we are at the bottom since cos(π) = −1. One full wave cycle starts at ϕ = 0 and goes on until ϕ = 2π. A periodic wave repeats itself after 2π since cos(ϕ + 2π) = cos(ϕ).12 12 Take note that it is conventional in theoretical physics to measure angles in multiplies of π, e.g. 180◦ ↔ π and for a full circle 360◦ ↔ 2π. ◃ The vector ⃗k is usually called the wave vector. The direction of ⃗k tells us in which direction the wave is traveling. For ex- ample, we have seen above (Eq. 5.11) that for ⃗k = (0, 0, k)T = k⃗ez, we are getting a wave which travels in the z-direction. The length of the wave vector |⃗k| describes how many oscilla- tions there are per meter. To understand this, imagine that we could stop the time, i.e. keep t ﬁxed and then move through space. As we move along the axis deﬁned by ⃗k we count how 144 no-nonsense electrodynamics many full wave shapes we encounter per meter. This number is the wave number. One full oscillation is over as soon as the phase of the wave has increased by 2π. Hence, we can say a bit more precisely that |⃗k| measures how many 2π cycles there are per meter.13 For this reason, |⃗k| is known as spatial 13 Formulated differently, |⃗k| tells us how much the phase changes as we move one meter along the wave at one ﬁxed point in time. angular frequency or wave number.14 14 This is in contrast to the temporal angular frequency ω, which tells us how many oscillations there are per second. For example, if we move 2 meters and observe that the phase changes by 20π, we know that the wave number is 10π radi- ans per meter.15 15 20π/2 m = 10π/ m The wave number is directly related to the wavelength λ: λ = 2π |⃗k| . (5.16) The wavelength is deﬁned as the spatial distance that we need to move until the phase of the wave has changed by 2π:1616 Formulated differently, the wavelength is the distance between adjacent identical parts of the wave. ◃ The constant ω is known as temporal angular frequency or simply angular frequency. The angular frequency describes how many oscillations there are per second. To understand it, imagine that we are at one ﬁxed point in space and time moves on. We now observe how the wave moves up and down at this one particular point. We count how often it undergoes a full oscillation, i.e. from maximum to maximum. electrodynamics 145 The result is the angular frequency of the wave. Formulated differently, ω tells us how much the phase changes during one second. For example, if we observe the wave for two seconds and the phase changes by 20π, i.e. we observe 10 full wave cycles, we know the angular frequency is 10π radians per second. The angular frequency is directly related to the period τ and ordinary frequency f of the wave ω = 2π τ = 2π f . (5.17) The period τ is the time the wave needs for one full oscilla- tion. ◃ The direction in which the amplitude vector ⃗E0 points, spec- iﬁes the geometrical orientation of the oscillation.17 Usually, 17 Take note that this is a different direction than the direction ⃗k in which the wave travels. the direction of ⃗E0 is called the polarization of the wave. The length of the amplitude vector |⃗E0| encodes the peak magnitude of the oscillation: 146 no-nonsense electrodynamics An extremely important observation is that the amplitude vector cannot point in any arbitrary direction for electromag- netic waves. We can see this since Gauss’s law for the electric ﬁeld for a system with a vanishing charge density reads ∇ · ⃗E = 0. This means, for example, that our solution discussed in Sec- tion 5.1 which travels in the z-direction ⃗E = ⃗E(z, t) can- not have an oscillating amplitude in the z-direction since ∂ ∂z Ez = 0. Formulated differently, a wave which travels in the z-direction depends only on z. However, ∂ ∂z Ez = 0 then implies that Ez = 0. Analogously, a wave which travels in the x direction ⃗E = ⃗E(x, t) cannot have an amplitude in the x-direction and a wave which travels in the y direction ⃗E = ⃗E(y, t) cannot have an amplitude in the y-direction. This means that electromagnetic waves cannot be polarized in the direction they are traveling. In other words, electro- magnetic waves are never longitudinally polarized (at least in a vacuum). One possibility to remember this is to use the experimental result that the speed of light is an absolute speed limit in nature.18 Nothing can move faster. Since an 18 This experimental fact is the basis for special relativity. We will talk about this in Chapter 6. electromagnetic wave already travels at the speed of light, no component can travel in the direction the wave is travel- ing, since otherwise this part of the wave would travel with a velocity higher than the speed of light. By using superpositions of plane waves, it is possible to construct waves with more complicated polarizations. For example, a linear combination of the form ⃗E = A (cos(kz − ωt)⃗ex + sin(kz − ωt)⃗ey) , (5.18) describes a circularly polarized wave. Our amplitude vector ⃗E0 circles in the xy-plane while the plane travels in the z- direction.1919 Reminder: a circular motion in physics is described by ⃗r = (cos(ωt) sin(ωt) ) . electrodynamics 147 In contrast, a linearly polarized wave looks like this ◃ The sign between the two terms in the cosine function deter- mines whether our wave moves up or down on the axis de- ﬁned by ⃗k. For example, a solution of the form ⃗E = ⃗E(x − ct) describes a wave which moves to the right on our x-axis, while a solution of the form ⃗E = ⃗E(x + ct) describes a wave which moves to the left. This interpretation comes about since if we focus on a ﬁxed point in our wave shape ⃗E(x − ct) and t increases, x also has to increase in order to keep ⃗E(x − ct) at the same value. In other words, this means that if we focus on a speciﬁc point in our wave shape, at a later point in time (a larger t), we will ﬁnd it at a larger x. Analogously, a solution of the form E = E(z − ct) describes a wave that moves up on our z-axis. Hence the solution, we 148 no-nonsense electrodynamics discussed in Section 5.1 describes a solution which moves in the positive direction on the z-axis. ◃ The absolute phase δ encodes the phase of the wave at ⃗r = 0 and t = 0. This quantity isn’t measurable since it depends on how we choose our coordinate system. However, it is still important if we consider superposition of waves. If we add two waves their relative absolute phase crucially determines if the am- plitude of the resulting wave is larger in which case we speak of constructive interference or smaller in which case we speak of destructive interference. electrodynamics 149 Now that we have a rudimentary understanding of the basic quantities associated with electromagnetic waves, we can talk about the most important of the more advanced properties. 5.5 Advanced properties of electromagnetic waves ◃ An extremely important question we haven’t answered so far is: How fast are electromagnetic waves traveling? Let’s focus on a speciﬁc point on our wave. In particular, to determine the velocity of the wave, we follow the movement of a point. To simplify the discussion, we restrict ourselves to a wave that moves in one-dimension. With this in mind, we can calculate the velocity by using our basic solution (Eq. 5.9) ⃗E = ⃗A cos (kx − ωt) = ⃗E0 cos (k(x − ω k t)) . (5.19) We assume that the speciﬁc point in our waveform ⃗Espec we are interested in is at t = t1 at x = x1: ⃗Espec = ⃗A cos (k(x1 − ω k t1)) . (5.20) At a later point in time t = t2, we will ﬁnd our speciﬁc point ⃗Espec at some new location x = x2: ⃗Espec = ⃗A cos (k(x2 − ω k t2)) . (5.21) This means that in the interval ∆t = t2 − t1 our speciﬁc point has traveled the distance ∆x = x2 − x1. Therefore, our point travels with velocity v = ∆x ∆t = x2 − x1 t2 − t1 . (5.22) 150 no-nonsense electrodynamics Since we are considering the same speciﬁc point ⃗Espec in Eq. 5.20 and Eq. 5.21, we can conclude x2 − ω k t2 = x1 − ω k t1↷ ω k = x2 − x1 t2 − t1 . (5.23) By comparing Eq. 5.22 with Eq. 5.23, we can conclude v = ω k . (5.24) In words, the velocity of each point in our wave form is given by the ratio of the angular frequency ω and the wave number k. We can also understand this from a different perspective. A velocity has units meter per second. The only combination of our basic wave quantities discussed in the previous section with units meter per second is2020 To be a bit more precise: the velocity we talk about here is the phase velocity. This name is used to make clear that it’s also possible to associate a different kind of velocity, called group velocity, to wave packets. The group velocity is the speed at which the envelope moves forward, while the phase velocity is the speed of the individual plane waves inside the packet. The group velocity and phase velocity are not always the same. v = λ τ (5.25) since the wavelength λ is measured in meters and the period in seconds. In words, this equation tells us that a wave travels one wavelength λ per period τ. We can rewrite the velocity of the wave v in terms of the angular frequency ω and wave number k ≡ |⃗k| as follows v = λ τ = 2π/k 2π/ω = ω k . (5.26) This is exactly the equation, we already derived above (Eq. 5.24). ◃ The formula v = ω/k is correct for waves in general. How- ever, we need to remember that we are dealing with electro- magnetic waves which are described by the wave equation (Eq. 3.38). Putting the general form of the solution (Eq. 5.9) into the wave equation yields2121 We saw this in Section 5.1. For- mulated differently, our general solution only fulﬁlls the wave equa- tion if this condition is satisﬁed. k2 = µ0ϵ0ω2 . (5.27) This equation is known as the dispersion relation and tells us how k and ω have to be related if our general solution (Eq. 5.9) describes electromagnetic waves. electrodynamics 151 An extremely important observation is that we can rewrite Eq. 5.27 as follows k2 = µ0ϵ0ω2 Eq. 5.27↷ k2 ω2 = µ0ϵ0↷ ω k = √ 1 µ0ϵ0 . (5.28) This is interesting because the ratio ω k is precisely the velocity of our wave (Eq. 5.26). Therefore, the velocity of electromag- netic waves is not some arbitrary number but actually a ﬁxed number which can be determined through the constants µ0 and ϵ0! Putting in the experimental values for µ0 and ϵ0 yields v = √ 1 µ0ϵ0↷ = √ √ √ √ 1 (4π × 10−7 m kg/C2)(8.854 × 10−12 C2 s2/kg m3)↷ = 2.9979 × 10 8 m/s . (5.29) This is the velocity at which electromagnetic waves prop- agate. Usually, we call this number the speed of light and denote it by c ≡ 2.9979 × 108 m/s . (5.30) ◃ An extremely important property of electromagnetic waves is that they can transport energy. Only thanks to this fact is life on earth possible since most of the energy we use is or was transported from the sun to earth via electromagnetic waves. To understand how this works, we ﬁrst need to calculate the energy stored in a particular electric and magnetic ﬁeld conﬁguration.22 Let’s imagine that we have a speciﬁc charge 22 Be warned that the following derivation is somewhat indirect. Nevertheless, the ﬁnal result is quite intuitive and worth the hassle. distribution and current density at time t. After some small 152 no-nonsense electrodynamics period dt, the charges have moved around a bit d⃗l = ⃗vdt, where ⃗v describes the velocity of the charges.23 Using the 23 We can either imagine that all charges move with the same velocity or that ⃗v describes the average velocity. Lorentz force law, we can then calculate that the work done by the electromagnetic ﬁeld on a single charge q is 24 24 Reminder: work is deﬁned as force times the path during which the force was applied. Moreover, take note that the magnetic force is always perpendicular to the velocity and therefore, no work is done by the magnetic ﬁeld. ⃗F · d⃗l = q(⃗E + ⃗v × ⃗B) · ⃗vdt = q⃗E · ⃗vdt . (5.31) To calculate the work done on all charges, we use q = ρdV and ⃗J = ρ⃗v. The rate at which work is done on all charges is therefore dW dt = ∫ V(⃗E · ⃗J)dV . (5.32) The lesson to take away is that we can interpret ⃗E · ⃗J as the work done per unit time, per unit volume. In other words, ⃗E · ⃗J is the power per unit volume. Next, we are interested in the energy stored in electromag- netic waves which can be present even though there are no charges and no current density ⃗J in the system. To derive a formula for such a situation, we use Maxwell’s equations and the product rule for the divergence operator: ⃗E · ⃗J = 1 µ0 ⃗E · (⃗∇ × ⃗B) − ϵ0⃗E · ∂⃗E ∂t this is the Ampere-Maxwell law, Eq. 3.32↷∇ · (⃗E × ⃗B) = ⃗B · (⃗∇ × ⃗E) − ⃗E · (⃗∇ × ⃗B) = 1 µ0 ( ⃗B · (⃗∇ × ⃗E) − ∇ · (⃗E × ⃗B)) − ϵ0⃗E · ∂⃗E ∂t↷Faraday’s law, Eq. 3.27, ⃗∇ × ⃗E = −∂⃗B/∂t = 1 µ0 ⃗B · (− ∂ ∂t ⃗B) − 1 µ0 ∇ · (⃗E × ⃗B) − ϵ0⃗E · ∂⃗E ∂t↷∂ ∂t ⃗B2 = ( ∂ ∂t ⃗B) · ⃗B + ⃗B · ( ∂ ∂t ⃗B) = 2⃗B · ∂ ∂t ⃗B = − 1 2µ0 ∂ ∂t ⃗B2 − 1 µ0 ∇ · (⃗E × ⃗B) − ϵ0 1 2 ∂ ∂t ⃗E2 . (5.33) Putting this into Eq. 5.32 yields electrodynamics 153 dW dt = ∫ V(⃗E · ⃗J)dV this is Eq. 5.32↷Eq. 5.33 = ∫ V (− 1 2µ0 ∂ ∂t ⃗B2 − 1 µ0 ∇ · (⃗E × ⃗B) − ϵ0 1 2 ∂ ∂t ⃗E2)) dV↷reorganizing the terms = − 1 2 ∂ ∂t ∫ V ( 1 µ0 ⃗B2 + ϵ0⃗E2)) dV − ∫ V 1 µ0 ∇ · (⃗E × ⃗B)dV↷Gauss’s theorem, ∫ V ∇ · ⃗A = ∮ S ⃗A = − 1 2 ∂ ∂t ∫ V ( 1 µ0 ⃗B2 + ϵ0⃗E2)) dV − ∮ S 1 µ0 (⃗E × ⃗B) · d⃗a↷deﬁnitions ≡ − dUem dt − ∮ S ⃗S · d⃗a . (5.34) This equation is known as the Poynting theorem. ⃗S ≡ 1 µ0 (⃗E × ⃗B) (5.35) is usually called the Poynting vector and describes the en- ergy ﬂux per unit time through a given surface, i.e. the en- ergy ﬂux. Moreover, Uem is the total energy stored in the electromagnetic ﬁeld conﬁguration. In words, Eq. 5.34 therefore tells us that the work done on charges by the Lorentz force is equal to the decrease in en- ergy stored in the ﬁeld dUem dt minus the energy which has ﬂowed through the surface S. In other words, if the energy stored in the electromagnetic ﬁeld gets smaller, it must have either been used to move charges around ( dW dt ) or has ﬂown out of the volume we are considering (∮ S ⃗S · d⃗a). Therefore, Eq. 5.34 is quite similar to the continuity equation (Eq. 3.8). However, instead of the conservation of electric charge, Eq. 5.34 describes the conservation of energy. We can make the analogy even more concrete by deriving the differential form of Eq. 5.34. To do this, we introduce the mechanical energy density dW dt ≡ d dt ∫ V umechdV (5.36) 154 no-nonsense electrodynamics and the electromagnetic energy density uem ≡ 1 2 (ϵ0⃗E2 + 1 µ0 ⃗B2) . (5.37) Using these deﬁnitions, we can rewrite Eq. 5.34 as follows dW dt = − dUem dt − ∮ S ⃗S · d⃗a↷deﬁnitions d dt ∫ V umechdV = − d dt ∫ V uemdV − ∮ S ⃗S · d⃗a↷ d dt ∫ V (umech + uem)dV = − ∮ S ⃗S · d⃗a↷Gauss’s theorem d dt ∫ V (umech + uem)dV = − ∫ V ∇ · ⃗SdV↷ d dt (umech + uem) = −∇ · ⃗S . (5.38) This equation is completely analogous to the continuity equa- tion (Eq. 3.8).25 Instead, of the charge density, we now have 25 The differential form of the continuity equation reads ∂ ∂t ρ = −∇ · ⃗J . the energy density on the left-hand side. Moreover, instead of the current density ⃗J, we now have the Poynting vector ⃗S on the right-hand side. While the current density describes how charges ﬂow through our system, the Poynting vector de- scribes how energy ﬂows through the system. In the context of electromagnetic waves the Poynting vector ⃗S is especially important since it describes how energy gets transported by the electromagnetic ﬁeld. Thus, to check the claim that elec- tromagnetic waves transport energy, we have to calculate the Poynting vector corresponding to an explicit wave solution. electrodynamics 155 We again use our explicit solution discussed in Section 5.1: ⃗S ≡ 1 µ0 (⃗E × ⃗B) Eq. 5.35↷explicit solutions, Eq. 5.2, Eq. 5.8 = 1 µ0 (A⃗ex cos(kz − ωt)) × ( k ω A⃗ey cos(kz − ωt))↷ = A2k µ0ω cos 2(kz − ωt)( ⃗ex ×⃗ey)↷⃗ex ×⃗ey = ⃗ez = A2k µ0ω cos 2(kz − ωt)⃗ez↷k2/ω2 = µ0ϵ0, Eq. 5.29 = A2√ ϵ0 µ0 cos 2(kz − ωt)⃗ez . (5.39) This is clearly non-zero and therefore, as promised, electro- magnetic waves do transport energy. We can understand this result a little better by calculating the corresponding electromagnetic energy density (Eq. 5.37):26 26 We will see in a moment why this is helpful. uem ≡ 1 2 (ϵ0⃗E2 + 1 µ0 ⃗B2)↷explicit solutions, Eq. 5.2, Eq. 5.8 = 1 2 (ϵ0(A⃗ex cos(kz − ωt))2 + 1 µ0 (A k ω⃗ex cos(kz − ωt))2)↷ = 1 2 (ϵ0(A2 cos2(kz − ωt)) + 1 µ0 (A2µ0ϵ0 cos 2(kz − ωt)))↷ = ϵ0 A2 cos2(kz − ωt) . (5.40) 156 no-nonsense electrodynamics Therefore, we can write Eq. 5.39 as follows ⃗S = A2√ ϵ0 µ0 cos 2(kz − ωt)⃗ez this is Eq. 5.39↷ϵ0 ϵ0 = 1 = A2√ ϵ0 µ0 ϵ0 ϵ0 cos 2(kz − ωt)⃗ez↷Eq. 5.37 = uem √ 1 ϵ0µ0⃗ez↷√ 1 µ0ϵ0 = c, Eq. 5.29 = uemc⃗ez . (5.41) In words, this means that the energy ﬂux ⃗S is simply the velocity of the wave ⃗v = c⃗ez times the energy density uem. This is completely analogous to how we can write the current density in terms of the charge density and the velocity.2727⃗j = ρ⃗v, Eq. 2.12. Before we move on, a few more comments on electromagnetic waves. ◃ There are lots of interesting phenomena associated with electrodynamical waves which are important, for example, in an engineering context. Using the explicit wave solutions discussed in the previous sections, it’s possible to derive how electromagnetic waves scatter, get reﬂected and behave in matter. These kind of investigations lead to a huge sub-ﬁeld of elec- trodynamics, commonly known as optics. electrodynamics 157 ◃ We haven’t talked about how electromagnetic waves can ac- tually be produced. We know that a stationary electric charge produces a static electric ﬁeld and an electric charge mov- ing with constant velocity produces a static magnetic ﬁeld. Electromagnetic waves are ﬁeld conﬁgurations involving changing electric and magnetic ﬁeld conﬁgurations.28 The 28 Recall that the main mechanism behind electromagnetic waves is that a changing electric ﬁeld produces a changing magnetic ﬁeld and, in turn, a changing magnetic ﬁeld produces a changing electric ﬁeld. crucial question is therefore: How can we produce changing ﬁeld conﬁgurations? The correct answer is: through accelerating charges. This means that an electromagnetic wave gets produced whenever we change the velocity of an electrically charged object. The charged object produces at each constant velocity a speciﬁc pattern in the magnetic ﬁeld. Each time the velocity of the object changes, a new pattern is generated. Hence, by chang- ing the velocity of a charged object, we can produce changing patterns in the magnetic ﬁeld. This is, for example, exactly what happens in an antenna. The current ﬂowing through an antenna is alternating (AC), which means it periodically changes its direction. Therefore, the electrons in the antenna are constantly accelerated and decelerated when the direction of the current changes. This is analogous to how we produce a wave in mechanics, for example, by shaking a rope: 158 no-nonsense electrodynamics Part III Get an Understanding of Electrodynamics You Can Be Proud Of \"More recently, the principle of local gauge invariance has blossomed into a unifying theme that seems capable of embracing and even synthesizing all the elementary interactions.\" Chris Quigg PS: You can discuss the content of Part III with other readers and give feedback at www.nononsensebooks.com/edyn/part3. electrodynamics 161 In this ﬁnal part, we will pick up a few loose ends: ◃ We will talk about the connection between Einstein’s theory of special relativity and electrodynamics and how it helps us to understand the origin of Maxwell’s equations. ◃ We will discuss gauge symmetry and how it can help to simplify calculations. ◃ In addition, we will investigate in what sense electrodynam- ics can be understood as a gauge theory. This is interesting since all fundamental interactions can be described using gauge theory. In other words, gauge theory allows us to understand all fundamental interactions from a common perspective. ◃ In the ﬁnal chapter, we will talk about books which cover speciﬁc aspects in more detail. In other words, if you’re interested in fundamental physics, this part is where things get really interesting. 6 Special Relativity In Section 5.5, we discovered a remarkable property of electro- magnetic waves:1 1 We derived this in Eq. 5.29. How- ever, take note that this is only true in a vacuum. An electromag- netic wave in matter travels with a smaller velocity since the constants ϵ0 and µ0 must be replaced with ϵ0 → ϵ, µ0 → µ . The speciﬁc values of ϵ and µ, and therefore of the speed of electromagnetic waves, depend on the material at hand. Moreover, take note that the speciﬁc value 2.9979 × 108 m/s is not important since it depends on our choice of units. For example, if we measure length in feet, we have 9.836 × 108 feet/s. In theoretical physics it’s often very convenient to use a special kind of units, known as natural units, where the speed of light is simply 1. The only thing that matters here is that c is non- zero and not inﬁnite, which is true for all sensible systems of units. The speed of electromagnetic waves is always exactly c = 1/√ϵ0µ0 = 2.9979 × 10 8 m/s. While this little fact can easily go unnoticed, it leads to ex- tremely wide-ranging consequences if taken seriously.2 Most 2 Historically, the importance of this property remained unnoticed until the famous Michelson-Morley experiment conﬁrmed it experi- mentally. Afterward, Einstein was the ﬁrst to take the result seriously and used it to develop his famous theory of special relativity. importantly, there are two noteworthy points here: ◃ The speed of electromagnetic waves is not inﬁnite. In partic- ular, this means that light needs some time to travel a given distance. Therefore, when we observe stars we actually see what they looked like many years ago since light needs many years to travel from distant stars to the earth. 164 no-nonsense electrodynamics In other words, when we observe the universe, we are actu- ally observing the past and don’t see what things are looking like right now. In addition, this means that we can only ob- serve a ﬁnite part of the universe since some parts are so far away that light hasn’t had enough time to reach us. The age of the universe (the time since the Big Bang) is ap- proximately t0 ≈ 13.79 billion years. This means that all parts of the universe further away than d ≈ ct0 are so far away that no light from there has ever reached the earth. ◃ The second important point is that we were able to determine the speed of light in full generality without making reference to anything. Usually the speed of an object depends on how we move relative to it, i.e. the frame of reference we are us- ing. For example, imagine that an observer standing at a train station measures that a train moves with 50 km h : A second observer who runs with 15 km h parallel to the same train, measures that the train moves with 35 km h . special relativity 165 Curiously, this does not happen for electromagnetic waves. Electromagnetic waves always travel with c = 1/√ϵ0µ0 = 2.9979 × 108 m/s, no matter how you move.3 3 As already mentioned above, the speed of light only has this value in free space and not if our wave moves in matter. The speed of electromagnetic waves in matter is lower. The curious fact of nature that the speed of electromagnetic waves always has exactly the same value, leads to all kinds of strange consequences. Taking it seriously leads to Einstein’s the- ory of special relativity. While there is no way we can discuss special relativity in detail, we should at least talk about the most famous phenomenon. Let’s imagine, a person sends a light pulse straight up where it is reﬂected by a mirror and ﬁnally reaches again the point from where it was sent: 166 no-nonsense electrodynamics We record three important events: ◃ A : the light pulse leaves the starting point ◃ B : the light pulse is reﬂected by the mirror ◃ C : the light pulse returns to the starting point. The time-interval between the two events A and C is44 Reminder: for a constant speed v we have v = ∆s ∆t , where ∆s is the distance and ∆t the time interval. Therefore, we have ∆t = ∆s v . ∆t = tC − tA = 2L c , (6.1) where L denotes the distance between the person and the mir- ror. So far, nothing interesting has happened. But this changes quickly as soon as we consider how a second person observes exactly the same situation. We imagine this second person moves with some constant speed u relative to the ﬁrst person. For simplicity, we assume that the origins of the two coordinate systems coincide when the light pulse is send off (tA). Moreover, we assume that each person stands at the origin of his coordinate system. A ﬁrst crucial observation is that the starting and end points of the light pulse have different coordinates for the second ob- server: special relativity 167 Mathematically, we have x′ A = 0 ̸= x′ C = u∆t′ → ∆x′ = u∆t′, (6.2) where we use primed coordinates for the coordinate system associated with the second person. In words, this means that for this second person the light apparently also has moved in the x-direction. In contrast, for the ﬁrst person xA = xC → ∆x = 0. (6.3) Now, what’s the time interval the second person measures between the event A and the event C?5 5 It will become clear in a moment, why this is an interesting question. As usual the time interval ∆t′ = t′ C − t′ A can be calculated as the distance l divided by the speed of the light pulse c. ∆t′ = l c (6.4) The distance l is for this second observer no longer simply L, but we can calculate it using the Pythagorean theorem6 6 See the triangle in the ﬁgure above. l = 2 √( 1 2 u∆t′)2 + L2. (6.5) The time interval measured by this second person is therefore 168 no-nonsense electrodynamics c∆t′ = 2 √( 1 2 u∆t′)2 + L2 this is Eq. 6.5 with Eq. 6.4 used on the LHS↷∆x′ = u∆t′, Eq. 6.2 = 2 √( 1 2 ∆x′)2 + L2↷ ∆t′ = 2 √( 1 2 ∆x′)2 + L2 c (6.6) For ∆x′ ̸= 0 this time interval is different from the time interval measured by the ﬁrst person: ∆t′ ̸= ∆t.7 In words, this means 7 Reminder: ∆t = 2L c , Eq. 6.1. that two observers moving relative to each other do not agree on the time interval between the two events A and C! This phenomenon is usually called time-dilation. Clocks tick differently for different observers and they count a different number of ticks between two events. Analogously, it’s possible to derive that different observers do not necessarily agree on the length of objects. This is known as length contraction and is another famous consequence of the constant speed of light. A third incredibly important con- sequence is that the speed of light c is an upper speed limit for everything physical. Unfortunately, as already mentioned above, discussing all these consequences in detail requires at least another book.88 Some of the best books on special relativity are listed in Chapter 9. special relativity 169 6.1 The origin of Maxwell’s equations Historically, electrodynamics was developed before special relativity. And as discussed in the previous section, it’s possible to derive special relativity using Maxwell’s equations. However, it’s also possible to turn this line of thought upside down. This means that we can use the experimental fact that the speed of light is constant for all observers to derive special relativity.9 9 As mentioned above, historically this fact was indeed discovered experimentally by the Michelson- Morley experiment.Then we can use special relativity to derive the correct equa- tions of electrodynamics, i.e. Maxwell’s equations. Faraday’s experiments etc. \u000f\u000f Michelson-Morley experiment c=const. \u000f\u000f Maxwell’s equations c=const. .. Special Relativitynn While the full story is far too long to include it here, let me sketch the main ideas.10 10 The full story can be found, for example, in Jakob Schwichtenberg. Physics from Symmetry. Springer, Cham, Switzerland, 2018a. ISBN 978- 3319666303 ◃ Whenever we can perform a transformation of some object and the end result is indistinguishable, we are dealing with a symmetry.11 We can then, motivated by the result of Michel- 11 A symmetry is a transformation which leaves the object in question unchanged. son and Morley, search for all transformations which leave the speed of light invariant. These transformations are the symmetries of special relativity. Formally, this set of transfor- mations is known as the Poincaré group. ◃ The second ingredient we need is the Lagrangian formal- ism. In modern physics, a theory is almost always deﬁned in terms of a Lagrangian. Each theory (Newtonian mechan- ics, quantum mechanics, . . .) corresponds to a speciﬁc La- grangian. The correct equations describing the theory can then be derived by minimizing the so-called action, which is the integral over the Lagrangian.12 12 In general, we search for extremal points of the action, i.e. sometimes the correct equations correspond to maxima of the action. The connection between Lagrangians and the corresponding equations of motion is given by the Euler- Lagrange equations. 170 no-nonsense electrodynamics ◃ The main idea is that the Lagrangian must be invariant un- der all symmetries of the given theory.13 This must be the 13 In general, only the action has to be invariant. However, if the Lagrangian is invariant, the action certainly is invariant too. case since we use the Lagrangian to derive the equations of motion. If the Lagrangian would change under a given symmetry transformation, we would get different equations of motions and hence the symmetry wouldn’t be a symme- try. This observation is one of the main reasons why the Lagrangian formalism is so popular in modern physics. ◃ This allows us to derive the correct Lagrangian describing special relativity by demanding that it has to be invariant under all relevant symmetries.1414 This requirement is not really enough to derive the correct La- grangian. Instead, the correct Lagrangian is the simplest non-trivial Lagrangian respecting the relevant symmetries. ◃ In addition, by investigating the symmetries of special rela- tivity carefully using the appropriate mathematical toolbox (group theory), we can also derive the correct Lagrangian describing electrodynamics. The crucial idea is that our sym- metry transformation can act on different kinds of objects (scalars, vectors, etc.).15 If we write down the simplest invari- 15 In mathematics this is known as representation theory. ant Lagrangian involving a vector ﬁeld Aµ, which describes nontrivial dynamics, we end up with the correct Lagrangian for electrodynamics:1616 If we follow the same line of thought for a scalar ﬁeld, we end up with the famous Klein-Gordon equation. For a third type of object our transformation can act on, known as spinors, we end up with the Dirac equation. Moreover, take note that there can be an additional term in the Lagrangian known as a mass term. Minimizing the action corresponding to the Lagrangian with this mass term leads to the so- called Proca equation. The absence of this mass term can be understood using gauge symmetry which is the topic of the next chapter. LMaxwell = 1 2 (∂µ Aν∂µ Aν − ∂µ Aν∂ν Aµ) = 1 4 FµνFµν , (6.7) where Fµν denotes the electromagnetic ﬁeld tensor and Aµ = (φ/c, Ai)T denotes the electromagnetic potential.17 Moreover, 17 The equivalence of the two forms of the Lagrangian given here is demonstrated in Appendix A.17.1. Aµ = (φ/c, −Ai)T, ∂µ = (∂0, ∂i)T, and ∂µ = (∂0, −∂i)T. By minimizing the corresponding action, we then end up with Maxwell’s equations. In fact, this way we only derive the inhomogeneous vacuum Maxwell equations:18 18 The vacuum Maxwell equations are those without any electric current and without any charge density, i.e. the ﬁrst two equations in Eq. 1.4 with J = 0 and ρ = 0. ∇ · ⃗E = 0 ∇ × ⃗B − µ0ϵ0 ∂⃗E ∂t = 0 The correct interplay between the electromagnetic potential Aµ and electric charges or currents can be derived using gauge special relativity 171 symmetry. Formulated differently, using gauge symmetry, we can derive why and how the charge density and the current density show up on the right-hand side in the inhomogeneous Maxwell equations. We will discuss gauge symmetry and gauge theory in general in two following chapters. The homogeneous Maxwell equations ∂λFµν + ∂µFνλ + ∂νFλµ = 0 (Eq. 1.1) (6.8) follow automatically from the deﬁnition of the electromagnetic ﬁeld tensor Fµν in terms of the potential Aµ:19 19 It may seem strange that two of the four incredibly important Maxwell equations are, in some sense, so trivial. Maybe it helps to think about them as consistency conditions which supplement the inhomogeneous Maxwell equations. In addition, take note that there is a deep and beautiful geometri- cal reason for the homogeneous Maxwell equations. In mathematical terms, Eq. 6.8 is a Bianchi identity and in words, Bianchi identities always encode the geometrical fact that \"the boundary of a boundary is zero\". This is discussed in a bit more detail in Appendix A.16.1. ∂λFµν + ∂µFνλ + ∂νFλµ↷Fµν = ∂µ Aν − ∂ν Aµ = ∂λ(∂µ Aν − ∂ν Aµ) + ∂µ(∂ν Aλ − ∂λ Aν) + ∂ν(∂λ Aµ − ∂µ Aλ)↷ = ∂λ∂µ Aν − ∂λ∂ν Aµ + ∂µ∂ν Aλ − ∂µ∂λ Aν + ∂ν∂λ Aµ − ∂ν∂µ Aλ↷∂µ∂ν = ∂ν∂µ = ∂λ∂µ Aν − ∂λ∂ν Aµ + ∂µ∂ν Aλ − ∂λ∂µ Aν + ∂λ∂ν Aµ − ∂µ∂ν Aλ↷ = ˘˘˘˘∂λ∂µ Aν − ˘˘˘˘∂λ∂ν Aµ + ˘˘˘˘∂µ∂ν Aλ − ˘˘˘˘∂λ∂µ Aν + ˘˘˘˘∂λ∂ν Aµ − ˘˘˘˘∂µ∂ν Aλ↷ = 0 ✓ An important point is that in order to write down the correct Lagrangian which does not change under Poincaré transforma- tions, we need to use Aµ (or equivalently Fµν = ∂µ Aν − ∂ν Aµ, Eq. 2.22). This is necessary because the electric ﬁeld and the magnetic ﬁeld are mixed through Poincaré transformations. We can understand this by recalling that non-zero electric ﬁeld strengths are created by stationary charges, while moving charges create non-zero magnetic ﬁeld strengths. But a charge which is at rest for us, appears moving for a different observer who moves relative to the charge. 172 no-nonsense electrodynamics This means that we calculate that the charge produces no mag- netic ﬁeld strength since the charge is stationary, while the second observer ﬁnds a non-zero magnetic ﬁeld strength. Who is right? We both are since the distinction between the electric and mag- netic ﬁelds depends on how we observe the ﬁeld. In reality, there is only one ﬁeld called the electromagnetic ﬁeld Fµν.2020 Take note that this is analogous to how space and time are mixed by Poincaré transformations. The mixing of space and time is what we called time dilatation and space contraction previously. This mixing makes it necessary to introduce the four vector xµ = (ct,⃗x)T. In physical terms this means that we need to consider spacetime instead of space and time separately. In practical terms, this means that to describe the electric and magnetic ﬁelds consistently, we need to use the electromagnetic tensor (Eq. 2.17) Fµν =      0 −E1/c −E2/c −E3/c E1/c 0 −B3 B2 E2/c B3 0 −B1 E3/c −B2 B1 0      , (6.9) which uniﬁes them into a single object. The transformation which transforms our perspective into the perspective of the moving observer is a Poincaré transformation known as a boost.21 What we have learned is therefore that a boost trans- 21 In general, a boost is a trans- formation to a different constant velocity. forms an electric ﬁeld strength into a magnetic ﬁeld strength and vice versa. To write down a theory that is invariant under boost, we need to use Fµν instead of the individual components ⃗E and ⃗B. Analogously, the electric potential φ and the vector potential Ai are mixed through Poincaré transformations. The four-vector special relativity 173 Aµ = (φ/c, Ai)T uniﬁes these two potentials and allows us to write down an invariant Lagrangian (Eq. 6.7). To summarize, we can say that from a relativistic perspective, the objects Aµ and Fµν are better suited to describe electrody- namics than the electric and magnetic ﬁelds we usually talk about. In the following chapters, we will see how gauge sym- metry allows us to understand electrodynamics and the objects Aµ and Fµν from a completely new perspective.7 Gauge Symmetry In this chapter we will unclutter the notation a bit by setting c = 1. In other words, we don’t write the speed of light everywhere. This is possible by using so-called \"natural units\". For our purposes it is suf- ﬁcient to remember that if we want results in SI-units, we must add c in a few places. But such details aren’t important for what follows because we are only interested in fundamental considerations. In most introductory textbooks, gauge symmetry is either only mentioned in passing or described as a technical trick to make calculations simpler. However, from a modern perspective we can understand gauge symmetry as the fundamental deﬁning feature of electrodynamics. We will start by discussing gauge symmetry in somewhat ab- stract terms but afterwards discuss what it means and how it helps us to understand electrodynamics. Gauge symmetry refers to the observation that we can’t mea- sure potentials like the electric potential A0 or the magnetic potential ⃗A directly. We can see this, for example, by noting 176 no-nonsense electrodynamics that the electric and magnetic ﬁeld strengths remain completely unchanged if we shift our potentials by a constant amount:11 Alternatively, we can understand it by recalling that we can only mea- sure potential energy differences. Potential energy differences remain unchanged by shifts of the potential φ1 − φ2 → φ1 + η − (φ2 + η) = φ1 − φ2 , where φ1 and φ2 denotes the po- tential energy of two objects in an electric ﬁeld and we shifted the whole potential by a constant amount η. A0 → A0 + η Ai → Ai + ξi . (7.1) We can check this statement explicitly2 2 Reminder, Eq. 2.20: Ei = ∂i A0 − ∂0 Ai Bi = ϵijk∂j Ak . Ei = ∂i A0 − ∂0 Ai → ˜Ei = ∂i(A0 + η) − ∂0(Ai + ξi) = ∂i A0 + ∂iη ︸︷︷︸ =0 −∂0 Ai − ∂0ξ ︸︷︷︸ =0 = ∂i A0 − ∂0 Ai = Ei ✓ (7.2) where we used that ξi and η are constant. Analogously, we ﬁnd Bi = ϵijk∂j Ak → ˜Bi = ϵijk∂j(Ak + ξk) = ϵijk∂j Ak + ϵijk ∂jξk ︸︷︷︸ =0 = ϵijk∂j Ak = Bi ✓ (7.3) Upon closer inspection, we can discover that we have even more freedom. We can not only add constants to the potentials but also derivatives of an arbitrary scalar function η(t,⃗x). In partic- ular, the electric and magnetic ﬁeld strengths remain completely unaltered by the transformations A0 → A0 + ∂0η(t,⃗x) Ai → Ai + ∂iη(t,⃗x) . (7.4) Again, we can check this explicitly: Ei = ∂i A0 − ∂0 Ai → ˜Ei = ∂i(A0 + ∂0η(t,⃗x)) − ∂0(Ai + ∂iη(t,⃗x)) = ∂i A0 + ∂i∂0η(t,⃗x) − ∂0 Ai − ∂0∂iη(t,⃗x) = ∂i A0 + ˘˘˘˘˘ ∂0∂iη(t,⃗x) − ∂0 Ai − ˘˘˘˘˘ ∂0∂iη(t,⃗x) = ∂i A0 − ∂0 Ai = Ei ✓ (7.5) gauge symmetry 177 and Bi = ϵijk∂j Ak → ˜Bi = ϵijk∂j(Ak + ∂kη(t,⃗x)) = ϵijk∂j Ak + ϵijk∂j∂kη(t,⃗x) ︸ ︷︷ ︸ =0 = ϵijk∂j Ak = Bi ✓ (7.6) where we used that ϵijk is antisymmetric but ∂j∂k is symmetric under the switching of the indices j ↔ k.3 Alternatively we can 3 This is an important general result. Every time we have a sum over something symmetric in its indices multiplied by something antisymmetric in the same indices, the result is zero: ∑ ij aijbij = 0 if aij = −aji and bij = bji holds for all i, j. We can see this by writing ∑ ij aijbij = 1 2 ( ∑ ij aijbij + ∑ ij aijbij) We are free to rename our indices i → j and j → i, which we use in the second term → ∑ ij aijbij = 1 2 ( ∑ ij aijbij + ∑ ij ajibji) Then we use the symmetry of bij and antisymmetry of aij, to switch the indices in the second term, which yields → ∑ ij aijbij = 1 2 ( ∑ ij aijbij + ∑ ij aji ︸︷︷︸ =−aij bji ︸︷︷︸ =bij ) = 1 2 ( ∑ ij aijbij − ∑ ij aijbij) = 0 also see this by writing the equation as a vector equation ⃗B = ∇ × ⃗A → ∇ × ( ⃗A + ∇η(t,⃗x)) = ∇ × ⃗A + ∇ × ∇η(t,⃗x) ︸ ︷︷ ︸ =0 = ∇ × ⃗A = ⃗B ✓ (7.7) where we used that the curl of a gradient is always zero.4 4 This is discussed in detail in Appendix A.16. The fact that everything we can measure remains completely unchanged by the transformations in Eq. 7.4 is known as gauge freedom or gauge symmetry.5 5 As mentioned above, a symmetry always refers to a set of transfor- mations which leaves our system unchanged. Gauge symmetry is an extremely useful discovery for the fol- lowing reason. We can describe electrodynamical systems us- ing the potentials A0, Ai. However, the physics in our system remains completely unchanged by a gauge transformation (Eq. 7.4). This means that we can use gauge transformations to simplify our calculations!6 6 Don’t worry if this seems like a cheap trick or otherwise strange to you because we will discuss what is really going on in the following chapter. For example, it is always possible to choose a scalar function η(t,⃗x) in such a way that ∇ · ⃗A = 0. This means that if we make this particular choice for our scalar function η(t,⃗x), all terms involving ∇ · ⃗A immediately drop out from all of our equa- tions and this often simpliﬁes our calculations tremendously. Choosing a speciﬁc scalar function is commonly called choos- ing a gauge. The choice leading to ∇ · ⃗A = 0 is known as the Coulomb gauge. This choice is useful, for example, as already 178 no-nonsense electrodynamics discussed in Section 4.2.3, because it simpliﬁes the Poisson equation for ⃗A. To understand how this works, imagine that we have found a potential ⃗A which yields the correct magnetic ﬁeld describing our system ⃗B = ∇ × ⃗A. In general, the divergence of this potential ⃗A is some non-vanishing scalar function ∇ · ⃗A(⃗x) = f (⃗x). Now, we can use our gauge freedom (Eq. 7.4) to modify the potential ⃗A′ = ⃗A + ∇η(t,⃗x) . (7.8) The divergence of this new potential reads ∇ · ⃗A′(t,⃗x) = ∇ · ⃗A(t,⃗x) + ∇ · ∇η(t,⃗x) = f (t,⃗x) + ∇ · ∇η(t,⃗x) . (7.9) Therefore, we can indeed achieve that ∇ · ⃗A′(t,⃗x) = 0 by using a gauge transformation (Eq. 7.4) with a scalar function ηsol(t,⃗x) which we can ﬁnd by solving ∇ · ∇ηsol(t,⃗x) = − f (t,⃗x) (7.10) since ∇ · ⃗A′(t,⃗x) ! = 0 ⇒ f (t,⃗x) + ∇ · ∇η(t,⃗x) ! = 0 . (7.11) It can be shown rigorously that a solution to Eq. 7.10 always exists. In practice, we don’t have to ﬁnd the function ηsol(t,⃗x) explicitly, but instead can simply use the knowledge that a function with the desired properties exists and set ∇ · ⃗A(t,⃗x) = 0. So far, gauge symmetry may seem like a boring mathematical trick, like voodoo or even both. However, gauge symmetry is really much more than that. Gauge symmetry is one of the guiding themes in modern physics and allows us to understand all fundamental interactions from a common perspective.77 Be warned that there are lots of discussions about in what sense gravity can be understood using gauge symmetry, but a proper discussion of this question is far beyond the scope of this book. While, once more, the full story is far too long to include it here, we can get at least some idea of how gauge symmetry gauge symmetry 179 and fundamental interactions are connected by considering a simple toy model.8 We will talk about this in the next chapter. 8 A complete discussion can be found in any textbook on gauge theory and, for example, in Jakob Schwichtenberg. Physics from Symmetry. Springer, Cham, Switzerland, 2018a. ISBN 978- 3319666303 However before we discuss it, we need to talk about a second puzzle piece which is necessary to understand gauge symmetry: quantum mechanics.9 9 This already demonstrates how gauge symmetry ties together seemingly unrelated parts of physics. Of course, we can’t discuss quantum mechanics properly here.10 10 If you’re interested in quan- tum mechanics, you might enjoy reading Jakob Schwichtenberg. No- Nonsense Quantum Mechanics. No- Nonsense Books, Karlsruhe, Ger- many, 2018b. ISBN 978-1719838719 But luckily we only need one small fact to move forward: in quantum mechanics, we describe what is going on using a com- plex function Ψ(⃗x) called the wave function. However, every- thing we can measure is a real number. This means that the relationship between quantities we can measure O and the wave function Ψ is always of the form O ≃ Ψ⋆Ψ, where the super- script ⋆ denotes complex conjugation. A direct consequence of this observation is that we have some freedom in our wave functions, analogous to how we have some freedom in our elec- tric and magnetic potentials. In particular, we always have the freedom to multiply our wave functions by a phase factor Ψ → ˜Ψ = eiϕΨ , (7.12) since O ≃ Ψ⋆Ψ ⇒ ˜Ψ⋆ ˜Ψ = (eiϕΨ)⋆eiϕΨ↷(eix)⋆ = e−ix = e−iϕΨ⋆eiϕΨ↷ = Ψ⋆ e−iϕeiϕ ︸ ︷︷ ︸ =1 Ψ↷ = Ψ⋆Ψ = O ✓ (7.13) One of the deepest observations in modern physics is that there is direct connection between the freedom in our wave function (Eq. 7.12) and the freedom in our electromagnetic potential (Eq. 7.4). How this comes about is the topic of the next chapter. 8 Electrodynamics as a Gauge Theory In this chapter, we will discuss the interplay between gauge symmetry and fundamental interactions in a bit more detail. Since we can’t discuss quantum mechanics and gauge theory in full detail, we will use a relatively simple toy model.1 1 You can ﬁnd a much more com- plete discussion of the ideas out- lined in this chapter in Jakob Schwichtenberg. Physics from Finance. No-Nonsense Books, Karlsruhe, Germany, 2019. ISBN 978-1795882415 At ﬁrst, it may seem like this toy model doesn’t really have any- thing to do with quantum mechanics, electrodynamics or even physics in general. However, this couldn’t be further from the truth. The interplay between interactions and gauge symmetry in the toy model is completely analogous to how things work in quantum mechanics and electrodynamics. We will discuss the connection between the toy model and our physical theories explicitly at the end of the chapter. For the moment, just take note that it’s not accidental that we use the same symbols Aµ, Fµν, Jµ to describe the toy model. Before we dive in, a short warning: we will discuss quite a bit of quantum mechanics in the following sections. But don’t worry if you don’t yet know anything about quantum mechanics and 182 no-nonsense electrodynamics can’t follow all of the arguments. Just keep reading and glimpse over the parts that you don’t understand. The most important ideas will still make sense and you can reread everything you didn’t understand once you know more about quantum me- chanics. 8.1 Symmetries intuitively Before we can discuss gauge symmetry, we have to talk about symmetries in general. So ﬁrst of all, what is a symmetry? Imagine a friend stands in front of you and holds a perfectly round ball in her hand. Then you close your eyes, your friend performs a transformation of the ball and afterward you open your eyes again. If now, for example, she rotates the ball while your eyes are closed, it is impossible for you to ﬁnd out whether she did anything at all. Hence, rotations are symmetries of the ball. In contrast, if she holds a cube, only very special rotations can be done without you noticing it. In general, all transformations which, in principle, change something but lead to an indistin- guishable result are symmetries. Formulated differently, a sym- metry takes us from one state to a different one which happens to have the same properties.22 In contrast, a redundancy takes us from one description of a state to another description of the same state. This will be discussed in detail below. It’s important to take note that with this deﬁnition, symmetries are observable properties of objects or systems. This is especially important in the context of gauge symmetries since here we electrodynamics as a gauge theory 183 need to be careful what transformations are symmetries and which are mere redundancies. This point can be confusing at ﬁrst and to understand it, we need to talk about subsystems and global or local transforma- tions of them. 8.1.1 Global vs. local symmetries A global transformation is one in which the whole ship is, for example, rotated as opposed to a transform in which only a part of it is rotated (which would be a local transformation). Clearly a global rotation of a ship takes us from one state to a physically different one since we are actively rotating the ship. However, if there is a physicist inside the ship with no possibility to look outside, there is no way for him to ﬁnd out whether he is in the original or the rotated ship.3 3 This thought experiment is known as Galileo’s ship experiment. Therefore, the subsystem has a global rotational symmetry. The crucial point is that, in principle, it would be possible to detect a difference since the ship and the rotated ship are two distinct states. In contrast, there is no way that how we could ever detect a global rotation of the whole universe. This is why the notion of subsystems is essential in this context 4. Moreover, in physics 4 D. Wallace and Hilary Greaves. Empirical consequences of symme- tries. British Journal for the Philosophy of Science, 65(1):59–89, 2014 184 no-nonsense electrodynamics we always consider sufﬁciently isolated subsystems, even if this is not explicitly stated and mathematically we take the limit |x| → ∞. Now what about local transformations? Our physicist inside the ship would immediately notice if we rotate only a part of the ship, e.g., a speciﬁc apparatus. Therefore, we don’t have local rotational symmetry. This is an important observation in the context of gauge theories where often special emphasis is put on local transformations. While global symmetries are common, local symmetries are rarely observed in nature.55 An example of a system with a local symmetry is a grid of com- pletely black balls. We can rotate each ball individually and it would be impossible to tell the difference. Now, after these preliminary remarks we can start to discuss gauge symmetries. In the following section, we introduce all relevant notions in the context of a simple toy model. This al- lows us to understand gauge symmetries without confusing abstractions and complicated formulas. 8.2 A toy gauge theory The toy model we will use in the following describes a sim- pliﬁed ﬁnancial market. It consists of several countries and the basic process we try to describe is that money can be car- electrodynamics as a gauge theory 185 ried around. This setup is arguably the simplest setup where a gauge symmetry shows up.6 6 The connection between ﬁnancial markets and gauge symmetries was ﬁrst put forward in [Ilinski, 1997] and later popularized in [Young, 1999] and [Maldacena, 2016]. First of all, let’s imagine that we have a common currency for several countries. For concreteness, we call this currency euro and the countries Germany, France, the United Kingdom and Italy. In addition, we consider this subsystem of the whole world iso- lated and assume that there is a trader who only does business within these countries. A crucial observation is that this sub- system has a global gauge symmetry since the absolute value of ﬁat money is, in general, not determined. We can change the units of the currency globally without any physical effect. For example, absolute prices are not ﬁxed by anything, only relative prices are.7 7 At least in our toy model, chang- ing the value of the currency has no effect since we imagine that all prices and wages are simply adjusted automatically. Of course, in the real world there could be psychological effects since people get used to certain prices. Imagine for example that our trader sells three tomatoes at €1 each and then uses this money to buy six apples at €0.5 each. The end result of such a process is completely unchanged if, for example, the government decides to print lots of euros such 186 no-nonsense electrodynamics that the value of each euro drops by a factor of ten. Afterwards, the trader gets €10 for each tomato but then needs to pay €5 per apple. So again, our trader starts with three tomatoes and ends up with six apples. This is no longer true if only one country, say France, decides to print lots of euros and give it to its people. Such a local change of the value of our currency certainly has a noticeable effect. The prices would only increase in France and our trader could sell his tomatoes in France at €10 each and buy apples at €0.05 in Germany.8 So when our trader starts again with three toma- 8 Take note that so far our toy model is completely static and therefore there is no dynamically adjusting of the prices. toes, he will now end up with sixty apples. In technical terms this example shows that our subsystem is not invariant under local transformations of the value of our currency. Alternatively, we can say that we have a global symmetry but not a local one. Now, in the real world there is more than one currency and we can imagine that we can change the numbers on the local currencies arbitrarily. Before we can discuss what this means for the symmetries of our system, we need to talk about the difference between two different kinds of transformations.99 To spoil the surprise: only invari- ance under active transformations means that we are dealing with a symmetry. Invariance under passive transformations merely indicates a redundancy in our description. 8.2.1 Active vs. passive transformations So far, we only considered active transformations. We discussed what happens when we rotate Galileo’s ship or when a country actually prints new money. These are real physical transforma- tions. However, there is also a different kind of transformation called passive transformations. A passive transformation is a change in how we describe a given system. electrodynamics as a gauge theory 187 For example, we can describe Galileo’s ship using curvilinear coordinates or a rotated coordinate system. Of course, such transformations never have any physical effect.10 In our ﬁ- 10 If we write down the equations describing our system appropriately this becomes immediately clear. We will discuss this explicitly below. nancial toy model, a passive transformation is a change of the money coordinate system, for example, when we drop digits from the euro. To summarize: passive transformations relate different descrip- tions of the same physical situation, while active transforma- tions relate different physical situations. A lot of confusion surrounding gauge symmetries can be traced back to confusion about these two kinds of transformations and it’s crucial to keep them separate. With this in mind, we are ﬁnally ready to discuss another in- credibly important distinction which is necessary to make sense of gauge transformations. 188 no-nonsense electrodynamics 8.2.2 Symmetries vs. redundancies As mentioned in the last section, passive transformations are always possible without physical implications since they are merely a change in our description of the system. However, if we now reconsider our ﬁnancial toy model, it is not immediately clear how this comes about. So far, locally adding or dropping zeroes from a given currency clearly makes a dif- ference. But this is only a problem in our description and not a physical effect. When we locally change the \"money coordinate system\" in a given country, we effectively introduce a new cur- rency. This new currency then has a speciﬁc value relative to the original currency. For concreteness, we now introduce independent local cur- rencies in, say Germany, which we call Deutsche Mark (DM). Moreover, we introduce Francs (F) in France, in England Pounds (P) and Lira (L) in Italy. This is only possible if we introduce some sort of bookkeepers who keep track of the values of local currencies and are able to exchange one currency for another. If we assume that the book- keepers always adjust their exchange rates perfectly whenever the value of our local currency changes, such changes have no noticeable effect. For example, let’s assume that the exchange rates are DM/P = 1 P/F = 2 F/L = 10 DM/L = 20 . (8.1) Now, if a trader starts with 1DM, he can trade it for 1P, then use it to trade it for 2F, then trade these for 20L and ﬁnally trade these back for 1DM. electrodynamics as a gauge theory 189 When a local currency changes, the bookkeepers simply adjust their exchange rates accordingly and the situation remains the same. Thanks to these bookkeepers, we can see that such passive transformations really make no difference as it should be. What we did here is a typical example of what is necessary to make the invariance under local passive transformations man- ifest. When we want to describe a system in such a way that arbitrary descriptions are allowed, we need to add new (math- ematical) ingredients. A passive transformation, by deﬁnition, cannot lead to a new physical situation and our bookkeepers make sure that this is indeed the case.11 11 As we will discuss below, the mathematical tools that allow us to describe a given system using arbitrary coordinate system are called connections. In physics, we call these connections gauge potentials. An important point is that, so far, our bookkeepers aren’t dy- namical actors which start actions on their own. In addition, they do not have any physical inﬂuence on the dynamics of the system. Instead their only task is to keep track of the local coordinate changes. Since it is important to keep passive and active transformations separate, we also need to keep invariance under them separate. We call invariance under passive transformations a redundancy and invariance under active transformations a symmetry. Using this deﬁnition, we can say that after the introduction of the bookkeepers our description of the system has a local redun- 190 no-nonsense electrodynamics dancy. Now what about local symmetry, i.e. invariance under active local transformations?1212 As discussed above, without the bookkeepers our ﬁnancial toy system was not invariant under active local transformation. At ﬁrst glance it may seem as if after the introduction of the bookkeepers we also have a local symmetry. However, this is not the case. The bookkeepers do not react to active transforma- tions. So far, they are purely mathematical objects which only exist to keep track of local passive transformations. An active transformation is a real physical change and our purely math- ematical bookkeepers cannot induce a physical change on their own which would cancel the active transformation and make the system invariant.1313 As we will discuss below, it is possible to turn the bookkeepers into active agents which are capable of inducing physical changes. Then we have to discuss the question of local symmetry again. However, it is already clear at this point that the answer crucially depends on the rules according to which the bookkeepers behave. In physics, we have to derive these rules from experiments. At this point we can already give a preliminary deﬁnition of the notion of local gauge symmetry, which, however, will be reﬁned later. A local gauge symmetry is not really a symmetry but rather a redundancy which appears in our description. The redundancy that exists in our description of the ﬁnancial toy example after the introduction of the bookkeepers is an example of a local gauge symmetry. 8.3 Gauge dynamics So far, our bookkeepers are purely mathematical ingredients which we introduced to make our description invariant under local passive transformations. Now, if we go back to our ﬁnancial toy example, we can imag- ine that bookkeepers can inﬂuence the dynamics of a system and even become dynamical actors on their own. First of all, we can imagine that there are imperfections in the exchange rates. If this is the case, we can imagine that our electrodynamics as a gauge theory 191 trader no longer trades goods, but starts to trade money. Now that we have local currencies and possibly imperfections in the exchange rates, this can be a lucrative business. This is an explicit example of how our bookkeepers can inﬂuence the dy- namics of the system. For example, let’s imagine the exchange rates set by the bank are as follows DM/P = 1 P/F = 2 F/L = 10 DM/L = 10 . (8.2) Now our trader is able to earn money simply by exchanging money. If he starts with 1DM, he can trade it for 1P, then use the pound to trade it for 2F, then trade these for 20L and ﬁnally trade these for 2DM. In the ﬁnancial world this is known as an arbitrage opportu- nity.14 14 Arbitrage is a risk-free possibility to earn money. In mathematics, we call the quantity which we here use to describe arbitrage opportunities the curvature. But wait, the exchange rates depend on the values of the local currencies which we can change at will . . . Does this mean that the amount of money our trader earns depends on these arbitrary choices? First of all, our exchange rates indeed do change when, for example, Italy decides to drop a zero from their currency. We 192 no-nonsense electrodynamics need to consistently take such a change of the local money coordinate system L → ˜L = L/10 into account and this means that we need to adjust the exchange rates accordingly: DM/P = 1 P/F = 2 F/L = 10 → F/ ˜L = F/(L/10) = 1 DM/L = 10 → DM/ ˜L = DM/(L/10) = 1 . (8.3) However, the amount of money our trader earns is unchanged by such a re-scaling! If he starts again with 1DM, he can still trade it for 1P, which he can trade for 2F, then trade these for 2L and ﬁnally trade these for 2DM. The ﬁnal result is the same as before. What we have learned here is a crucial aspect of every gauge theory. An important task is to ﬁnd quantities which do not depend on local conventions. For example, the possibility to earn risk-free money is independent of how we choose our local coordinate system. There is another way to make risk-free money which involves interest rates. Let’s assume that the interest rate is 2 % per year for the Deutsche Mark. Moreover, we assume that the exchange rate changes over time and is now DM/L = 10 (8.4) electrodynamics as a gauge theory 193 and in one year DM/L = 5 . (8.5) In this scenario it’s possible to earn money as follows. Our trader can borrow 1DM and after 1 year he has to pay 1.02DM back. Moreover, he can now exchange his 1DM for 10L. After one year he can then exchange his 10L for 2DM. Therefore, after paying back the 1.02DM he has made 0.98DM in proﬁt. This is another example of a risk-free arbitrage trade. The crucial point in these examples is that the situation in our toy model is a very different one whether there is an arbitrage opportunity or not. Formulated differently, when there is an arbitrage opportunity, our bookkeepers are more than mere mathematical tools since their exchange rates actively shape the dynamics of the system.15 However, so far, our bookkeepers 15 In physics, the situation with an arbitrage opportunity corre- sponds to a system with non-zero curvature. But take note that this curvature is not necessarily dy- namical. Only when the curvature is dynamical are we dealing with a gauge ﬁeld. Otherwise we are describing physics happening in a static curved space. are still not dynamical. Their exchange rates don’t change and while the bookkeepers have a real effect on the system they only yield the static background in front of which all dynamical actors do their business. However, next we can imagine that our bookkeepers become dynamical actors. In the real world, banks calculate exchange rates, but they are also institutions that live in the real world, not only in our description of ﬁnancial markets, and make deci- sions dynamically. Promoting bookkeepers to dynamical objects which follow their own rules is a crucial step to make our toy model more realistic.16 16 In physics, a dynamical object is something with its own equation of motion. While we can always introduce local redundancies into our description of a given system, we are only dealing with a gauge theory when the bookkeepers are dynamical objects in the system and not only artifacts of our description. The deﬁning feature of a gauge theory is that the bookkeepers, which are necessary to make the description of the system invariant under 194 no-nonsense electrodynamics local passive transformations, are physical actors that shape the dynamics of the system. In practice this means that exchange rates are adjusted dynamically depending on what else happens in the system. We will discuss the explicit rules according to which this happens in Section 8.3.2. Before we move on and discuss gauge symmetries and gauge theories in more mathematical terms, there is a question which we still need to answer. Now that we have promoted our bookkeepers to dynamical agents, do we end up with a local symmetry? Our bookkeepers are now parts of the system and not just of our description and they can therefore induce real changes. Thus, in principle, it’s possible that the action of an active trans- formation is canceled through a bookkeeper. However, this is not automatically the case and in general, we still don’t have a local symmetry. This will become a lot clearer as soon as we dis- cuss gauge symmetries in the context of physical theories like electrodynamics. But ﬁrst we have to introduce the proper mathematical notions which can all be easily understood in the context of our toy model. 8.3.1 Mathematical description of the toy model Mathematically, we imagine that our countries live on a lat- tice. Each point on the lattice is labelled by d-numbers: ⃗n = (n1, n2, · · · , nd). In other words, each country can be identiﬁed by a vector ⃗n which points to its location. We can move from one country to a neighboring country by using a basis vector ⃗ei, where i denotes the direction we are moving. For example, ⃗e2 = (0, 1, 0 · · · , 0). electrodynamics as a gauge theory 195 We denote the exchange rates between the country labelled by the vector ⃗n and its neighbor in the i-direction by R⃗n,i. For ex- ample, if the country at the location labelled by ⃗n uses Deutsche Marks and its neighbor in the 2-direction uses Francs, R⃗n,2 tells us how many Francs we get for each Deutsche Mark. In physics, we usually introduce the corresponding logarithms R⃗n,i ≡ eAi(⃗n) , (8.6) where Ai(⃗n) ≡ ln(R⃗n,i). The next ingredient that we need is a notation for gauge trans- formations. In our toy model, a gauge transformation is a change of currencies and directly impacts, for example, the exchange rates. We use the notation f (⃗n) to denote a change of the currency in the country at ⃗n by a factor of f (⃗n). In addition, we again introduce the corresponding logarithm f (⃗n) ≡ eϵ(⃗n) . (8.7) In general, when we perform such a gauge transformation in the country labelled by ⃗n and also in the neighboring country in the i-direction, the corresponding exchange rate changes as follows17 17 The currency in the country at ⃗n gets multiplied by f⃗n and the currency in its neighbor in the i- direction by f⃗n+⃗ei . Therefore, the exchange rate gets modiﬁed by the ratio of these two factors. R⃗n,i → f (⃗n +⃗ei) f (⃗n) R⃗n,i . (8.8) In terms of the logarithms this equation reads R⃗n,i = eAi(⃗n) → f (⃗n +⃗ei) f (⃗n) R⃗n,i↷Eq. 8.7 = eϵ(⃗n+⃗ei) eϵ(⃗n) eAi(⃗n)↷ = eAi(⃗n)+ϵ(⃗n+⃗ei)−ϵ(⃗n) (8.9) and we can conclude Ai(⃗n) → Ai(⃗n) + ϵ(⃗n +⃗ei) − ϵ(⃗n) . (8.10) We learned above that an import aspect of the system is whether arbitrage opportunities exist. An arbitrage opportunity exists 196 no-nonsense electrodynamics when we can trade currencies in such a way that we end up with more money than we started with. But we can only make such a statement when the starting currency and the ﬁnal cur- rency are the same. Only then can we be certain whether the ﬁnal amount of money is larger than the initial amount. There- fore, we need to trade money in a loop. The total gain we can earn by following a speciﬁc loop can be quantiﬁed by G = R⃗n,iR⃗n+⃗ei,j 1 R⃗n+⃗ej,i 1 R⃗n,j . (8.11) When this gain factor is larger than one, we can earn money by trading money following the loop, if it is smaller than one we lose money. To understand the deﬁnition of the gain factor, imagine that we start with 1DM. We trade it for Pounds and R⃗n,1 = 1 tells us that we get 1P. Afterwards, we trade our Pounds for Francs and R⃗n+⃗e1,2 = 2 tells us that we get in total 2F. Afterwards, we trade our Franc for Lira. R⃗n+⃗e2,1 = 10 tells us that we get 1/10 Franc for each Lira. Hence, we have to calculate 2F/R⃗n+⃗e2,1 = 20L. Finally, we use that R⃗n,2 = 10 tells us that we get 10L for each Deutsche Mark and therefore calculate 20L/R⃗n,2 = 2DM.1818 The crucial point is that our exchange rates R⃗n,i always tell us how many of the currency at the neighboring country in the i-direction we get for each unit of the local currency at the country at ⃗n. Hence, we sometimes have to divide by the corresponding exchange rate to calculate the resulting amount of a new currency. electrodynamics as a gauge theory 197 Once more we introduce the corresponding logarithm G ≡ eFij(⃗n) (8.12) and again, we can rewrite our equation in terms of the loga- rithms19 19 In physics Fij is directly related to components of the magnetic ﬁeld. For example, F12 = −B3.Fij(⃗n) = Aj(⃗n +⃗ei) − Aj(⃗n) − [Ai(⃗n +⃗ej) − Ai(⃗n)] . (8.13) A crucial consistency check is that G and Fij are unchanged by gauge transformations. We already argued above that an arbitrage opportunity is something real and thus cannot depend on local choices of the coordinate system. Quantities like this are usually called gauge invariant. So in words, G and Fij(⃗n) encode what is physical in the structure of exchange rates.20 20 A single exchange rate Ai(⃗n) is gauge dependent and can there- fore, for example, be set to zero simply by changing a local money coordinate system. Moreover, an important technical observation is that Fij(⃗n) is antisymmetric: Fij(⃗n) = −Fji(⃗n), which follows directly from the deﬁnition. So far, we only talked about spatial exchange rates. However, there are also temporal exchange rates, i.e. interest rates. A clever trick to incorporate this is to introduce time as the zeroth- coordinate like we do in special relativity. In other words, in addition to speciﬁc locations (countries) our lattice now contains copies of these locations at different points in time. This means that a point on the lattice is speciﬁed by d + 1 coordinates ⃗n = (n0, n1, n2, · · · , nd) and the zeroth component indicates the point in time. Then, Eq. 8.13 reads21 21 The non-vanishing components of Fµν(⃗n) with either µ = 0 or ν = 0 are directly related to what we call electric ﬁeld in physics. For example, F10 = E1. Fµν(⃗n) = Aν(⃗n +⃗eµ) − Aν(⃗n) − [Aµ(⃗n +⃗eν) − Aµ(⃗n)] , (8.14) where previously i, j ∈ {1, 2, . . . , d} and now µ, ν ∈ {0, 1, 2, . . . , d}. In the continuum limit, where the lattice spacing goes to zero, Eq. 8.10 becomes22 22 To understand this, take note that in Eq. 8.14 we get in this limit the difference quotient. Compare this equation with Eq. 7.4.Aµ(xµ) → Aµ(xµ) + ∂ϵ ∂xµ (8.15) and Eq. 8.14 reads23 23 This is exactly how we deﬁned the ﬁeld strength tensor in Sec- tion 2.4. 198 no-nonsense electrodynamics Fµν(xµ) ≡ ∂Aν ∂xµ − ∂Aµ ∂xν . (8.16) We can not only earn money by trading money itself, but also by trading goods like, for example, copper. Depending on the local prices it can be lucrative to buy copper in one country, bring it to another country, sell it there, and then go back to the original country to compare the ﬁnal amount of money with the amount of money we started with. The gain factor for such a process is given by g = p(⃗n +⃗ei) p(⃗n)R⃗n,i . (8.17) To understand this deﬁnition, imagine that we start with 10DM and the price for one kilogram of copper in Germany is p(⃗n) = 10DM. This means that we can buy exactly 1 kilogram of cop- per. Then we can go to the neighboring country and sell our copper for, say, 30F since p(⃗n +⃗e1) = 30F. Afterward, we can go back to Germany and exchange our 30F for 15DM since, say, R⃗n,i = 0.5. Therefore, we have made in total 5DM. Again, a gain factor larger than one means that we earn money and a gain factor smaller than one that we lose money.2424 If you are unsure which quantity goes in the numerator and which in the denominator, ask yourself: Would it increase our proﬁt if the given quantity is larger? If the answer is yes we write in the nu- merator, if not in the denominator. For example, a higher price of cop- per in France certainly increases our proﬁt. Hence p(⃗n +⃗ei) is written in the numerator. Similarly, a higher price of copper in Germany would lower our proﬁt and therefore, we write it in the denominator. Again, we introduce the corresponding logarithm g ≡ eJi(⃗n) (8.18) and Eq. 8.17 then reads in terms of the corresponding loga- electrodynamics as a gauge theory 199 rithms g = p(⃗n +⃗ei) p(⃗n)R⃗n,i↷ eJi(⃗n) = eϕ(⃗n+⃗ei) eϕ(⃗n)eAi(⃗n)↷ Ji(⃗n) = ϕ(⃗n +⃗ei) − ϕ(⃗n) − Ai(⃗n) . (8.19) The amount of money we earn depends on the amount of cop- per we carry around. Thus, in general, we have Ji(⃗n) = q(ϕ(⃗n +⃗ei) − ϕ(⃗n) − Ai(⃗n)) , (8.20) where q is the amount of copper involved in the trade. Com- pletely analogous to what we did above, we can generalize this formula for situations that involve time by replacing i ∈ {1, 2, . . . , d} with µ ∈ {0, 1, 2, . . . , d}). Jµ(⃗n) = q(ϕ(⃗n +⃗eµ) − ϕ(⃗n) − Aµ(⃗n)) . (8.21) In addition to the interpretation as a gain factor, there is another way we can look at the four quantities Jµ(⃗n). As mentioned above, the amount of money we can earn in a copper trade Jµ is proportional to the amount of copper in- volved. Hence, we can use Jµ as a measure, for example, of the amount of copper that ﬂows between countries. In the trade described by Ji(⃗n) copper is transported from the country at ⃗n to the neighboring country at ⃗n +⃗ei. Hence, Ji(⃗n) is a measure of the amount of copper that ﬂows between the two countries. The trade related to the gain factor J0(⃗n) does not involve the exchange of copper between neighboring countries. Instead, J0(⃗n) tells us how much money we can earn by buying copper and selling it at a later point in time in the same country. Again, J0(⃗n) is directly proportional to the amount of copper involved. Hence, J0(⃗n) is a measure of the amount of copper in the coun- try at ⃗n. This is an important idea since so far we had nothing in our description that contained any information about the amount of 200 no-nonsense electrodynamics copper at a certain location or about how copper ﬂows through the system. Copper is represented in our description in some- what abstract terms by its price. However, the amount of copper in a given country is not proportional to the local price of cop- per. The local price depends on the local money coordinate system and therefore cannot represent directly a quantity like the amount of copper. In other words, the amount of copper in a given country must be represented by a gauge indepen- dent quantity like J0(⃗n). The same is true for the ﬂow of copper Ji(⃗n).2525 In physics J0 = cρ (see Section 2.2) describes the charge density while Ji describes the current density (see Section 2.3). Now that we’ve introduced a mathematical notation to describe our toy model, we can also write down the speciﬁc rules accord- ing to which the system behaves. 8.3.2 Gauge rules First of all, of course, it’s possible to consider various kinds of dynamics within our ﬁnancial toy model which correspond to different kinds of laws. However, in the following we discuss a very particular set of rules which correspond to what is known as Maxwell’s equations in physics. To derive these, we start with the crucial assumption that cop- per is conserved (i.e. no copper is destroyed or produced).26 26 By invoking Noether’s ﬁrst theo- rem and a Lagrangian formulation this can be derived using the global symmetry discussed above. This means that whenever the amount of copper decreases in a given country it must have gone somewhere. Equally, whenever the amount of copper increases in a country it must have come from somewhere. In words, this means:27 27 We only consider elementary cop- per trade loops. In these elementary loops copper always ﬂows \"from left to right\". For example, Ji(⃗n) is proportional to the amount of copper that ﬂows from ⃗n to ⃗n +⃗ei. A positive Ji(⃗n) therefore represents an outﬂow of copper. Analogously, the ﬂow involved in the process which yields Ji(⃗n −⃗ei) also goes from left to right. However, a pos- itive Ji(⃗n −⃗ei) means a net inﬂow. Therefore, we need a relative minus sign. If we have a positive quantity on the left-hand side, the amount of copper increases and this must correspond to a net inﬂow. change of amount of copper in country ⃗n = total net ﬂow which we can write in mathematical terms as J0(⃗n +⃗e0) − J0(⃗n) = − ( d ∑ i=1 Ji(⃗n) − d ∑ i=1 Ji(⃗n −⃗ei) ) electrodynamics as a gauge theory 201 where we sum over all neighboring countries. We can also write this as J0(⃗n +⃗e0) − J0(⃗n) + ( d ∑ i=1 Ji(⃗n) − d ∑ i=1 Ji(⃗n −⃗ei) ) = 0 . (8.22) and we can visualize this equation as follows: In the continuum limit, Eq. 1.4 becomes the usual continuity equation28 28 Here and in the following, we use the usual summation convention, i.e. there is an implicit sum over all indices that appear in pairs. 3 ∑ µ=0 ∂µ Jµ = 0 . (8.23) Now, what we really want is an equation that let’s us under- stand how arbitrage opportunities show up and evolve as time passes. Moreover, we already know that the good quantities to describe our system are Jµ and Fµν since these do not depend on local conventions. The quantities Jµ contain information about the positions and ﬂow of copper, while Fµν represent arbitrage opportunities. However, the ﬁrst naive guess to write Fµν on one side and Jµ on the other side of an equation fails because Jµ has only one index but Fµν has two. There is one additional piece of information that we can use, namely Eq. 8.23. If the right-hand side of an equation yields zero when we take the derivative ∂µ Jµ, the left-hand side has, of course, to be zero too. But there 202 no-nonsense electrodynamics is no reason why ∂µFµν should be zero and this is another hint that our ﬁrst naive guess is wrong. The crucial observation is that 3 ∑ µ=0 3 ∑ ν=0 ∂ν∂µFµν = 0 , (8.24) since Fµν is antisymmetric but partial derivatives commute ∂µ∂ν = ∂ν∂µ.29 This suggests that we try 29 The sum over something antisym- metric times something symmetric is always zero. We discussed this explicitly in Chapter 7. This is anal- ogous to how the integral over a symmetric function (e.g. cos(x)) times an antisymmetric function (e.g. sin(x)) over a symmetric interval yields exactly zero: ∫ a −a sin x cos x = 0. 3 ∑ ν=0 ∂νFµν = µ0 Jµ , (8.25) where we introduced the proportionality constant µ0 which encodes how strongly the pattern of arbitrage opportunities react to the presence and ﬂow of copper. This equation has exactly one free index (µ) on both sides and most importantly, both sides yield zero if we calculate the derivative 3 ∑ µ=0 3 ∑ ν=0 ∂µ∂νFµν = µ0 3 ∑ µ=0 ∂µ Jµ↷Eq. 8.23 and Eq. 8.24 0 = 0 ✓ Eq. 8.25 is the famous inhomogeneous Maxwell equation.3030 Take note that this derivation of the inhomogeneous Maxwell equations is analogous to how the Einstein equation is often derived in textbooks. For an alternative derivation using a similar toy model, see [Maldacena, 2016]. For our discrete system it reads d ∑ ν=1 Fµν(⃗n) − d ∑ ν=1 Fµν(⃗n −⃗eν) = µ0 Jµ(⃗n) . (8.26) Moreover, Eq. 1.4 with µ = 0 reads d ∑ ν=1 F0ν(⃗n) − d ∑ ν=1 F0ν(⃗n −⃗eν) = µ0 J0(⃗n)↷ d ∑ i=1 F0i(⃗n) − d ∑ ν=1 F0i(⃗n −⃗ei) = µ0 J0(⃗n) , (8.27) where we used that F00(⃗n) = 0, since Fµν is antisymmetric. We have on the right-hand side µ0 J0(⃗n), which is proportional to the amount of copper located at ⃗n. Thus, for µ = 0 Eq. 8.25 electrodynamics as a gauge theory 203 gives us information about the pattern of exchange rates around a country in which copper is present. In the continuum limit, Eq. 8.27 becomes 3 ∑ i=0 ∂i F0i = µ0 J0 , (8.28) which is known as Gauss’s law. Similarly, for µ = i ∈ {1, 2, 3}, we get equations that give us information about the pattern of exchange rates which are present whenever copper ﬂows d ∑ ν=1 Fiν(⃗n) − d ∑ ν=1 Fiν(⃗n −⃗eν) = µ0 Ji(⃗n) . (8.29) In the continuum limit, this equation becomes ∂0F0i + 3 ∑ j=0 ∂jFij = µ0 Ji , (8.30) which is known as the Ampere-Maxwell law. There are two important lessons in this section. Firstly, we saw why redundancies can be useful. By thinking about redundan- cies we learn which quantities are independent of local conven- tions. Secondly, we derived the correct equation describing the interplay between goods like copper and arbitrage opportuni- ties by using the ideas that we should describe the system with gauge invariant quantities and that copper is conserved.31 31 Take note that the homogeneous Maxwell equation 0 = ∂α ( 1 2 ϵαβγδ Fδγ ) = ∂λ Fµν + ∂µ Fνλ + ∂ν Fλµ (8.31) follows directly from the deﬁnition of Fµν (Eq. 8.14). This is discussed in Appendix A.16.1. Now, we move on and discuss the simplest instances of gauge symmetries in physics. 8.4 Gauge symmetry in physics 8.4.1 Gauge symmetry in Quantum Mechanics In our ﬁnancial toy model, we used local prices p(⃗n) to describe copper. Analogously, in quantum mechanics we use the wave 204 no-nonsense electrodynamics function Ψ(x) to describe particles like, for example, an electron. A wave function is a complex function which can be written in polar form Ψ(x) = R(x)eiϕ(x) . (8.32) Observables are related to products of the form ψ⋆ i (x) ˆOΨ(x), where ˆO denotes an operator. Therefore, we can multiply the wave function by a global phase factor without changing any- thing32 32 This is a global transformation, analogous to, for example, a rota- tion of the whole subsystem. Under such a global rotation, all vectors have to be rotated accordingly. Analogously, all wave functions must be transformed, i.e. ψi → eiϵψi too. Ψ(x) → eiϵΨ(x) (8.33) since ψ⋆ i (x) ˆOΨ(x) → ψ⋆ i (x)e−iϵ ˆOeiϵΨ(x) = ψ⋆ i (x) ˆOΨ(x) (8.34) It is important to note that this is an observable symmetry and is completely analogous to, for example, the rotational sym- metry of Galileo’s ship. To understand this, imagine that a quantum physicist sits inside the ship which represents our subsystem. This quantum physicist performs an experiment with electrons which are injected through a small slit. We can perform a global transformation as given in Eq. 8.33 by using a phase shifter. So we phase shift our electron before we insert it into the ship. The crucial point is that it’s impossible for the quantum physicist inside the box to ﬁnd out whether we per- formed such a phase shift or not. Therefore, a global phase shift is indeed a symmetry. Now, what about local phase shifts in quantum mechanics? A local phase shift is a transformation of the form Ψ(x) → eiϵ(x)Ψ(x) , (8.35) electrodynamics as a gauge theory 205 where the transformation parameter ϵ(x) is now a function of the location x, i.e. no longer globally the same. Again it’s important to keep in mind that this kind of transformation can be understood in an active and in a passive sense. We can notice immediately that our description is not invari- ant since, for example, for the momentum operator ˆp = −i∂x contains a derivative:33 33 For simplicity, we restrict our- selves to one spatial dimension and work with ¯h = 1.ψ⋆ i (x) ˆpΨ(x) →ψ⋆ i (x)e−iϵ(x) ˆpeiϵ(x)Ψ(x)↷ = −iψ⋆ i (x)e−iϵ(x)∂xeiϵ(x)Ψ(x)↷ = −iψ⋆ i (x)∂xΨ(x) + ψ⋆ i (x)(∂xϵ(x))Ψ(x)↷ ̸= ψ⋆ i (x) ˆpΨ(x) . (8.36) However, if we interpret the local transformation in the pas- sive sense, it shouldn’t make any difference.34 Analogous to 34 Reminder: passive transformation = coordinate transformation.what we did in our ﬁnancial toy model, we can achieve this by introducing a bookkeeper Aµ which keeps track of such local changes of the phase. In particular, we replace the momentum operator with the so-called covariant momentum operator ˆP = −i∂x − Ax . (8.37) This bookkeeper Ax becomes under a local phase shift (Eq. 8.35)35 35 This is exactly the continuum limit of Eq. 8.10. Ax → Ax + ∂xϵ(x) . (8.38) After the introduction of this bookkeeper our description is indeed invariant under local phase shifts. ψ⋆ i (x) ˆPΨ(x) →ψ⋆ i (x)e−iϵ(x) ˆPeiϵ(x)Ψ(x)↷ = ψ⋆ i (x)e−iϵ(x)( − i∂x − Ax − ∂xϵ(x))eiϵ(x)Ψ(x)↷ = −iψ⋆ i (x)∂xΨ(x) + ψ⋆ i (x)(∂xϵ(x))Ψ(x)↷ − ψ⋆ i (x)AxΨ(x) − ψ⋆ i (x)(∂xϵ(x))Ψ(x)↷ = ψ⋆ i (x) ˆPΨ(x) . (8.39) 206 no-nonsense electrodynamics But are local phase shifts also a real symmetry of quantum mechanics? To understand this, we again ask our quantum physicist in the ship to perform an experiment with electrons. However, this time we do not perform the phase shifts globally but locally. This means that we put the phase shifter inside the ship. Now, the quantum physicist can ﬁnd out that a phase shift did happen, even if he can’t see the phase shifter directly. All he has to do is perform a double slit experiment. The result of the double slit experiment is dramatically altered by a local phase shift36. 36 Y. Aharonov and D. Bohm. Signiﬁcance of electromagnetic potentials in the quantum the- ory. Phys. Rev., 115:485–491, 1959. doi: 10.1103/PhysRev.115.485. [,95(1959)]; and Gerard ’t Hooft. Gauge theories of the forces be- tween elementary particles. Sci. Am., 242N6:90–116, 1980. [,78(1980)] Therefore, we can conclude that local phase shifts are not sym- metries of quantum mechanics. Now, it’s again possible that our bookkeepers Aµ become real dynamical actors and are completely analogous to what we dis- cussed for our ﬁnancial toy model. The theory which describes the dynamics of the bookkeepers is known as electrodynamics. 8.4.2 Gauge Symmetry in Electrodynamics The equations of electrodynamics (Maxwell’s equations) also posses a global symmetry Aµ(x) → Aµ(x) + aµ , (8.40) electrodynamics as a gauge theory 207 where aµ are arbitrary real numbers. This comes about since we can only measure potential differences.37 37 We discussed this in the previous chapter. Like in the two previous examples, the invariance under this global transformation is a real observable symmetry. To under- stand this, we again imagine a physicist in a ship which this time, however, is isolated from the ground. Charging the ship leads to a global increase of the electric po- tential A0(x) → A0(x) + φ0, while Ai(x) remains unchanged. However, this change has no measurable effect inside the sub- system since Bi = ϵijk∂j Ak and Ei = −∂i A0 − ∂t Ai are un- changed. So, as long as we raise the electric potential A0 globally inside the ship, there is no possibility for the physicist to detect that we changed the potential.38 38 If this is unclear, recall that, for example, a bird can walk on an uninsulated power line without any injuries. This is possible because the bird walks solely in the subsys- tem \"power line\" and the electric potential is only high relative to the potential at the ground. Alterna- tively, imagine Faraday sitting in a metal cage which is insulated from the ground. It’s possible to electrify the cage without any notable effect inside the cage. In other words, we can raise the electric potential globally inside such a Faraday cage without inducing any measurable effect [Aharonov and Bohm, 1959]. We can understand this by recalling that in Section 4.1.2, we discov- ered that the electric ﬁeld inside a charged sphere is zero. Similarly to what we discussed above, we can also argue that a local shift of the electromagnetic potential is not a symmetry of the system. To understand this, imagine that we only change the potential of a single object inside the ship. Clearly the physi- cist would have no problem ﬁnding this out. The most important point is that in electrodynamics, our book- keepers Aµ are dynamical physical actors and they can induce real physical changes. A famous example of this phenomenon is the so-called Aharonov-Bohm effect, where a non-zero Aµ induces a phase shift in the wave function.39 39 Y. Aharonov and D. Bohm. Signiﬁcance of electromagnetic potentials in the quantum theory. Phys. Rev., 115:485–491, 1959. doi: 10.1103/PhysRev.115.485. [,95(1959)] 208 no-nonsense electrodynamics Moreover, completely analogous to what we did in Eq. 8.14 we can deﬁne the quantity4040 In our ﬁnancial toy model, this quantity encodes how much money we can make by trading money in a loop (i.e. about an arbitrage opportunity). Fµν(xµ) ≡ ∂Aν ∂xµ − ∂Aµ ∂xν , (8.41) which encodes in gauge invariant terms information about the presence of electromagnetic ﬁelds. In the context of electrody- namics, the quantity in Eq. 8.41 is known as the ﬁeld strength tensor. With this in mind, we can now put the puzzle pieces together and disentangle real symmetries from mere redundancies in quantum mechanics and electrodynamics. 8.4.3 Putting the puzzle pieces together First, we noted that there is a global symmetry in quantum mechanics (Eq. 8.33) Ψ(x) → eiϵΨ(x) , (8.42) but not a local one (Eq. 8.35) Ψ(x) → eiϵ(x)Ψ(x) . (8.43) However, we then learned that we can rewrite our equations such that they are invariant under local transformations by introducing bookkeepers Aµ. We then argued that the invariance under global transforma- tions represents a real symmetry, while the invariance under electrodynamics as a gauge theory 209 local transformations is only a redundancy. Formulated differ- ently, we have invariance under active global transformations and also passive local transformations if we formulate the the- ory appropriately. However, quantum mechanics is not invari- ant under active local phase shifts. Analogous to what we did in the ﬁnancial toy model, we then argued that our bookkeepers Aµ can also appear as real dynam- ical parts of the system and not only as purely mathematical bookkeepers. The theory which describes the dynamics of the bookkeepers is electrodynamics. The bookkeepers Aµ then be- come what we usually call the electromagnetic potential. The crucial point is then that as soon as we have a system where the bookkeepers are no longer purely mathematical objects but physical parts of it, they can induce measurable changes. An important example is the Aharonov-Bohm effect, where a non- zero potential induces a phase shift in the wave function41. 41 Y. Aharonov and D. Bohm. Signiﬁcance of electromagnetic potentials in the quantum theory. Phys. Rev., 115:485–491, 1959. doi: 10.1103/PhysRev.115.485. [,95(1959)] What this implies immediately is that, in principle, it’s possible to cancel any local phase shift using an electromagnetic poten- tial Aµ. However, it’s important to take note that we still do not have a local symmetry when an electromagnetic potential is present, for example, in the ship in which our quantum physi- cist detects local phase shifts. It is instructive to reformulate this point using the notions of \"active transformation\" and \"passive transformation\" which were introduced in Section 8.2.1. A passive transformation is simply a change of the coordinate system and therefore cannot lead to any physical change. All we achieve through a passive transformation is a different de- scription of the same physical situation. Therefore, when we perform a passive transformation, we must be careful to keep our description consistent. In particular, this means that when- ever we perform a local passive transformation, we have to accompany Ψ → eiϵ(x)Ψ (8.44) 210 no-nonsense electrodynamics always with Aµ → Aµ + ∂µϵ(x) . (8.45) When we perform a passive transformation these two transfor- mations always go hand in hand. In contrast, an active transformation means that a real physical change happens and therefore the physical situation doesn’t need to remain unchanged. Especially, when we induce an active local phase shift Ψ → eiϵ(x)Ψ , (8.46) the corresponding transformation of Aµ does not happen auto- matically. Otherwise we wouldn’t be able to detect local phase shifts in experiments. It is possible to cancel the phase shift in Ψ by using Aµ. But this only happens when we actively pre- pare the system in a particular way, for example, by using an Aharonov-Bohm type setup. In other words, the active shifts in Ψ and Aµ are two separate transformations which can happen independently. To summarize: quantum mechanics and electrodynamics are invariant under active global gauge transformations. Hence, the global gauge symmetry is a real symmetry. However, only our description is invariant under passive local transformations. Therefore, local gauge symmetry is a redundancy. Now, after these discussions of gauge symmetries in intuitive and concrete physical terms, it’s time to move on and discuss how we can deﬁne them mathematically. 8.5 Gauge symmetries mathematically First of all, symmetries are described mathematically using group theory. A group consists of all transformations which leave the given system invariant and an operation which allows us to connect transformations.4242 For further details, see, for exam- ple, [Schwichtenberg, 2018a] electrodynamics as a gauge theory 211 In our ﬁnancial toy model, the global symmetry group is the dilation group which consists of all possible dilatations f = eϵ , with ϵ ∈ R . (8.47) of the given currency. The mathematical name for this group is GL+(1, R), the one-dimensional real general linear group with positive determinant. In electrodynamics, the global symmetry group is U(1) and consists of all possible phase shifts f = eiϵ , with ϵ ∈ R . (8.48) The difference between GL+(1, R) and U(1) is the factor of i in the exponent of the transformation operators.43 43 As a result, U(1) is compact while GL+(1, R) is not. An important idea was then to introduce bookkeepers Aµ in order to make the theory locally redundant. These bookkeepers allow us to use arbitrary local coordinate systems. In mathemat- ical terms, we then have a local gauge symmetry, which is really just a redundancy in our description. So after the introduction of the bookkeepers, we can perform local dilatations44 44 To unclutter the notation, we restrict ourselves to one spatial dimension.f (x) = eϵ(x) , with ϵ(x) ∈ C∞ (8.49) since our bookkeepers adjust accordingly (Eq. 8.15). Analo- gously, in electrodynamics we can then perform local phase shifts (Eq. 8.35) f (x) = eiϵ(x) , with ϵ(x) ∈ C∞ . (8.50) The crucial point is that our transformation parameters ϵ(x) are functions of the location x. In other words, we can now shift the prices, or analogously the phase, at each point in space x by a different amount. Only global shifts were permitted without the bookkeepers which meant that the prices, or analogously the phases, were shifted by exactly the same amount everywhere. After the introduction of the bookkeepers, we have the freedom to perform independent GL+(1, R) transformations at each point in space. In our ﬁnancial toy model, this means that we 212 no-nonsense electrodynamics have a copy of the gauge group GL+(1, R) at each point in space. Taken together these copies yield the group of gauge transformations. Analogously, in electrodynamics our symmetry group is U(1) and we also have a copy of U(1) above each point in space.45 45 It’s crucial to keep the notions of a \"gauge group\" and a \"group of gauge transformations\" separate. While the former is usually simple and ﬁnite-dimensional, the latter is a lot more complicated and inﬁnite-dimensional. This comes about since the group of gauge transformations is a group of smooth functions on spacetime that take values in the gauge group. Since each Lie group can be understood as a manifold, the geo- metrical picture involving ﬁber bundles emerges. For example, U(1) transformations (Eq. 8.33) are unit complex numbers, which all lie on the unit circle in the complex plane. Hence, ge- ometrically we can imagine U(1) as a circle and therefore that there is a little circle attached to each spacetime point. Since pictures with lots of cir- cles quickly become confusing, it is conventional to \"cut\" the circle and turn it into a line. We then have to remember that the end points P have to be identiﬁed. The ﬁnal geometrical picture is known as a ﬁber bundle: electrodynamics as a gauge theory 213 A ﬁnal thing that we need to talk about are our bookkeepers, which we introduced to make the theory locally invariant. In mathematical terms the bookkeepers Aµ are called connec- tions. A connection is a tool that allows us to compare prices or phases at different locations since it keeps track of how the local coordinate systems are deﬁned and encodes information about the structure of the space we are moving on. As already mentioned above, there are two situations where it’s necessary to introduce connections. On the one hand, we need connections to allow for arbitrary local coordinate systems. Here, the connection keeps track of these local coordinate sys- tems and lets us compare prices or phases deﬁned according to different local conventions. On the other hand, connections are essential whenever the space we are interested in is curved. The prototypical example of a curved space is a sphere. To compare vectors at two different points on a sphere (e.g. to calculate a derivative), we need a procedure to move one vector to the location of the second one consistently. The needed procedure is known as parallel transport. To un- derstand it, imagine that you are walking on the sphere while holding a stick in your hand. While you walk, you always do your best to keep the stick straight. If you do this, you are paral- lel transporting the stick. 214 no-nonsense electrodynamics Mathematically, the inﬁnitesimal parallel transport of a vector Vα(x) is deﬁned as Vα(x + dx) = Vα(x) − Γα βγ(x)V β(x)dxγ , (8.51) where Γi jk denotes the corresponding connection. An important observation is that if the space you are moving in is curved, it’s possible that the stick does not end up in its starting position if you move along a closed curve. Hence, the difference between the original vector and the vector which was parallel transported along an inﬁnitesimal closed curve encodes information about the local curvature. Therefore, we imagine that our vector moves from A to B via two different paths. Taken together these two paths yield a closed curve and we can electrodynamics as a gauge theory 215 calculate Vα(A → C → B) − Vα(A → D → B) = R ν αβ µV βdxµdxν + . . . , (8.52) where R ν αβ µ denotes the corresponding (Riemann) curvature tensor R ν αβ µ = ∂αΓ µ β ν − ∂βΓ µ α ν + Γ µ α κΓ κ β ν − Γ µ β κΓ κ α ν . (8.53) Now, it is also possible that we have curvature in an internal space. For example, for the U(1) symmetry described above, we can imagine that the various U(1) copies are glued together non-trivially. In this case, we again need a connection that allows us to con- sistently move our wave function from one point to another Ψ(xµ + ∆xµ) = Ψ(xµ) − Aµ(xµ)Ψ(xµ)∆xµ , (8.54) where Aµ(xµ) denotes the connection. Moreover, completely analogously, we can imagine that we don’t end up with the same wave function when we move along a closed curve. If this is the case, we know that our internal space is curved and hence, we deﬁne Fµν(xµ) ≡ ∂Aν ∂xµ − ∂Aµ ∂xν (8.55) as a measure of the curvature. Take note that this is exactly how we deﬁned the quantity which encodes information about 216 no-nonsense electrodynamics arbitrage opportunities in Eq. 8.14 and the quantity which tells us that there is a non-zero electromagnetic ﬁeld in Eq. 8.41. An important point is that connections can be non-zero, even though the curvature is zero. In this case, our connections are necessary only because of our choice of the local coordinate systems and not as a result of the physical situation itself. If the curvature is zero, it is possible to ﬁnd a choice of local coordi- nate systems such that no connection is necessary. However, whenever the curvature is non-zero, connections are essential and can’t be removed by a clever choice of local coordinate sys- tems. This will be discussed further in the next section. 8.5.1 Gauge connections in Quantum Mechanics and the toy model We can modify our equations in quantum mechanics to make them invariant under local U(1) transformations. To achieve this, we need to introduce the connection Aµ. However, this connection is again a purely mathematical bookkeeper, as long as the curvature tensor vanishes4646 Here curvature is not referring to a property of spacetime but of our internal space. Fµν(xµ) = 0 , (8.56) where (Eq. 8.16) Fµν(xµ) ≡ ∂Aν ∂xµ − ∂Aµ ∂xν . (8.57) But if Fµν ̸= 0, we can’t get rid of Aµ everywhere at the same time by a change of coordinate systems, since this would imply Fµν = 0, which corresponds to a different physical situation. Moreover, Fµν only becomes the dynamical actor that we call electromagnetic ﬁeld if there are nontrivial equations of motion for the connection (the Maxwell equations). Finally, we can also come back to our ﬁnancial toy model. Here, we can introduce arbitrary local currencies and this makes it electrodynamics as a gauge theory 217 necessary to introduce bookkeepers Aµ(⃗n) which are able to handle the exchange of currencies. However, these bookkeepers are purely mathematical parts of our description as long as there is no arbitrage opportunity Fµν(⃗n) = 0 (8.58) where (Eq. 8.14) Fµν(⃗n) = Aν(⃗n +⃗eµ) − Aν(⃗n) − [Aµ(⃗n +⃗eν) − Aµ(⃗n)] , (8.59) The bookkeepers are only indispensable when there are real ar- bitrage opportunities Fµν(⃗n) ̸= 0. Moreover, as soon as there are equations of motion for them, they become dynamical actors. So in summary, while connections also appear when we write down the equations of a given model in a more general way, they have no measurable effect since we are still describing the same model. The physical situation is only then a different one when the corresponding curvature is non-zero. In this case, our connections are no longer optional but essential parts of the model and have a measurable effect on the dynamics. Moreover, they become dynamical actors only when they change dynam- ically, i.e. they follow their own equations of motion. Theories with a dynamical connection are what we call gauge theories. Since in electrodynamics we have Maxwell’s equations for the connection Aµ and, in general, a non-zero curvature Fµν, we can understand electrodynamics as a gauge theory. 9 Further Reading Recommen- dations As mentioned already in the preface, the content of this book is far from comprehensive. There are hundreds of different as- pects of electrodynamics that I haven’t even said a word about. Electrodynamics is almost two centuries old and thousands of people have worked on it. Unsurprisingly, no single book can capture it all. However, there are lots of excellent books that cover various aspects extremely well. Since for every good book there are at least 20 bad ones which are not worth your time, I recommend some of my favorites below. So, start by picking the ones that interest you most, and dig in1. 1 If you need further or more spe- cialized reading recommendations, you should visit: www.physicstravelguide.com This is an expository physics wiki where anyone can help to collect the best resources on any physics topic + publish student-friendly explanations. My favorite student-friendly electrodynamics textbooks are ◃ Vol. 2 of the Feynman Lectures2 2 Richard Feynman. The Feynman lectures on physics. Addison-Wesley, San Francisco, Calif. Harlow, 2011. ISBN 9780805390650 ◃ A student’s guide to Maxwell’s equations by Daniel A. Fleisch3 3 Daniel Fleisch. A student’s guide to Maxwell’s equations. Cambridge University Press, Cambridge, UK New York, 2008. ISBN 978- 0521701471 220 no-nonsense electrodynamics ◃ Introduction to Electrodynamics by David J. Grifﬁths44 David Grifﬁths. Introduction to electrodynamics. Pearson Education Limited, Harlow, 2014. ISBN 9781292021423 ◃ Electricity and Magnetism by Edward M. Purcell5 5 Edward Purcell. Electricity and magnetism. Cambridge University Press, Cambridge, 2013. ISBN 9781107014022 If you want to dive deeper, you might try ◃ Modern Electrodynamics by Andrew Zangwill6 6 Andrew Zangwill. Modern electro- dynamics. Cambridge University Press, Cambridge, 2013. ISBN 9780521896979 ◃ Classical Electrodynamics by John David Jackson7 7 John Jackson. Classical electrody- namics. Wiley, New York, 1999. ISBN 9780471309321 However, be warned that these books aren’t as student-friendly as the books mentioned above. To learn more about vector and tensor calculus, I strongly rec- ommend ◃ Div, Grad, Curl, and All that by H. M Schey88 H. M. Schey. Div, grad, curl, and all that : an informal text on vector calculus. W.W. Norton & Company, New York, 2005. ISBN 9780393925166 ◃ A Student’s Guide to Vectors and Tensors by Daniel A Fleisch9 9 Daniel Fleisch. A student’s guide to vectors and tensors. Cambridge University Press, Cambridge New York, 2012. ISBN 9781139031035 ◃ The vector calculus series by Kalid Azad10 10 https://betterexplained. com/articles/category/math/ vector calculus/ Good books to learn more about special relativity are ◃ Special Relativity by Anthony French11 11 A. P. French. Special relativity. Norton, New York, 1968. ISBN 9780393097931 ◃ Special Relativity for Beginners by Jürgen Freund12 12 Juergen Freund. Special rela- tivity for beginners : a textbook for undergraduates. World Scientiﬁc, Sin- gapore, 2008. ISBN 9789812771599 ◃ Spacetime Physics by Edwin F. Taylor and John A. Wheeler13 13 Edwin Taylor. Spacetime physics : introduction to special relativity. W.H. Freeman, New York, 1992. ISBN 9780716723271 If you want to learn more about gauge theory and group theory, you might enjoy my ﬁrst book ◃ Physics from Symmetry14 14 Jakob Schwichtenberg. Physics from Symmetry. Springer, Cham, Switzerland, 2018a. ISBN 978- 3319666303 further reading recommendations 221 To learn more about ﬁber bundles and differential geometry, good starting points are ◃ Fiber Bundles and Quantum Theory by Herbert J. Bernstein and Anthony V. Phillips15 15 H. J. Bernstein and A. V. Phillips. Fiber Bundles and Quantum The- ory. Sci. Am., 245:94–109, 1981. doi: 10.1038/scientiﬁcamerican0781-122◃ A pictorial introduction to differential geometry, leading to Maxwell’s equations as three pictures by Jonathan Gratus16 16 J. Gratus. A pictorial introduction to differential geometry, leading to Maxwell’s equations as three pictures. ArXiv e-prints, September 2017 If you’re looking for an introductory book on quantum mechan- ics you might enjoy my book ◃ No-Nonsense Quantum Mechanics17 17 Jakob Schwichtenberg. No- Nonsense Quantum Mechanics. No- Nonsense Books, Karlsruhe, Ger- many, 2018b. ISBN 978-1719838719 One Last Thing It’s impossible to overstate how important reviews are for an author. Most book sales, at least for books without a marketing budget, come from people who ﬁnd books through the recom- mendations on Amazon. Your review helps Amazon ﬁgure out what types of people would like my book and makes sure it’s shown in the recommended products. I’d never ask anyone to rate my book higher than they think it deserves, but if you like my book, please take the time to write a short review and rate it on Amazon. This is the biggest thing you can do to support me as a writer. Each review has an impact on how many people will read my book and, of course, I’m always happy to learn about what people think about my writing. PS: If you write a review, I would appreciate a short email with a link to it or a screenshot to Jakobschwich@gmail.com. This helps me to take note of new reviews. And, of course, feel free to add any comments or feedback that you don’t want to share publicly. Part IV Appendices A Vector Calculus A whole new set of mathematical tools is necessary to describe electrodynamics. Most of them are part of what is usually called vector calculus and this is what this appendix is about. How- ever, we will not talk about vector calculus in general but in- stead restrict ourselves to those tools which are necessary to understand electrodynamics.1 1 The only exception is the outer product which is only mentioned for completeness. There are two ways of approaching this appendix. One possibil- ity is to read it before you start reading the main chapters about electrodynamics. Alternatively, it is also possible to simply start reading the chapters about electrodynamics and then read about the concepts whenever we really need them. As usual, we’ll start with a birds-eye overview and afterwards dive into the details. 228 no-nonsense electrodynamics Basic Quantities To describe electrodynamics, we need more than simple num- bers.2 For example, charged objects get pushed by other charged 2 In mathematical terms a simple number is a scalar. The deﬁning feature of a scalar is that it doesn’t change if we change our coordi- nate system while, for example, a vector gets rotated if we rotate our coordinate system. objects in a particular direction. For this reason, we need vectors ⃗v, which not only allow us to describe how strongly an object gets pushed but also in which direction. Moreover, to get a deep understanding of electrodynamics, we need to understand the interplay between the electric and the magnetic ﬁeld. This in- terplay can be best understood by making use of a particular tensor Fµν, which allows us to describe more than one direction at once.33 Scalars, vectors and tensors are discussed in Appendix A.1 In addition, since charged objects can inﬂuence each other with- out directly touching each other, we need to introduce so-called ﬁelds. A ﬁeld is a mathematical object which assigns a partic- ular quantity (scalar, vector, tensor) to each point in space. In particular, the electric ﬁeld ⃗E(⃗x) and magnetic ﬁeld ⃗B(⃗x) are mathematically vector ﬁelds. They describe at each location ⃗x a particular ﬁeld strength and direction in which a charged object would get pushed. Moreover, the electromagnetic ﬁeld tensor Fµν(⃗x) assigns a tensor to each location ⃗x and is therefore mathematically a tensor ﬁeld.44 We talk about ﬁelds in Ap- pendix A.4. Take note that in physics our ﬁelds usually also change in time, i.e. Fµν = Fµν(t,⃗x) and ⃗E = ⃗E(t,⃗x) etc. Basic Operations There are different ways how we can combine vectors and vec- tor ﬁelds.5 One possibility is to multiply two vectors ⃗v and ⃗w 5 The reason why we care about these products is, well, that we need them to describe nature. and, as a result, get a matrix ⃗v ⊗ ⃗w ≡ ⃗v⃗wT =   v1 v2 v3    (w1 w2 w3)↷ =   v1w1 v1w2 v1w3 v2w1 v2w2 v2w3 v3w1 v3w2 v3w3    . vector calculus 229 This way of combining two vectors is known as the outer prod- uct. Another closely related possibility to combine two vectors is ⃗v · ⃗w ≡ ⃗vT ⃗w = (v1 v2 v3)   w1 w2 w3   ↷ = v1w1 + v2w2 + v3w3 . Here the result is a simple number and this way of combining two vectors is known as the inner product or the scalar product or the dot product.6 6 The dot product is the topic of Appendix A.2. A third product allows us to combine two vectors in such a way that we get a vector as a result: ⃗v × ⃗w =   v1 v2 v3    ×   w1 w2 w3   ↷ =   v2w3 − v3w2 v3w1 − v1w3 v1w2 − v2w1    . This way of combining two vectors is known as the cross prod- uct.7 7 The cross product is discussed in Appendix A.3. To summarize: two vectors ⃗v, ⃗w dot product ⃗v·⃗w tt cross product ⃗v×⃗w \u000f\u000f outer product ⃗v⊗⃗w * a scalar a vector a matrix Integrals There are different kinds of integrals for ﬁelds. The simplest type is a line integral8 8 See Appendix A.5. 230 no-nonsense electrodynamics ∫ C φ(⃗r)dl , (A.1) where we integrate a scalar ﬁeld φ(⃗r) along some curve C. We need to be a bit more careful when we integrate a vector ﬁeld along some path P ∫ P ⃗V(⃗x) · d⃗l = ∫ P ⃗V ·⃗t(l)dl , (A.2) since we need to take the direction of ⃗V(⃗x) at each location ⃗x into account. This type of integral is known as a path integral.99 The path integral is the topic of Appendix A.6. In addition, we can consider higher-dimensional integrals like a surface integral1010 See Appendix A.8. ∫ S φ(⃗x)da , (A.3) where a scalar ﬁeld φ(⃗r) is integrated over some surface S. The analogous integral for vector ﬁelds is again a bit more com- plicated ∫ S ⃗V(⃗x) · d⃗a = ∫ S ⃗V(⃗x) · ⃗nda , (A.4) since again, we need to take the direction of ⃗V(⃗x) at each lo- cation ⃗x into account. A surface integral for a vector ﬁeld is usually called a ﬂux integral.1111 We discuss how this name comes about in detail in Appendix A.9. Differential Operators There are different ways of gathering information about how a given ﬁeld changes through space. For an ordinary func- tion f (x) this kind of information is encoded in the deriva- tive ∂x f (x). For a scalar ﬁeld φ(⃗x), we need to describe how it changes as we move in the three spatial directions (x, y, z): ∇φ(⃗x) =   ∂x ∂y ∂z    φ(⃗x) =   ∂xφ(⃗x) ∂yφ(⃗x) ∂zφ(⃗x)    . (A.5) vector calculus 231 This is known as the gradient of φ(⃗x).12 12 We discuss the gradient in Ap- pendix A.10. Moreover, vector ﬁelds ⃗F(⃗x) additionally assign a direction to each point in space. Information about how this direction changes as we move through space is encoded in the diver- gence13 13 The divergence is the topic of Appendix A.11. ∇ · ⃗F(⃗x) =   ∂x ∂y ∂z    ·   Fx(x) Fy(x) Fz(x)    = ∂x Fx(⃗x) + ∂yFy(⃗x) + ∂zFz(⃗x) . (A.6) and in the curl14 14 The meaning of the curl of a vector ﬁeld is discussed in detail in Appendix A.12. ∇ × ⃗F(⃗x) =   ∂x ∂y ∂z    ×   Fx(⃗x) Fy(⃗x) Fz(⃗x)    =   ∂yFz(⃗x) − ∂zFy(⃗x) ∂zFx(⃗x) − ∂x Fz(⃗x) ∂x Fy(⃗x) − ∂yFx(⃗x)    . (A.7) The divergence tells us if the arrows around a given location point in the same direction or away from the location. The curl tells us whether or not the arrows circle around the location. It is important to keep in mind that many quantities like diver- gence and curl were speciﬁcally introduced because we need them in electrodynamics. In other words, the main motivation for why these quantities are introduced is that we need them to describe nature. Fundamental Theorems There are several theorems which allow us to understand the interplay between the various differential operators (gradient, divergence, curl) and the various types of integrals (line inte- gral, path integral, surface integral, ﬂux integral). The basic message in all these theorems is that the integral of a derivative over a region is equal to the sum over the values of the quantity at the boundary of the region.15 15 Take note that the same is true for the fundamental theorem of calculus ∫ b a d f (x) dx dx = f (b) − f (a). Here our region is the line from a to b. The theorem allows us to replace the integral over this region by a sum over the values of f (x) at the boundary of the line, i.e. a and b. 232 no-nonsense electrodynamics For example, the fundamental theorem for gradients ∫ P ∇φ(⃗x) · d⃗s = φ(⃗r1) − φ(⃗r0) , (A.8) tells us that the path integral over the gradient of the scalar ﬁeld φ(⃗x) is equal to the difference of the values of φ(⃗x) at the boundary of the path P.16 16 The fundamental theorem for gradients is the topic of Ap- pendix A.13. The boundary of a path is the starting and the end point of the path (⃗r0 and ⃗r1). Analogously, the fundamental theorem for divergences17 17 The fundamental theorem for di- vergences is also known as Gauss’s theorem, Green’s theorem or simply the divergence theorem. We talk about it in detail in Appendix A.13. ∫ V ∇ · ⃗F(⃗x)dV = ∮ S ⃗F(⃗x) · d⃗S (A.9) tells us that the volume integral over the divergence of a vector ﬁeld ⃗F(⃗x) is equal to the sum over the values of ⃗F(⃗x) at the boundary of V.18 18 The boundary of a volume V is what we call its surface S. More- over, recall that an integral can be understood as a ﬁnely-grained sum. This means that on the right-hand side, we really sum over all val- ues of the vector ﬁeld ⃗F(⃗x) on the surface S. Finally, the fundamental theorem for curls19 19 The fundamental theorem for curls is also known as Stokes’ theorem. We discuss it in detail in Appendix A.15. ∫ S ∇ × ⃗F(⃗x) · d⃗S = ∮ P ⃗F(⃗x) · d⃗l (A.10) tells us that the surface integral over the curl of a vector ﬁeld ⃗F(⃗x) is equal to the sum over the values of ⃗F(⃗x) at the boundary of S.20 20 The boundary of a surface S is a path P around it. Now, let’s talk about these concepts in detail. vector calculus 233 A.1 Scalars, Vectors, Tensors In somewhat naive terms, we can say that a scalar is simply a number, a vector an arrow and a tensor something more compli- cated like a matrix. scalar vector rank 2 tensor (x)   x y z      M11 M12 M13 M21 M22 M23 M31 M32 M33    We use scalars to describe quantities that only have a magnitude (or size) and no associated direction, for example, like temper- ature. In contrast, a vector is a mathematical object that we use whenever we want to describe something which is characterized by its magnitude and direction. An example of a quantity that we need to describe using a vector is the velocity of an object. We need a vector here since we not only need to describe how fast the object is moving but also in which direction. Another important example of a vector quantity is a force. Here, we describe how strongly something gets pushed in a particular direction. A tensor is an object we use to describe quantities which are characterized by their magnitudes and multiple directions. The number of directions necessary determines the rank of the ten- sor. For example, if two directions are needed, we need a rank 2 tensor.21 One of the most important tensors is the electromag- 21 Using this deﬁnition, we can also understand scalars and vectors as tensors. A scalar encodes a magnitude and zero directions. For this reason, a scalar is often called a rank 0 tensor. A vector encodes a magnitude and exactly one direction. Therefore, we can call a vector a rank 1 tensor. netic ﬁeld tensor which is a rank 2 tensor. We need one direc- tion to describe the electric ﬁeld and another one to describe the magnetic ﬁeld. We can construct a tensor by using the matrix product (\"row times column\") of two vectors22 22 Take note that this is not the dot product, which is deﬁned as ⃗v · ⃗w = ⃗vT ⃗w = (v1 v2 v3)  w1 w2 w3   and yields a simple number. The superscript \"T\" denotes transposi- tion. ⃗v⃗wT =   v1 v2 v3    (w1 w2 w3) =   v1w1 v1w2 v1w3 v2w1 v2w2 v2w3 v3w1 v3w2 v3w3    (A.11) 234 no-nonsense electrodynamics A useful way to think about scalars, vectors and tensors is in terms of how they react to transformations of our coordinate system, e.g., rotations. ◃ A scalar remains completely unchanged. ◃ A vector transforms exactly like a position vector ⃗r. For ex- ample, if we rotate our coordinate system using a rotation matrix R, i.e. ⃗r → R⃗r, any vector ⃗v gets rotated completely analogous to how ⃗r gets rotated: ⃗v → R⃗v. ◃ The transformation behavior of a tensor is more complicated. For example, a rank 2 tensor transforms like the product of two vectors in Eq. A.11: M → RMRT. We can understand this by transforming the product in Eq. A.11 explicitly: M ≡ ⃗v⃗wT → (R⃗v)(R⃗w)T = R⃗v⃗wT RT ≡ RMRT , where we used that each vector transforms like a position vector, ⃗v → R⃗v, ⃗w → R⃗w. In general, we need one transfor- mation matrix per tensor rank.2323 From this perspective it again makes sense to call a scalar a rank 0 tensor, since we need no transformation matrix. The second statement may seem strange or even trivial. How- ever, this deﬁnition is actually useful since, in principle, we can write any three quantities below each other between two big brackets. For example, we could write the pressure P, tempera- ture T and entropy E of a gas between two big brackets   P T E    . But still this object is not a vector since it doesn’t transform like a position vector. These kind of thoughts are especially important in the context of special relativity. In special relativity our main focus are events in spacetime, which we can describe using four-vectors xµ = (ct, x1, x2, x3)T.2424 Take note that the speed of light c appears here and in other four- vectors since all components of a vector must have the same units since otherwise we can’t mix them. Since t has units [s], we multiply it by the only fundamental velocity that we have: c. The result ct has units [ m s s]=[m] which is the same as the other components. An event is characterized by a location (x1, x2, x3) and a point in time t. It makes sense to deﬁne four-vectors in special rela- tivity since t and (x1, x2, x3) are mixed through transformations vector calculus 235 of our coordinate system (\"boosts\"). This is analogous to how (x1, x2, x3) are mixed through rotations and hence we write them together as a vector. A crucial task is then to identify which quantities transform together like a four-vector. A fa- mous example is the four-momentum pµ = (E/c, p1, p2, p3), where ⃗p = (p1, p2, p3) is the ordinary momentum vector, E the energy and c denotes the speed of light. Another example is the electromagnetic potential Aµ = (φ/c, A1, A2, A3), where φ denotes the electric potential and ⃗A = (A1, A2, A3) the magnetic vector potential.25 25 Take note that there are objects which transform non-trivially but not like a position vector. The most famous example are spinors, which are mathematical objects that we need to describe elementary particles like electrons in quantum ﬁeld theory. In modern physics, simple scalars, vectors and tensors are often not enough to describe what is going on. Instead, we need scalar ﬁelds, vector ﬁelds, and tensor ﬁelds. We will talk about these mathematical tools in a moment. However, ﬁrst we will talk about the geometrical meaning of the dot and cross product since these are extremely important in electrodynamics.26 26 We will not discuss the outer product, since we don’t need it in electrodynamics. A.2 The dot product As mentioned above, the dot product allows us to combine two vectors ⃗v, ⃗w in such a way that the result is a number27 27 If we combine two vector func- tions, we get a scalar function. ⃗v · ⃗w =   v1 v2 v3    ·   w1 w2 w3    = v1w1 + v2w2 + v3w3 . (A.12) In words, we can summarize the idea behind it as follows: The scalar product of two vectors ⃗v · ⃗w yields the projection of the ﬁrst vector ⃗v onto the axis deﬁned by the second vector ⃗w times the length of the second vector. How does this interpretation ﬁt together with the formula given in Eq. A.12? 236 no-nonsense electrodynamics To understand this, we need to talk about the projection of some vector ⃗v onto the axis deﬁned by a second vector ⃗w.2828 The easiest way to understand projections in general is to consider projections onto the coordinate axis ⃗ex,⃗ey,⃗ez. The projection of some vector ⃗v onto a coordinate axis like ⃗ex is simply what we usually call the ﬁrst component v1. In words, the meaning of this component is how much our vector ⃗v spreads out in the x-direction. Analogously, the projection of ⃗v onto ⃗ey is what we call the second component v2 and it tells us how much ⃗v spreads out in the y-direction. This allows us to write any vector in terms of basis vectors as follows ⃗v = v1⃗ex + v2⃗ey + v3⃗ez . (A.13) By looking at the ﬁgure above, we can see that the correct for- mula for the projection of ⃗v onto ⃗w is projection of |⃗v| onto the axis deﬁned by ⃗w = |⃗v| cos θ (A.14) where θ denotes the angle between the two vectors. The state- ment from above in mathematical form therefore reads ⃗v · ⃗w = |⃗v| cos θ|⃗w| . (A.15) Therefore, the question we now need to answer is: how is this formula related to the usual formula in Eq. A.12? To answer this question, we write our two general vectors in terms of our basis vectors: ⃗v = vx⃗ex + vy⃗ey + vz⃗ez ⃗w = wx⃗ex + wy⃗ey + wz⃗ez . We can then rewrite our dot product in terms of dot products of the basis vectors: ⃗v · ⃗w = |⃗v||⃗w| cos(θ)↷ = [vx⃗ex + vy⃗ey + vz⃗ez] · [wx⃗ex + wy⃗ey + wz⃗ez]↷ = vxwx(⃗ex ·⃗ex) + vxwy(⃗ex ·⃗ey) + vxwz(⃗ex ·⃗ez) + vywx(⃗ey ·⃗ex) + vywy(⃗ey ·⃗ey) + vywz(⃗ey ·⃗ez) + vzwx(⃗ez ·⃗ex) + vzwy(⃗ez ·⃗ey) + vzwz(⃗ez ·⃗ez) . vector calculus 237 Next we use that our basis vectors are normalized (⃗ex ·⃗ex = 1) and orthogonal (⃗ex ·⃗ey = 0): ⃗v · ⃗w = vxwx(1) + vxwy(0) + vxwz(0) + vywx(0) + vywy(1) + vywz(0) + vzwx(0) + vzwy(0) + vzwz(1)↷ = vxwx + vywy + vzwz ✓ We can visualize this calculation as follows: This tells us that we really can understand the result of the dot product as the projection of ⃗v onto ⃗w times the length of ⃗w. An important example is the dot product of a vector with itself, ⃗v · ⃗v. In words, the result is the projection of ⃗v onto itself times the length of ⃗v. Since the projection of ⃗v onto itself simply yields the full vector length, we simply get the length of the vector squared ⃗v · ⃗v = |⃗v| cos 0|⃗v| = |⃗v|2 . (A.16) 238 no-nonsense electrodynamics Example: dot product of two vectors The dot product of ⃗v =  1 4 9   and ⃗w =  2 2 1   (A.17) is given by ⃗v · ⃗w =  1 4 9   ·  2 2 1   = 2 + 8 + 9 = 19 . (A.18) A.3 The cross product As mentioned above, the cross product allows us to combine two vectors ⃗A, ⃗B in such a way that the result is a vector2929 If we combine two vector func- tions, we get another vector func- tion. ⃗A × ⃗B =   A1 A2 A3    ×   B1 B2 B3    =   A2B3 − A3B2 A3B1 − A1B3 A1B2 − A2B1    . (A.19) In words, we can summarize the idea behind it as follows:3030 The direction in which the result- ing vector points can be determined by the right-hand rule. The cross product of two vectors ⃗A × ⃗B yields a vector perpendicular to ⃗A and ⃗B whose magnitude is the area of the parallelogram spanned by ⃗A and ⃗B. vector calculus 239 Now, how does this interpretation ﬁt together with the formula in Eq. A.19? To understand this, we ﬁrst need to recall that the formula for the area of a parallelogram is base times height. Here, our base is given by the length of the vector ⃗A and the height by sin(θ)|⃗B|, where θ is the angle between ⃗A and ⃗B. The area of the parallelogram spanned by ⃗A and ⃗B is therefore area = | ⃗A| sin(θ)|⃗B| . (A.20) Therefore, the question we now need to answer is: how is this formula related to the usual formula in Eq. A.19? To answer this question, we write our two general vectors in terms of our basis vectors: ⃗v = vx⃗ex + vy⃗ey + vz⃗ez ⃗w = wx⃗ex + wy⃗ey + wz⃗ez . We can then rewrite our cross product in terms of cross prod- 240 no-nonsense electrodynamics ucts of the basis vectors: |⃗v × ⃗w| = ∣ ∣[vx⃗ex + vy⃗ey + vz⃗ez] × [wx⃗ex + wy⃗ey + wz⃗ez]∣ ∣↷ = |vxwx(⃗ex ×⃗ex) + vxwy(⃗ex ×⃗ey) + vxwz(⃗ex ×⃗ez) + vywx(⃗ey ×⃗ex) + vywy(⃗ey ×⃗ey) + vywz(⃗ey ×⃗ez) + vzwx(⃗ez ×⃗ex) + vzwy(⃗ez ×⃗ey) + vzwz(⃗ez ×⃗ez)| . Next we use that the cross product of a vector with itself yields zero (⃗ex ×⃗ex = 0) and that the cross product of two basis vectors yields the third basis vector (⃗ex ×⃗ey = ⃗ez):3131 This is necessarily the case since the vector that we ﬁnd by calculat- ing the cross product of two vectors, ⃗a ≡ ⃗b ×⃗c, is orthogonal to the two vectors in the product. Here ⃗a is orthogonal to ⃗b and ⃗c. |⃗v × ⃗w| = |vxwx(0) + vxwy(⃗ez) + vxwz(−⃗ey) + vywx(−⃗ez) + vywy(0) + vywz(⃗ex) vzwx(⃗ey) + vzwy(−⃗ex) + vzwz(0)| = | (vywz − vzwy)⃗ex + (vzwx − vxwz)⃗ey + (vxwy − vywx)⃗ez| Therefore, we can conclude that the cross product really yields a vector whose length is the area of the parallelogram spanned by ⃗A and ⃗B. Example: cross product of two vectors The cross product of ⃗A =  1 4 9   and ⃗B =  2 2 1   (A.21) is given by ⃗A × ⃗B =  1 4 9   ×  2 2 1   =  4 − 18 18 − 1 2 − 8   =  −14 17 −6   . (A.22) A.4 Fields An ordinary function f (x) is an object which eats some number x and spits out another number f (x), e.g., f (3) = 7. In physics, we often need functions which eat a location ⃗r and spit out a number f (⃗r), e.g., f (2, 3, 1) = 9. We usually call an object like this a scalar ﬁeld.3232 In physics, a ﬁeld is a quantity which exists everywhere in space at the same time. This is in contrast to ordinary objects like, say, a ball which only exists at one particular location. vector calculus 241 A scalar ﬁeld assigns a number to each point in space.33 This 33 Reminder: scalar is another word for a simple number. The deﬁning feature of a scalar is that it remains completely unchanged under coordinate transformations like, for example, rotations. In contrast, a vector, in general, gets changed if we rotate our coordinate system. number is what we call the ﬁeld strength at the given point. A scalar ﬁeld is the proper mathematical tool to describe, for example, temperature. The value of the temperature ﬁeld at each point is simply the temperature there. Completely analogously, we can deﬁne so-called vector ﬁelds. While a scalar ﬁeld assigns a simple number to each point in space, a vector ﬁeld assigns vectors. The length of the vector at each location represents the ﬁeld strength at the point. In addition, the vector also deﬁnes a direction. 242 no-nonsense electrodynamics A vector ﬁeld is the proper mathematical tool to describe, for example, air. The ﬁeld strength (vector length) at each point represents the velocity of the air molecules. Moreover, the di- rection in which the vector points at each location encodes in which direction the air ﬂows. Analogously, we can also deﬁne so-called tensor ﬁelds. A ten- sor ﬁeld assigns a tensor to each point in space.3434 Reminder: a rank 2 tensor is a matrix. Higher rank tensors are even more abstract objects. A scalar is simply a number and can be treated as a rank 0 tensor. A vector has exactly one index and can therefore be treated as a rank 1 tensor. A tensor ﬁeld is useful, for example, to describe inertia. The matrix at each point in this case encodes information about how difﬁcult it is to move in each of the three directions (x, y, z).3535 Another way to think about it is as three vectors attached to each point. To summarize, we use scalar ﬁelds to describe quantities which are completely described by their magnitude at each point in space. In contrast, we use vector ﬁelds to describe quantities vector calculus 243 which are completely described by their magnitude and a direc- tion at each point in space. Tensor ﬁelds are useful to describe quantities for which we need more than just a magnitude and a direction to describe them. Formulated differently: ◃ A scalar ﬁeld eats a location ⃗r1 = (1, 3, 4)T and spits out a number, e.g., φ(⃗r1) = 3. For a different location ⃗r2 = (9, 2, 4)T, we possibly get a different number: φ(⃗r2) = 9.2. ◃ A vector ﬁeld eats a location ⃗r1 = (1, 3, 4) and spits our a vector, e.g., ⃗A(⃗r1) = (1, 3, 2)T. Again, if we put in a different location ⃗r2 = (9, 2, 4)T, we possibly get a different vector: ⃗A(⃗r1) = (3, 11, 9)T. ◃ A tensor ﬁeld eats a location ⃗r1 = (1, 3, 4)T and spits out a tensor, e.g., M(⃗r1) =   1 2 3 3 4 5 3 4 5   . If we put in a different location ⃗r2 = (9, 2, 4)T, we possibly get a different tensor: M(⃗r2) =   8 9 9 4 3 22 3 3 15   . Take note that in the following, we will use the notions of \"scalar ﬁeld\" and \"scalar function\" or \"vector ﬁeld\" and \"vec- tor function\" interchangeably. 244 no-nonsense electrodynamics A.5 Line integral The simplest kind of integral involving ﬁelds is when we inte- grate a scalar ﬁeld φ over a curve C: ∫ C φ(⃗r)dl . We call an integral of this form a line integral and its meaning can be summarized as follows We calculate the magnitude of the scalar ﬁeld φ(⃗r) at each point on the curve C and then sum over all these individual contributions. This type of integral is useful, for example, to calculate the total mass of a thin wire if only the (non-constant) mass density ρ(x) is known or to calculate the total charge if only the charge density is known. Let’s discuss the idea behind the line integral in a bit more detail. The main idea is that we divide the curve C, e.g. our wire, into N short segments ∆xi. vector calculus 245 We can then calculate, for example, the mass contained in each short segment by multiplying the length of the segment ∆xi with the mass density ρi in this region: mass in each segment i = ρi∆xi . (A.23) The total mass of the wire is then given by the sum over these N individual contributions mass of the wire ≃ N ∑ i ρi∆xi . (A.24) This formula is not quite exact since we need to use some aver- age value of the mass density ρi in each segment. The formula becomes exact in the limit where the length of each segment ∆xi goes to zero. In this limit, the sum becomes an integral and we are then left with mass of the wire = ∫ L 0 ρ(x)dx , (A.25) where L denotes the length of the wire. Take note that it’s also possible to consider more complicated curves C. In general, we have to ﬁnd a parametrization of the curve ⃗r(l), where l is a parameter like, for example, time. In the simple case above, our curve was a line and we can parameter- ize it using ⃗r = (x, 0, 0)T with x running from 0 to L. 246 no-nonsense electrodynamics Example: line integral over circular path If we consider, for example, a circular path we have to use a parameter- ization of the form ⃗r(ϕ) =  R cos(ϕ) R sin(ϕ) 0   , (A.26) where R denotes the radius of the circle. The line integral of a speciﬁc scalar function, for example, φ(⃗x) = x2y2, can then be calculated as follows ∮ C φ(⃗r) dl = ∫ 2π 0 φ(⃗r) Rdϕ = ∫ 2π 0 (R cos(ϕ))2(R sin(ϕ))2 Rdϕ = R5 π 4 . (A.27) Take note that the symbol ∮ is used to indicate an integral over a closed curve. vector calculus 247 A.6 Path integral The path integral is deﬁned as the integral of a vector ﬁeld ⃗V(⃗x) over some path P: ∫ P ⃗V · d⃗l = ∫ P ⃗V ·⃗t(l)dl , where l parameterizes the path P and ⃗t(l) yields a vector tan- gential to the path at each point l on the path. In words, the path integral can be described as follows36 36 Reminder: as discussed in Ap- pendix A.2, the dot product yields the projection of the ﬁrst vector onto the direction deﬁned by the second vector. We calculate the component of the vector ﬁeld ⃗V in the direction d⃗l of the path P at each point. The path integral is then the sum over all these individual contributions. The path integral is important, for example, to calculate the work done by some force and also appears in Maxwell’s equa- tions. Let’s discuss how this interpretation of the path integral comes about in detail. Using the line integral discussed in the previous section, we cal- culate a sum over the values of a scalar function φ(⃗x) on some line C. In other words, we calculate a sum over the magnitude of φ(⃗x) in some region deﬁned by the line L. Now, if we want to integrate a vector function ⃗A(⃗x), there is one additional thing that we need to take into account. The main difference between a scalar function and a vector function is that the latter not only has a magnitude at each point in space, but also a direction.37 Therefore, when we integrate over a 37 The difference between scalar and vector functions is discussed in Appendix A.4. 248 no-nonsense electrodynamics vector function ⃗A(⃗x), we somehow need to take this additional information into account. To understand this, let’s consider a concrete example. The increment of work W done by a speciﬁc force ⃗F is given by the product of the magnitude of the force times the displace- ment of the object ∆x: ∆W = |⃗F|∆x . (A.28) However, this simple formula is only correct if the force is con- stant and if the displacement is in the same direction as the force.38 We therefore need to reﬁne our formula to take these 38 To understand how the displace- ment can be in a different direction than the force, imagine an object ﬂying with some initial velocity. While the force will push the object in a speciﬁc direction, the object will still ﬂy forward. The total displacement is then a sum of the displacement caused by the force and its forward movement. two possibilities into account. First of all, if the force is not constant, we can use the same trick that we already used for the line integral: we divide our path P into N short segments ∆li and then sum over all these individual contributions W = N ∑ i ∆Wi ≃ N ∑ i |⃗Fi|∆li , (A.29) where ⃗Fi denotes the force applied in the segment ∆li. In the limit of vanishing segment length, we again end up with an integral39 39 Since |⃗F(l)| is a scalar function, we have a simple line integral. W = ∫ P |⃗F(l)| dl , (A.30) where l parameterizes our path P. Now, how can we take into account that, in general, the dis- placement is not in the same direction as the force? The crucial idea is that we only count the component of the force in the direction of the displacement ∆⃗li. In other words, we need to project the force onto our path P and only then sum over these contributions. Luckily, we already know the mathematical tool which allows us to this: the dot product.40 The component of 40 The dot product is discussed in Appendix A.2. the force in the direction of the displacement ∆⃗li is ∆W = ⃗F · ∆⃗li = |⃗F||∆⃗li| cos(θ) , (A.31) where θ is the angle between ⃗F and ∆⃗li. vector calculus 249 Therefore, in the most general case, we need to calculate ⃗F · ∆⃗li for each segment and then sum over these contributions W = ∫ P ⃗F(⃗l) · d⃗l , (A.32) This type of integral is known as a path integral. The path integral over a circular path ∮ P ⃗V(⃗l) · d⃗l is an ex- tremely important special case which shows up, for example, in Maxwell’s equations. The resulting quantity that we get when we integrate a vector ﬁeld over a circular path is known as the circulation of the vector ﬁeld.41 41 The circulation of a vector ﬁeld is the topic of Appendix A.7. Now, a question we still need to answer is: How can we cal- culate d⃗l for complicated paths? This is the topic of the next section. 250 no-nonsense electrodynamics A.6.1 Tangent vector As mentioned in the previous section, the vector d⃗l at each point on the path P points in the direction of the displacement. For this reason, d⃗l is known as a tangent vector. At each point in space, we have a different tangent vector and for this reason it is often helpful to write d⃗l =⃗t(s)ds , (A.33) where s parameterizes our path P. Now how can we calculate ⃗t(s)? To understand this, imagine an object moves along the path P. We describe the path using a speciﬁc trajectory ⃗r(s), where you can think of s, for example, as the time. Our task is to ﬁnd a vector which at each point s is tangential to the path ⃗r(s). Luckily, in physics there is one extremely familiar vector func- tion with exactly this property: the velocity of the object. The velocity encodes the rate at which the location of our object changes and, in addition, also in which direction it moves. In particular, given the location of our object at some speciﬁc point in time s0, we can calculate the location at a later point in time s1 using ⃗r(s1) ≃ ⃗r(s0) + ⃗v(s0)∆s (A.34) where ∆s ≡ s1 − s0. vector calculus 251 This means that the velocity vector function ⃗v(s) ≡ d⃗r(s) ds (A.35) yields at each point s a vector tangential to the path ⃗r(s). To get the normalized tangent vectors, all we have to do is di- vide ⃗v(s) by its length42 42 We use normalized tangent vectors since, for example, the work done often does not depend on the velocity. All we need is a tangent vector which allows us to project the vector ﬁeld onto the path. An additional factor, like the velocity, can always be added additionally, if necessary; but is not essential to the deﬁnition of a path integral. ⃗t(s) = ⃗v(s) |⃗v(s)| . (A.36) This is how we can calculate the tangent vectors for a general path P. We can also see this formally as follows ∫ ⃗V · d⃗r = ∫ ⃗V · d⃗r dt dt = ∫ ⃗V · ⃗vdt (A.37) Example: work done along a rectangular path For concreteness, let’s imagine we have a force described by the vector ﬁeld ⃗F =  Lx2 − y3 −xy2 xyz   (A.38) and we want to calculate the work done if we apply this force along a small square with edge length 2L around the origin in the xy plane. 252 no-nonsense electrodynamics To calculate the path integral ∮ ⃗F · d⃗r = ∮   Lx2 − y3 −xy2 xyz   · d⃗r , (A.39) we divide the square into four parts. These can be parameterized by 1. ⃗r1(t) =   x y z   =   t L 0   , d⃗r1 dt =   1 0 0   , where t runs from L to −L. 2. ⃗r2(t) =   x y z   =   −L t 0   , d⃗r2 dt =   0 1 0   , where t runs from L to −L. 3. ⃗r3(t) =   x y z   =   t −L 0   , d⃗r3 dt =   1 0 0   , where t runs from −L to L. 4. ⃗r4(t) =   x y z   =   L t 0   , d⃗r4 dt =   0 1 0   , where t runs from −L to L. vector calculus 253 The total path integral is therefore ∮ □   Lx2 − y3 −xy2 0   · d⃗r = ∮ □   Lx2 − y3 −xy2 0   · d⃗r dt dt↷ = ∫ −L L   Lt2 − L3 −tL2 0   ·   1 0 0   dt + ∫ −L L   L3 − t3 Lt2 0   ·   0 1 0   dt + ∫ L −L   Lt2 + L3 −tL2 0   ·   1 0 0   dt + ∫ L −L   L3 − t3 −Lt2 0   ·   0 1 0   dt↷ = − ∫ L −L(Lt2 − L3)dt − ∫ L −L Lt2dt + ∫ L −L(Lt2 + L3)dt + ∫ L −L(−Lt2)dt↷ = ∫ L −L(2L3 − 2Lt2)dt = [2L3t − 2 3 Lt3]L −L↷ = 4L4 − 4 3 L4 = 8 3 L4 . (A.40) A.7 Circulation integral The circulation of a vector ﬁeld ⃗V(⃗x) is deﬁned as Circulation(⃗x) = ∮ P ⃗V(⃗x) ·⃗tdt, where t is a parameter which parameterizes our path and ⃗t a unit vector tangential to the path. In other words, the circulation is simply the path integral over the vector ﬁeld for a circular path.43 43 Information about the circulation at a single point is encoded in the curl of the vector ﬁeld. This is the topic of Appendix A.12.Intuitively, as the name already indicates, it’s a measure for how much our vector ﬁeld circulates. 254 no-nonsense electrodynamics A.8 Surface integral Given a scalar ﬁeld φ(⃗x), we can not only calculate the integral over some curve C, but also the integral over a surface S: ∫ S φ(⃗x)da We calculate the magnitude of the scalar ﬁeld φ(⃗x) at each point on the surface S and then sum over all these individual contributions. This type of integral is useful, for example, to calculate the total mass of a thin plate if only the (non-constant) mass density ρ(⃗x) is known or to calculate the total charge if only the charge density is known. Let’s discuss how this interpretation of the surface integral comes about in detail. The main idea is completely analogous to what we already discussed for the line integral in Appendix A.5. We divide our vector calculus 255 surface S into small segments δa and then use that the mass of each segment is given by the product of δa and the mass surface density in this region ρi: mass in each segment i = ρi∆ai . (A.41) The total mass of the plate is then given by the sum over these N individual contributions mass of the plate ≃ N ∑ i ρi∆ai . (A.42) This formula is not quite exact since we need to use some aver- age value of the mass density ρi in each segment. The formula becomes exact in the limit where the area of each segment ∆ai goes to zero. In this limit, the sum becomes an integral and we are then left with mass of the plate = ∫ S ρ(⃗x)da . (A.43) 256 no-nonsense electrodynamics A.8.1 Example: surface integral Let’s imagine, we want to calculate the surface integral for the scalar function φ(⃗x) = x2y2 over a square around the origin in the xy-plane. The surface integral can therefore be calculated as follows ∫ □ φ(⃗x)da = ∫ L −L ∫ L −L φ(⃗x) dxdy↷ = ∫ L −L ∫ L −L x2y2 dxdy↷ = [ ∫ L −L 1 3 x3y2dy]L −L↷ = ∫ L −L 1 3 L3y2dy − ∫ L −L 1 3 (−L)3y2dy↷ = 2L3 3 ∫ L −L y2dy↷ = 2L3 3 [ 1 3 y3]L −L↷ = 2L3 3 ( 1 3 L3 − 1 3 (−L)3)↷ = 4L6 9 . (A.44) vector calculus 257 A.9 Flux Integral Given a vector ﬁeld ⃗V(⃗x), we can not only calculate the integral along some path P, but also the integral through a surface S:44 44 We discussed the path integral in Appendix A.6. ∫ S ⃗V · d⃗a = ∫ S ⃗V · ⃗nda , where ⃗n is a unit vector normal to the surface S. In words, the surface integral can be described as follows45 45 Reminder: as discussed in Ap- pendix A.2, the dot product yields the projection of the ﬁrst vector onto the direction deﬁned by the second vector. We calculate the component of the vector ﬁeld ⃗V normal to the surface at each point on the surface. The surface integral is then the sum over all these individual contributions. The ﬂux integral is important, for example, to describe how a ﬂuid ﬂows through a pipe and, of course, because it appears in Maxwell’s equations. Let’s discuss how this interpretation of the ﬂux integral comes about in detail. Using the surface integral discussed in the previous section, we calculate a sum over the values of a scalar function φ(⃗x) on some surface S. In other words, we calculate a sum over the magnitude of φ(⃗x) in some region S. Now, if we want to integrate a vector function ⃗V(⃗x) over some surface, there is again one additional thing that we need to take into account: a vector function also describes a particular direc- tion at each point in space, not only a magnitude.46 Therefore, 46 The difference between scalar and vector functions is discussed in Appendix A.4. when we integrate over a vector function ⃗V(⃗x), we somehow need to take this additional information into account. To understand this, let’s consider a concrete example. 258 no-nonsense electrodynamics Let’s assume our vector ﬁeld describes a current density ⃗J(⃗x).4747 The current density describes in which direction and how much electric charge passes per unit area. We discuss the current density in detail in Section 2.3. For concreteness, let’s assume our current density is given by the number density ρ(⃗x) times their average velocity ⃗v:48 48 The number density describes the number of particles per cubic meter. ⃗J(⃗x) = ρ(⃗x)⃗v . (A.45) This quantity has units of particles per square meter per second. In words, it tells us how many particles pass each unit area per unit time. Therefore, if our current density is uniform ⃗J and perpendicular to some speciﬁc surface S, the ﬂux through S is simply the number of particles per second through S ≡ ﬂux through S ≡ |⃗J| × surface area . However, analogous to what we did in Appendix A.6, this for- mula needs to be reﬁned if we want to calculate the ﬂux in more general situations. If the current density is not constant, we can use the same trick that we already used for the surface integral: we divide our surface S into N small segments ∆ai and then sum over all these individual contributions ﬂux through S ≃ N ∑ i |⃗Ji|∆ai , (A.46) where ⃗Ji is the average current density in the region ∆ai. In the limit of vanishing segment areas, we end up again with an integral4949 Since |⃗J(⃗x)| is a scalar function, we simply have a surface integral. ﬂux through S = ∫ S |⃗J|da . (A.47) Now, how can we take into account the direction of our vector ﬁeld at each point on the surface? In our concrete example, the direction of the vector at each point indicates the direction in which our particles ﬂow. There- fore, if we want to calculate how many particles really pass a speciﬁc surface, we need to be careful since, in general, the full ﬂow does not \"hit\" the surface. vector calculus 259 This means that if we use Eq. A.47, we would make a big mis- take since far fewer particles really pass the surface S. To un- derstand this problem, we use the fact that we can split up our general vector ⃗J(⃗r0) at a particular point ⃗r0 on the surface in terms of a component tangential J∥ to the surface and a compo- nent J⊥ normal to the surface ⃗J(⃗r0) = J⊥⃗e⊥ + J∥⃗e∥ . (A.48) This is useful since the component tangential to the surface can be neglected completely. A current density vector tangential to the surface means that the particles ﬂow along the surface and therefore do not penetrate it. In general, we can ﬁnd the relevant component of ⃗J(⃗x) at each point on the surface by using the dot produce once more. In particular, the dot product between ⃗J(⃗x) and a unit vector ⃗n normal to the surface S yields exactly the relevant component J⊥.50 50 Recall that the dot product between a vector and a unit vector yields exactly the component of vector along the direction of the unit vector. 260 no-nonsense electrodynamics Therefore, in the most general case, we need to calculate ⃗J · ∆⃗ai for each segment and then sum over these contributions5151 Take note that, in general, we have a different normal vector at different points on the surface. ﬂux through S = ∫ S ⃗J(⃗x) · d⃗a = ∫ S ⃗J(⃗x) · ⃗nda . (A.49) Again an important special case is the ﬂux through a closed surface. In this case, we usually denote the ﬂux like this ﬂux through closed surface S = ∮ S ⃗J(⃗x) · d⃗a = ∮ S ⃗J(⃗x) · ⃗nda . (A.50) vector calculus 261 Example: ﬂux integral Let’s imagine that we want to calculate the ﬂux integral of the vector function ⃗F =   xz −yz 2y2   (A.51) over a square around the origin in the xy-plane. The unit vector normal to the square is ⃗n =  0 0 1   . (A.52) The ﬂux integral can therefore be calculated as follows ∫ □ ⃗F · d⃗a = ∫ □   xz −yz 2y2   ·   0 0 1   da↷ = 2 ∫ A y2da↷ = 2 ∫ L −L dx ∫ L −L y2dy↷ = 2[x]L −L[ 1 3 y3]L −L↷ = 2 · 2L · 2 3 L3 = 8 3 L4 (A.53) 262 no-nonsense electrodynamics A.10 Gradient The gradient is an operator which transforms a scalar function f (⃗x) into a vector function:5252 The symbol ∇ is usually called \"del\" or \"nabla\". It is usually written like a vector ∇ =  ∂x ∂y ∂z   . The nabla symbol is not only important here but is also used to get a scalar function from a given vector function using the divergence ∇ · ⃗v(⃗x), c.f. Appendix A.11. Moreover, we use the nabla symbol to deﬁne the curl of a vector ﬁeld ∇ × ⃗v(⃗x) , c.f. Appendix A.12. ∇ f (⃗x) =   ∂x ∂y ∂z    f (⃗x) =   ∂x f (⃗x) ∂y f (⃗x) ∂z f (⃗x)    . The meaning of the resulting vector function, called the gradi- ent, can be summarized as follows:53 53 We discussed ﬂux in Ap- pendix A.9. The gradient of a scalar function is a vector function ⃗G(⃗x) = ∇ f (⃗x) which describes the rate of change of the scalar function in the three coordinate directions. In particular, this means that the x-component ∂x f (⃗x) of the resulting vector function ∇ f (⃗x) tells us the slope of the scalar ﬁeld f (⃗x) in the x-direction. Analogously, the y-component ∂y f (⃗x) tells us the slope in the y-direction and ∂z f (⃗x) the slope in the z-direction. In this sense, the gradient is simply an exten- sion of the usual derivative ∂x f (x) for functions which depend on more than one coordinate. For example, let’s imagine we have a scalar function f (⃗x) which describes the height of terrain above sea level. If we now evalu- ate the gradient ∂x f (⃗x) at one speciﬁc location ⃗G(⃗r) and use its ﬁrst component, it tells us whether we move up or down if we move in the x-direction. In addition, it also tells us how steep the slope is. To describe the slope in any possible direction, we need the slope in the three basis directions x, y and z and this is why the gradient has three components.5454 Take note that the gradient assigns a vector to each location. This vector points uphill in the direction of steepest slope and its magnitude |∇ f (⃗x)| yields the slope in this particular direction. vector calculus 263 Example: gradient The gradient of the scalar ﬁeld φ(⃗x) = x2y + y2 − zx (A.54) is ∇φ(⃗x) =  ∂x ∂y ∂z   (x2y + y2 − zx)↷ =      ∂x(x2y + y2 − zx) ∂y(x2y + y2 − zx) ∂z(x2y + y2 − zx)     ↷ =  2xy − z x2 + 2y −x   . (A.55) 264 no-nonsense electrodynamics A.11 Divergence The divergence ∇· is an operator which transforms a vector function ⃗F(⃗x) = (Fx(x), Fy(x), Fz(x))T into a scalar function:5555 Reminder: the little dot · denotes the scalar product. ∇ · ⃗F(⃗x) =   ∂x ∂y ∂z    ·   Fx(⃗x) Fy(⃗x) Fz(⃗x)    = ∂x Fx(⃗x) + ∂yFy(⃗x) + ∂zFz(⃗x) . The meaning of the resulting scalar function, called the diver- gence, can be summarized as follows:5656 We discussed ﬂux in Ap- pendix A.9. The divergence of a vector function is a scalar function D(⃗x) = ∇ · ⃗F(⃗x) which describes the amount of ﬂux D(⃗x) entering or leaving a given point ⃗x. So, analogous to how the charge density ρ(⃗x) encodes informa- tion about how the total charge q is distributed throughout the system, the divergence ∇ · ⃗F(⃗x) tells us how ﬂux is distributed. While the charge density ρ(⃗x) is deﬁned as net charge per unit volume, the divergence ∇ · ⃗F(⃗x) is deﬁned as net ﬂux per unit volume:5757 We will discuss below how this interpretation ﬁts together with the deﬁnition in terms of derivatives given above. Divergence = Flux Volume . (A.56) In this sense, an alternative name for the divergence of a vector ﬁeld could be \"ﬂux density\". From a slightly different perspective, we can say that the diver- gence of a vector ﬁeld tells us how much the ﬁeld \"diverges\" from a point.58 58 In physics, how much a ﬁeld diverges from a point depends on how much charge is present at this point. We will talk about this in more detail below. vector calculus 265 Let’s talk about these deﬁnitions and interpretations in a bit more detail. As mentioned above, our goal is to deﬁne a quantity which describes the ﬂux of a vector ﬁeld at a single point.59 59 The main motivation to introduce this quantity in physics is that it allows to switch from complicated surface and volume integrals to a point-wise description. The divergence is deﬁned, for example, to write Maxwell’s equations in a differential form. This is discussed in Chapter 3.In Section A.9, we deﬁned the ﬂux through a surface S as φ = ∫ S ⃗F · d⃗S. (A.57) The key idea is now that we shrink the surface S until we are left with the neighborhood of a single point. However, if we simply take the limit limS→0 in Eq. A.57 we ﬁnd that the ﬂux vanishes: lim S→0 ∫ S ⃗F · d⃗S = 0 . (A.58) Therefore, this is not a useful quantity to describe the ﬂux at a single point since it is simply zero for every point and thus contains no speciﬁc information. To get a meaningful quantity, we need to somehow account for the vanishing surface area in such a way that the total result is something constant, not something vanishing. A clever idea is to divide Eq. A.57 by the volume correspond- ing to the surface S. Each time we divide our surface area the volume gets divided too. Hence, the effect induced by the van- ishing surface area is canceled by the vanishing volume and the 266 no-nonsense electrodynamics result is something ﬁnite: lim V→0 1 V ∫ S ⃗F · d⃗S ≡ ∇ · ⃗F , (A.59) where S is the surface of the volume V and therefore also shrinks as we make the volume smaller. The resulting non- vanishing quantity ∇ · ⃗F is known as the divergence of ⃗F. In words, the divergence is the ﬂux out of the volume V per unit volume in the limit that V becomes inﬁnitesimally small. The deﬁnition in Eq. A.59 makes the interpretation of the divergence as ﬂux density (Eq. A.56) precise. Take note that ∇ · ⃗F is a scalar quantity, just like the ﬂux φ. We can understand the importance of this deﬁnition from a physical point of view. The divergence of a vector ﬁeld is only non-zero if there is a source for the vector ﬁeld present.60 This 60 In electrodynamics, our sources are electric charges. is described by Gauss’s law (Eq. 3.14): ∮ S ⃗E · ⃗dS = 1 ϵ0 ∫ V ρdV = ρ0 ϵ0 V , where we assumed, for simplicity, that the charge density is constant ρ(⃗x) = ρ0. On the right-hand side we have the total charge contained in the volume V. Therefore, as we shrink the surface to zero, our volume shrinks to zero too. This means that lim S→0 ∮ S ⃗E · d⃗S = lim V→0 ρ0 ϵ0 V = 0 . However, we can extract information about the vector ﬁeld at a single point by getting rid of the volume on the right-hand side:6161 It’s important to keep in mind that S is the surface of the volume V. Hence, if V shrinks to zero, its surface automatically shrinks to zero. lim V→0 1 V ∮ S ⃗E · d⃗S = ρ0 ϵ0 . The construction appearing on the left-hand side is what we call the divergence and using the deﬁnition in Eq. A.59 our equation reads ∇ · E = ρ0 ϵ0 . This is what we call Gauss’ law in differential form (Eq. 3.16). For many practical purposes this differential form is a lot more useful than the integral form since it contains information about the ﬁeld at single points and not only on surfaces. vector calculus 267 Now, how is the strange looking deﬁnition of the divergence in Eq. A.59 related to the simple deﬁnition given at the beginning of this section?62 62 Reminder: ∇ · ⃗F(⃗x) = ∂x F1(⃗x) + ∂y F2(⃗x) + ∂z F3(⃗x) . To understand this, we consider the ﬂux through a small rect- angular parallelepiped explicitly and take the limit where the volume of the parallelepiped shrinks to zero.63 We assume that 63 Take note since we are really interested in the limit ∆V → 0, the actual shape of the volume doesn’t matter and we simply choose a convenient one. the parallelepiped has edges of length ∆x, ∆y, ∆z parallel to the coordinate axes. The ﬂux through this parallelepiped is given by the sum over the ﬂux through the six sides. We start with the face that we call S1, as indicated in the ﬁgure above. The ﬂux through S1 is given by ∫ S1 ⃗F · ⃗ndS = ∫ S1 ⃗F · ⃗exdS = ∫ S1 FxdS . (A.60) Since our parallelepiped is small, the integral is approximately equal to the value of ⃗F at the middle of the side times the area of the side:64 64 The coordinates of the middle of the side S1 are (x + ∆x 2 , y, z) and the area of S1 is ∆y∆z. Moreover, take note that in the limit V → 0 the formula we derive using this approximation becomes exact. ∫ S1 ⃗F · ⃗ndS ≃ Fx(x + ∆x 2 , y, z)∆y∆z . (A.61) Analogously, the ﬂux through the opposite side S2 is65 65 The normal vector always points outwards and we therefore have −⃗ex as our normal vector. More- over, the coordinates of the center of S2 are (x − ∆x 2 , y, z).∫ S2 ⃗F · ⃗ndS ≃ −Fx(x − ∆x 2 , y, z)∆y∆z . (A.62) The total ﬂux through the two sides S1 and S2 is therefore66 66 We will talk about the contribu- tions from the remaining four sides in a moment. 268 no-nonsense electrodynamics ∫ S1+S2 ⃗F · ⃗ndS = ∫ S1 ⃗F · ⃗ndS + ∫ S2 ⃗F · ⃗ndS↷Eq. A.61 and Eq. A.62 ≃ Fx(x + ∆x 2 , y, z)∆y∆z − Fx(x − ∆x 2 , y, z)∆y∆z↷∆x ∆x = 1 = Fx(x + ∆x 2 , y, z) − Fx(x − ∆x 2 , y, z) ∆x ∆x∆y∆z↷∆x∆y∆z = ∆V = Fx(x + ∆x 2 , y, z) − Fx(x − ∆x 2 , y, z) ∆x ∆V . Once more we can see that the ﬂux vanishes for ∆V → 0. How- ever, the quantity 1 ∆V ∫ S1+S2 ⃗F · ⃗ndS ≃ Fx(x + ∆x 2 , y, z) − Fx(x − ∆x 2 , y, z) ∆x is non-vanishing in the limit ∆V → 0: lim ∆V→0 1 ∆V ∫ S1+S2 ⃗F · ⃗ndS = lim ∆V→0 Fx(x + ∆x 2 , y, z) − Fx(x − ∆x 2 , y, z) ∆x↷if ∆V → 0, ∆x → 0 = lim ∆x→0 Fx(x + ∆x 2 , y, z) − Fx(x − ∆x 2 , y, z) ∆x↷ = ∂Fx ∂x . Here we used that the quantity appearing on the right-hand side is exactly the deﬁnition of the partial derivative in terms of the difference quotient. This is the ﬂux per unit volume through the two sides S1 and S2 in the limit ∆V → 0. As mentioned above, the total ﬂux through the parallelepiped is given by the sum over the ﬂux through all six sides. The cal- culations for the remaining four sides are completely analogous and yield ∂Fy ∂y plus ∂Fz ∂z . The total ﬂux per unit volume is there- fore lim ∆V→0 1 ∆V ∫ S ⃗F · ⃗ndS = ∂Fx ∂x + ∂Fy ∂y + ∂Fz ∂z . (A.63) vector calculus 269 We can see, as promised above, that the two deﬁnitions of the divergence are really equivalent. In practice, of course, the sim- ple formula in terms of derivatives is usually used. Example: divergence The divergence of the vector ﬁeld ⃗A(⃗x) =  x2y yz yz2   (A.64) is ∇ · ⃗A(⃗x) =  ∂x ∂y ∂z   ·  x2y yz yz2  ↷ = ∂x(x2y) + ∂y(yz) + ∂z(yz2)↷ = 2xy + z + 2yz . (A.65) 270 no-nonsense electrodynamics A.12 Curl The curl ∇× is an operator which transforms a vector function ⃗F(⃗x) = (Fx(x), Fy(x), Fz(x))T into a different vector function:6767 Reminder: × denotes the cross product. ∇ × ⃗F(⃗x) =   ∂x ∂y ∂z    ×   Fx(⃗x) Fy(⃗x) Fz(⃗x)    =   ∂yFz(⃗x) − ∂zFy(⃗x) ∂zFx(⃗x) − ∂x Fz(⃗x) ∂x Fy(⃗x) − ∂yFx(⃗x)    The meaning of the resulting vector function, called the curl, can be summarized as follows:6868 We discussed the circulation of a vector ﬁeld in Appendix A.7. The idea of a circulation at a single point may seem strange at ﬁrst glance. However, in principle, the idea is completely analogous to the idea behind the divergence of a vector ﬁeld which describes the ﬂux at a single point. Moreover, to understand it, imagine a little paddle wheel at each location. At locations with non-zero curl, the paddle wheel will rotate. The curl of a vector function is a vector function ⃗C(⃗x) = ∇ × ⃗F(⃗x) which describes the circulation ⃗C(⃗r) of the vector ﬁeld ⃗F(⃗x) at a given point ⃗r. So, analogous to how the divergence ∇ · ⃗F(⃗x) tells us how ﬂux is distributed through the system, the curl ∇ × ⃗F(⃗x) tells us how circulation is distributed throughout the system. While the divergence is deﬁned as ﬂux per unit volume, the curl is deﬁned as circulation per area: Curl = Circulation Area . (A.66) In this sense, an alternative name for the curl of a vector ﬁeld could be \"circulation density\". 69 69 We will discuss below how this interpretation ﬁts together with the deﬁnition in terms of derivatives given above. Let’s talk about these deﬁnitions and interpretations in a bit more detail. vector calculus 271 In Section A.7, we deﬁned the circulation ⃗C(⃗x) of a vector ﬁeld ⃗F(⃗x) along a path P by Circulation(⃗x) = ∮ P ⃗F(⃗x) ·⃗t ds, (A.67) where s is a parameter which parameterizes our path and ⃗t(s) a unit vector tangential to the path. Now, the idea is completely analogous to what we discussed in Appendix A.11. We shrink the path P until we are left with the neighborhood of a single point. However, if we simply take the limit limP→0 in Eq. A.57, we ﬁnd zero: lim P→0 ∮ P ⃗F ·⃗t ds = 0 . (A.68) Therefore, this is not a useful quantity to describe the circu- lation at a single point. However, we can construct a useful quantity by dividing our circulation by the area S enclosed by the path and then take the limit70 70 The line of thought here is com- pletely analogous to what we discussed in Appendix A.11. Take note that since P is the path around S, the limit limS→0 implies auto- matically that P shrinks to zero too. lim S→0 1 S ∮ P ⃗F ·⃗t ds ̸= 0 . (A.69) This quantity describes the circulation around the point en- closed by the speciﬁc path P. However, there are many different paths around a given point and therefore Eq. A.69 doesn’t contain all possible information about how the vector ﬁeld ⃗F curls around the point in question. Instead, we need to calculate Eq. A.69 for multiple paths. We must choose the paths in such a way that they can be used as basic building blocks for all possible paths. For example, in three dimensions we can use one path Pyx lying in the yx- plane (with normal vector ⃗ez), another one Pxz in the xz-plane (with normal vector ⃗ey) and a third one Pyz in the zy-plane (with normal vector ⃗ex). The circulation around these three paths contains complete information about how ⃗F curls. In particular, it is useful to write the resulting three basic circu- lations as a vector 272 no-nonsense electrodynamics ⃗C(⃗x) = (circulation around Pyz) ⃗ex + (circulation around Pxz) ⃗ey + (circulation around Pyx) ⃗ez ≡ ∇ × ⃗F(⃗x) (A.70) since we can then calculate the correct circulation around a general path with normal vector ⃗n using ⃗n · ⃗C(⃗x).7171 For example, for ⃗n = ⃗ex, we get the correct result for our path Pyz. This idea allows us to derive the formula for the curl as a vector ﬁeld involving derivatives, as stated at the beginning of this section. Let’s see how this works in practice. We start with a small rectangular path Pyx in the yx-plane with edge lengths ∆x and ∆y. This path consist of four straight lines. vector calculus 273 Therefore, to calculate the circulation along the path Pyx, we need to calculate the contributions from these four individual lines. We start with the contribution from PB: ∫ PB ⃗F ·⃗t ds = ∫ PB Fx dx . (A.71) Since our rectangle is small, we can approximate the integral as the value of Fx at the center of the line times the length of the line72 72 The coordinate of the center of the line PB is (x, y + ∆y 2 , 0) and the length of the line PB is ∆x. The vector tangential to the path is ⃗ex. Moreover, take note that in the limit S → 0, the ﬁnal formula will become exact. ∫ PB ⃗F(⃗x) ·⃗ex ds = ∫ PB Fx(⃗x) dx ≃ Fx (x, y − ∆y 2 , 0) ∆x . (A.72) For the opposite line PT, we can calculate analogously73 73 The coordinate of the center of the line PT is (x, y − ∆y 2 , 0) and the length of the line PT is ∆x. However, take note that the vector tangential to the path is now −⃗ex. ∫ PT ⃗F(⃗x) · (−⃗ex) ds = − ∫ PT Fx(⃗x) dx ≃ −Fx (x, y + ∆y 2 , 0) ∆x . (A.73) The total circulation along the two edges PB and PT is there- fore74 74 We will talk about the contribu- tions from the remaining sides in a moment. ∫ PB+PT ⃗F ·⃗t ds = ∫ PB ⃗F ·⃗t ds + ∫ PT ⃗F ·⃗t ds↷Eq. A.72 and Eq. A.73 ≃ Fx (x, y − ∆y 2 , 0) ∆x − Fx (x, y + ∆y 2 , 0) ∆x↷∆y ∆y = 1 = Fx (x, y − ∆y 2 , 0) − Fx (x, y + ∆y 2 , 0) ∆y ∆x∆y↷∆x∆y = ∆Syx = Fx (x, y − ∆y 2 , 0) − Fx (x, y + ∆y 2 , 0) ∆y ∆Syx . (A.74) We can see that the circulation vanishes for ∆Syx → 0. However, the quantity 1 ∆Syx ∫ PB+PT ⃗F ·⃗t ds ≃ Fx (x, y − ∆y 2 , 0) − Fx (x, y + ∆y 2 , 0) ∆y (A.75) is non-vanishing in the limit ∆Syx → 0: 274 no-nonsense electrodynamics lim ∆Syx→0 1 ∆Syx ∫ PB+PT ⃗F ·⃗t ds = lim ∆Syx→0 1 ∆S Fx (x, y − ∆y 2 , 0) − Fx (x, y + ∆y 2 , 0) ∆y↷ = − lim ∆y→0 Fx (x, y + ∆y 2 , 0) − Fx (x, y − ∆y 2 , 0) ∆y↷ = − ∂Fx ∂y . Here we used that the quantity appearing on the right-hand side is exactly the deﬁnition of the partial derivative in terms of the difference quotient. Completely analogously, we can calculate the contributions from the remaining two paths PL and PR: lim ∆Syx→0 1 ∆Syx ∫ PL+PR ⃗Fy ·⃗t ds = ∂Fy ∂x . (A.76) Therefore, the total circulation per unit area along the path Pyx is lim ∆Syx→0 1 ∆Syx ∫ Pyx ⃗F ·⃗t ds = lim ∆S→0 1 ∆S ∫ PB+PT +PL+PR ⃗F ·⃗tds = ∂Fy ∂x − ∂Fx ∂y . (A.77) Moreover, following exactly the same steps we can calculate the circulation per unit area along the paths Pxz and Pyz : lim ∆Sxz→0 1 ∆Sxz ∫ Pxz ⃗F ·⃗t ds = ∂Fx ∂z − ∂Fz ∂x lim ∆Syz→0 1 ∆Syz ∫ Pyz ⃗F ·⃗t ds = ∂Fz ∂y − ∂Fy ∂z . (A.78) vector calculus 275 Therefore, using Eq. A.70, we can conclude that ∇ × ⃗F(⃗x) = lim ∆Syz→0 lim ∆Sxz→0 lim ∆Syx→0     1 ∆Syz ∫ Pyz ⃗F ·⃗t ds 1 ∆Sxz ∫ Pxz ⃗F ·⃗t ds 1 ∆Syx ∫ Pyx ⃗F ·⃗t ds     =    ∂yFz(x) − ∂zFy(x) ∂zFx(x) − ∂x Fz(x)) ∂x Fy(x) − ∂yFx(x)    which is exactly the formula given at the beginning of this chap- ter. Example: curl The curl of the vector ﬁeld ⃗A(⃗x) =  x2y yz yz2   (A.79) is ∇ × ⃗A(⃗x) =  ∂x ∂y ∂z   ×  x2y yz yz2  ↷ =      ∂y(yz2) − ∂z(yz) ∂z(x2y) − ∂x(yz2) ∂x(yz) − ∂y(x2y)     ↷ =  z2 − y 0 − 0 0 − x2  ↷ =  z2 − y 0 −x2   . (A.80) 276 no-nonsense electrodynamics A.13 The fundamental theorem for gradients The fundamental theorem for gradients reads ∫ P ∇φ(⃗x) · d⃗s = φ(⃗r1) − φ(⃗r0) (A.81) where ⃗r0 is the starting point and ⃗r1 the end point of the path P. Therefore, in words it tells us The path integral of the gradient of a scalar ﬁeld is equal to the value of the ﬁeld at the end point minus its value at the starting point. In other words, it tells us that taking the gradient and calculat- ing the path integral are inverse operations, analogous to how taking the derivative and integrating are inverse operations for ordinary functions. To understand the theorem, let’s imagine we want to measure the height of a building. There are two possibilities: ◃ A rough approximation can be calculated by climbing the stairs and measuring the time that we need to get to the top ∆t. We can then multiply this result by our average velocity to calculate the height of the building height ≃ v∆t . (A.82) We get a better result, by measuring the time we need for each step ∆xi and then multiplying it by our velocity vi dur- ing this step. This yields the height of each step and the total height is therefore height ≃ N ∑ i vi∆ti , (A.83) vector calculus 277 where N denotes the number of steps. In the limit of an inﬁnitesimal step size, the sum becomes an integral and we get an exact formula for the height of the building:75 75 Take note that, formally, this is the same result that we get when we use a ruler to measure the rise at each step ∆xi and then add all these results up: N ∑ i ∆xi → ∫ H 0 dx = ∫ T 0 dx(t) dt dt, where H denotes the height of the building and T is the total time that we need to get to the top. height = ∫ T 0 v(t)dt = ∫ T 0 dx(t) dt dt . (A.84) This is exactly the method used on the left-hand side in Eq. A.81.76 76 The only difference is that in Eq. A.81 our function φ(⃗x) does depend on x, y, z and not only on x. Hence, to get the total height, we need to take the contributions coming from x, y, z into account. ◃ Alternatively, we can use an altimeter to measure the height above sea level at the top and bottom of the building and subtract these two results: height = x(top) − x(bottom) . (A.85) This is exactly the method used on the right-hand side in Eq. A.81. 278 no-nonsense electrodynamics A.14 The fundamental theorem for divergences a.k.a. Gauss’s theorem The fundamental theorem for divergences reads ∫ V ∇ · ⃗F(⃗x)dV = ∮ S ⃗F(⃗x) · d⃗S (A.86) where S is the surface of the volume V. Therefore, in words it tells us The volume integral of the divergence of a vector ﬁeld is equal to the integral of the ﬁeld over the surface of the volume. The theorem is important because it allows us to replace ﬂux integrals with volume integrals and vice versa. This is useful, for example, if we want to rewrite Maxwell’s equations in a more compact form. We can understand in intuitive terms why the theorem is true as follows. Let’s imagine that the vector ﬁeld ⃗F describes the ﬂow of a particular quantity, say, an incompressible ﬂuid like water. The ﬂux integral on the right-hand side in Eq. A.86 gives us the total amount of water that ﬂows through the surface S per unit time:7777 We discussed the notion of \"ﬂux integral\" in Appendix A.9. ﬂux = ∮ S ⃗F(⃗x) · d⃗S . Now, analogous to how there are two ways of measuring the total height of a building, there is an alternative method to measure this total amount of water.7878 The two methods to measure the height of a building were discussed in Appendix A.13 in the context of the fundamental theorem for gradients. To understand the second method, which is described by the left-hand side in Eq. A.86, we need to recall what the divergence vector calculus 279 of a vector ﬁeld ∇ · ⃗F(⃗x) means. A non-zero divergence at a particular location always indicates that there is a source or a sink for the quantity in question.79 If there is a water source 79 A non-zero divergence means that the vectors of the ﬁeld spread out from the location in question. In physical terms, this means that there must be a source for the vector ﬁeld. In the case of water, our source would be, for example, a faucet. inside the volume, a particular amount of water is necessarily forced out of the volume since the ﬂuid is incompressible. This means, that an alternative method to determine the amount of water ﬂowing through the surface is to count the number of sources and sinks inside the volume.80 80 A sink acts like a \"negative source\" and hence yields a negative contribution to the total ﬂow. If there are more sinks than sources inside the volume, the total ﬂow through the surface is negative, which means that water ﬂows into the volume. ﬂux = ∑ sources (- sinks) = ∫ V ∇ · ⃗F(⃗x) dV This is what the left-hand side in Eq. A.86 describes. Alternatively, we can understand the left-hand side by recalling that the divergence of a vector ﬁeld is deﬁned as the ﬂux den- sity.81 Hence, integrating the divergence over a volume yields 81 This was discussed in Ap- pendix A.11.the total ﬂux. We can also understand this in more mathematical terms. We start with the ﬂux integral on the right-hand side in Eq. A.86 ∮ S ⃗F(⃗x) · d⃗S . (A.87) The main idea is that we can divide the volume enclosed by the surface S into N subvolumes Vi. 280 no-nonsense electrodynamics The total ﬂux through the surface S is then equal to the ﬂux through the surfaces of all subvolumes ∮ S ⃗F(⃗x) · d⃗S ≃ N ∑ i ∫ Si ⃗F(⃗x) · d⃗S . (A.88) This is not obvious but we can understand this statement by considering the ﬂux through the surfaces of two adjacent sub- volumes A crucial observation is that the ﬂux through the common face S0 of the two volumes cancels exactly. This happens because the ﬂux is deﬁned as the projection of the vector ﬁeld onto the outward pointing normal vector.82 The ﬂux through the ﬁrst 82 This is discussed in Ap- pendix A.9. subvolume therefore contains a contribution of the form ∫ S0 ⃗F(⃗x) · ⃗n1dS , (A.89) vector calculus 281 while the ﬂux through the second subvolume contains a contri- bution of the form ∫ S0 ⃗F(⃗x) · ⃗n2dS . (A.90) Since ⃗n2 = −⃗n1 it follows that 83 83 In the limit of inﬁnitesimal subvolumes, the two contributions cancel exactly. ∫ S0 ⃗F(⃗x) · ⃗n1dS + ∫ S0 ⃗F(⃗x) · ⃗n2dS = ∫ S0 ⃗F(⃗x) · ⃗n1dS + ∫ S0 ⃗F(⃗x) · (−⃗n1)dS ≃ 0 . (A.91) We can therefore conclude that the ﬂux through all common faces of the subvolumes cancel and the only non-vanishing contributions are those on the boundary of the volume. This is exactly the statement in Eq. A.88. However, this formula is only exact in the limit of inﬁnitesimal subvolumes. To make sense of our formula in this limit, we modify it as follows84 84 It will become clear in a moment why this modiﬁcation is helpful. ∮ S ⃗F(⃗x) · d⃗S ≃ N ∑ i ∫ Si ⃗F(⃗x) · d⃗S this is Eq. A.88↷∆Vi ∆Vi = 1 = N ∑ i ( 1 ∆Vi ∫ Si ⃗F(⃗x) · d⃗S)∆Vi . (A.92) If we now take the limit lim∆Vi→0 the expression between the large brackets is exactly the formal deﬁnition of the divergence (Eq. A.59). Moreover, the sum becomes an integral and we can therefore conclude that ∮ S ⃗F(⃗x) · d⃗S = ∫ V ∇ · ⃗F(⃗x)dV , (A.93) which is exactly the fundamental theorem for divergences (Eq. A.86) stated at the beginning of this section. A.15 The fundamental theorem for curls a.k.a. Stokes’ theorem The fundamental theorem for curls reads ∫ S ∇ × ⃗F(⃗x) · d⃗S = ∮ P ⃗F(⃗x) · d⃗l (A.94) 282 no-nonsense electrodynamics where P is the boundary of the surface S. Therefore, in words it tells us The surface integral of the curl of a vector ﬁeld is equal to the integral of the ﬁeld over the boundary of the surface. The theorem is important because it allows us to replace path integrals with surface integrals and vice versa. This is useful, for example, if we want to rewrite Maxwell’s equations in a more compact form. We can understand why the theorem is correct as follows. The path integral on the right-hand side in Eq. A.94 describes the circulation of the vector ﬁeld ⃗F(⃗x).8585 The circulation of a vector ﬁeld was discussed in Appendix A.7. The curl on the left-hand side in Eq. A.94 is a measure for the circulation at a particular point. In other words, the curl de- scribes a circulation density.86 Hence, by integrating the curl 86 This was discussed in Ap- pendix A.12. over a particular area yields the total circulation. We can also understand this in more mathematical terms. We start with the circulation integral on the right-hand side in Eq. A.94:8787 Take note that the steps we follow here are completely analogous to the steps we followed to derive the fundamental theorem for divergences in Appendix A.13. vector calculus 283 ∮ P ⃗F(⃗x) · d⃗l (A.95) The main idea is that we can divide the surface enclosed by the path P into N small surfaces Si. The total circulation along the path P is then equal to the circu- lation along the boundaries of all these small surfaces ∮ P ⃗F(⃗x) · d⃗l ≃ N ∑ i ∮ P ⃗F(⃗x) · d⃗l . (A.96) This is not obvious but we can understand this statement by considering the circulation along the boundary of two small surfaces. The crucial observation is that the circulation along the common edge cancels: ∮ B A ⃗F(⃗x) · d⃗l + ∮ A B ⃗F(⃗x) · d⃗l = 0 . (A.97) 284 no-nonsense electrodynamics We can therefore conclude that the circulation along all com- mon edges vanishes and the only non-vanishing contributions are those on the boundary of the surface.88 This is exactly the 88 We can understand that the actual shape of the surface doesn’t matter. There are many possible surfaces with the same boundary path P. However, since the contributions from small rectangles on the surface cancel anyway, the exact shape doesn’t matter. statement in Eq. A.96. This formula is only exact in the limit of inﬁnitesimal surfaces lim∆Si→0. Before we consider this limit, we rewrite our equation as follows89 89 It will become clear in a moment why this modiﬁcation is helpful. ∮ P ⃗F(⃗x) · d⃗l ≃ N ∑ i ∮ P ⃗F(⃗x) · d⃗l this is Eq. A.96↷∆Si ∆Si = 1 = N ∑ i ( 1 ∆Si ∮ P ⃗F(⃗x) · d⃗l)∆Si . (A.98) If we now take the limit lim∆Si→0 the expression between the large brackets is exactly the formal deﬁnition of the curl (Eq. A.69). Moreover, the sum becomes an integral, and we can therefore conclude ∮ P ⃗F(⃗x) · d⃗l = ∫ S ∇ × ⃗F(⃗x) · d⃗S , (A.99) which is exactly the fundamental theorem for curls (Eq. A.94). A.16 Vector identities There are many useful mathematical relations involving the various quantities introduced in the previous sections. These so-called vector identities can often be used to make formulas shorter and calculations simpler. For example, we have learned that we can construct a vector function ⃗g(⃗x), called the gradient, from any given scalar func- tion ⃗g(⃗x) = ∇ f (⃗x). An important fact is that a vector function which is constructed this way is always curl-free:9090 We will see why this is true below. Take note that in contrast, a general vector ﬁeld, i.e. one which isn’t constructed as the gradient of a scalar ﬁeld, can have a non-zero curl. ∇ × ⃗g = ∇ × ∇ f (⃗x) = 0 . (A.100) This observation is important in physics for the following rea- son. vector calculus 285 The work W done by a particular force along a speciﬁc path P between two points A and B is given by W = ∫ P ⃗F ·⃗tds . (A.101) We can now check if ∇ × ⃗F is zero. If yes, we know immediately that we can write ⃗F as the gradient of a scalar function:91 91 We use the fundamental theorem for gradients, which is completely analogous to the usual fundamental theorem ∫ b a ∂x f (x) dx = f (b) − f (a). In other words, we simply use that integration and differentiation \"cancel\" each other. W = ∫ P ⃗F ·⃗tds = ∫ P ∇ f ·⃗tds = f (B) − f (A) (A.102) where A and B denotes the starting and endpoint of the path P. This means that the work done between A and B does not depend on the particular path between these two points. In addition, if we consider a circular path we have A = B and therefore ﬁnd W = ∮ P ⃗F ·⃗tds = ∮ P ∇ f ·⃗tds = f (A) − f (A) = 0 . (A.103) In words, this means that if the force can be written as the gra- dient of some potential, the work done along a circular path is always zero. Forces with this property are known as conserva- tive forces, since no energy gets lost along the way.92 92 If work is needed to move along a circular path, some energy has to go missing, e.g. in the form of friction. Now, Eq. A.100 tells us that we can write the force as the gra- dient of some potential if the curl vanishes. This means that for any given force ⃗F(⃗x), we can check immediately whether it’s conservative or not, simply by calculating the curl ∇ × ⃗F(⃗x). If the curl vanishes, we know that ⃗F(⃗x) is a conservative force.93 93 In physics, a vector function describing a conservative force ⃗F(⃗x) can be thought of to be originating from a corresponding potential φ: ⃗F = −∇φ. Famous examples are the Lorentz force and the gravitational force. To summarize: ∫ P ⃗F ·⃗tds is path independent oo // ⃗F = ∇φ oo // ∇ × ⃗F = 0 Now, why is Eq. A.100 true? It’s possible to check a vector identity like the one in Eq. A.100 286 no-nonsense electrodynamics simply by brute force: ∇ × ∇ f (⃗x) = ∇ ×   ∂x f (⃗x) ∂y f (⃗x) ∂z f (⃗x)   ↷ =   ∂y∂z f (⃗x) − ∂z∂y f (⃗x) ∂z∂x f (⃗x) − ∂x∂z f (⃗x) ∂x∂y f (⃗x) − ∂y∂x f (⃗x)   ↷∂x∂y = ∂y∂x etc. =   ∂z∂y f (⃗x) − ∂z∂y f (⃗x) ∂z∂x f (⃗x) − ∂z∂x f (⃗x) ∂x∂y f (⃗x) − ∂x∂y f (⃗x)   ↷ =   0 0 0    □ (A.104) However, we can also understand it intuitively. If the scalar ﬁeld f (⃗x) describes the height of a given terrain, the gradient describes its slope. Equation A.100 then tells us that there is no circulation in the arrows describing the slope. This is certainly a sensible statement since we can’t walk from A to B going uphill and then from B to A walk uphill again.94 In other 94 Recall that the gradient vectors always point in the direction of the biggest slope. A situation with a circulating slope would represent a situation like those that are depicted in the famous paintings by M. C. Escher. words, we can’t go uphill both ways. Alternatively, we can interpret Eq. A.100 for the case where the scalar ﬁeld f (⃗x) describes the gravitational potential. The gradi- ent ∇ f (⃗x) then describes the gravitational force. Now, Eq. A.100 states that a block slides downwards without spinning (at least in the absence of friction). vector calculus 287 There is another particular important vector identity which we can use to understand the origin of the homogeneous Maxwell equations. This identity and its geometrical meaning are the topic of the next section. A.16.1 The Bianchi identity One of the homogeneous Maxwell equations reads ∇ · ⃗B = 0. In terms of the vector potential ⃗A, we can write it as95 95 Reminder, Eq. 2.20: ⃗B = ∇ × ⃗A∇ · (∇ × ⃗A) = 0 . (A.105) An important observation is now that this equation is true for any vector ﬁeld ⃗A. In other words, the divergence of the curl is always zero for any vector ﬁeld. This means that Eq. A.105 is a general vector identity analogous to ∇ × ∇ f (⃗x) = 0 which we discussed in the previous section. Again, it’s possible to check the validity of Eq. A.105 simply by brute force. However, there is also a different and more insightful way to see why Eq. A.105 is always true. The main idea is to integrate Eq. A.105 and then apply Stokes’s theorem and Gauss’s theorem96: 96 We discussed Stokes’ theorem in Appendix A.15 and Gauss’s theorem in Appendix A.13. ∫ V ∇ · (∇ × ⃗A)dV = ∮ δV=S(∇ × ⃗A) · d⃗S using Gauss’s theorem Eq. A.86↷Stoke’s theorem Eq. A.94 = ∮ δS ⃗A · d⃗l↷δS = δδV = 0 = 0 ✓ (A.106) The crucial observation in the last step is that we integrate over the boundary of the surface of the volume. But the surface of a volume has no boundary and therefore the integral vanishes. 288 no-nonsense electrodynamics This is true in general. A boundary doesn’t have a boundary. One way of understanding why this is true is by noting that every potential boundary point has already been used to deﬁne the boundary in the ﬁrst place. For example, the boundary of a disk is a circle but a circle has no boundary.97 Or, the boundary 97 In contrast, a line has a boundary which consists of the two end points. However, this boundary (the two end points) do not have a boundary themselves. of a ball is a sphere but a sphere has no boundary. To summarize: the deep reason why ∇ · (∇ × ⃗A) = 0 (Eq. A.105) is true is because the boundary of a boundary is always zero.98. 98 This interpretation of Bianchi identities is promoted mainly in [Misner, 1973] There are many similar identities involving ∇, commonly known as vector identities. Below you’ll ﬁnd a list of par- ticularly important vector identities. They can all be checked explicitly, analogous to what we did in Eq. A.104. vector calculus 289 A.16.2 Summary of vector identities ⃗∇ · (⃗∇ × ⃗A) ≡ div(rot ⃗A) = (⃗∇ × ⃗∇) · ⃗A ≡ 0 ⃗∇ × (⃗∇ϕ) ≡ rot grad ϕ = (⃗∇ × ⃗∇)ϕ ≡ 0 ⃗∇ · ( ⃗Aϕ) = ϕ⃗∇ · ⃗A + ⃗A · ⃗∇ϕ ⇐⇒ div( ⃗Aϕ) = ϕ div ⃗A + ⃗A · grad ϕ ⃗∇ × ( ⃗Aϕ) = ϕ⃗∇ × ⃗A − ⃗A × ⃗∇ϕ ⇐⇒ rot( ⃗Aϕ) = ϕ rot ⃗A − ⃗A × grad ϕ ⃗∇ · ( ⃗A × ⃗B) = ⃗B · (⃗∇ × ⃗A) − ⃗A · (⃗∇ × ⃗B) ⇐⇒ div( ⃗A × ⃗B) = ⃗B · rot ⃗A − ⃗A · rot ⃗B ⃗∇ × ( ⃗A × ⃗B) = (⃗B · ⃗∇) ⃗A − ( ⃗A · ⃗∇)⃗B + ⃗A(⃗∇ · ⃗B) − ⃗B(⃗∇ · ⃗A) ⇐⇒ rot( ⃗A × ⃗B) = (⃗B grad) ⃗A − ( ⃗A grad)⃗B + ⃗A(div ⃗B) − ⃗B(div ⃗A) ⃗∇( ⃗A · ⃗B) = (⃗B · ⃗∇) ⃗A + ( ⃗A · ⃗∇)⃗B + ⃗A × (⃗∇ × ⃗B) + ⃗B × (⃗∇ × ⃗A) ⇐⇒ grad( ⃗A · ⃗B) = (⃗B · grad) ⃗A + ( ⃗A · grad)⃗B + ⃗A × rot ⃗B + ⃗B × rot ⃗A ⃗∇ · (⃗∇ϕ) ≡ div(grad ϕ) ≡ ∆ϕ = ∂2 ϕ ∂x2 + ∂2 ϕ ∂y2 + ∂2 ϕ ∂z2 , ∆ = Laplace Operator ⃗∇ × (⃗∇ × ⃗A) ≡ rot(rot ⃗A) = ⃗∇(⃗∇ · ⃗A) − (⃗∇ · ⃗∇) ⃗A ≡ grad div ⃗A − ∆ ⃗A 290 no-nonsense electrodynamics A.17 Index notation and Maxwell’s equations In this section, we will neglect the constant c to unclutter the nota- tion. In other words, we work in \"natural units\" where c = 1. This is often extremely convenient for fundamental considerations. However, if you want to get some number you can compare to an experiment (which usually use SI-units) you have to remember to insert the speed of light c in a few places. In this appendix, we discuss how Maxwell’s equations (Eq. 1.1) can be written in terms of the electric and magnetic ﬁelds (Eq. 1.4). Before we can do that, we need to recall some deﬁ- nitions and talk about various conventions.9999 Don’t get demotivated if not every step in the following calcu- lations is perfectly clear. It simply takes some time to get used to the index notation used in special rela- tivity. Moreover, the details are not really important for the purpose of this book and it is a perfectly valid approach to simply take the equiv- alence of Eq. 1.1 and Eq. 1.4 for granted. It probably makes more sense to check the equivalence later once you are more familiar with the notation used in special relativity. First of all, take note that the ﬁeld-strength tensor has in total 4 × 4 = 16 components.100 But not all these components are 100 Reminder: Greek indices like µ, ν or σ are always summed from 0 to 3: xµyµ = ∑3 µ=0 xµyµ. independent. This follows because the ﬁeld-strength tensor Fµν is antisymmetric Fµν = −Fνµ, which we can see by looking at the deﬁnition in terms of the potential Fµν = ∂µ Aν − ∂ν Aµ . (A.107) An antisymmetric (4 × 4) matrix has only 6 independent com- ponents and it is conventional to label the independent compo- nents as follows Fµν =      F00 F01 F02 F03 F10 F11 F12 F13 F20 F21 F22 F23 F30 F31 F32 F33     ↷ ≡      0 −E1 −E2 −E3 E1 0 −B3 B2 E2 B3 0 −B1 E3 −B2 B1 0      . In index notation, this means that101101 Reminder: Roman indices always run from 1 to 3. Fi0 = Ei . (A.108) vector calculus 291 For example, F10 = E1 or F20 = E2. The remaining three inde- pendent components, are in index notation, given by Fij = −ϵijkBk . (A.109) For example,102 102 Reminder: whenever an index appears twice in a term, an implicit sum is assumed. This is known as Einstein’s summation convention.F12 = −ϵ12kBk↷ = − ϵ121︸︷︷︸ =0 B1 − ϵ122︸︷︷︸ =0 B2 − ϵ123︸︷︷︸ =1 B3↷ = −B3 . (A.110) Before we can rewrite Maxwell’s equations in terms of the elec- tric and magnetic ﬁelds, there is one more thing that we need to talk about. Whenever an index appears twice but one time as a superscript and one time as a subscript, as it is the case in Maxwell’s equa- tions, there is an implicit minus sign between the 0-term and the remaining terms:103 103 Don’t worry if you don’t under- stand the following explanations immediately. It takes some time getting used to this Minkowski no- tation. For us it is enough to know how to interpret a term of the form xµyµ like it appears in Maxwell’s equations when we write them in terms of the ﬁeld-strength tensor. xµyµ = 3 ∑ µ=0 xµyµ↷ = x0y0 − 3 ∑ i=1 xiyi↷ = x0y0 − x1y1 − x2y2 − x3y3 . The reason for this convention is that the scalar product in special relativity reads104 104 The scalar product is the cor- rect method to combine two vectors to get something invari- ant, i.e. a scalar. The naive sum x0y0 + x1y1 + x2y2 + x3y3 is not invariant under Poincaré trans- formations and therefore not the correct scalar product. For some more information on special relativ- ity, see Chapter 6. xµyµηµν , (A.111) where ηµν is the Minkowski metric105 105 A metric is a mathematical tool which tells us the distance between two points. In the usual Euclidean space of classical mechanics, the metric is simply the unit matrix. However, in special relativity we are talking about distances in Minkowski spacetime and the correct way to measure distances is to use the Minkowski metric. ηµν =      1 0 0 0 0 −1 0 0 0 0 −1 0 0 0 0 −1      . (A.112) 292 no-nonsense electrodynamics So we have xµηµνyν = (x0 x1 x2 x3)      1 0 0 0 0 −1 0 0 0 0 −1 0 0 0 0 −1           y0 y1 y2 y3      = x0y0 − x1y1 − x2y2 − x3y3 . (A.113) To unclutter the notation, it is convenient to get rid of the Minkowski metric by introducing superscript indices: yµ ≡ ηµνyν . (A.114) We can then write the scalar product as xµyνηµν ≡ xµyµ . (A.115) With this in mind, we can rewrite Maxwell’s equations (Eq. 1.1) ∂νFµν = µ0 Jµ ∂λFµν + ∂µFνλ + ∂νFλµ = 0 in terms of the components Ei and Bi. We start with the inhomogeneous Maxwell equations106106 The inhomogeneous Maxwell equations are those without a zero on the right-hand side. ∂σ Fρσ = Jρ (A.116) For the three components ρ → i = 1, 2, 3, we have Ji = ∂σ Fiσ↷Einstein’s summation convention = ∂0Fi0 − ∂k Fik↷Fk0 = Ek, Eq. A.108 and Fik = ϵikl Bl, Eq. A.109 = ∂0Ei + ϵikl∂kBl↷∇ × ⃗B is ϵikl ∂k Bl in index notation. ∴ ⃗J = ∂t⃗E + ∇ × ⃗B . (A.117) vector calculus 293 For the remaining component (ρ → 0), we have J0 = ∂σ F0σ↷Einstein’s summation convention = ∂0F00 − ∂k F0k↷F00 = 0 and F0k = −Fk0 = ∂k Fk0↷Fk0 = Ek, Eq. A.108 = ∂kEk . (A.118) and we can conclude that ∇ · ⃗E = J0 . (A.119) Analogously, we can rewrite the homogeneous Maxwell equa- tions ∂λFµν + ∂µFνλ + ∂νFλµ = 0 (A.120) in terms of Ei and Bi. First of all, take note that we can write the homogeneous Maxwell equations as ϵµνλδ∂λFµν = 0 . This follows when we multiply Eq. A.120 by ϵµνλδ 0 = ϵµνλδ∂λFµν + ϵµνλδ∂µFνλ + ϵµνλδ∂νFλµ↷renaming indices = ϵµνλδ∂λFµν + ϵλµνδ∂λFµν + ϵνλµδ∂λFµν↷switching indices of ϵλµνδ = ϵµνλδ∂λFµν + ϵµνλδ∂λFµν − ϵµνλδ∂λFµν↷ = ϵµνλδ∂λFµν ✓ We can then start by looking at the 0-component of this equa- tion (δ → 0):107 107 δ is the only free index, i.e. the only index which does not appear twice in each term. 294 no-nonsense electrodynamics 0 = ϵµνλ0∂λFµν↷ϵµνλδ is zero if two indices are equal = ϵijk∂k Fij↷Fij = ϵijl Bl, Eq. A.109 = ϵijk∂k(ϵijl Bl)↷ = ϵijkϵijl∂kBl↷ϵijkϵijl = 2δkl, where δkl is the Kronecker delta = 2δkl∂kBl↷ = 2∂l Bl . We can therefore conclude that ∇ · ⃗B = 0 . (A.121) Analogously, we can take a look at the remaining components (δ → i) and derive ∇ × ⃗E + ∂t⃗B = 0 . (A.122) This is the conventional form of the homogeneous Maxwell equation which is used for real-world applications. A.17.1 Electrodynamical Lagrangian In this appendix, we want to understand why we can write the electrodynamical Lagrangian in the following two forms: LMaxwell = 1 2 (∂µ Aν∂µ Aν − ∂µ Aν∂ν Aµ) = 1 4 FµνFµν . (A.123) vector calculus 295 We can see the equivalence as follows LMaxwell = 1 4 FµνFµν↷ = 1 4 (∂µ Aν − ∂ν Aµ)(∂µ Aν − ∂ν Aµ)↷ = 1 4 (∂µ Aν∂µ Aν − ∂µ Aν∂ν Aµ↷ − ∂ν Aµ∂µ Aν + ∂ν Aµ∂ν Aµ)↷renaming dummy indices = 1 4 (∂µ Aν∂µ Aν − ∂µ Aν∂ν Aµ↷ − ∂µ Aν∂ν Aµ + ∂µ Aν∂µ Aν)↷ = 1 4 (2∂µ Aν∂µ Aν − 2∂µ Aν∂ν Aµ)↷ = 1 2 (∂µ Aν∂µ Aν − ∂µ Aν∂ν Aµ) ✓ (A.124) B Taylor Expansion The Taylor expansion is one of the most useful mathematical tools and we need it all the time in physics to simplify compli- cated systems and equations. We can understand the basic idea as follows: Imagine that you sit in your car and wonder what your exact location l(t) will be in 10 minutes: l(t0 + 10 minutes) =? ◃ A ﬁrst guess is that your location will be exactly your current location l(t0 + 10 minutes) ≈ l(t0) . Given how large the universe is and thus how many possible locations there are, this is certainly not too bad. ◃ If you want to do a bit better than that, you can also include your current velocity ˙l(t0) ≡ ∂tl(t)∣ ∣t0.1 The total distance 1 Here ∂t is a shorthand notation for ∂ ∂t and ∂tl(t) yields the velocity (rate of change). After taking the derivative, we evaluate the velocity function l(t) ≡ ∂tl(t) at t0: l(t0) = ∂tl(t)∣ ∣t0 . you will travel in 10 minutes if you continue to move at your current velocity is this velocity times 10 minutes: ˙l(t0) × 10 minutes . Therefore, your second estimate is your current location plus the velocity you are traveling times 10 minutes l(t0 + 10 minutes) ≈ l(t0) + ˙l(t0) × 10 minutes . (B.1) 298 no-nonsense electrodynamics ◃ If you want to get an even better estimate you need to take into account that your velocity can possibly change. The rate of change of the velocity ¨l(t0) = ∂2 t l(t)∣ ∣t0 is what we call acceleration. So in this third step, you additionally take your current acceleration into account22 The factor 1 2 and that we need to square the 10 minutes follows since, to get from an acceleration to a location, we have to integrate twice: ∫ dt ∫ dt ¨x(t0) = ∫ dt ¨x(t0)t = 1 2 ¨x(t0)t2 where ¨x(t0) is the value of the acceleration at t = t0 (= a constant). l(t0 + 10 minutes) ≈ l(t0) + ˙l(t0) × 10 minutes + 1 2 ¨l(t0) × (10 minutes)2 . ◃ Our estimate will still not yield the perfect ﬁnal location since, additionally, we need to take into account that our acceleration could change during the 10 minutes. We could therefore additionally take the current rate of change of our acceleration into account. This game never ends and the only limiting factor is how pre- cisely we want to estimate our future location. For many real- world purposes, our ﬁrst order approximation (Eq. B.1) would already be perfectly sufﬁcient. The procedure described above is exactly the motivation behind the Taylor expansion. In general, we want to estimate the value of some function f (x) at some value of x by using our knowl- edge of the function’s value at some ﬁxed point a. The Taylor series then reads33 Here the superscript n denotes the n-th derivative. For example f (0) = f and f (1) is ∂x f . f (x) = ∞ ∑ n=0 f (n)(a)(x − a)n n! = f (0)(a)(x − a)0 0! + f (1)(a)(x − a)1 1! + f (2)(a)(x − a)2 2! + f (3)(a)(x − a)3 3! + . . . , (B.2) where f (a) is the value of the function at the point a we are ex- panding around. Moreover, x − a is analogous to the 10 minute timespan we considered above. If we want to know the location at x = 5:10 pm by using our knowledge at a = 5:00 pm, we get x − a = 5:10 pm − 5:00 pm = 10 minutes. Therefore, this equation is completely analogous to our estimate of the future location we considered previously. taylor expansion 299 To understand the Taylor expansion a bit better, it is helpful to look at concrete examples. We start with one of the simplest but most important examples: the exponential function. Putting f (x) = ex into Eq. B.2 yields ex = ∞ ∑ n=0 (e0)(n)(x − 0)n n! The crucial puzzle pieces that we need are therefore (ex)′ = ex and e0 = 1. Putting this into the general formula (Eq. B.2) yields ex = ∞ ∑ n=0 e0(x − 0)n n! = ∞ ∑ n=0 xn n! (B.3) This result can be used as a deﬁnition of ex. Next, let’s assume that the function we want to approximate is sin(x) and we want to expand it around x = 0. Putting f (x) = sin(x) into Eq. B.2 yields sin(x) = ∞ ∑ n=0 sin(n)(0)(x − 0)n n! The crucial information we therefore need is (sin(x))′ = cos(x), (cos(x))′ = − sin(x), cos(0) = 1 and sin(0) = 0. Because sin(0) = 0, every term with even n vanishes, which we can use if we split the sum. Observe that ∞ ∑ n=0 n = ∞ ∑ n=0(2n + 1) + ∞ ∑ n=0(2n)↷ 1 + 2 + 3 + 4 + 5 + 6 . . . = 1 + 3 + 5 + . . . + 2 + 4 + 6 + . . . (B.4) Therefore, splitting the sum into even and odd terms yields sin(x) = ∞ ∑ n=0 sin(2n+1)(0)(x − 0)2n+1 (2n + 1)! + ∞ ∑ n=0 sin(2n)(0)(x − 0)2n (2n)!↷sin(0) = 0 = ∞ ∑ n=0 sin(2n+1)(0)(x − 0)2n+1 (2n + 1)! . (B.5) 300 no-nonsense electrodynamics Moreover, every even derivative of sin(x) (i.e., sin(2n)) is again sin(x) or − sin(x). Therefore, the second term vanishes since sin(0) = 0. The remaining terms are odd derivatives of sin(x), which are all proportional to cos(x). We now use sin(x)(1) = cos(x) sin(x)(2) = cos′(x) = − sin(x) sin(x)(3) = − sin′(x) = − cos(x) sin(x)(4) = − cos′(x) = sin(x) sin(x)(5) = sin′(x) = cos(x) The general pattern is sin(2n+1)(x) = (−1)n cos(x), as you can check by putting some integer values for n into the formula4. 4 sin(1)(x) = sin(2·0+1)(x) = (−1)·0 cos(x) = cos(x), sin(3)(x) = sin(2·1+1)(x) = (−1)1 cos(x) = − cos(x) Thus, we can rewrite Eq. B.5 as sin(x) = ∞ ∑ n=0 sin(2n+1)(0)(x − 0)2n+1 (2n + 1)!↷ = ∞ ∑ n=0 (−1)n cos(0)(x − 0)2n+1 (2n + 1)!↷cos(0) = 1 = ∞ ∑ n=0 (−1)n(x)2n+1 (2n + 1)! . (B.6) This is the Taylor expansion of sin(x), which we can also use as a deﬁnition of the sine function. C Delta Distribution The delta distribution was invented as a tool that allows us to describe point sources. For example, in electrodynamics, an electron is a point source of the electromagnetic ﬁeld. Usually, in electrodynamics, we describe the locations of charges using a quantity called charge density ρ(⃗x). A charge density encodes the amount of charge per unit volume. Hence, if we integrate it over some volume, we get the total charge contained in the volume1 1 This is discussed in more detail in Section 2.2.total charge inside V = ∫ V ρ(⃗x)dV . (C.1) Now, how can we describe that there is only a single charge at one particular location? In other words: what’s the charge density for a single point charge? We write the charge density of a point charge as ρp(⃗x) = qδ(⃗x − ⃗x0) , (C.2) where q is the charge of the point charge, ⃗x0 its location and δ(⃗x − ⃗x0) the delta distribution. The deﬁning property of the delta distribution is that any integral over a volume V1 which contains the location of the point charge, yields exactly q: total charge inside V1 = ∫ V1 ρp(⃗x)dV = ∫ V1 qδ(⃗x − ⃗x0)dV = q (C.3) 302 no-nonsense electrodynamics but an integral over a different volume V2 which does not con- tain the point ⃗x0 yields exactly zero: total charge inside V2 = ∫ V2 ρp(⃗x)dV = ∫ V2 qδ(⃗x − ⃗x0)dV = 0 . (C.4) This means that the δ(⃗x − ⃗x0) yields zero for all ⃗x, except for ⃗x = ⃗x0. A good way to understand the delta distribution2 (also known 2 The delta distribution is not really a function in the strict mathematical sense and therefore a new word was invented: distribution. as the Dirac delta) is to recall a simpler but analogous math- ematical object: the Kronecker delta δij, which is deﬁned as follows: δij =   1 if i = j 0 if i ̸= j (C.5) In matrix form, the Kronecker delta is simply the unit matrix3. 3 For example, in two-dimensions 1(2×2) = (1 0 0 1 ) . (C.6) The Kronecker delta δij is useful because it allows us to pick one speciﬁc term of any sum. For example, let’s consider the sum 3 ∑ i=1 aibj = a1bj + a2bj + a3bj (C.7) and let’s say we want to extract only the second term. We can do this by multiplying the sum by the Kronecker delta δ2i: 3 ∑ i=1 δ2iaibj = δ21︸︷︷︸ =0 a1bj + δ22︸︷︷︸ =1 a2bj + δ23︸︷︷︸ =0 a3bj = a2bj. (C.8) In general, we have 3 ∑ i=1 δikaibj = akbj. (C.9) delta distribution 303 The delta distribution δ(x − y) is a generalization of this idea for integrals instead of sums.4 This means that we can use the 4 To unclutter the notation, we restrict ourselves to one-dimension.delta distribution to extract speciﬁc terms from any given inte- gral:5 5 Take note that this implies the statement made above for f (x) = q: ∫ dxqδ(x − y) = q . ∫ dx f (x)δ(x − y) = f (y). (C.10) In words, this means that the delta distribution allows us to extract exactly one term - the term x = y - from the inﬁnitely many terms which we sum over as indicated by the integral sign. For example, ∫ dx f (x)δ(x − 2) = f (2) . Now, one example where the Kronecker delta appears is ∂xi ∂xj = δij . (C.11) The derivative of ∂xx = 1, whereas ∂xy = 0 and ∂xz = 0. Completely analogously, the delta distribution appears as fol- lows: ∂ f (xi) ∂ f (xj) = δ(xi − xj). (C.12) The delta distribution is also often introduced by the following deﬁnition: δ(x − y) = { ∞ if x = y, 0 if x ̸= y , (C.13) which is somewhat analogous to the deﬁnition of the Kronecker delta in Eq. C.5. Moreover, when we use a constant function in Eq. C.10, for example, f (x) = 1, we get the following remark- able equation: ∫ dx1δ(x − y) = 1. (C.14) The thing is that Eq. C.10 tells us that if we have the delta dis- tribution δ(x − y) together with a function under an integral, the result is the value of the function at y = x. Here, we have a constant function and its value at y = x is simply 1. 304 no-nonsense electrodynamics In words, these properties mean that the delta distribution is an inﬁnitely thin (only non-zero at y = x) and also an inﬁnitely high function that yields exactly one if we integrate it all over space. Bibliography Y. Aharonov and D. Bohm. Signiﬁcance of electromagnetic potentials in the quantum theory. Phys. Rev., 115:485–491, 1959. doi: 10.1103/PhysRev.115.485. [,95(1959)]. H. J. Bernstein and A. V. Phillips. Fiber Bundles and Quantum Theory. Sci. Am., 245:94–109, 1981. doi: 10.1038/scientiﬁcamerican0781-122. Jennifer Coopersmith. The lazy universe : an introduction to the principle of least action. Oxford University Press, Oxford New York, NY, 2017. ISBN 9780198743040. Richard Feynman. The Feynman lectures on physics. Addison-Wesley, San Francisco, Calif. Harlow, 2011. ISBN 9780805390650. Daniel Fleisch. A student’s guide to Maxwell’s equations. Cam- bridge University Press, Cambridge, UK New York, 2008. ISBN 978-0521701471. Daniel Fleisch. A student’s guide to vectors and tensors. Cam- bridge University Press, Cambridge New York, 2012. ISBN 9781139031035. A. P. French. Special relativity. Norton, New York, 1968. ISBN 9780393097931. Juergen Freund. Special relativity for beginners : a textbook for undergraduates. World Scientiﬁc, Singapore, 2008. ISBN 9789812771599. 306 no-nonsense electrodynamics J. Gratus. A pictorial introduction to differential geometry, leading to Maxwell’s equations as three pictures. ArXiv e- prints, September 2017. David Grifﬁths. Introduction to electrodynamics. Pearson Educa- tion Limited, Harlow, 2014. ISBN 9781292021423. Kirill Ilinski. Physics of Finance. 1997. John Jackson. Classical electrodynamics. Wiley, New York, 1999. ISBN 9780471309321. Juan Maldacena. The symmetry and simplicity of the laws of physics and the Higgs boson. Eur. J. Phys., 37(1):015802, 2016. doi: 10.1088/0143-0807/37/1/015802. Charles Misner. Gravitation. W.H. Freeman and Company, New York, 1973. ISBN 9780716703440. Edward Purcell. Electricity and magnetism. Cambridge Univer- sity Press, Cambridge, 2013. ISBN 9781107014022. H. M. Schey. Div, grad, curl, and all that : an informal text on vector calculus. W.W. Norton & Company, New York, 2005. ISBN 9780393925166. Jakob Schwichtenberg. Physics from Symmetry. Springer, Cham, Switzerland, 2018a. ISBN 978-3319666303. Jakob Schwichtenberg. No-Nonsense Quantum Mechanics. No-Nonsense Books, Karlsruhe, Germany, 2018b. ISBN 978- 1719838719. Jakob Schwichtenberg. Physics from Finance. No-Nonsense Books, Karlsruhe, Germany, 2019. ISBN 978-1795882415. Gerard ’t Hooft. Gauge theories of the forces between elemen- tary particles. Sci. Am., 242N6:90–116, 1980. [,78(1980)]. Edwin Taylor. Spacetime physics : introduction to special relativity. W.H. Freeman, New York, 1992. ISBN 9780716723271. D. Wallace and Hilary Greaves. Empirical consequences of symmetries. British Journal for the Philosophy of Science, 65(1): 59–89, 2014. bibliography 307 K. Young. Foreign exchange market as a lattice gauge the- ory. American Journal of Physics, 67(10):862–868, 1999. doi: 10.1119/1.19139. URL https://doi.org/10.1119/1.19139. Andrew Zangwill. Modern electrodynamics. Cambridge Univer- sity Press, Cambridge, 2013. ISBN 9780521896979. Index Ampere-Maxwell law, 79 amplitude, 145 angular frequency, 144 spatial, 143 temporal, 144 Bianchi identity, 171, 287 charge, 32 charge density, 35 static, 95 circulation, 253 connection, 213 continuity equation, 54 Coulomb gauge, 178 Coulomb potential, 105 Coulomb’s law, 104 cross product, 238 curl, 270 current, 37 curvature, 215 dipole, 110 dipole moment, 113 divergence, 264 dot product, 235 Einstein summation convention, 13 electric charge, 32 electric current, 37 steady, 95 electric ﬁeld, 44 dipole, 110 general charge distribution, 111 point charge, 101 sphere, 106 electric permittivity, 63 Electrodynamics, 133 electromagnetic ﬁeld, 42 electromagnetic potential, 48 electromagnetic wave energy, 151 Electrostatics, 93, 95 Faraday’s law, 74 ﬁber bundle, 212 ﬁeld, 240 scalar, 241 vector, 241 ﬁeld energy, 151 ﬂux, 257 fundamental theorem calculus, 276 curls, 281 divergences, 278 gradients, 276 fundamental theorem of calculus, 231 gauge connection, 213 electrodynamics, 217 quantum mechanics, 216 toy model, 216 310 no-nonsense electrodynamics gauge group, 211 gauge symmetry, 175 electrodynamics, 206 quantum mechanics, 203 gauge theory, 181 dynamics, 190 Gauss’s law electric ﬁeld, 61 magnetic ﬁeld, 69 gradient, 262 Greek indices, 13 Green’s function, 121 group of gauge transformations, 211 Laplace equation, 121 line integral, 244 Lorentz force law, 21, 22, 57 application, 115, 126 magnetic charge density, 69 magnetic ﬁeld, 44 wire, 123 magnetic monopole, 69 magnetic multipole moments, 131 Magnetostatics, 93, 95 magnets, 72 Maxwell’s equations, 22 covariant, 20 homogeneous, 171 index notation, 290 inhomogeneous, 202 origin, 169 multipole expansion, 114 Newton’s second law, 115 path integral, 247 phase, 143 phase factor, 179 phase shift, 138 photons, 49 Poisson equation, 121 polarization, 145 longitudinal, 146 transversal, 146 redundancy, 188 scalar, 233 scalar ﬁeld, 42, 241 special relativity, 163 spin, 72 summation convention, 13 superposition, 93, 111 surface integral, 254 symmetry, 182 global, 183 local, 183 tangent vector, 250 Taylor expansion, 112, 297 tensor, 233 vector, 242 tensor ﬁeld, 42, 242 test charge, 45 transformation active, 186 transformations passive, 186 vector, 233 vector calculus, 227 vector ﬁeld, 42, 241 solenoidal, 72 vector identities, 284 wave equation, 24, 82 electric ﬁeld, 83 explicit solution, 134 general solution, 138 magnetic ﬁeld, 83 wave equations monochromatic solutions, 139 plane wave solutions, 139 standing wave solutions, 140 wave function, 179 wave number, 143 wave vector, 143 wavelength, 144","libVersion":"0.3.2","langs":""}
{"path":"Books and Papers/Data and Programming/Cyclic Neural Networks.pdf","text":"Cyclic Neural Network Liangwei Yang 1 , Hengrui Zhang 1 , Zihe Song 1 , Jiawei Zhang 2 , Jing Ma3 , Weizhi Zhang 1 and Philip S. Yu 1 1Department of Computer Science, University of Illinois Chicago, United States 2IFM Lab, University of California, Davis, United States 3University of Electronic Science and Technology of China, China {lyang84, hzhan55, zsong29, wzhan42, psyu}@uic.edu, jiwzhang@ucdavis.edu, jingma@uestc.edu.cn Abstract This paper answers a fundamental question in ar- tificial neural network (ANN) design: We do not need to build ANNs layer-by-layer sequentially to guarantee the Directed Acyclic Graph (DAG) prop- erty. Drawing inspiration from biological intelli- gence (BI), where neurons form a complex, graph- structured network, we introduce the groundbreak- ing Cyclic Neural Networks (Cyclic NNs). It em- ulates the flexible and dynamic graph nature of biological neural systems, allowing neuron con- nections in any graph-like structure, including cy- cles. This offers greater adaptability compared to the DAG structure of current ANNs. We further develop the Graph Over Multi-layer Perceptron, which is the first detailed model based on this new design paradigm. Experimental validation of the Cyclic NN’s advantages on widely tested datasets in most generalized cases, demonstrating its supe- riority over current BP training methods through the use of a forward-forward (FF) training algo- rithm. This research illustrates a totally new ANN design paradigm, which is a significant departure from current ANN designs, potentially leading to more biologically plausible AI systems. 1 Introduction Artificial intelligence (AI) has reshaped our daily lives and is expected to have a much greater impact in the foreseeable future. Lying behind the most profound AI applications [Sil- ver et al., 2017; OpenAI, 2023; Ramesh et al., 2021; Jumper et al., 2021], artificial neural networks (ANN) are designed specifically for different domains to fit the training data such as multi-layer perception (MLP) [Rumelhart et al., 1986], convolution neural network (CNN) [LeCun et al., 1995] and Transformer [Vaswani et al., 2017]. Regardless of the net- work structure, neural networks are stacked layer-by-layer to form deep ANNs for greater learning capacity. It has been a de facto practice until now that data is first fed into the in- put layer and then propagated through all the stacked layers to obtain the final representations at the output layer. In this paper, we seek to answer a fundamental question in ANNs: “Do we really need to stack neural networks layer-by-layer sequentially?”. (a) Connectome of BI (C. elegans) Computation Block Input Computation Block Output Computation Block (b) Computation structure of AI Figure 1: Neuron connection between Biology Neural Network and Artificial Neural Network To answer this question, let’s first examine the evidence from biological intelligence (BI). Neuroscientists have stud- ied the biological neurons for decades. The connectome of C. elegans is the most thoroughly studied biological neural system, and biologists depicted the most detailed connection between 302 biological neurons [White et al., 1986; Cook et al., 2019] as shown in Figure 1(a). Rather than being stacked layer-by-layer, all the neurons form a complicated connection graph, where each can connect to several other neurons within the system. We are even unable to determine which neuron serves as the input/output within the neural system to process information. The same findings have also been observed in the latter more complicated neural systems, such as the biol- ogy neural connectome of drosophila larva [Winding et al., 2023], zebrafish [Brooks et al., 2022] and the mouse [Sporns and Bullmore, 2014]. Observed biological intelligence ex- hibits graph-structured, flexible, and dynamic neural systems, which are apparently different from the current layer-by-layer AI models we build nowadays, as depicted in Figure 1(b). To dive further, the difference in the neural system struc- ture between BI and AI is actually caused by the learning rules. The Hebb’s Rule [Hebb, 2005], depicted as “Neurons that fire together wire together”, is recognized as the fun- damental learning way of biological neurons. The Spike- Timing-Dependent Plasticity (STDP) learning is then pro- posed to consider the relative spiking time of pre-synapse andarXiv:2402.03332v1 [cs.NE] 11 Jan 2024 post-synapse neurons further. Both learning rules of BI are localized, i.e., the learning occurs on each neuron within its local influence scope. The localized learning rules grant the flexibility of each neuron on its connections to other neurons, which leads to the complicated graph-structured BI system. Conversely, for AI systems, the backward propagation (BP) algorithm [Rumelhart et al., 1986] has dominated the training of ANNs. Data is fed into the ANNs from the input layer, for- ward propagates layer by layer to the last layer, calculates a global loss for the whole ANN based on the ground-truth, and then reversely backward propagates the error signals layer by layer to the input layer. In this procedure, ANNs are trained by a global loss function and the ANNs must guarantee the error from global loss can be backpropagated layer by layer. This requirement prevent current ANNs from forming cycles for the ease of gradient back-propagation. As a result, most current ANNs are nearly all DAG structured. To mitigate the biological implausible nature of the BP al- gorithm, the forward-forward (FF) algorithm [Hinton, 2022] is recently proposed to train ANNs. FF algorithm constructs good/bad samples and computes a loss function on each layer to differentiate between these samples. Similar to Hebb’s Rule and STDP learning, the FF algorithm is a localized learning method. This motivates us to conduct research on the Cyclic NN in this paper and to see whether the more com- plicated graph-structured ANNs can benefit AI systems. With cyclic structure within the neural network, Cyclic NN greatly increases the design space of ANNs beyond the DAG struc- ture, and also largely enhances information communication between layers. Cyclic NN distinguishes itself from the current layer-by- layer ANNs in many aspects. 1) More flexible neuron con- nections. In Cyclic NN, neurons can connect to each other to form any type of graph without any constraint. For ex- ample, the neurons can form a cycle connection as a loop. The flexible connection design makes Cyclic NN more like the biological neural system. 2) Localized training. Instead of current dominating global loss-guided BP-based training, Cyclic NN is based on localized training, i.e., each neuron is optimized with its own local loss function. There is no gradi- ent propagating between neurons. 3) Computation. The neu- ron within Cyclic NN is considered the computational neuron with greater computation capacity, such as a linear layer or a transformer block. It is inspired by the study of biological neuron [Beniaguev et al., 2021], which empirically proves the learning capacity of a biological neuron is similar to that of an MLP. We take this observation and propose the computational neuron in Cyclic NN with more complicated computation. In summary, our contributions can be summarized as follows: • Conceptually, we compare the difference between BI and AI to answer a fundamental question in ANN de- sign: We do not need to organize ANNs in a layer-by- layer manner. • Methodologically, we propose the ground-breaking Cyclic neural network, a novel ANN design paradigm that supports a much more flexible connection between neurons, which discards current layer-by-layer design constraints and is more biologically plausible. • We test the novel design paradigm on the most general- ized case and propose Graph Over Multi-layer Percep- tron, the first detailed model based on Cyclic NN. • Experimentally, we demonstrate the advantage of the proposed Cyclic neural network on widely tested datasets. At the same time, we are the first to beat the current dominating BP training using the FF training al- gorithm by the supported flexible network design pro- posed in this paper. 2 Cyclic Neural Network In this section, we first illustrate the novel Cyclic neural net- work framework in detail and then discuss its tremendous ad- vantages. One Cyclic NN model is one graph G = (V, E), where V = (N1, N2, ..., N|V|) is the computational neuron set and E = (S1, S2, ..., S|E|) is the synapse set denoting the connections among neurons. Similar to the BI system, Ni (∀i ∈ {1, 2, · · · , |V|}) is the neuron that tackles the detailed computation, and Sj (∀j ∈ {1, 2, · · · , |E|}) is the synapse that propagates information between computational neurons. In the Cyclic NN, computational neurons can be connected flexibly in any way, like the BI system. 2.1 Computational Neuron The computational neuron N acts as the computa- tion/optimization unit in Cyclic NN. Different from the cur- rent ANN in a neuron, it indicates a dinput to 1 mapping, we grant N with stronger computation power. It is motivated by the research that proves the computation power of a sin- gle biological neuron is similar to an MLP [Beniaguev et al., 2021]. In Cyclic NN, N is parameterized by a function f RdN in →RdN out N (h N in ) = h N out that maps from a d N in -dimensional representation h N in to a dout-dimensional representation h N out. fN can be parameterized by any computation block, such as MLP, CNN, LSTM, and Transformer. d N in is the input dimen- sion that is decided by the output of its pre-synapse neurons, and dN out is the output dimension of N . Similar to the bi- ological neurons, each computational neuron functions as a computation/optimization unit. Compared with current ANN neurons, the computational neuron is more independent dur- ing optimization. 2.2 Synapse In neuroscience, synapses stand as pivotal junctions, or- chestrating the complex symphony of neural communication. They serve as the critical interface between neurons, facili- tating the transmission of information through chemical and electrical signals. In a Cyclic NN, we model the synapse S as the edge between neurons defined in Section 2.1. Each synapse S1,2 = (N1 → N2) is a directional edge from com- putational neuron N1 to N2. It indicates the output of N1, h N1 out , will be propagated to N2 as part of its input h N2 in . Differ- ent from the current ANNs with DAG structure, the synapses between neurons can be organized as any connected graph structure, including the cyclic neural network. 2.3 Local Optimization Local optimization is the bedrock to support Cyclic NN, which is also a distinguishing point compared with current ANNs. For current ANNs, inputs are propagated through lay- ers to obtain the final representation. Designing a global loss function Lglobal based on the final representation and ground- truth labels is the de facto practice to train ANNs. Lglobal is optimized with BP algorithm to propagate the error signal layer-by-layer, which also prohibits the formation of compu- tation cycle. Conversely, Cyclic NN depends on local opti- mization, i.e., each computational neuron is optimized locally without gradients propagated from other computational neu- rons. For the computational neuron N , Cyclic NN has a local loss function LLocal to optimize its parameters. The local op- timization principle is similar to the BI system, where each neuron has the ability to learn from its local context. 2.4 Inference During the inference phase, we design a readout layer to gather the output of all the neurons within the model as hreadout = freadout([h N1 out , h N2 out , ..., h NV out ]). hreadout collects all the encoded information and acts as the final representation for the inference task. For example, hreadout can be fed into a classifier for the classification task, where the classifier is trained together with the computational neurons. 2.5 Advantages of Cyclic NN Cyclic NN has its unique advantages over current ANNs, which can be summarized as follows: • Flexibility: Cyclic NN enables more flexible neuron connections. Neurons can be connected to form loops without any constraint. • Extensibility: As computational neurons are more inde- pendent of other neurons, Cyclic NN can be easily ex- tended. We can insert a new computational neuron into the current trained Cyclic NN without much impact. • Parallelism: Due to the local optimization, each compu- tational neuron can be optimized immediately when the data comes without the need to wait for the gradient to propagate back. It supports greater parallelism because all computational neurons can be optimized at the same time without waiting for a gradient. • Privacy: In privacy-sensitive scenarios such as federated learning [He et al., 2021], the gradient leakage can lead to privacy issue. Without gradient propagated through layers, Cyclic NN will not have privacy leakage issues. • Biological Similarity: Compared with current ANNs, Cyclic NN is more similar to biological intelligence. The design of computational neurons, synapses, and lo- cal optimization are all grounded in biological observa- tion. 3 Graph Over Multi-layer Perceptron In this section, we propose the first Cyclic NN under the most generalized case, Graph Over Multi-Layer Perceptron (GOMLP), to show the design principle of Cyclic NN. As shown in Figure 2(c) and (d), GOMLP is designed by build- ing a graph structure over the multi-layer perception to solve the classification task. GOMLP includes input construction, computation graph structure, synapse propagation, readout layer, optimization, and inference. 3.1 Input Construction For the classification task, each sample is symbolized as the feature-label pair (hi, yi), where hi is the representation of sample i and yi is the corresponding label. To enable the local optimization illustrated in Section 3.4, a fusion function is used to construct the input as: hpos = ffusion(h, ytrue) = h||ytrue, hneg = ffusion(h, yfalse) = h||yfalse, (1) hneu = ffusion(h, yneutral) = h||yneu, hpos, hneg, and hneu are the constructed input for local op- timization of different parts. ffusion is a function to fuse in- formation between feature and label, which is defined as a concat function (||) in this paper. ytrue is the one-hot vector of ground-true label, yfalse is the one-hot vector of a randomly sampled false label. For yneu, we place an 1 Class Number on all the dimensions of one-hot vector to indicate hneutral is neutral to all classes. To be noted that the ffusion can be designed as any proper function to fuse information of the input feature and the label. In our study, we design it as a simple concat function same as [Hinton, 2022]. 3.2 Computation Graph The computation graph G contains the computational neurons V and the synapses E. Each computational neuron N ∈ V is a local module for calculation and optimization, while each synapse S defines how the information propagates between computational neurons. G can be defined as a graph genera- tor: G = Generator(|V|, |E|). (2) The above-generated graph G denotes a general graph struc- ture. Meanwhile, to justify the effectiveness of the proposed Cyclic NN, we test multiple graph generators to build the graph. • Chain graph. Neurons are organized layer-by-layer as shown in Figure 2(b). In this case, GOMLP degrades to Hinton’s method [Hinton, 2022]. • Cycle graph. Neurons form a cycle by connecting the neurons head-to-tail as shown in Figure 2(c). • Complete graph. Each neuron connects to all the other neurons, as shown in Figure 2(d). • Watts-Strogatz (WS) graph [Watts and Strogatz, 1998]. It is a random graph generation model that produces graphs with small-world properties, including short av- erage path lengths and high clustering. • Barab´asi–Albert (BA) graph [Albert and Barab´asi, 2002]. It generates random scale-free networks using a preferential attachment mechanism.InputClassifier (a) Layer-by-layer MLP (BP: global loss) InputClassifier (b) Layer-by-layer MLP(FF: local loss) OutputOutputOutputOutput ReadoutInputClassifier (c) Graph Over MLP (Cycle Graph) Output ReadoutOutputOutputOutputInputClassifier (d) Graph Over MLP (Complete Graph) Output ReadoutOutputOutputOutput Cyclic Neural NetworksDirected Acyclic Neural Networks Forward Propagation Backward Propagation Loss Function Figure 2: Comparison between different types of MLP structure. Neuron Update In GOMLP, each neuron is parameterized by a linear layer. At each propagation, neuron N is updated by (we omit the notation of N in equation for simplicity): hout = σ(W˜hin), (3) where σ is the Relu activation function [Nair and Hinton, 2010], W ∈ RdN out×dN in is N ’s parameter. d N out is N ’s output dimension, which is a pre-defined dimension size, and d N in is N ’s input dimension, which is defined by the output of N ’s pre-synapse neurons. ˜hin is the normalized input as: ˜hin = hin ∥hin∥2 , (4) where hin is computational neuron N ’s input. Synapse Propagation Each synapse S = (Ni → Nj) is a directional edge from computational neuron Ni to Nj, which indicates Ni is the pre-synapse neuron of Nj and h Ni out (the output of Ni) will be propagated to Nj. Assume for neuron N , we obtain a set of pre-synapse neurons (N1, N2, ..., Nn) based on the topology of G. Then, in each propagation, N receives the output of all its pre-synapse neurons along the synapses, and fuse the information to form its input by a concatenation function: hin = h||h N1 out ||h N2 out ||, ..., ||h Nn out , (5) where || is the concat function, h is the input representation constructed in Section 3.1. Then we can obtain hin,pos, hin,neg, hin,neu by providing hpos, hneg, hneu separately. As we relax the layer-by-layer restriction, the differentiation between the input/hidden/output layers is also relaxed. In this case, we directly put the input h to all computational neurons. Thus, the input dimension size of N , dN in = dh + dN1 out + d N2 out + ... + d Nn out . 3.3 Readout Layer The readout layer is designed to collect information from all computational neurons and make the collective decision on the studied classification task. The input of the readout layer is the concat function of all computational neurons: h readout in = freadout(h N∗ out ) = || |V| i=1(h Ni out ), (6) where || is the concat function. Then, the readout layer casts all the input representations to the output dimension: ˆy = Softmax(Wreadouth readout in ), (7) where Wreadout ∈ RClass Number×d(hreadout in ) is the parameter of the readout layer and ˆy is the prediction vector on classes. 3.4 Local Optimization GOMLP comprises two parts that hold parameters: the com- putational neurons, and the readout layer. All parameters are optimized locally without gradient propagates between each parts. The computational neuron and readout layer are op- timized differently with different inputs. The optimization is shown in Algorithm 1, which contains the optimization of each computational neuron and the readout layer. The hyper-parameter T indicates the number of synapse propa- gation over G. Computational Neuron Optimization computational neurons are optimized to differentiate the pos- itive examples from negative ones. For computational neu- ron N , its optimization involves hin,pos and hin,neg. After the computational neuron update (Equation 3), we can get hout,pos and hout,neg, respectively. Then, following [Hinton, 2022], a goodness function is used to calculate the goodness score on the representation: p(h) = σ(∑ i h 2 i − θ ∗ d(h)), (8) where p(h) is the goodness score to judge h, d(h) is the di- mension size of h, σ is the relu activation function and θ is the threshold hyper-parameter. The binary cross-entropy loss is used to optimize each computational neuron: LN = − 1 |D| ∑ D (log(p(hout,pos)) − log(p(hout,neg))), (9) where D is the dataset. The optimization of computational neurons aims to increase the neuron’s output for positive sam- ples while decreasing the neurons’ output for negative sam- ples. It enables each computational neuron its own ability to differentiate positive examples from negative ones. Readout Layer Optimization The readout layer is designed to accomplish the classifica- tion task for GOMLP. It reads the information from all com- putational neurons, and makes the decision over classes. To relieve the label leakage issue, the readout layer is only op- timized with hneutral. We use a multi-class cross-entropy loss to optimize the readout layer: LReadout(y, ˆy) = − 1 |D| ∑ |D| C∑ c=1 yc log( ˆyc), (10) where C is the number of classes, y is the one-hot vector of ground-truth label and ˆy is the prediction from Equation 7. Though the optimization of the computational neuron and readout layer are localized and different, these two parts com- plement each other. Computational neurons aim to extract the hidden representations of each sample, and the readout layer aims to make the final decision based on the extracted infor- mation. During the inference time, we pair each test sample with the neutral label to construct hneu. It then propagates through the GOMLP to obtain its representation on each computa- tional neuron. Finally, we predict its class with the largest logit from the output of the readout layer. 4 Experiments In this section, we conduct experiments on three datasets to test the performance and characteristics of GOMLP. 4.1 Experimental setup Datasets We conduct experiments on three widely studied datasets from both computer vision and natural language processing domains. Data statistics are shown in Table 1. For each dataset, the split of training and test follows the original set- ting. We further extract 20% samples from the training data as validation sets to tune hyper-parameters. Algorithm 1 Graph Over MLP Input: dataset D Parameter: G = (V, E), Wreadout Output: Optimized G, Wreadout 1: while Not Converged do 2: Obtain inputs by Equation 1 from D 3: Let t = 0 4: while t < T do 5: for N ∈ V do 6: Synapse Propagate by Eq. 5 7: computational neuron Update by Eq. 3 8: Optimize N by Eq. 9 9: end for 10: t = t + 1 11: end while 12: Calculate the output of Readout layer by Eq. ?? 13: Optimize Wreadout by Eq. 10 14: end while 15: return G = (V, E), Wreadout • MNIST [LeCun et al., 1989]. It contains handwritten digits from 0-9, which is the most accessible and used datasets in the field of machine learning. • NewsGroup [Lang, 1995]. It is a collection of approxi- mately 20,000 newsgroup documents, partitioned across 20 different newsgroups. This dataset is widely used for experiments in text applications of machine learning techniques, such as text classification and text clustering. • IMDB [Maas et al., 2011]. It is a movie review dataset crawled from IMDB. It is the most widely studied dataset for binary sentiment classification. For MNIST, we directly use its flattened pixel values as the input of all methods and replace the first 10 pixels with labels as the fusion function, which is the same as [Hinton, 2022] and leads to an input dimension of 28 ∗ 28 = 784. For NLP datasets (NewsGroup, IMDB), we use BERT [Devlin et al., 2018] to encode the sentences into a fixed-length tensor (768) as the input. The fusion function is the concat function, which leads to an input dimension of 768 + 20 = 788 for News- Group and 768 + 2 = 770 for IMDB dataset, respectively. Table 1: Dataset Statistics Dataset MNIST NewsGroup IMDB Training Samples 50,000 9,314 20,000 Validation Samples 10,000 2,000 5,000 Test Samples 10,000 7,532 25,000 Dimensions 784 788 770 Classes 10 20 2 Baselines In this paper, we aim to reveal the advantages of graph- structured multi-layer perceptron. We compared GOMLP with a variant of different methods, which can be differen- tiated by two attributes (Training and Graph). Training in- dicates the training method, where BP indicates Backward Propagation [Rumelhart et al., 1986] and FF indicates the Forward-forward algorithm [Hinton, 2022]. The graph indi- cates the graph structure of computational neurons. To make a fair comparison, we keep 4 layers of MLP for all methods. The special cases are further illustrated as: • BP-Chain*: Layer-by-layer neural networks trained with BP as depicted in Figure 2(a). It is the current de- fault way of building and training ANNs. • FF-Chain: Layer-by-layer neural networks trained with FF as depicted in Figure 2(b). This setting is the same as [Hinton, 2022]. • BP-Chain: A modified version of BP-Chain*, where we use the structure of Figure 2(b) with readout function and trained with BP. It adds direct local supervision on each layer. FF-Cycle, FF-WSGraph, FF-BAGraph, and FF-Complete are different versions of GOMLP, where the training is FF and only the graph generator defined in Eq. 2 differs. Experimental Setting We use Adam [Kingma and Ba, 2014] optimizer to train the model until convergence for all experiments. The learning rate and weight decay are tuned within (0.1,0.01,0.001) and (0.0, 1e-2, 1e-4, 1e-6, 1e-8), respectively. The early stop tech- nique is applied to avoid overfitting, where we stop training if there is no improvement on the validation set for continuous 10 epochs. For each setting, we report the mean and variance on 20 experiments with different random seed. 4.2 Overall Comparison Table 2: Error rate (%) ↓ on different datasets. Train Graph MNIST NewsGroup IMDB BP Chain* 1.77±0.16 42.11±0.92 17.16±0.19 FF Chain 1.83±0.2 43.88±0.38 18.75±0.92 BP Chain 1.74±0.11 38.85±0.42 17.27±0.13 FF Cycle 1.80±0.14 43.54±0.41 18.97±0.49 FF WSGraph 1.70±0.17 38.38±0.13 17.93±0.38 FF BAGraph 1.64±0.08 38.41±0.14 18.20±0.67 FF Complete 1.54±0.05 38.366±0.06 17.58±0.20 The overall experiment result is shown in Table 2. We show the error rate of different methods on different datasets (the lower, the better). Best performance is marked bold. From the table, we can have several interesting and exciting findings: • FF-Complete achieves the best performance on MNIST and NewsGroup datasets, and it also achieves compara- ble results to the best one on the IMDB dataset. It is the first FF-trained model that outcompetes the BP-trained model. It is an exciting observation of the effectiveness of the FF algorithm compared with the BP algorithm. • FF-Chain performs worse than BP-Chain* on all datasets. This observation is on par with [Hinton, 2022], where the FF lags behind the BP training algorithm 1 2 3 4 5 6 T 1.5 1.6 1.7 1.8 1.9Error Rate (%) MNIST 0 1 2 3 4 5 5 10 15 20 25Error Rate (%) MNIST 1 2 3 4 5 6 T 38.2 38.3 38.4 38.5 38.6 38.7 38.8 38.9 39.0Error Rate (%) NewsGroup 0 1 2 3 4 5 42.5 45.0 47.5 50.0 52.5 55.0 57.5 60.0Error Rate (%) NewsGroup 1 2 3 4 5 6 T 17.6 17.8 18.0 18.2 18.4 18.6 18.8Error Rate (%) IMDB 0 1 2 3 4 5 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0Error Rate (%) IMDB Figure 3: Parameter sensitivity of T and θ when they both follow layer-by-layer organization as a chain graph. However, we can outcompete BP-Chain* when we organize the layers as a graph structure. This finding inevitably reveals the advantages of GOMLP by organizing multi-layer perceptron as a flexible graph structure. • FF-Cycle achieves similar performance with FF-Chain on three datasets. It is reasonable because there is only one edge difference between these two methods. When we build more complex graphs (WSGraph, BAGraph, Complete Graph), we can observe much better perfor- mance immediately. It shows the benefits of enriching the communication between layers by the GOMLP. • BP-Chain is better than BP-Chain* in most cases. Com- pared with BP-Chain*, BP-Chain further adds layer- wise optimization directly from the final loss function. It indicates the advantageous layer-wise optimization, which provides new guidelines when designing layer- by-layer neural networks. In summary, the experiment results answer that we do not need to stack neural networks layer-by-layer sequentially, and we can organize the neural networks as a flexible, complex graph structure like the brain. More excitingly, we can out- perform the current de facto layer-by-layer neural network design paradigm with the cyclic neural network, and provide a totally new way of building ANNs. 4.3 Hyper-paramter Sensitiviey In this section, we test the impact of hyper-parameters (T and θ) designed within GOMLP. Experiment results on three datasets are shown in Figure 3. T controls the number of propagation between computa- tional neurons. A larger T indicates more times the infor- mation is propagated between computational neurons. We can observe an error rate trend that first decreases and then increases on all three datasets. It indicates that the computa- tional neurons need a suitable propagation number. When T is small, computational neurons can not draw sufficient lessons from each other. When T is large, computational neu- rons are over-propagated, which leads to the over-smoothing problem. Different from over-smoothing in graph neural net- work [Chen et al., 2020] where the graph is data and the over- smoothed node representation, in GOMLP, the graph is the model, and the over-smoothing occurs among computational neurons. θ controls the goodness threshold of each computational neuron. We can observe a sharp error rate decrease when θ increases from 0 to 1, and then it gets stable with larger θ. It indicates the existence of the goodness threshold matters more than the threshold value. When θ = 0, there is little room to optimize the computational neuron towards the neg- ative sample, which can lead to the training collapse as the computational neuron can not differentiate the negative sam- ple. When θ is larger, there is more room to optimize the goodness score toward a negative sample, as all the goodness scores under the threshold can represent a negative sample. θ is a critical component of GOMLP. 4.4 Ablation Study Table 3: Error rate (%) ↓ of Ablation study. Model MNIST NewsGroup IMDB FF-Complete 1.54 38.36 18.20 -LN 2.34 47.61 22.94 -LReadout 95.58 95.55 44.36 This section studies the impact of different optimization modules within GOMLP, including the computational neuron optimization LN and readout layer optimization LReadout. We conduct experiments on the FF-Complete structure, and the results are summarized in Table 3. We can have the follow- ing observations: 1) The error rate increases when removing any optimization module, indicating the usefulness of each component. 2) GOMLP falls to a very large error rate (nearly random guess) when removing LReadout. It is reasonable as we depend on the readout layer to complete the final clas- sification task. Without optimization on the readout layer, GOMLP falls into random guess even with optimized compu- tational neuron’s input. 3) The error rate increases by remov- ing LN . It shows the computational neuron’s optimization can provide a more informative goodness score for the read- out layer to complete the classification task. LN and LReadout complement each other within GOMLP, and they collectively make the best performance. 5 Related Work 5.1 Localized Learning Algorithm Although end-to-end backpropagation has become the dom- inant training algorithm for deep neural networks, studies have revealed notable limitations in such end-to-end train- ing with global objectives [Bengio et al., 2015; Crick, 1989]. Numerous works have proposed alternative training meth- ods to make ANNs more biologically plausible. Inspired by Hebbian theory [Hebb, 2005], Hebbian Learning [Gerstner and Kistler, 2002] updates weights locally between two ac- tive, connected neurons, ensuring long-term stability so pre- viously learned information is not lost. Addressing the weight transportation problem of backpropagation when applied to biological neurons, Feedback alignment methods [Lillicrap et al., 2016; Nøkland, 2016] replace downstream synaptic weights with random weights, eliminating the need for feed- back weights in neurons. Unlike backpropagation, which re- quires two types of computations in forward and backward passes, Equilibrium Propagation [Scellier and Bengio, 2017] performs both inference and weight updates using only one type of computation. The approach in [Nøkland and Eidnes, 2019] reduces memory consumption and increases training parallelism by adopting subnetworks and layer-wise training. Additionally, [Hinton, 2022] introduces a simple yet efficient local objective function that measures the goodness of posi- tive and negative data, along with a local optimization method to train ANNs. Localized learning algorithm is the bedrock to support the cyclic structure within neural network. In this paper, we are the first to beat global BP training with pure localized learn- ing algorithm by the help of cyclic network structure pro- posed in this paper. 5.2 Graph Generator With a number of computational neurons as nodes, there are plenty of algorithms to generate computation graphs. The simplest model to generate a random graph is the Erd˝os–R´enyi model [ERDdS and R&wi, 1959], which iter- ates all possible edges and adds them with a probability of p ∈ [0, 1]. [Batagelj and Brandes, 2005] propose a more ef- ficient algorithm to create a graph with a linear computation cost in terms of running time and space requirement. Ran- dom Geometric graphs [Penrose, 2003] place nodes on geo- metric planes and form edges within a fixed distance, resem- bling real-world social networks. The Spectrum Graph Forge [Baldesi et al., 2018] creates a random graph that has the same eigenstructure as a given adjacency matrix. The Wax- man model [Waxman, 1988] generates large-scale graphs that have properties similar to those of communication networks. K-regular graphs [STEGER and WORMALD, 1999] are sim- ple graphs in which every node has the same degree. The Watts-Strogatz model [Watts and Strogatz, 1998] produces small-world graphs that guarantee high clustering coefficients and low averages of shortest path distances. The Barab´asi- Albert model [Albert and Barab´asi, 2002] generates graphs by continuously adding nodes that attach to existing nodes with high degrees, known as scale-free graphs. Due to the constraint of BP training algorithm, current ANNs only fall into a very specialized case considering the graph structure (Directed Acyclic Graph). In this paper, we propose a ground-breaking cyclic NN design paradigm to support all kinds of graph structure, which is more flexible and with higher biology similarity. 6 Conclusion In summary, this research introduces Cyclic Neural Net- works (Cyclic NNs), a novel ANN architecture inspired by the complex, graph-like neural networks in biological intelli- gence. This groundbreaking design diverges from traditional directed acyclic ANN structures. Our findings, demonstrated through the Graph Over Multi-layer Perceptron model and validated on various datasets, show enhanced performance over conventional backpropagation methods. This significant development paves the way for more efficient and biologi- cally realistic AI systems, representing a major shift in ANN design. References R´eka Albert and Albert-L´aszl´o Barab´asi. Statistical mechan- ics of complex networks. Reviews of modern physics, 74(1):47, 2002. Luca Baldesi, Carter T. Butts, and Athina Markopoulou. Spectral graph forge: Graph generation targeting modu- larity. In IEEE INFOCOM 2018 - IEEE Conference on Computer Communications, page 1727–1735. IEEE Press, 2018. Vladimir Batagelj and Ulrik Brandes. Efficient generation of large random networks. Physical review. E, Statisti- cal, nonlinear, and soft matter physics, 71 3 Pt 2A:036113, 2005. Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and Zhouhan Lin. Towards biologically plausible deep learning. arXiv preprint arXiv:1502.04156, 2015. David Beniaguev, Idan Segev, and Michael London. Single cortical neurons as deep artificial neural networks. Neuron, 109(17):2727–2739, 2021. Paul Brooks, Andrew Champion, and Marta Costa. Map- ping of the zebrafish brain takes shape. Nature Methods, 19(11):1345–1346, 2022. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing prob- lem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelli- gence, volume 34, pages 3438–3445, 2020. Steven J Cook, Travis A Jarrell, Christopher A Brittin, Yi Wang, Adam E Bloniarz, Maksim A Yakovlev, Ken CQ Nguyen, Leo T-H Tang, Emily A Bayer, Janet S Duerr, et al. Whole-animal connectomes of both caenorhabditis elegans sexes. Nature, 571(7763):63–71, 2019. Francis Crick. The recent excitement about neural networks. Nature, 337:129–132, 1989. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. P ERDdS and A R&wi. On random graphs i. Publ. math. debrecen, 6(290-297):18, 1959. Wulfram Gerstner and Werner M. Kistler. Mathematical for- mulations of hebbian learning. Biological Cybernetics, 87:404–415, 2002. Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao Sun, Lifang He, Liangwei Yang, Philip S Yu, Yu Rong, et al. Fedgraphnn: A federated learning system and benchmark for graph neural networks. arXiv preprint arXiv:2104.07145, 2021. Donald Olding Hebb. The organization of behavior: A neu- ropsychological theory. Psychology press, 2005. Geoffrey Hinton. The forward-forward algorithm: Some pre- liminary investigations. arXiv preprint arXiv:2212.13345, 2022. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasu- vunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with al- phafold. Nature, 596(7873):583–589, 2021. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ken Lang. Newsweeder: Learning to filter netnews. In Ma- chine learning proceedings 1995, pages 331–339. Elsevier, 1995. Yann LeCun, Bernhard Boser, John Denker, Donnie Hen- derson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back- propagation network. Advances in neural information pro- cessing systems, 2, 1989. Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995. Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights sup- port error backpropagation for deep learning. Nature com- munications, 7(1):13276, 2016. Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vec- tors for sentiment analysis. In Proceedings of the 49th an- nual meeting of the association for computational linguis- tics: Human language technologies, pages 142–150, 2011. Vinod Nair and Geoffrey E Hinton. Rectified linear units im- prove restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML- 10), pages 807–814, 2010. Arild Nøkland and Lars Hiller Eidnes. Training neural net- works with local error signals. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, vol- ume 97 of Proceedings of Machine Learning Research, pages 4839–4850. PMLR, 09–15 Jun 2019. Arild Nøkland. Direct feedback alignment provides learning in deep neural networks. Advances in neural information processing systems, 29, 2016. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. Mathew Penrose. Random geometric graphs, volume 5. OUP Oxford, 2003. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Inter- national Conference on Machine Learning, pages 8821– 8831. PMLR, 2021. David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986. Benjamin Scellier and Yoshua Bengio. Equilibrium prop- agation: Bridging the gap between energy-based models and backpropagation. Frontiers in computational neuro- science, 11:24, 2017. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mas- tering the game of go without human knowledge. nature, 550(7676):354–359, 2017. Olaf Sporns and Edward T Bullmore. From connections to function: the mouse brain connectome atlas. Cell, 157(4):773–775, 2014. A. STEGER and N. C. WORMALD. Generating random reg- ular graphs quickly. Combinatorics, Probability and Com- puting, 8(4):377–396, 1999. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Duncan J Watts and Steven H Strogatz. Collective dynam- ics of ‘small-world’networks. nature, 393(6684):440–442, 1998. B.M. Waxman. Routing of multipoint connections. IEEE Journal on Selected Areas in Communications, 6(9):1617– 1622, 1988. John G White, Eileen Southgate, J Nichol Thomson, Sydney Brenner, et al. The structure of the nervous system of the nematode caenorhabditis elegans. Philos Trans R Soc Lond B Biol Sci, 314(1165):1–340, 1986. Michael Winding, Benjamin D Pedigo, Christopher L Barnes, Heather G Patsolic, Youngser Park, Tom Kazimiers, Akira Fushiki, Ingrid V Andrade, Avinash Khandelwal, Javier Valdes-Aleman, et al. The connectome of an insect brain. Science, 379(6636):eadd9330, 2023.","libVersion":"0.3.2","langs":""}
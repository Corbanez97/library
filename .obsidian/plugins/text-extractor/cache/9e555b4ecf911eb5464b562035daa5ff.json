{"path":"Books and Papers/Masters Points of Interest/Machine_Learning_Phases_of_Matter.pdf","text":"LETTERS PUBLISHED ONLINE: 13 FEBRUARY 2017 | DOI: 10.1038/NPHYS4035 Machine learning phases of matter Juan Carrasquilla1* and Roger G. Melko1,2 Condensed-matter physics is the study of the collective behaviour of inﬁnitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits 1. This complexity is reﬂected in the size of the state space, which grows expo- nentially with the number of particles, reminiscent of the ‘curse of dimensionality’ commonly encountered in machine learning 2. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recog- nize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks 3, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern soft- ware libraries 4,5, neural networks can be trained to detect multi- ple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state conﬁgura- tions sampled with Monte Carlo 6,7. Conventionally, the study of phases in condensed-matter systems is performed with the help of tools that have been carefully designed to elucidate the underlying physical structures of various states. Among the most powerful are Monte Carlo simulations, which consist of two steps: a stochastic importance sampling over state space, and the evaluation of estimators for physical quantities calculated from these samples 7. These estimators are constructed on the basis of a variety of physical motivations; for example, the availability of an experimental measure such as a specific heat; or, the encoding of a theoretical device such as an order parameter 1. However, some technologically important states of matter, such as topologically ordered states 1,8, can not be straightforwardly identified with standard estimators 9,10. Machine learning, already explored as a tool in condensed- matter research 11\u001516, provides a complementary paradigm to the above approach. The ability of modern machine learning techniques to classify, identify, or interpret massive data sets such as images foreshadows their suitability to provide physicists with similar analyses on the exponentially large data sets embodied in the state space of condensed-matter systems. We first demonstrate this on the prototypical example of the square-lattice ferromagnetic Ising model, H D \u0000J P hiji ˙ z i ˙ z j . We set J D 1 and the Ising variables ˙ z i D \u00061 so that for N lattice sites, the state space is of size 2N . Standard Monte Carlo techniques provide samples of configurations for any temperature T , weighted by the Boltzmann distribution. The existence of a well-understood phase transition at temperature Tc (ref. 17), between a high-temperature paramagnetic phase and a low-temperature ferromagnetic phase, allows us the opportunity to attempt to classify the two di˙erent types of configurations without the use of Monte Carlo estimators. Instead, we construct a fully connected feed-forward neural network, implemented with TensorFlow 4, to perform supervised learning directly on the thermalized and uncorrelated raw configurations sampled by a Monte Carlo simulation. As illustrated in Fig. 1a, the neural network is composed of an input layer with values determined by the spin configurations, a 100-unit hidden layer of sigmoid neurons, and an analogous output layer. When trained on a broad range of data at temperatures above and below Tc, the neural network is able to correctly classify data in a test set. Finite-size scaling is capable of systematically narrowing in on the thermodynamic value of Tc in a way analogous to measurements of the magnetization: a data collapse of the output layer (Fig. 1b) leads to an estimate of the critical exponent \u0017 ' 1.0 \u0006 0.2, while a size scaling of the crossing temperature T \u0003=J estimates Tc=J ' 2.266 \u0006 0.002 (Fig. 1c). One can understand the training of the network through a simple toy model involving a hidden layer of only three analytically `trained' perceptrons, representing the possible combinations of high- and low-temperature magnetic states exclusively on the basis of their magnetization. Similarly, our 100-unit neural network relies on the magnetization of the configurations in the classification task. Details about the toy model, the 100-unit neural network, as well as a low- dimensional visualization of the training data, which may be used as a preprocessing step to generate the labels if they are not available a priori, are discussed in the Supplementary Figs 1, 2, and 4. We note that in a recent development, a closely related neural-network- based approach allows for the determination of critical points using a confusion scheme 18, which works even in the absence of labels. Finally, we mention that similar success rates occur if the model is modified to have antiferromagnetic couplings, H D J P hiji ˙ z i ˙ z j , illustrating that the neural network is not only useful in identifying a global spin polarization, but an order parameter with other ordering wavevectors (here q D .ˇ, ˇ/). The power of neural networks lies in their ability to generalize to tasks beyond their original design. For example, what if one was presented with a data set of configurations from an Ising Hamiltonian where the lattice structure (and therefore its Tc) is not known? We illustrate this scenario by taking our above feed-forward neural network, already trained on configurations for the square-lattice ferromagnetic Ising model, and provide it a test set produced by Monte Carlo simulations of the triangular lattice ferromagnetic Ising Hamiltonian. In Fig. 1d\u0015f we present the averaged output layer versus T , the corresponding data collapse, and a size scaling of T \u0003=J , allowing us to successfully estimate the critical parameters Tc=J D 3.65 \u0006 0.01 and \u0017 ' 1.0 \u0006 0.3 consistent with the exact values 19. We now turn to the application of such techniques to problems of greater interest in modern condensed matter, such as disordered or topological phases, where no conventional order parameter exists. Coulomb phases, for example, are states of frustrated lattice models where local energetic constraints imposed by the Hamiltonian lead to extensively degenerate classical ground states. We consider a two-dimensional square-ice Hamiltonian given by H D J P v Q 2 v, where the charge Qv D P i2v ˙ z i is the sum over the Ising variables located in the lattice bonds incident on vertex v, as shown in Fig. 2. In a conventional approach, the ground states 1Perimeter Institute for Theoretical Physics, Waterloo, Ontario N2L 2Y5, Canada. 2Department of Physics and Astronomy, University of Waterloo, Ontario N2L 3G1, Canada. *e-mail: jcarrasquilla@perimeterinstitute.ca NATURE PHYSICS | VOL 13 | MAY 2017 | www.nature.com/naturephysics 431 © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. LETTERS NATURE PHYSICS DOI: 10.1038/NPHYS4035 1.0 1.5 2.0 2.5 3.0 3.5 T/J 0.0 0.2 0.4 0.6 0.8 1.0Outputlayer a L = 10 L = 20 L = 30 L = 40 L = 60 L = 10 L = 20 L = 30 L = 40 L = 60 2.0 2.5 3.0 3.5 4.0 4.5 5.0 0.0 0.2 0.4 0.6 0.8 1.0Outputlayer 0.0 0.2 0.4 0.6 0.8 1.0Outputlayer 0.0 0.2 0.4 0.6 0.8 1.0Outputlayer d 0.00 0.05 0.10 1/L 2.26 2.28 2.30 2.32c 3.64 3.66 3.68 3.70T∗/JT∗/J f Input Hidden Output −10 −5 0 5 10 tL1/ν T/J 0.00 0.05 0.10 1/L −10 −5 0 5 10 tL1/ν b e Figure 1 | Machine learning the ferromagnetic Ising model. a, The output layer averaged over a test set as a function of T=J for the square-lattice ferromagnetic Ising model. The inset in a displays a schematic of the fully connected neural network used in our simulations. b, Plot showing data collapse of the average output layer as a function of tL1=\u0017 , where t D (T \u0000 Tc)=J is the reduced temperature. Linear system sizes L D 10, 20, 30, 40 and 60 are represented by crosses, up triangles, circles, diamonds and squares, respectively. c, Plot of the ﬁnite-size scaling of the crossing temperature T\u0003=J (down triangles). d–f, Analogous data to a–b, but for the triangular Ising ferromagnet using the neural network trained for the square-lattice model. The vertical orange lines signal the critical temperatures of the models in the thermodynamic limit, Tc=J D 2= ln (1 C p2) for the square lattice17 and Tc=J D 4= ln 3 for the triangular lattice19. The dashed vertical lines represent our estimates of Tc=J from ﬁnite-size scaling. The error bars represent one standard deviation statistical uncertainty (see Supplementary Information). and the high-temperature states are distinguished by their spin\u0015spin correlation functions: power-law decay at T D 0, and exponential decay at T D 1. Instead we feed raw Monte Carlo configurations to train a neural network (Fig. 1a) to distinguish ground states from high-temperature states (Fig. 2a,b). For a square-ice system with N D 2 \u0002 16 \u0002 16 spins, we find that a neural network with 100 hidden units successfully distinguishes the states with a 99% accuracy. The network does so solely based on spin configurations, with no information about the underlying lattice\u0016a feat di˚cult for the human eye, even if supplemented with a layout of the underlying Hamiltonian locality. We now examine an Ising lattice gauge theory, the prototypical example of a topological phase of matter, without an order param- eter at T D 0 (refs 8,20). The Hamiltonian is H D \u0000J P p Q i2p ˙ z i , where the Ising spins live on the bonds of a two-dimensional square lattice with plaquettes p (see Fig. 2c). The ground state is again a degenerate manifold 8,21 with exponentially decaying spin\u0015spin cor- relations. As in the square-ice model, we attempt to use the neural network in Fig. 1a to classify the high- and low-temperature states, but find that the training fails to classify the test sets to an accuracy of over 50%\u0016equivalent to simply guessing. Instead, we employ a convolutional neural network (CNN) 3,22 which readily takes advan- tage of the two-dimensional structure as well as the translational invariance of the model. We optimize the CNN in Fig. 2d using Monte Carlo configurations from the Ising gauge theory at T D 0 and T D 1. The CNN discriminates high-temperature from ground states with an accuracy of 100% in spite of the lack of an order parameter or qualitative di˙erences in the spin\u0015spin correlations. We find that the discriminative power of the CNN relies on the det- ection of satisfied local energetic constraints of the theory, namely whether Q i2p ˙ z i is either C1 (satisfied) or \u00001 (unsatisfied) on each plaquette of the system (see the Supplementary Fig. 5). We construct an analytical model to explicitly exploit the presence of local constraints in the classification task, which discriminates our test sets with an accuracy of 100% (see Supplementary Fig. 6). Notice that, because there is no finite-temperature phase transition in the Ising gauge theory, we have restricted our analysis to temperatures T D 0 and T D 1, only. However, in finite systems, violations of the local constraints are strongly suppressed, and the system is expected to slowly cross over to the high-temperature phase. The crossover temperature T \u0003 happens as the number of thermally excited defects ˘ N exp.\u00002J \f/ is of the order of one, implying T \u0003=J ˘ 1= ln p N (ref. 23). As the presence of local defects is the mechanism through which the CNN decides whether a system is in its ground state or not, we expect that it will be able to detect the crossover temperature in a test set at small but finite temperatures. In Fig. 3 we present the results of the output neurons of our analytical model for di˙erent system sizes averaged over test sets at di˙erent temperatures. We estimate the inverse crossover temperature \f \u0003J based on the crossing point of the low- and high-temperature output neurons. As expected theoretically, this depends on the system size, and as shown in the inset in Fig. 3, a clear logarithmic crossover is apparent. This result showcases the ability of the CNN to detect not only phase transitions, but also non-trivial crossovers between topological phases and their high- temperature counterparts. 432 © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. NATURE PHYSICS | VOL 13 | MAY 2017 | www.nature.com/naturephysics NATURE PHYSICS DOI: 10.1038/NPHYS4035 LETTERS p 2×2 maps (64 per sublattice) Fully connected layer (64) v Softmax Dropout regularization a High-temperature state b Ising square-ice ground state c Ising lattice gauge theory d Figure 2 | Typical conﬁgurations of square-ice and Ising gauge models. a, A high-temperature state. b, A ground state of the square-ice Hamiltonian. c, A ground state conﬁguration of the Ising lattice gauge theory. Dark circles represent spins up, while white circles represent spins down. The vertices and plaquettes deﬁning the models are shown in the insets of b and c. d, Illustration of the convolutional neural network of the Ising gauge theory. The convolutional layer applies 64 2 \u0002 2 ﬁlters to the conﬁguration on each sublattice, followed by rectiﬁed linear units (ReLu). The outcome is followed by a fully connected layer with 64 units and a softmax output layer. The green line represents the sliding of the maps across the conﬁguration. −1 0 1 2 3 4 βJ 0.0 0.2 0.4 0.6 0.8 1.0Output layer L = 4 L = 8 L = 12 L = 16 L = 20 L = 24 L = 28 100 101 102 L 1.0 1.5 2.0 2.5 3.0 3.5β∗JFigure 3 | Detecting the logarithmic crossover temperatures in the Ising gauge theory. Output neurons for diﬀerent system sizes averaged over test sets versus \fJ. Linear system sizes L D 4, 8, 12, 16, 20, 24 and 28 are represented by crosses, up triangles, circles, diamonds, squares, stars and hexagons. The inset displays \f\u0003J (octagons) versus L in a semilog scale. The error bars represent one standard deviation statistical uncertainty. A final implementation of our approach on a system of non- interacting spinless fermions subject to a quasi-periodic poten- tial 24 demonstrates that neural networks can distinguish metallic from Anderson localized phases, and can be used to study the localization transition between them (see the Supplementary Figs 3 and 4). We have shown that neural network technology, developed for applications such as computer vision and natural language processing, can be used to encode phases of matter and discriminate phase transitions in correlated many-body systems. In particular, we have argued that neural networks encode information about conventional ordered phases by learning the order parameter of the phase, without knowledge of the energy or locality conditions of the Hamiltonian. Furthermore, we have shown that neural networks can encode basic information about unconventional phases such as the ones present in the square-ice model and the Ising lattice gauge theory, as well as Anderson localized phases. These results indicate that neural networks have the potential to represent ground state wavefunctions. For instance, ground states of the toric code 1,8 can be represented by convolutional neural networks akin to the one in Fig. 2d (see Supplementary Fig. 6 and Supplementary Table 1). We thus anticipate their use in the field of quantum technology 25, such as quantum error correction protocols 26, and quantum state tomography 27. As in all other areas of `big data', we are already witnessing the rapid adoption of machine learning techniques as a basic research tool in condensed matter and statistical physics. Data availability. The data that support the plots within this paper and other findings of this study are available from the corresponding author upon request. Received 27 June 2016; accepted 11 January 2017; published online 13 February 2017 NATURE PHYSICS | VOL 13 | MAY 2017 | www.nature.com/naturephysics © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. 433 LETTERS NATURE PHYSICS DOI: 10.1038/NPHYS4035 References 1. Wen, X. Quantum Field Theory of Many-Body Systems: From the Origin of Sound to an Origin of Light and Electrons (Oxford Graduate Texts, OUP Oxford, 2004). 2. Bellman, R. Dynamic Programming 1st edn (Princeton Univ. Press, 1957). 3. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning (MIT Press, 2016); http://www.deeplearningbook.org 4. Abadi, M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems (2015); http://tensorflow.org 5. Bergstra, J. et al. Theano: a CPU and GPU math expression compiler. Proc. Python Sci. Comput. Conf. (SciPy)(2010). 6. Avella, A. & Mancini, F. Strongly Correlated Systems: Numerical Methods (Springer Series in Solid-State Sciences, 2013). 7. Sandvik, A. W. Computational studies of quantum spin systems. AIP Conf. Proc. 1297, 135\u0015338 (2010). 8. Kitaev, A. Fault-tolerant quantum computation by anyons. Ann. Phys. 303, 2\u001530 (2003). 9. Levin, M. & Wen, X.-G. Detecting topological order in a ground state wave function. Phys. Rev. Lett. 96, 110405 (2006). 10. Kitaev, A. & Preskill, J. Topological entanglement entropy. Phys. Rev. Lett. 96, 110404 (2006). 11. Arsenault, L.-F. , Lopez-Bezanilla, A., von Lilienfeld, O. A. & Millis, A. J. Machine learning for many-body physics: the case of the Anderson impurity model. Phys. Rev. B 90, 155136 (2014). 12. Kusne, A. G. et al. On-the-fly machine-learning for high-throughput experiments: search for rare-earth-free permanent magnets. Sci. Rep. 4, 6367 (2014). 13. Kalinin, S. V., Sumpter, B. G. & Archibald, R. K. Big-deep-smart data in imaging for guiding materials design. Nat. Mater. 14, 973\u0015980 (2015). 14. Ghiringhelli, L. M., Vybiral, J., Levchenko, S. V., Draxl, C. & Sche˜er, M. Big data of materials science: critical role of the descriptor. Phys. Rev. Lett. 114, 105503 (2015). 15. Schoenholz, S. S., Cubuk, E. D., Sussman, D. M., Kaxiras, E. & Liu, A. J. A structural approach to relaxation in glassy liquids. Nat. Phys. 12, 469\u0015471 (2016). 16. Mehta, P. & Schwab, D. J. An exact mapping between the variational renormalization group and deep learning. Preprint at http://arXiv.org/abs/1410.3831 (2014). 17. Onsager, L. Crystal statistics. I. A two-dimensional model with an order-disorder transition. Phys. Rev. 65, 117\u0015149 (1944). 18. van Nieuwenburg, E. P. L., Liu, Y.-H. & Huber, S. Learning phase transitions by confusion. Nat. Phys. http://dx.doi.org/10.1038/nphys4037 (2017). 19. Newell, G. F. Crystal statistics of a two-dimensional triangular Ising lattice. Phys. Rev. 79, 876\u0015882 (1950). 20. Kogut, J. B. An introduction to lattice gauge theory and spin systems. Rev. Mod. Phys. 51, 659\u0015713 (1979). 21. Castelnovo, C. & Chamon, C. Topological order and topological entropy in classical systems. Phys. Rev. B 76, 174416 (2007). 22. LeCun, Y., Bottou, L., Bengio, Y. & Ha˙ner, P. Gradient-based learning applied to document recognition. IEEE Proc. 86, 2278\u00152324 (1998). 23. Castelnovo, C. & Chamon, C. Entanglement and topological entropy of the toric code at finite temperature. Phys. Rev. B 76, 184442 (2007). 24. Aubry, S. & André, G. Analyticity breaking and anderson localization in incommensurate lattices. Ann. Isr. Phys. Soc. 3, 133 (1980). 25. Amin, M. H., Andriyash, E., Rolfe, J., Kulchytskyy, B. & Melko, R. Quantum Boltzmann machine. Preprint at http://arXiv.org/abs/1601.02036 (2016). 26. Torlai, G. & Melko, R. G. A neural decoder for topological codes. Preprint at http://arXiv.org/abs/1610.04238 (2016). 27. Landon-Cardinal, O. & Poulin, D. Practical learning method for multi-scale entangled states. New J. Phys. 14, 085004 (2012). Acknowledgements We would like to thank G. Baskaran, C. Castelnovo, A. Chandran, L. E. Hayward Sierens, B. Kulchytskyy, D. Schwab, M. Stoudenmire, G. Torlai, G. Vidal and Y. Wan for discussions and encouragement. We thank A. Del Maestro for a careful reading of the manuscript. This research was supported by NSERC of Canada, the Perimeter Institute for Theoretical Physics, the John Templeton Foundation, and the Shared Hierarchical Academic Research Computing Network (SHARCNET). R.G.M. acknowledges support from a Canada Research Chair. Research at Perimeter Institute is supported through Industry Canada and by the Province of Ontario through the Ministry of Research & Innovation. Author contributions All authors contributed significantly to this work. Additional information Supplementary information is available in the online version of the paper. Reprints and permissions information is available online at www.nature.com/reprints. Correspondence and requests for materials should be addressed to J.C. Competing ﬁnancial interests The authors declare no competing financial interests. 434 © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. NATURE PHYSICS | VOL 13 | MAY 2017 | www.nature.com/naturephysics","libVersion":"0.3.2","langs":""}
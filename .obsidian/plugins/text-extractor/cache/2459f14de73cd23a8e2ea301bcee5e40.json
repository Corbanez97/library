{"path":"Books and Papers/Astrophysics and Cosmology/Dissertação_Mestrado_UEL_Final_Sanderson_Carlos_Ribeiro.pdf","text":"SANDERSON CARLOS RIBEIRO RECONSTRUÇÃO DA EQUAÇÃO DE ESTADO DA ENERGIA ESCURA Londrina 2023 SANDERSON CARLOS RIBEIRO RECONSTRUÇÃO DA EQUAÇÃO DE ESTADO DA ENERGIA ESCURA Dissertação apresentada ao Departamento de Física da Universidade Estadual de Londrina, como requisito para o grau de Mestre. Orientador: Prof. Dr. Sandro Dias Pinto Vitenti Londrina 2023 SANDERSON CARLOS RIBEIRO RECONSTRUÇÃO DA EQUAÇÃO DE ESTADO DA ENERGIA ESCURA Tese apresentada ao Departamento de Física da Universidade Estadual de Londrina, como requi- sito para o grau de Mestre. BANCA EXAMINADORA Prof. Dr. Sandro Dias Pinto Vitenti Universidade Estadual de Londrina Profª. Dra. Mariana Penna Lima Universidade de Brasília Prof. Dr. Álvaro Gomes dos Santos Universidade de Brasília Londrina, 12 de janeiro de 2023. AGRADECIMENTOS Agradeço à minha família, em especial à minha mãe, Sedenir pelo apoio e por ter me ensinado o verdadeiro valor de ser uma pessoa honesta, com caráter, simples e educada. Eu dedico mais essa etapa da minha carreira a você, afinal não iria tão longe sem seu esforço e amor. Agradeço também ao meu irmão, Sandro, assim como a minha irmã, Samanta. Agradeço a todos os professores do Departamento de Física da UEL e da Unicentro que conheci por terem me passado um pouco do conhecimento que adquiriram ao longo de suas vidas. Com certeza seus ensinamentos foram fundamentais para me tornar o profissional que sou hoje me inspirando sempre cada vez mais. Em especial agradeço ao meu orientador, Prof. Dr. Sandro Dias Pinto Vitenti, por ter paciência comigo e por me orientar ao infinito e além pelo Universo. Agradeço aos meus amigos e amigas de longa data que ainda permaneceram em minha vida como Bruno Wogt pela amizade desde o ensino médio (faz tempo isso), Guilherme Henri- que que me ajudou muito no início e que espero que tenha feito a escolha certa para sua vida e Dionatan que sempre foi um grande amigo, agradeço por sua amizade e por você e sua família me acolherem. Dentre tantos nomes também menciono os de Gabriel Carneiro, Natalia Silva, Cheila Sumenssi e Milena Malherbi dentre vários outros(as) que foram fundamentais para o meu crescimento profissional e social. Vocês foram e continuam sendo especiais em minha vida. Agradeço pela amizade e ajuda nos estudos aos meus amigos e amigas que fiz no curso de pós graduação em Física da UEL, em especial menciono os integrantes do grupo de Cos- mologia e Gravitação: Caio, Henrique, Cinthia, Eduardo e William. Não poderia esquecer de agradecer ao Ricardo que conheço há alguns anos, mas que fui realmente admirar nesse período em que dividimos um lar em Londrina para buscarmos nossos sonhos. E por fim, Demétrio, que apesar de dizer que não é “demérito” nenhum conhecê-lo, saiba que foi uma honra conhe- cer você, pois não teria ido tão longe sem sua ajuda e amizade, então eu digo que você é uma pessoa “de mérito” e que merece tudo o que tem de mais maravilhoso nesse vasto Universo. Sou eternamente grato à todos vocês. E parafraseando Carl Sagan: “Diante da vastidão do tempo e da imensidão do Universo, é um imenso prazer para mim dividir um planeta e uma época com vocês”. No meu TCC mencionei que eu era agnóstico. Ainda sou em alguns aspectos, mas acabei me encontrando mais com Deus nesses últimos tempos e só tenho a agradecer por tudo. Agradeço à CAPES pelo apoio financeiro. “Um mundo... isso mesmo! Para vocês, que bus- cam a verdadeira liberdade, existe um mundo in- teiro bem diante de seus olhos! Se os seus sonhos inacabáveis guiam seus caminhos, lutem para que eles virem realidade sob a bandeira de seus ideais!” — One Piece. “Tudo o que você for fazer tem que ser bom para você e para mais alguém.” — Glauco Marques (Ator, Dublador e Diretor de Dublagem). RIBEIRO, Sanderson Carlos. Reconstrução da Equação de Estado da Energia Escura 2022. Trabalho de Conclusão de Curso (Mestrado em Física) – Universidade Estadual de Londrina, Londrina, 2022. RESUMO Na Cosmologia, medidas de distância são atualmente a ferramenta mais poderosa no estudo da expansão do Universo sem especificar seu conteúdo de matéria ou qualquer teoria gravitacional. Nesse presente trabalho supomos uma cosmologia padrão descrita pelo Princípio Cosmológico (homogeneidade e isotropia em grandes escalas) e a teoria da Relatividade Geral de Einstein, com o objetivo de reconstruir a equação de estado da energia escura w(z) em função do redshift z, usando uma abordagem paramétrica e independente de modelo, de modo que apenas os dados irão nos dizer como fazer essa reconstrução. Adotamos essa suposição, como ponto de partida, justamente para entender melhor como a densidade da energia escura afeta a expansão do Uni- verso. Neste caso, ressaltamos que a nossa reconstrução é paramétrica no que diz respeito a distribuição de probabilidade dos dados que supomos ser normalmente distribuída, ou seja uma gaussiana, de modo que podemos usufruir do teorema do limite central tal que a distribuição amostral da média de uma determinada estimativa também é normalmente distribuída, indepen- dentemente do tamanho da amostra. Além disso é independente de modelo apenas no que se concerne à energia escura. Essa independência de modelo no nosso caso é muito sutil e está restrita apenas a hipótese de que a energia escura é um fluido barotrópico que satisfaz a conser- vação do tensor energia-momento para um fluido perfeito. A parte mais extensa desse trabalho foi a implementação e execução dos códigos computacionais e o estudo sobre o método de re- construção, cuja tecnicalidade utilizada na reconstrução de w(z) é chamada de splines cúbicas, uma técnica muito utilizada na interpolação polinomial de funções numericamente. Todas as estimativas foram obtidas através de cálculo numérico e devidamente analisadas de acordo com as suas regiões de confiança e resultados existentes na literatura. As análises dos resultados fo- ram divididas em três etapas de modo que houve implementação de dados observacionais mais recentes e/ou em maior quantidade da 1ª para a 2ª etapas além de uma mudança significativa no intervalo de redshift da reconstrução devido a calibração do modelo feita na 1ª etapa. Na 3ª etapa executamos o método de Markov Chain Monte Carlo para os mesmos dados e intervalo de redshift que foram usados na 2ª etapa para obtermos uma maior confiança nas incertezas das estimativas, tendo em vista que o método da mtriz de Fisher usado nas outras etapas não é con- fiante para sistemas degenerados como o do nosso trabalho. Nessa etapa final constatamos com mais confiança que, para baixos redshifts (0 ≤ z ≤ 3), a equação de estado da energia escura w(z) apresenta um comportamento bimodal de modo que obtivemos valores altamente concen- trados em w = −1 e 0 < w < 1/3. Essa é uma constatação tratada como novidade em nosso trabalho, que não é conflitante com o que sabemos atualmente sobre Cosmologia, devido ao fato de que também obtivemos resultados próximos da literatura como ˆw = −1.087 ± 0.03592 para w constante, que é consistente com um comportamento de constante cosmológica. Palavras-chave: Cosmologia. Energia Escura. Reconstrução. Equação de Estado. RIBEIRO, Sanderson Carlos. Reconstruction of the Dark Energy Equation of State 2022. Masters Degree Dissertation – Universidade Estadual de Londrina, Londrina, 2022. ABSTRACT In Cosmology, distance measurements are currently the most powerful tool to study the expan- sion of the Universe without specifying its content of matter or any gravitational theory. In this present work we assume a standard cosmology described by the Cosmological Principle (homogeneity and isotropy on large scales) and Einstein’s theory of General Relativity, with the aim of reconstructing the equation of state for dark energy w(z) as a function of redshift z, using a parametric and model-independent approach, so that only the data will tell us how to do this reconstruction. We adopted this assumption as a starting point precisely to better un- derstand how the density of dark energy affects the expansion of the Universe. In this case, we emphasize that our reconstruction is parametric with regard to the probability distribution of the data that we assume is normally distributed, that is, a Gaussian, so that we can take advantage of the central limit theorem such that the sampling distribution of the mean of a given estimate is also normally distributed, regardless of sample size. Furthermore, it is model-independent only with regard to dark energy. This model independence in our case is very subtle and is res- tricted only to the hypothesis that dark energy is a barotropic fluid that satisfies the conservation of the energy-momentum tensor for a perfect fluid. The most extensive part of this work was the implementation and execution of computational codes and the study of the reconstruction method, whose technicality is used in the reconstruction of w(z) is called cubic splines, a te- chnique widely used in polynomial interpolation of functions numerically. All estimates were obtained through numerical calculation and duly analyzed according to their confidence regions and existing results in the literature. The analyzes of the results were divided into three stages so that there was implementation of more recent observational data and/or in greater quantity from the 1st to the 2nd stages, in addition to a significant change in the redshift interval of the recons- truction due to the calibration of the model made in the 1st stage. In the 3rd stage, we executed the Markov Chain Monte Carlo method for the same data and redshift interval that were used in the 2nd stage, in order to obtain greater confidence in the uncertainties of the estimates, con- sidering that the method of Fisher’s matrix used in the other steps is not reliable for degenerate systems like the one in our work. In this final step we found with more confidence that, for low redshifts (0 ≤ z ≤ 3), the dark energy equation of state w(z) presents a bimodal behavior so that we obtained highly concentrated in w = −1 and 0 < w < 1/3. This is a finding treated as new in our work, which is not in conflict with what we currently know about Cosmology, due to the fact that we also obtained results close to the literature, such as ˆw = −1.087 ± 0.03592 for w constant, which is consistent with cosmological constant behavior. Keywords: Cosmology. Dark Energy. Reconstruction. Equation Of State. 8 SUMÁRIO 1 INTRODUÇÃO 15 2 OBJETIVOS 18 3 FUNDAMENTAÇÃO TEÓRICA 19 3.1 O ESTUDO DO UNIVERSO: COSMOLOGIA . . . . . . . . . . . . . . . . . . 19 3.2 MODELOS COSMOGÔNICOS . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.2.1 Mitos da criação . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.3 MODELOS GEOCÊNTRICO E HELIOCÊNTRICO . . . . . . . . . . . . . . . 23 3.4 COSMOLOGIA BÁSICA E A RELATIVIDADE RESTRITA . . . . . . . . . . . 24 3.5 RELATIVIDADE GERAL E A COSMOLOGIA MODERNA . . . . . . . . . . . 25 3.6 CONCEITO DE ESPAÇO-TEMPO E A MATEMÁTICA NA RELATIVIDADE GE- RAL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.7 HOMOGENEIDADE E ISOTROPIA DO UNIVERSO: O PRINCÍPIO COSMO- LÓGICO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.8 MODELOS COSMOLÓGICOS . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.9 PARÂMETROS COSMOLÓGICOS . . . . . . . . . . . . . . . . . . . . . . . . 39 3.9.1 O parâmetro e a constante de Hubble . . . . . . . . . . . . . . . . . 39 3.9.2 Tempo de Hubble . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.9.3 Distância ou comprimento de Hubble . . . . . . . . . . . . . . . . . 45 3.9.4 Redshift (Desvio para o vermelho) . . . . . . . . . . . . . . . . . . . 46 3.9.5 Distância comóvel em termos do redshift . . . . . . . . . . . . . . . 47 3.10 DINÂMICA DO UNIVERSO EM EXPANSÃO: A MÉTRICA FLRW E AS EQUA- ÇÕES DE FRIEDMANN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.11 COMPONENTES DO UNIVERSO . . . . . . . . . . . . . . . . . . . . . . . . 63 3.11.1 Matéria Bariônica . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 3.11.2 Matéria Escura . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 3.11.3 Energia Escura . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.12 RECONSTRUÇÃO DA EQUAÇÃO DE ESTADO DA ENERGIA ESCURA . . . . . 68 3.12.1 Equação de estado . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 3.12.2 A Física no método de reconstrução . . . . . . . . . . . . . . . . . . 70 3.12.3 Abordagem paramétrica, não-paramétrica, dependência e indepen- dência de modelo . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4 DADOS OBSERVACIONAIS 78 4.1 DADOS DE SUPERNOVAS DO TIPO IA . . . . . . . . . . . . . . . . . . . . . 79 4.1.1 1ª etapa - SNe Ia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4.1.2 2ª e 3ª etapas - SNe Ia . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4.2 OSCILAÇÕES ACÚSTICAS DE BÁRIONS - BAO . . . . . . . . . . . . . . . . 87 4.2.1 1ª etapa - BAO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.2.2 2ª e 3ª etapas - BAO . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 4.3 MEDIDAS DO PARÂMETRO DE HUBBLE: CRONÔMETROS COSMOLÓGICOS 92 4.3.1 1ª etapa - CCH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4.3.2 2ª e 3ª etapas - CCH . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 4.4 MEDIDAS DA CMB PELO WMAP . . . . . . . . . . . . . . . . . . . . . . . 98 4.4.1 1ª etapa - Distance priors . . . . . . . . . . . . . . . . . . . . . . . . 98 5 METODOLOGIA 101 5.1 ABORDAGEM ADOTADA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 5.2 DESCRIÇÃO DO FATOR DE ESCALA COMO UM POLINÔMIO . . . . . . . . 101 5.3 TÉCNICA DE spline PARA INTERPOLAÇÃO POLINOMIAL . . . . . . . . . . 103 5.4 EQUAÇÃO DE ESTADO DA ENERGIA ESCURA POR PARTES . . . . . . . . . 107 5.5 IMPLEMENTAÇÃO DO SOFTWARE WSPLINE . . . . . . . . . . . . . . . . . 109 5.6 PROCEDIMENTO DE ESTIMAÇÃO NUMÉRICO . . . . . . . . . . . . . . . . 119 5.7 MÉTODOS ESTATÍSTICOS EMPREGADOS NAS ANÁLISES . . . . . . . . . . 122 5.7.1 Método da máxima verossimilhança . . . . . . . . . . . . . . . . . . 122 5.7.2 Método da matriz de Fisher e de Markov Chain Monte Carlo . . . 127 5.7.3 Teorema do limite central e Regra 68-95-99 . . . . . . . . . . . . . . 129 6 RESULTADOS E DISCUSSÕES 135 6.1 DISCUSSÃO SOBRE A RECONSTRUÇÃO . . . . . . . . . . . . . . . . . . . . 135 6.2 1ª ETAPA - RESULTADOS E DISCUSSÕES . . . . . . . . . . . . . . . . . . . 139 6.3 2ª ETAPA - RESULTADOS E DISCUSSÕES . . . . . . . . . . . . . . . . . . . 142 6.4 3ª ETAPA - RESULTADOS E DISCUSSÕES . . . . . . . . . . . . . . . . . . . 145 7 CONSIDERAÇÕES FINAIS 157 A TABELAS DE BEST-FITS E GRÁFICOS 159 A.1 3ª ETAPA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 A.1.1 Gráficos de densidade de probabilidade e corner plots de MCMC . 162 B ANÁLISE ILUSTRATIVA DOS VALORES DA VARIÁVEL IMPLEMENTADA COM RELAÇÃO AO REDSHIFT 167 10 LISTA DE FIGURAS 3.1 Alguns monumentos megalíticos associados à fenômenos astronômicos. Fonte: WikiLivros. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.2 Retina fazendo a função janela na observação de um ponto do copo d’água. Fonte: Autor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.3 O Universo dentro de 500.000 anos-luz: As galáxias satélites da Via Láctea. Fonte: Richard Powell. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.4 Ilustração de 1 milhão de anos-luz de afastamento da Via Láctea. Fonte: Ri- chard Powell. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.5 Ilustração de 10 milhões de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.6 Ilustração de 100 milhões de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.7 Ilustração de 1 bilhão de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.8 Representação de um Universo homogêneo e não isotrópico. Fonte: Autor. . . . 36 3.9 Representação de um Universo homogêneo e não isotrópico, observado em di- ferentes direções. Fonte: Autor. . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.10 Representação de um Universo homogêneo e isotrópico. Fonte: Autor. . . . . . 37 3.11 CMB observada pelo Planck. Fonte: European Space Agency (ESA) - Planck. . 37 3.12 Etapas de inferência da medida de H0 realizada pela equipe SH0ES. . . . . . . 42 3.13 Representações gráficas das possíveis dinâmicas do Universo para o caso de matéria e representação da reta tangente a curvatura do Modelo Cosmológico Padrão que se trata do parâmetro de Hubble. No ponto em que t = t0 temos a constante de Hubble. Fonte: Autor. . . . . . . . . . . . . . . . . . . . . . . . . 44 3.14 Gráfico do tempo em bilhões de anos em função do parâmetro de Hubble. No ponto em que H = H0 = 67.4 km s−1 Mpc −1 temos a idade estimada do Universo, que é aproximadamente t = 14 bilhões de anos. Fonte: Autor. . . . . 45 3.15 Representação de uma medida de distância entre dois pontos, A e B, no espaço tridimensional euclidiano. Fonte: Autor. . . . . . . . . . . . . . . . . . . . . . 50 3.16 Representação da geometria dos tipos de Universo. Fonte: Autor. . . . . . . . . 52 3.17 Representação da expansão do Universo. Fonte: Canal D-Dimensões. . . . . . 68 3.18 Classificação da matéria. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 3.19 Representação de um experimento no qual soltamos uma bolinha de uma certa altura e esta apresenta uma velocidade descontínua em seu percurso no trajeto delimitado por b. Fonte: Autor. . . . . . . . . . . . . . . . . . . . . . . . . . . 74 11 3.20 Representação de uma caixa dividida em outros 4 subsistemas. O número de repetições aumenta de acordo com a diminuição da escala. Fonte: Autor. . . . . 76 4.1 Representação trigonométrica para as duas contribuições da escala acústica: a radial r|| e a transversal r⊥. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 5.1 Representação gráfica do fenômeno de Runge. A curva vermelha é a chamada função de Runge. A curva azul é uma interpolação polinomial em 5ª ordem dessa mesma função, usando 6 pontos de interpolação igualmente espaçados. E a curva verde é uma interpolação, também da mesma função, em 9ª ordem com 10 pontos de interpolação igualmente espaçados. Fonte: Wikimedia. . . . . . . 104 5.2 Representação gráfica de uma função usando a técnica de spline. Fonte: Autor. 106 5.3 Grade de representação dos valores de z com divisão em 1000 partes iguais. . . 112 5.4 Grade ilustrativa dos valores de α com divisão em 12 partes iguais no intervalo de redshift 0 ≤ z ≤ 10 10. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 5.5 Grade ilustrativa dos valores de α com divisão em 12 partes iguais no intervalo de redshift 0 ≤ z ≤ 10 4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 5.6 Parâmetros fixos e livre do nosso modelo. . . . . . . . . . . . . . . . . . . . . 120 5.7 Mapa conceitual referente ao procedimento de estimação numérico deste pre- sente trabalho para as 1ª e 2ª Etapas. . . . . . . . . . . . . . . . . . . . . . . . 123 5.8 Representação gráfica de um ajuste de curvas usado na discussão sobre a distri- buição χ 2 ser chamada de verossimilhança. . . . . . . . . . . . . . . . . . . . 124 5.9 Representação da região de confiança entre valores verdadeiros w0, w1 (hipó- tese) e valores estimados ˆw1 e ˆw1 (hipótese alternativa) para 1σ de confiança cuja forma geométrica é dada pela equação paramétrica da elipse. Fonte: Autor. 130 5.10 Regiões de confiança usando a regra 68-95-99 para 1σ, 2σ e 3σ de confiança. Fonte: Autor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 5.11 Regiões de confiança usando a regra 68-95-99 para 1σ, 2σ e 3σ de confiança para o primeiro exemplo discutido. Fonte: Autor. . . . . . . . . . . . . . . . . 132 5.12 Regiões de confiança usando a regra 68-95-99 para 1σ, 2σ e 3σ de confiança para o segundo exemplo discutido. Fonte: Autor. . . . . . . . . . . . . . . . . 133 6.1 Região de confiança de ˆH0 obtida da configuração de 5 nós (TTT) na 3ª etapa associada a 1σ = 68.3% de confiança. . . . . . . . . . . . . . . . . . . . . . . 148 6.2 Região de confiança de ˆH0 e da medida do Planck H P 0 , ambas associadas a 1σ = 68.3% de confiança. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 6.3 Gráfico de densidade de probabilidade para o best-fit de 5 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. . . . . . . . . . 150 6.4 Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (ex- terna/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4 e M1 (configuração de 5 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. . . . . . . . . . . . . . 152 6.5 Região de confiança de ˆw para o modelo XCDM na 3ª etapa associada com 3σ = 99.7% de confiança. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 6.6 Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (ex- terna/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w, e M1 (Modelo XCDM (w constante)). A área sombreada na distribuição marginal representa 1σ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 A.1 Gráfico de densidade de probabilidade para o best-fit de 6 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. . . . . . . . . . 162 A.2 Gráfico de densidade de probabilidade para o best-fit de 7 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. . . . . . . . . . 162 A.3 Gráfico de densidade de probabilidade para o best-fit de 8 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. . . . . . . . . . 163 A.4 Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (ex- terna/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4, w5 e M1 (configuração de 6 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. . . . . . . . . . . . . 164 A.5 Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (ex- terna/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4, w5, w6 e M1 (configuração de 7 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. . . . . . . . . 165 A.6 Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (ex- terna/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4, w5, w6, w7 e M1 (configuração de 8 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. . . . . . . . . 166 13 LISTA DE TABELAS 3.1 Equações de Maxwell na forma integral e diferencial. . . . . . . . . . . . . . . 27 3.2 Alguns dos parâmetros cosmológicos do Planck considerando Ωκ = 0 (Uni- verso plano) com 68% de confiança combinado com TT, TE, EE+lowE+lenteamento. 60 3.3 Modelos cosmológicos e parâmetros determinados pelo Pantheon+ SH0ES com 68% de confiança. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.1 Parâmetros de SNe Ia de Betoule et al. (2014). . . . . . . . . . . . . . . . . . . 82 4.2 Dados de BAO usados na 1ª etapa deste trabalho. A medida 1 não apresenta covariância. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.3 Dados de BAO usados na 2ª e 3ª etapas deste trabalho. O valor apresentado em Ross et al. (2015) é de 664 ± 25 Mpc da quantidade DV (z)rs,fid rs , entretanto aqui apresentamos ele com base na referência Alam et al. (2021). . . . . . . . . . . 93 4.4 Dados de cronômetros cosmológicos H(z) em unidades de km s −1 Mpc−1 usa- dos na 1ª etapa. A 21ª medida foi dada em 4.54. . . . . . . . . . . . . . . . . . 97 4.5 Dados de cronômetros cosmológicos H(z) em unidades de km s −1 Mpc−1 usa- dos na 2ª e 3ª etapas. São uma reprodução direta da Tabela 1 de Gomez-Valent e Luca Amendola (2018). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 5.1 Valores aproximados de redshift para as principais fases do Universo e objeto ou fenômeno descrito de acordo com z. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.1 Tipos de dados booleanos para os parâmetros da reconstrução em que T (true) e F (False) se referem ao parâmetro livre e fixo, respectivamente. . . . . . . . . 136 6.2 Principais medidas dos parâmetros cosmológicos que usaremos para as análises de alguns resultados em particular do nosso modelo. . . . . . . . . . . . . . . . 138 6.3 Best-fit da configuração de 5 nós (TTT) e 8 nós (TTF) na 1ª etapa com 0 ≤ z ≤ 1080. A verossimilhança total é −2 ln L ≈ 701.33 e −2 ln L ≈ 701.89, respectivamente. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 6.4 Best-fit da configuração de 5 nós (TTT) e 8 nós (TTF) na 2ª etapa com 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1482.402 e −2 ln L ≈ 1481.436, respectivamente. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 6.5 Best-fit da configuração de 5 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1484. . . . . . . . . . . . . . . . . . . . . . 147 6.6 Best-fit do modelo XCDM (w constante) na configuração TTT da 3ª etapa no intervalo de redshift de 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1503. 153 A.1 Best-fit da configuração de 6 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1483. . . . . . . . . . . . . . . . . . . . . . 159 A.2 Best-fit da configuração de 7 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1483. . . . . . . . . . . . . . . . . . . . . . 160 A.3 Best-fit da configuração de 8 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1483. . . . . . . . . . . . . . . . . . . . . . 161 15 1 INTRODUÇÃO Atualmente, na Cosmologia, temos que medidas de distância são poderosas ferramentas no estudo da expansão do Universo sem especificar seu conteúdo de matéria ou qualquer teoria gravitacional. As medidas de distância são um modo pelo qual interpretamos o mundo e no nosso caso, o Universo. Este trabalho, em particular, adotou uma abordagem paramétrica e independente de mo- delo na reconstrução da equação de estado da energia escura w(z). Modelos podem ter uma abordagem paramétrica que consiste em se atribuir uma dis- tribuição de probabilidade para os dados, e não-paramétrica em que usamos os dados para construir a sua distribuição de probabilidade. Além disso, a curva do fenômeno estudado, pode ser dependente de modelo, quando usamos uma função a priori para descrever os dados, e inde- pendente de modelo quando usamos os dados para reconstruir a função que melhor se ajusta a distribuição do fenômeno. A nossa reconstrução é paramétrica no que diz respeito a distribuição de probabilidade dos dados que supomos ser normalmente distribuída, i.e. uma gaussiana, de modo que po- demos usufruir do teorema do limite central tal que a distribuição amostral da média de uma determinada estimativa também é normalmente distribuída, independentemente do tamanho da amostra. Nem sempre essa aproximação é boa para análises, mas como um primeiro método, conseguimos analisar as regiões de confiança do modelo com grande precisão. Além disso a reconstrução é independente de modelo no que se concerne apenas à energia escura, que será reconstruída a partir dos dados. Desta forma todo o restante do arcabouço teórico ainda é dependente de modelo. Neste caso, supomos uma cosmologia padrão descrita pelo Princípio Cosmológico (homogeneidade e isotropia em grandes escalas) com métrica de fundo sendo a de Friedmann-Lemaître-Robertson-Walker (FLRW) e a teoria da Relatividade Geral de Einstein. Adotamos essa suposição, como ponto de partida, justamente para entender melhor como a den- sidade de energia escura afeta a expansão do Universo. De acordo com essa característica vamos supor, por exemplo, que conhecemos certos constituintes do Universo, como a matéria escura e bárions, mas não sabemos o que é energia escura, que por hipótese supomos ser um fluido baro- trópico. Assim, podemos supor uma equação de estado da forma w(z) = pDE/ρDE em que pDE e ρDE são a pressão e densidade da energia escura, respectivamente e w(z) é um parâmetro o qual pode ser considerado constante ou dependente do tempo e que representa, em nosso trabalho, a equação de estado da energia escura. A independência de modelo no nosso caso é muito sutil e está restrita apenas a hipótese de que a energia escura é um fluido barotrópico que satisfaz a conservação do tensor energia-momento para um fluido perfeito. Na literatura o modelo que considera a equação de estado da energia escura livre e as demais componentes fixas é chamado de wCDM. A energia escura representa em torno de 69% de toda a composição do nosso Universo 16 [1], sendo considerada pelo Modelo Cosmológico Padrão a responsável pela expansão acele- rada do Universo. Desta forma é interessante reconstruirmos essa componente, devido ao fato de que ela tem grande importância para a Cosmologia. Entretanto, a energia escura não é um observável direto, pois não conseguimos medir o quanto de energia escura existe em uma certa região do espaço com o uso de telescópios, por exemplo. Para contornar isso usamos os chamados observáveis cosmológicos como Supernovas do tipo Ia, oscilações acústicas de bári- ons e cronômetros cosmológicos, para ter acesso a dados observacionais provenientes de seus redshifts z. Com esses dados é possível se estimar w(z) e assim determinar a Cosmologia que mais se ajusta aos resultados obtidos com a reconstrução e as técnicas empregadas. Pois se não sabemos muito sobre a teoria, o melhor que podemos fazer é adivinhar o que os dados e seus erros nos dizem. Em outro caso, podemos reconstruir o modelo através dos dados. Em poucas palavras essa é a principal característica atribuída ao método de reconstrução. Em particular, evitamos escolhas arbitrárias da forma de w(z), aproximando-a em par- tes, onde cada uma é representada por um polinômio de 3ª ordem, ou seja, uma spline cúbica. Adotando as condições de contorno embutidas nesse tipo de reconstrução, bem como a técnica chamada not-a-knot de Carl de Boor [2] conseguimos reconstruir numericamente a curva que melhor se ajusta aos dados observacionais, uma vez que w(z) é uma função indireta dos dados. Assim, com os resultados mais a hipótese para o nosso modelo pretendemos inferir a melhor aproximação/estimação para a equação de estado da energia escura w(z). A estruturação dessa presente dissertação de mestrado é a seguinte: No Capítulo 2 apresentamos os principais objetivos desse trabalho de pesquisa. No Capítulo 3 temos uma revisão acerca da teoria necessária para o desenvolvimento deste presente trabalho. Ressalto que as seções voltadas aos Modelos Cosmogônicos, Modelo Geocêntrico e Heliocêntrico e a Cosmologia Básica são apenas a título de curiosidade para que possamos entender o avanço, de acordo com a história, da Cosmologia até o seu patamar atual. Fica a cargo do leitor a sua leitura. Pois são seções que não influenciam na dinâmica do trabalho. As outras seções servem para introduzir alguns parâmetros e discutir mais acerca do método de reconstrução e dos diferentes tipos de abordagens de modelos. O Capítulo 4 apresenta uma pequena introdução aos tipos de observáveis cosmológicos que foram usados na reconstrução de w(z) bem como os dados utilizados e suas verossimilhan- ças. Este capítulo foi dividido em 1ª e 2ª etapas para cada observável cosmológico, pois tivemos uma implementação de dados mais recentes e/ou em maior quantidade de uma etapa para outra. A calibração do modelo realizada na 1ª etapa de 0 ≤ z ≤ 1080 nos permitiu excluir os observá- veis cosmológicos chamados de distance priors da radiação cósmica de fundo para as demais análises, pois ficou constatado que estes deixavam o nosso modelo muito dependente em altos redshifts. Dessa forma restringimos o nosso modelo de 0 ≤ z ≤ 3.0 para as demais etapas. Restrição essa que foi possível também devido ao grande número de dados nesse intervalo de redshifts em particular. No Capítulo 5 discutimos sobre a abordagem adotada, o método de reconstrução envol- 17 vendo splines cúbicas e os procedimentos matemáticos que foram implementados numerica- mente com o auxílio da biblioteca NumCosmo. Os resultados e discussões acerca deste trabalho são apresentados no Capítulo 6, que é dividido em três etapas. Na 1ª e 2ª etapas usamos os dados observacionais relatados nas sub- seções dedicadas a cada etapa no Capítulo 4. Os dados da 2ª etapa são os mesmos utilizados na 3ª e última etapa, a qual empregamos o método de Markov Chain Monte Carlo para análise das regiões de confiança do nosso modelo em baixos redshifts. Analisamos alguns casos par- ticulares para cada etapa tendo em vista que por se tratar de um sistema muito degenerado, o método da matriz de Fisher na determinação das regiões de confiança da 1ª e 2ª etapas não é confiável. Assim as análises mais concretas dos nossos resultados foram feitas na 3ª etapa, na qual constatamos um comportamento bimodal para a equação de estado da energia escura. Por fim, apresentamos no Capítulo 7 as nossas considerações finais. 18 2 OBJETIVOS As medições de distância são atualmente poderosas ferramentas para se estudar a expan- são do Universo sem entrar em detalhes sobre o seu conteúdo de matéria nem qualquer teoria da gravitação. Assumindo um Universo que obedece o Princípio Cosmológico e descrito pela Relatividade Geral de Einstein os objetivos principais deste presente trabalho são: 1. Aplicar um método independente de modelo, mas paramétrico para reconstruir a equação de estado da energia escura w(z) através do uso de splines cúbicas. 2. Reconstruir a evolução recente do Universo e entender melhor o que os dados podem dizer a respeito da sua fase. 3. A partir dos resultados, se a hipótese de que o Universo descrito pela métrica de FLRW é sustentável, pretendemos estimar o comportamento da energia escura analisando a sua equação de estado w(z) e assim, verificar a eficácia do método de reconstrução e modelo, bem como do código desenvolvido. 19 3 FUNDAMENTAÇÃO TEÓRICA Este capítulo é dedicado a revisão da teoria necessária para o desenvolvimento deste presente trabalho. 3.1 O ESTUDO DO UNIVERSO: COSMOLOGIA A Cosmologia 1, em poucas palavras, é o estudo científico do Universo. Nos dias atuais ela é uma área bem consolidada, de tal forma que podemos tratá-la como um ramo da Física. Apesar disso, ainda é comum as pessoas associarem a Cosmologia à Astrofísica, outra área da Física, e essa associação decorre do fato de ambas seguirem caminhos paralelos. A Astrofísica é o estudo dos processos físicos e químicos envolvidos em fenômenos astronômicos. Neste contexto, ela estuda a estrutura e evolução estelar (incluindo a geração e transporte de energia dentro das estrelas presentes no Universo), as propriedades do meio interestelar e suas intera- ções com sistemas estelares, e a estrutura e dinâmica de sistemas estelares, como aglomerados estelares e galáxias. A Cosmologia, por sua vez, está mais interessada em estudar a natureza, origem e evolução do universo [4]. Mais formalmente, a Cosmologia, como ciência, utiliza o método científico na criação de modelos e hipóteses/teorias para tentar entender o Universo como um todo estudando sua composição e a organização da matéria no espaço, além, é claro, de entender como se dá a evolução do mesmo ao longo do tempo, desde o “Big Bang” [5]. É Claro que a Cosmologia não tenta descrever tudo. Por exemplo, não tenta descrever uma pessoa, casa ou mesmo a Terra, pois existem outras ciências que podem estudar essas coisas. O que a Cosmologia tenta descrever é o Universo em grandes escalas, e neste caso o seu principal objeto de estudo são objetos e/ou fenômenos cosmológicos que estão há milhões ou bilhões de anos-luz 2 da Terra. Primeiramente, veremos como a Cosmologia evoluiu historicamente, desde os principais mitos da criação até a cosmologia moderna atual. 3.2 MODELOS COSMOGÔNICOS Muito antes da descoberta da gravitação por Sir Isaac Newton, que consideramos como o início da Cosmologia baseada no método científico, a humanidade, desde seus primórdios, vêm observando o Universo e fazendo inferências sobre o céu noturno e diurno de acordo com suas culturas, conhecimentos, e habilidades na produção de instrumentos e/ou locais utilizados para as observações. Eles não sabiam tanto quanto o homem moderno sabe e, portanto, teria 1A palavra Cosmologia vêm da Grécia Antiga, com a junção das palavras gregas “Cosmos” que significa mundo ordenado, organizado ou Universo, e da palavra “logia”, que por sua vez vem da palavra grega “logos” e significa pensamento ou discurso racional, conhecimento [3]. Neste caso, a Cosmologia pode ser pensada, a grosso modo, como o estudo do Universo de forma racional. 2Medida de distância cosmológica. Temos que 1 ano-luz = 9.46073 × 10 15m [6]. 20 sido fácil identificarem erroneamente ou descreverem inadequadamente os fenômenos naturais, como eclipses solares e lunares, que por sua vez, neste caso em particular, geralmente eram associados com prenúncios de acontecimentos ruins para diversos povos [7]. A criação do Universo, na antiguidade, era associada aos chamados mitos de criação que de certa forma podem ser considerados os primeiros modelos cosmológicos criados. Devido ao fato de serem baseados em divindades e/ou mitos sobre a criação, atualmente são chamados de modelos cosmogônicos, para diferenciá-los dos modelos baseados no método científico. Esses modelos são baseados na definição de Cosmogonia, que segundo o dicionário Michaelis se trata de um conjunto de teorias, princípios ou doutrinas, com base científica, religiosa ou meramente mítica, que procura explicar e descrever a origem e a formação do Universo. De certa forma, todos os modelos cosmológicos são baseados em Cosmogonia, pois tentam explicar a origem do Universo de acordo com suas concepções. Entretanto, a palavra modelo cosmogônico parece não se atrelar aos modelos cosmológicos atuais, pois estes por sua vez, estão mais interessados em explicações científicas do que associar a criação de tudo à divin- dades e/ou mitos de criação. Deste modo, usualmente, na história, nos referimos a Cosmogonia como sendo um pensamento tipicamente dominante antes do advento da filosofia na Grécia, fomentada sobre mitos e criações divinas do Universo. Apenas a partir do século VI a.C. que podemos dizer que teve o advento da Cosmologia, com uma visão mais racional, baseada em elementos terrenos. Neste presente capítulo, trataremos como modelos cosmogônicos aqueles que são base- ados em mitos da criação em sua construção. Esses modelos são cercados de poesia, pois as mitologias estão em todos os lugares, e nos oferecem exemplos da riqueza dos pensamentos e da cultura de diversos povos. Essa característica, ao meu ver, se tornou a principal nesses modelos cosmológicos primitivos. 3.2.1 Mitos da criação Não temos na história em si uma data que nos diga exatamente quando o homem come- çou a se questionar acerca do Universo que o rodeava. Podemos até, de certa forma, pensar que os Homo sapiens, que viveram a mais de 300 mil anos atrás, também poderiam estar interessa- dos sobre tal assunto. Afinal, o que será que eles pensavam ao olhar para o céu noturno e ver vários pontos luminosos, que hoje conhecemos por estrelas? Enfim, é graças aos povos antigos que, nos dias atuais, podemos ver monumentos in- críveis em nosso planeta, como o círculo de Stonehenge (Figura 3.1(a) [8]), na Inglaterra, que estima-se ter sido construído entre 3000 e 1700 a.C. 3 Associamos o Stonehenge ao estudo as- 3Temos muito mistério cercando a construção e utilidade do Stonehenge, pois evidências arqueológicas atuais mostram que que as pedras chegam a pesar cerca de 30 toneladas, e mais ainda, que algumas destas foram trans- portadas por mais de 150 km, provenientes do país de Gales [9]. Quanto a sua utilidade, o monumento poderia ser usado em cerimônias ou rituais religiosos, mas também é quase certo que poderia ser usado como um tipo de observatório. 21 tronômico, pois existem muitas teorias bem fundamentadas sobre seu uso em observações 4, como na previsão da posição do Sol, da Lua e de Eclipses. Podemos citar também o Crome- leque dos Almendres (Figura 3.1(b) [8]), localizado em Évora, Portugal, que assim como o Stonehenge, também era um círculo de pedras. E além deles temos o Newgrange (Figura 3.1(c) [8]), que se trata de uma espécie tumba, localizada no Condado de Meath na Irlanda, construída de modo que, no solstício de inverno (nascer do Sol do dia mais curto do ano), um raio de sol devidamente colimado por uma pequena abertura ilumina o piso da câmara no final de um longo corredor. Todos esses monumentos citados são considerados megalíticos por terem sido construídos com grandes estruturas de pedras correspondentes ao período neolítico, cerca de 10 mil anos atrás, na pré-história [10, 8]. (a) Stonehenge. (b) Cromeleque dos Almendres. (c) Entrada do Newgrange. Figura 3.1: Alguns monumentos megalíticos associados à fenômenos astronômicos. Fonte: WikiLivros. Além desses monumentos temos registros astronômicos que confirmam o interesse do homem pelo céu desde muitos anos atrás. Esses documentos são datados de aproximadamente 3.000 a.C. e são de autoria de diferentes povos, como os babilônios, egípcios, assírios e chineses. Foi devido a esses registros que a Cosmologia começou a engatinhar, pois esses povos deixaram modelos de universos os quais foram usados pelos gregos para criar a primeira Cosmologia com ideais científicos há mais de 2000 anos, que mais tarde seria a responsável pela descoberta de Ptolomeu, que acarretaria em grandes descobertas para o desenvolvimento da Cosmologia atual. Então vale a pena analisarmos um pouco sobre algumas dessas concepções do Universo concebidas por esses povos antigos. • Cosmologia Maia: A civilização maia surgiu por volta do ano 1800 a.C. na região que hoje compreende algumas regiões da América Central. Esse povo se destaca pelo fato de desenvolverem a Astronomia ao fazer observações dos objetos celestes e com isso 4Podemos citar os trabalhos de Gerald Hawkins em 1965 e de Fred Hoyle em 1972. 22 prever eclipses, tudo a olho nu. Eles cultuavam Vênus de forma religiosa e segundo o que sabemos sobre os maias, o Universo era representado por uma grande e sagrada árvore, que estaria no centro do mundo, com seus ramos sustentando o céu e suas raízes se estendendo pelo submundo [10]. • Cosmologia Babilônica: O povo babilônico viveu na região da Mesopotâmia, entre os famosos rios Tigre e Eufrates. Seus registros são muito sistemáticos devido ao fato de que desenvolveram uma sofisticada matemática de base 60.5 Eles usavam blocos de argila para observações e com isso observavam o movimento das estrelas e planetas. Entretanto, todo esse conhecimento astronômico foi empregado em mitos, pois segundo os registros eles acreditavam que o movimento dos corpos celestes influenciava nos acontecimentos da Terra. Além disso, os babilônios acreditavam que o Universo estaria dividido em 6 níveis, com 3 firmamentos e três terras baseadas em toda a sua cultura religiosa e mítica [10]. • Cosmologia Egípcia: Apesar de serem considerados um povo bem desenvolvido no ramo astronômico, a Cosmologia egípcia não é tão boa quanto a dos babilônios. Para os egíp- cios antigos, o Divino era e é a fonte de toda a sua existência. Ou seja, é através de forças Divinas que o Universo foi criado e está sendo mantido. Neste caso, todos os aspectos acerca do conhecimento do Egito Antigo são creditados aos atributos/aspectos/qualidades de divindades, por eles chamados de neteru (deuses/deusas 6). Para esse povo, a religião era uma tradição de forma que todo o seu conhecimento, baseado na chamada consciência cósmica (ter consciência de que a aplicação de realidades metafísicas na vida diária seria a responsável pelo condicionamento físico), foi incorporado em suas atividades diárias, como agricultura, produções de ferramentas, construções e até mesmo atos de guerras ou jogos. Todas essas atividades tinham uma correspondência divina, isto é, cada ação, não importava quão mundana fosse, era tratada como divina [10, 11]. O estudo do céu para esse povo tinha mais interesses práticos, como a criação do calen- dário. Segundo o que consta na história, os egípcios antigos acreditavam que a Terra era retangular e continha o Rio Nilo no seu centro. O céu era sustentado por colunas nos quatro pontos desse retângulo e para explicar o movimento do Sol recorreram ao deus do Sol Rá, enquanto que as demais estrelas eram apenas sustentadas no céu por cabos fortes, sem nenhuma explicação mítica ou divina para os seus movimentos [10]. • Cosmologia Indiana: A Cosmologia hindu nos remete ao fato do Universo ser cíclico, passando por períodos de criação e destruição, de modo eterno. Para eles o Universo era obra de Brahma, o deus da criação, onde cada dia de sua vida representaria um kalpa 5Essa matemática ainda existe nos dias atuais, como na divisão da circunferência em 360 graus e em como dividimos as horas em 60 minutos e cada minuto em 60 segundos. 6Tradução essa muito comum na literatura, entretanto o correto seria se referir a neteru como os princípios e funções divinas do Deus Uno Supremo do Egito Antigo [11]. 23 (uma criação do Universo), onde ao final de cada kalpa o Universo seria destruído e pos- teriormente criado novamente por Shiva, que anuncia a criação e destruição do Universo, com o uso de um tambor em sua mão direita e uma chama em sua mão esquerda, respec- tivamente [10]. • Cosmologia Chinesa: A Cosmologia chinesa nos remete a um mito que datado do sé- culo 3 a.C. Segundo esse mito, no início existia uma espécie de nuvem chamada de ovo cósmico, devido a sua forma, e os céus e a Terra eram um só até a existência de um ser chamado Pangu, que seria o responsável pela origem do Universo. Com suas habilidades conseguiu separar os céus e a Terra, fazendo com que a porção mais leve, representada pelo Yin, fosse para cima e a mais pesada, representada pelo Yang, fosse para baixo, ge- rando assim o nosso Universo. Após sua morte, o corpo de Pangu origina as montanhas, rios, Lua, Sol e etc. Além disso, os chineses eram considerados precisos em seus regis- tros, uma vez que conseguiram registrar a explosão de uma supernova em 1054 a.C. e inferir com grande exatidão a localização dessa estrela na Nebulosa do Caranguejo [10]. Mapas conceituais sobre essas principais noções de Cosmologia para diferentes povos podem ser encontrados na referência [10]. 3.3 MODELOS GEOCÊNTRICO E HELIOCÊNTRICO Uma outra tentativa de tentar entender como o Universo funcionava reside nos chamados modelos geocêntrico e heliocêntrico. O modelo geocêntrico tinha como principal característica a Terra estática, em torno da qual giravam o Sol, a Lua, e todos os outros corpos celestes existentes no Universo, e de certa forma a Terra seria o centro deste, daí o nome geocêntrico. Essa ideia firmada pelos gregos, principalmente Aristóteles, surgiu devido a interpretação equivocada de que a Terra não se movia, pois não tinham essa impressão. E essa afirmação fazia sentido para com as suas crenças de que a Terra era o centro da criação, e com suas observações do céu [7]. Este modelo estava tão incrustado na época que questioná-lo poderia ser tratado como ato de heresia. No entanto existiam pessoas capazes de tais atos, e um deles chamava-se Nicolau Copérnico que analisou o movimento retrógrado dos planetas e pôs o Sol como o centro do Universo, criando o chamado modelo heliocêntrico. Esse modelo foi corroborado ainda mais com as observações de Galileu Galilei, que foram capazes de nos dizer que a Terra não era o centro do nosso sistema, mas sim o Sol, e que ao redor deste orbitavam os demais planetas do atual sistema conhecido por Solar. Galileu foi condenado por heresia, passando o resto e sua vida preso, sendo perdoado pela igreja só 350 anos após a sua morte [7]. Uma característica em comum em ambos os modelos provém da existência de um quinto elemento, comumente chamado de quintessência, que seria o responsável pela estabilização dos modelos, ou seja um elemento que mantinha os planetas, luas e tudo mais no Universo estáveis. Algo muito parecido com o que fazemos atualmente através da energia e matéria escura. 24 A criação do Universo antigamente era sempre associada a vários mitos de criação de acordo com diferentes povos. Entretanto a Cosmologia, como ciência pode ser dividida em duas eras: a era da Cosmologia Básica e da Cosmologia Moderna. 3.4 COSMOLOGIA BÁSICA E A RELATIVIDADE RESTRITA A Cosmologia básica é baseada na gravitação newtoniana, descoberta por Sir Isaac New- ton (1642-1727) no século XVII, e devido a isso, também é conhecida como Cosmologia New- toniana. A descoberta de Newton é talvez, uma das histórias mais conhecidas da Física, tanto pela sua beleza poética quanto pela lenda que aumentou cada vez mais sobre Newton e a maçã. A história mais concreta a respeito desta descoberta foi relatada em um manual britânico para treinamento de professores de ciência da seguinte maneira [12]: “Um dia Newton estava sentado sob uma macieira em um jardim. Ele viu uma maçã caindo de uma árvore. Veio à sua mente um pensamento de que devia haver alguma razão para a maçã cair no chão e não ir para cima. Assim ele chegou à conclusão de que existe uma força exercida pela Terra que puxa (atrai) todos os objetos para baixo em sua direção. Depois ele deu a essa força o nome de força da gravidade.” Esse acontecimento o fez pensar que, talvez, o poder responsável pela queda da maçã, também atuasse, na Lua, de modo que essa estaria continuamente caindo para a Terra, o que de certa forma a impediria de se afastar7. Assim, ele chegou à conclusão de que existe uma força (gravidade) exercida pela Terra que puxa, ou seja, atrai, todos os objetos para baixo em sua direção [13]. Algumas variantes dessa história dizem que a maçã caiu em sua cabeça e outras que ele na verdade estaria apenas passeando por um jardim. De fato, Newton, com 20 e poucos anos na década de 60, retornou à sua cidade natal para passar um tempo enquanto a instituição na qual estudava, Universidade de Cambridge, estava fechada por causa da peste negra, e foi nessa década que descobriu a teoria da gravidade. Existia apenas uma macieira no jardim de sua casa (local que era e ainda é carinhosamente chamada de Woolsthorpe Manor) localizada no condado de Lincolnshire, na Inglaterra. Essa macieira acabou sendo arrancada por uma forte tempestade em 1816, no entanto suas raízes permaneceram e geraram uma descendente que permanece até os dias atuais [12, 13, 14]. Newton publicou sua descoberta no chamado Principia, sua obra máxima na qual tam- bém consta as suas famosas três leis de movimento dos corpos. No Livro III, Proposição VII e corolário temos o seguinte enunciado para a Lei da Gravitação Universal [15]: 7Ressaltando que Newton já tentava entender porque a Lua não se afastava da Terra e com isso, podemos constatar que todo esse raciocínio não foi de forma alguma instantâneo, devido as circunstâncias da queda da maçã. 25 “A interação gravitacional entre dois corpos pode ser expressa por uma força central, atrativa, diretamente proporcional às massas dos corpos e inversamente proporcional ao quadrado da distância entre eles.” Matematicamente, essa lei é expressa em módulo por: F = Gm1m2 r2 , (3.1) em que G = 6.67428 × 10 −11 Nm 2/kg 2 é a constante gravitacional [6], m1 e m2 são as massas dos corpos e r é a distância entre eles. Essa expressão nos diz que se aumentarmos ou dimi- nuirmos a distância entre os corpos r a força gravitacional irá diminuir ou aumentar de forma inversamente proporcional, respectivamente. Foi graças a Newton que ficou provado as três Leis dos Movimentos Planetários de J. Kepler (1571-1630) [7]. Aplicando esse conceito sobre a Cosmologia poderíamos entender como os objetos as- tronômicos e cosmológicos se comportam no Universo na presença de outros objetos. Entre- tanto, essa concepção acabou se tornando problemática, pois acreditava-se, até o final do século XIX que as posições e movimentos dos objetos no espaço poderiam ser medidos com relação à um referencial absoluto que era imóvel e preenchido com um meio invisível a qual o deram o nome de éter. Em 1887 o experimento do interferômetro de Michelson-Morley, foi proposto na tentativa de se detectar o éter, mas acabou falhando e mais ainda, indicou que a luz sempre viajava à mesma velocidade com relação a um observador em movimento ou não [16]. Esses problemas foram resolvidos apenas em 1905 com Albert Einstein (1879-1955) que formulou a chamada Relatividade Restrita (RR) ou Especial que se baseava em dois princípios ou postulados [16]: Postulado 1: As leis da Física são as mesmas em todos os referenciais inerciais. Postulado 2: A velocidade da luz no vácuo tem o mesmo valor c, independentemente da velocidade do observador ou da fonte que a emitiu. 3.5 RELATIVIDADE GERAL E A COSMOLOGIA MODERNA No início do século XX teve o advento da Cosmologia Moderna, com as descobertas feitas por Einstein no seu trabalho sobre a teoria da Relatividade Geral (RG), uma teoria capaz de descrever a interação gravitacional, considerada uma das quatro interações básicas da natu- reza. A formulação da RG não foi resultado de um experimento que necessitava de explicação mas sim o desejo de se incluir na teoria da relatividade a descrição de todos os fenômenos na- turais. No entanto Einstein, em 1907, não compreendia como a única exceção para atingir este objetivo residia nos fenômenos envolvendo a força da gravidade. Anos mais tarde, no escritório de patentes de Berna, na Suíça, no qual trabalhava lhe ocorreu a seguinte explicação em suas próprias palavras [16, 17]: 26 “Foi então que me ocorreu a ideia mais feliz da minha vida — der glücklichste Gedanke meines Lebens. o campo gravitacional tem apenas uma existência relativa, de modo similar ao campo elétrico gerado pela indução eletromagnética, pois para um observador que cai em queda livre do telhado de uma casa, pelo menos na sua vizinhança imediata, não há campo gravitacional. De fato, se o observador soltar alguns corpos, então estes, em relação a ele, permanecerão em repouso ou em movimento uniforme, independentemente da sua natureza particular ou física (nessa observação, a resistência do ar, naturalmente, é ignorada). Portanto, o observador tem o direito de interpretar seu estado como de repouso.” A partir dessa ideia Einstein elaborou o princípio da equivalência, que se tornaria o postulado básico da RG. Ele pode ser enunciado da seguinte maneira [16]: Um campo gravitacional homogêneo é equivalente, sob todos os aspectos, a um referencial uniformemente acelerado. De acordo com esse princípio podemos concluir que a massa m da 2ª Lei de Newton é equivalente a massa m da Lei da gravitação, embora a formulação clássica da teoria não nos ofereça nenhuma explicação plausível para tal igualdade [16]. Para mais detalhes a respeito do Princípio da Equivalência veja a Ref. [17]. A Cosmologia, antes de Einstein era um ramo da Astronomia pouco consolidado, no qual existiam apenas explicações filosóficas através de teorias e achismos por parte dos cosmó- logos, como são chamados os estudiosos desta área. Estudar o Universo significa compreender o seu funcionamento, seja estudando o seu passado, presente ou futuro afim de obter informações importantes para o conhecimento científico. Esta é uma das principais funções da Cosmologia. Para isto, os cosmólogos utilizam-se de várias ferramentas para seu auxílio, como o telescópio Hubble, satélites e o telescópio espacial James Webb que se trata do maior e mais poderoso telescópio já construído para a ciência espacial, que possivelmente alterará fundamentalmente a nossa compreensão atual do Universo. Todas essas ferramentas fornecem dados importantes para o estudo destes pesquisadores. Demonstrando assim, que o avanço tecnológico foi, e ainda é, fundamental para obtermos informações acerca do Universo [5, 18]. A Cosmologia é um pouco diferente de outras áreas da Física pois ela funciona mais como uma investigação de uma hipótese do que uma ciência experimental. Não temos a liber- dade de experimentar, ou seja, repetir certos experimentos e observar/analisar o que acontece. Neste caso a Cosmologia é uma ciência do tipo observacional. Conseguimos substituir a ex- perimentação na Cosmologia usando a mecânica estatística. Por exemplo, podemos pressupor que o Universo é homogêneo e isotrópico, isto significa dizer que seções/pedaços do Universo possuem o mesmo comportamento, nos proporcionando obter ensembles de volumes peque- nos dentro de um Universo grande, sendo que cada volume pequeno deste se trata de um dado diferente de uma mesma estatística. Naturalmente, quanto maior o volume menor será a possi- bilidade deste feito devido a variância cósmica, que se trata da incerteza estatística inerente às 27 observações do Universo em grandes escalas de distância (entraremos em mais detalhes sobre ela na Seção 3.12.3), que surge no maior volume possível, dada pelo raio de Hubble. Ao anali- sar o maior volume que podemos ver do Universo não conseguimos obter uma repetição deste pois não conseguimos, por exemplo, nos mover 13 bilhões de anos-luz para a direita e analisar novamente o mesmo volume. Mesmo assim, estatisticamente você consegue ter uma noção do que acontece em grandes escalas. A Cosmologia é descrita pela gravitação. Isto se deve a dois motivos: um preconceito teórico 8 no qual é pressuposto a não existência de forças de longo alcance, o que na prática não é verdade 9, podendo ser constatado através de observações. E o outro motivo vem do fato da gravitação ser descrita por uma teoria bem estabelecida atualmente que é a RG. A descrição do Universo é um dos principais objetivos da Cosmologia. Entender a vastidão deste Universo que nos cerca é com certeza uma das maiores fontes de interesse por parte deste ramo fundamentado na interpretação dada pela gravitação apresentada na teoria da Relatividade Geral de Einstein. Mas o que seria a descrição do Universo? Vamos fazer uma analogia. Quando escrevemos as equações de Maxwell para o ele- tromagnetismo do lado direito da igualdade temos termos que possuem alguma relação com a corrente elétrica i, como mostra a Tabela 3.1 elaborada com referência em [19]. Tabela 3.1: Equações de Maxwell na forma integral e diferencial. Nome Forma Integral Forma Diferencial Lei de Gauss ∮ S E · dA = Qint ϵ0 ∇ · E = ρ ϵ0 Lei de Gauss para o magnetismo ∮ S B · dA = 0 ∇ · B = 0 Lei de Ampère-Maxwell ∮ C B · dl = µ0(i + ϵ0 dΦE dt ) ∇ × B = µ0J + µ0ϵ0 ∂E ∂t Lei de Faraday-Lenz ∮ C E · dl = −dΦB dt ∇ × E = −∂B ∂t Esta corrente pode ser descrita de várias formas, como átomos, partículas subatômicas ou densidade de corrente por exemplo. E se puder ser duas coisas, estaríamos de certo modo 8Esse é um preconceito sobre teorias, na qual a comunidade científica acaba desprezando teorias devido ao fato de não serem comprovadas experimentalmente ou não terem um bom embasamento teórico. 9Por exemplo, o caso da força eletromagnética, que é uma força de longo alcance e a desprezamos para estudar o Universo, devido a suposição de que, em média, este é neutro. Logo, supondo que a força mais importante é a gravitação, e que realmente conseguimos constatar que esta descreve bem o Universo, usamos um precon- ceito teórico e acabamos por desprezar as demais forças de longo alcance. Claro que isso tudo pode ser testado posteriormente, sendo que os resultados realmente nos dizem que podemos fazer tais suposições. 28 colocando propriedades diferentes para a corrente, pois fundamentalmente essas formas são di- ferentes. Uma coisa é descrever um átomo, próton, elétron e outra coisa seria descrever uma distribuição contínua de cargas. No eletromagnetismo isto é possível, dar descrições fundamen- talmente diferentes para a corrente no lado direito das equações de Maxwell. Esta afirmação se deve ao fato de que a estrutura destas equações é linear e teorias lineares tem uma vasta gama de simplificações associadas a elas. Como trabalhar com a média destas teorias. Ou seja, digamos que ao invés de lidarmos com cargas pontuais queremos lidar com uma distribuição contínua de cargas. Analisar as médias não muda a estrutura de uma teoria. Sair de uma descrição pon- tual para uma contínua não altera minha teoria se esta for linear. Na RG isto não é possível, pois trata-se de uma teoria de fato não-linear, que possui uma relação com a renormalização de um campo. Digo isto pois existem teorias não lineares que aceitam um princípio de super- posição 10, que serve para escrever conjuntos completos de soluções e entender a teoria como um todo. Com a renormalização de um campo não estamos atribuindo novos operadores de interação, pois ela está ligada intimamente a não linearidade, ao passo que em teorias lineares o surgimento de novos termos pode ocorrer ao realizarmos tal renormalização. Todas essas discussões foram expostas apenas com o intuito de se concluir que em suma, a Cosmologia Moderna, com seus alicerces fundamentados sobre a RG, é o pontapé inicial para as tentativas atuais de descrição do Universo. 3.6 CONCEITO DE ESPAÇO-TEMPO E A MATEMÁTICA NA RELATIVIDADE GERAL Os conceitos de espaço e tempo são de grande importância para a humanidade e, ao longo dos anos, com o avanço científico e tecnológico, foram se aprimorando até as suas defini- ções atuais. Pensamos no espaço como um tipo de entidade que possui três dimensões (largura, altura e espessura), enquanto que o tempo seria uma espécie de fluxo, que guia a evolução do nosso Universo. Essas duas entidades podem trabalhar juntas como um tipo de palco, onde se dá a peça que descreve o Universo. Os atores, nesta analogia, se tratam de tudo o que existe no Universo, como planetas, estrelas e seres terrestres ou não. Para Newton esse palco seria o mesmo para qualquer observador, atuando ou não nesta peça [21]. Entretanto, para Einstein, em sua teoria da Relatividade Geral usando resultados da RR, o espaço e tempo deixam de ser o palco para se tornarem atores da própria peça. Neste caso, como atores da peça, o espaço e tempo passam a ser tratados como um contínuo de quatro dimensões, chamado espaço-tempo, que seria uma espécie de união entre essas duas entidades. A presença de matéria e energia consegue de certa forma modificar este “ator”. Sendo mais formal a presença de estrelas e galáxias curvariam a estrutura do espaço-tempo [21]. 10Este é o princípio segundo o qual, em muitos casos, o efeito total pode ser calculado somando-se efeitos parciais. A superposição ocorre quando pegamos uma combinação linear de soluções. No caso da gravitação, esse princípio pode ser aplicado no cálculo da força gravitacional total a que uma partícula está sujeita na presença de outras partículas. Neste caso, para obtermos a força total sobre uma partícula basta somarmos vetorialmente as forças que as outras partículas exercem sobre ela [20]. 29 Deste modo, o espaço e o tempo estão intimamente ligados. Assim, quando dois eventos ocorrem em locais separados, o espaço entre eles é ambíguo devido ao fato de que observadores viajando a diferentes velocidades irão medir distâncias diferentes. Além disso, o tempo entre cada evento também depende de cada observador. Só por meio de artifícios matemáticos que podemos determinar a separação entre os eventos de tal forma que a combinação entre espaço e tempo nos forneça valores sobre os quais todos os observadores concordam [22]. Segundo a RG de Einstein, os efeitos gravitacionais são incluídos em sistemas físicos através de uma métrica gµν(x) e, se conhecemos a distribuição de matéria e energia, o conjunto de equações que nos fornece essa métrica é chamado de Equações de Einstein (EE) [21]: Gµν = 8πG c4 Tµν, (3.2) onde G é a constante gravitacional, c = 2.99792458 × 10 8 m/s é a velocidade da luz no vácuo [6], e Tµν é o tensor energia-momento que se trata do objeto mais geral que carrega a informação da massa, energia e momento do sistema em estudo. As EE podem ser interpretadas como o espaço-tempo dizendo à matéria como se com- portar e a matéria dizendo como o espaço-tempo deve se curvar. Do lado esquerdo da igualdade dada em 3.2 temos o chamado tensor de Einstein: Gµν = Rµν − 1 2gµνR, (3.3) que carrega a informação sobre a curvatura do espaço-tempo em análise. Além da métrica gµν, esse tensor é definido em termos de dois outros objetos: o tensor de Ricci11 Rµν definido como: Rµν ≡ Rρµρν = ∂ρΓ ρ µν − ∂νΓ ρ ρµ + Γρ ρλΓ λ νµ − Γρ νλΓλ ρµ, (3.4) e o escalar de curvatura R: R = Rµµ = gµνRµν = gµν(∂ρΓ ρ µν − ∂νΓ ρ ρµ + Γρ ρλΓ λ νµ − Γ ρ νλΓ λ ρµ), (3.5) em que Rρµρν é o tensor de Riemann que descreve a curvatura intrínseca do espaço e é utilizado para construir outros tensores que auxiliam na descrição da curvatura do espaço-tempo na RG [23], e 11A interpretação geométrica do tensor de Ricci é que ele descreve o quanto um elemento de volume difere no espaço curvo em comparação com o espaço euclidiano ou plano. Diferentes componentes do tensor de Ricci descrevem como o elemento de volume evolui à medida que se move ao longo de uma geodésica (curva que conecta dois pontos no espaço) em qualquer direção. Em resumo o o tensor de Ricci então, em certo sentido, lhe diria quanto o volume de um elemento no espaço cartesiano, por exemplo, muda quando vai para um espaço riemanniano, que seria o espaço curvo representado numa variedade Riemanniana, que nada mais é do que um espaço onde os vetores da base podem variar de um ponto para outro. Fisicamente ele descreve o quanto o volume do espaço-tempo de um objeto muda devido às marés gravitacionais na RG. Isso ocorre porque, geometricamente, o tensor de Ricci descreve as mudanças de volume devido à curvatura do espaço-tempo, e esta por sua vez é equiparada às forças de maré [23]. 30 Γ ρ νµ = 1 2gρλ(∂νgµλ + ∂µgνλ − ∂λgνµ), (3.6) é o símbolo de Christofell ou a conexão afim, que em resumo representam os coeficientes de conexão da conexão Levi-Civita. Em um sentido geométrico, eles descrevem mudanças nos vetores de base ao longo de um determinado sistema de coordenadas. Fisicamente, os símbolos de Christoffel representam forças fictícias induzidas por um referencial não inercial [23]. Resolver as EE, Eq. 3.2, significa resolver dez equações diferenciais parciais não li- neares em gµν e isso não é nada fácil (lembrando que estamos considerando o espaço-tempo quadridimensional e o fato de que o tensor de Einstein é simétrico, já que impomos que o tensor energia-momento é simétrico 12). Como já mencionado essa equação nos permite obter a mé- trica gµν quando conhecemos o tensor energia-momento do sistema físico em estudo. Só assim conseguimos determinar como o espaço-tempo tem sua estrutura geométrica modificada pela presença de matéria [21]. Em seus estudos Einstein introduziu a chamada constante cosmológica (Λ) estabele- cendo a noção de um Universo estático, de curvatura positiva (Universo fechado) e, portanto, finito. Essas duas características pareciam concordar plenamente com a física da época, pois realmente a visão que os cientistas tinham do universo era que este era estático e o fato deste ser finito evitava problemas da existência de grandezas infinitas nas condições de contorno, algo que era indesejável para qualquer teoria física. A constante foi introduzida justamente para con- trabalancear os efeitos de atração da gravitação. Em outras palavras enquanto que a gravitação atraia as galáxias, planetas, dentre outros objetos celestes, a constante cosmológica os repelia com a mesma intensidade atrativa. Devido a isso o Universo em sua grandiosidade tendia a ser estático [25]. Com Λ as EE são escritas como: Gµν + Λgµν = 8πG c4 Tµν. (3.7) 3.7 HOMOGENEIDADE E ISOTROPIA DO UNIVERSO: O PRINCÍPIO COSMOLÓGICO Nas equações representativas da RG, não sabemos o que se trata o termo à direita da igualdade de tais. Este é o chamado Fitting Problem (Problema de Ajuste). Então o máximo que fazemos é supor termos do lado direito. Na descrição do Universo, do ponto de vista prá- tico, precisamos de simplificações. Como não temos uma teoria de soluções completas da RG, não conseguimos lidar com a teoria em toda a sua complexidade. Logo, usando preconceitos 12Tratamos isso como uma espécie de postulado pois um tensor de energia-momento simétrico é necessário, no contexto da RG, quando os campos de matéria são acoplados à gravidade. Isso é claro pois as EE relacionam um tensor de curvatura simétrico G µν à T µν e neste caso as equações seriam inconsistentes entre si se T µν não fosse também simétrico. Além disso, é fácil provar que realmente T µν é simétrico. A referência [24] mostra essa propriedade utilizando um argumento físico: basta considerar as tensões de cisalhamento sobre um cubo muito pequeno de aresta L e massa-energia igual a T 00L 3 que inevitavelmente ele teria aceleração angular infinita caso o tensor não fosse simétrico. 31 teóricos podemos estender toda a teoria para um patamar descritível. Ao fazer isto estamos dizendo que não existe um lugar privilegiado no Universo, dizendo de certo modo que este é homogêneo. A partir disso, junto com a noção de isotropia, temos o chamado Princípio Cosmológico que se trata de um pressuposto básico para se estudar Cosmologia. Segundo este princípio “O Universo em grande escala é homogêneo e isotrópico” e, atualmente, nenhuma contestação foi capaz de contradizer esta afirmação. Como sabemos o Universo é inomogêneo, pois a densidade no ar é diferente da que está dentro de uma pessoa, por exemplo. Esta afirmação de algo ser homogêneo ou inomogêneo surge acompanhada de uma afirmação de escala, ou seja, uma coisa inomogênea numa certa escala pode ser homogênea em outra. Observe a Figura 3.2. A retina, ao observamos um copo d’água, faz a função janela 13 de tal forma que um ponto observado na água se trata de uma média de vários outros pontos. O observador verifica que a água é homogênea quando a observa dentro do copo transparente, mesmo se ele estiver observando apenas um ponto da água. No entanto, se ele usasse um mi- croscópio, por exemplo, enxergaria pontos pequenos, que representam moléculas que formam a estrutura da água, nessa mesma porção e constataria que esta é inomogênea. Ou seja, na Figura 3.2, temos a descrição homogênea na imagem a esquerda e inomogênea à direita, de acordo com a escala observada pela retina ou um zoom no mesmo ponto observado por ela, respectiva- mente. Para analisar estatisticamente dados obtidos de um satélite ou telescópio por exemplo, temos que saber qual a função janela de tais objetos. Figura 3.2: Retina fazendo a função janela na observação de um ponto do copo d’água. Fonte: Autor. 13A função janela trata-se basicamente de como é feita a coleta de dados e o seu agrupamento para obtenção de uma determinada média estatística. 32 A homogeneidade no Universo se aplica de maneira semelhante a do exemplo no copo d’água. Como dito o Universo é homogêneo em grande escala. Para entender melhor isto vamos acompanhar as seguintes ilustrações que comparam representações de diferentes porções espaciais do Universo. A unidade de medida ly apresentadas nas Figuras se trata da abreviação em inglês de ano-luz (light-year) [26]. Na Figura 3.3 podemos visualizar a nossa galáxia (Via Láctea - Milk Way em inglês) cer- cada por outras galáxias pequenas, comumente chamadas de galáxias satélites14 da Via Láctea [26]. Figura 3.3: O Universo dentro de 500.000 anos-luz: As galáxias satélites da Via Láctea. Fonte: Richard Powell. Ao nos afastarmos em torno de 1 milhão de anos-luz da Via Láctea podemos observar novas galáxias cujo nome recebido é de Grupo Local como ilustrado na Figura 3.4 [26]. Ainda mais distante, em torno de 10 milhões de anos-luz de afastamento em relação à Via Láctea podemos observar outras galáxias compondo tal ilustração. Aqui surge o superaglo- merado de Virgem como visto na Figura 3.5 [26]. Prosseguindo com o afastamento, temos que em torno de 100 milhões de anos-luz po- demos observar outros superaglomerados como ilustrado na Figura 3.6 [26]. E por fim, chegamos na escala de 1 bilhão de anos-luz de afastamento da Via Láctea, a qual vemos o Universo em grande escala. Veja a Figura 3.7 [26]. Estas figuras apresentadas representam até onde podemos enxergar no Universo tendo como centro a nossa galáxia. Fica então visível o uso da homogeneidade no estudo do Universo, pois esta significa distribuição uniforme de matéria. Quanto mais nos afastamos da Via Láctea mais encontramos a mesma quantidade de galáxias (e também de aglomerados e superaglome- rados de galáxias), em todas as partes do Universo. Claro que isto tudo tratando o mesmo em 14Em termos gerais, uma galáxia pode ser considerada satélite quando esta orbita uma outra galáxia de tama- nho maior, que neste aspecto pode ser chamada de galáxia primária. Esse fenômeno ocorre devido a interação gravitacional que existe entre as galáxias menores com a maior [27]. 33 Figura 3.4: Ilustração de 1 milhão de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. Figura 3.5: Ilustração de 10 milhões de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. grande escala. Se pegarmos apenas uma pequena porção de qualquer uma das Figuras 3.3, 3.4, 3.5 e 3.6 certamente não encontraremos homogeneidade nesta distribuição de matéria. Contudo na Figura 3.7, temos certeza que observamos homogeneidade até mesmo em partes dela, pois se trata da maior distância observada do nosso Universo em grande escala [26]. As figuras ilustrativas das galáxias são uma aproximação para o que já obtivemos com a sonda espacial Hubble, que se trata neste caso de um dos objetos que faz a função janela na Cosmologia. Dessa forma, com as observações do Hubble, além de outras observações do céu com outros instrumentos 15, podemos constatar que existe homogeneidade em nosso Universo. 15O Sloan Digital Sky Survey (SDSS) fornece evidências de que a distribuição de galáxias é homogênea em 34 Figura 3.6: Ilustração de 100 milhões de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. Figura 3.7: Ilustração de 1 bilhão de anos-luz de afastamento da Via Láctea. Fonte: Richard Powell. Em outras palavras podemos entender a homogeneidade como uma maneira de dizer escalas superiores a 300 anos-luz de distância; consulte [28] para mais informações. 35 que não há lugar privilegiado, ou especial, no Cosmo, todos fazemos parte de um todo [29]. Outro aspecto importante no Princípio Cosmológico é o de que o Universo é isotrópico em grande escala. A isotropia é um termo que nos fala da ausência de direções privilegiadas [29]. Considere a seguinte analogia para entendermos melhor como funciona a isotropia no Universo: Imaginemos o céu azul e sem nuvens de um dia ensolarado. Podemos encontrar várias regiões de igual luminosidade, com o mesmo brilho, mostrando o mesmo tom de azul no céu. Entretanto, existe uma região muito mais brilhante, exatamente onde está o Sol no céu. Isso significa que não percebemos isotropia com relação à luminosidade em todo o céu de um dia ensolarado, porque há uma direção onde existe um brilho muito mais intenso em comparação com outras regiões. Na direção do Sol o brilho é mais intenso que em outra região, ou seja não existe isotropia no céu ensolarado do exemplo [26]. Mas onde podemos observar a isotropia no Universo? Na escala apresentada na Figura 3.7 observamos isotropia devido ao fato de que em grande escala não existe direção privilegiada. Considere a radiação nesta escala, ela é a mesma vinda de todas as direções. Com relação a matéria também temos a mesma afirmação, pois observamos a mesma distribuição e quantidade desta pelo Universo [26]. Logo, concluímos que o espaço possui propriedades isotrópicas, isto é, o espaço não possui direção. Se você for um astronauta, por exemplo e pegar como referência você flutuando no espaço e queira ir para a direita, estaria dando uma direção para sua trajetória, porém isto não quer dizer que você está na direção direita do espaço, pois não existe direções nele. No entanto, em pequena escala fica visível que o brilho do Sol é mais intenso na direção deste. Aí está, considere sempre, isso mesmo, sempre em grande escala o estudo do Universo. Para entender de forma definitiva tal aspecto do Universo considere um observador si- tuado em qualquer ponto de um Universo isotrópico. Este deve observar, em qualquer direção que olhe, a mesma distribuição de matéria. Logo, este Universo também é homogêneo [26]. Mas isso não quer dizer que em um Universo homogêneo podemos observar isotropia, ou seja se um brilho é mais homogêneo no Sol do que em outro ponto afastado deste estaremos dando uma direção para tal afirmação e tirando a isotropia do Universo. Logo, fica evidente que a homogeneidade é uma consequência da isotropia. Considere como experimento mental um Universo no qual estruturas vermelhas estão homogeneamente distribuídas como representado na Figura 3.8 baseada em [26]. Este Universo não possui o mesmo aspecto se for observado de direções diferentes. No caso, observá-lo na direção de A é diferente de observá-lo nas direções B ou C (veja a Figura 3.9 baseada em [26]). Neste aspecto ao “corrermos” o observador sobre a direção A, veremos uma linha, ao passo que se “corrermos” esse mesmo observador na direção de B e C veremos pontos, pois não teremos noção de altura nessas direções, e como sabemos uma linha reta projetada no plano é um ponto. Caso nosso Universo hipotético seja formado por estruturas pontuais, como as da Figura 36 Figura 3.8: Representação de um Universo homogêneo e não isotrópico. Fonte: Autor. Figura 3.9: Representação de um Universo homogêneo e não isotrópico, observado em diferen- tes direções. Fonte: Autor. 3.10 baseada em [26], teremos então um Universo homogêneo e isotrópico, pois ao observamos esse Universo nas direções A, B e C (aplicando a isotropia) obteríamos os mesmos aspectos (ou seja, sua homogeneidade). Este é um exemplo de Universo que obedece o Princípio Cosmoló- gico [26]. É importante notar, também, que a homogeneidade como consequência da isotropia só é válida se a isotropia for válida para qualquer ponto do Universo em questão [26]. Isto é, se em 37 Figura 3.10: Representação de um Universo homogêneo e isotrópico. Fonte: Autor. qualquer direção que eu observe meu Universo em qualquer ponto deste eu obtenho os mesmos aspectos então, o meu Universo é homogêneo. Essa característica fica ainda mais evidenciada ao analisarmos a principal função da sonda espacial Planck, que seria a observação da Radiação Cósmica de Fundo (do inglês Cosmic Microwave Background abreviada para CMB). A CMB, em poucas palavras, se trata de uma radiação proveniente das primeiras épocas do Universo, uma assinatura dos estágios iniciais deste quando matéria e luz começaram a seguir caminhos independentes. Essa radiação é uniforme de tal modo que ela indica o caminho que as pertur- bações da matéria tomaram desde o início do Universo, e assim essa foi uma maneira pela qual os cosmólogos inferiram, ao observar o mapa da CMB (Figura 3.11 [30]), que o Universo é isotrópico e, portanto, homogêneo. Figura 3.11: CMB observada pelo Planck. Fonte: European Space Agency (ESA) - Planck. Em resumo o Princípio Cosmológico é fundamental para termos um alicerce no estudo da Cosmologia e enquanto ele não for refutado o Universo ao qual pertencemos é homogêneo e isotrópico (por conveniência se diz homogêneo antes de isotrópico, apesar da homogeneidade 38 ser consequência da isotropia). 3.8 MODELOS COSMOLÓGICOS Partindo da formulação da RG de Einstein podemos conceber os chamados modelos cosmológicos. Como já mencionamos, diferentemente dos modelos cosmogônicos, os modelos que trataremos nessa seção são baseados no método científico, sem associações com mitos de criação. Os modelos cosmológicos são modelos criados para descrever o principal objeto de es- tudo na Cosmologia, o Universo, por meio de equações matemáticas. Em outras palavras, um modelo cosmológico é uma representação matemática que procura descrever o Universo obser- vável, bem como a sua história. Neste aspecto os modelos cosmológicos seriam a receita de bolo da Cosmologia, ou seja, se queremos estudar Cosmologia, antes de mais nada precisare- mos de bons argumentos matemáticos para descrevê-la. Daí o nome modelo cosmológico, pois é um modelo criado com o objetivo de explicar matematicamente a Cosmologia. Para que um modelo cosmológico seja bem aceito pela comunidade científica ele deve satisfazer algumas exigências que o definiriam como um bom modelo descritivo. Elas podem ser elencadas da seguinte maneira, sem ordem de importância: I. O modelo deve estar de acordo com o conhecimento físico atual. II. O modelo deve explicar algum fenômeno já conhecido. III. O modelo deve ser capaz de prever os resultados de determinados experimentos ainda não realizados ou explicar os problemas que o modelo padrão não explica. Outros critérios são por vezes levados em consideração, mas estes são os mais impor- tantes e os mais notáveis na criação de um modelo cosmológico. O primeiro modelo cosmológico foi desenvolvido por Albert Einstein em 1917, dois anos após a formulação da teoria da RG. Segundo Einstein o Universo deveria ser estático e para tal introduziu uma constante em suas equações chamada de constante cosmológica, que seria a responsável pela estaticidade 16 do Universo. Alguns anos depois, em 1922, A. Friedmann trabalhou com a hipótese de um Universo em expansão. Posteriormente, em 1927, o físico e padre belga G. Lemaître [31] ajustou o mo- delo de Friedmann. Os dados obtidos por Edwin Hubble, em 1929, provaram que de fato o Universo estaria em expansão. Deste modo, o modelo cosmológico de Einstein, apesar de bem fundamentado, não conseguia explicar a expansão do Universo e por isso não estava de acordo com o que sabíamos. Anos mais tarde, em 1935, os físicos americanos H. P. Robertson (1903- 1961) [32] e A. G. Walker (1909-2001) [33], com a suposição de homogeneidade e isotropia, chegaram a constatação de que a métrica estudada por Friedmann e Lemaître era única, ou seja, 16Qualidade ou condição de algo estático. 39 o que Robertson e Walker fizeram foi mostrar que esse era o modelo mais geral consistente com homogeneidade e isotropia [33]. Devido a isso atualmente nos referimos à métrica do Universo que obedece o PC como métrica de Friedmann-Lemaître-Robertson-Walker (FLRW). Esse foi o pontapé inicial para o Modelo Cosmológico Padrão (MCP), aceito atualmente, também cha- mado de ΛCDM, onde a letra maiúscula Λ representa a energia escura que nesse caso é atribuída à constante cosmológica e CDM vêm do inglês Cold Dark Matter (Matéria Escura Fria). Esse modelo depende da densidade de matéria no Universo e admite alguns pressupostos para sua validade que são: I. O Universo é homogêneo, o que significa que ele tem as mesmas propriedades em toda a sua escala cosmológica. II. As leis da Física são universais e as mesmas onde quer que estejamos. III. O Universo é isotrópico e está em expansão. No entanto, este modelo nos diz que o Universo era mais denso e quente no passado, o que por sua vez originou o chamado modelo do “Big Bang”. Em suma esse modelo usa três parâmetros que caracterizam totalmente a evolução do universo. Esses parâmetros são: I. O parâmetro de Hubble que caracteriza a taxa de expansão do universo. II. O parâmetro de densidade de matéria que mede a razão entre a densidade de matéria do universo e uma certa densidade chamada densidade crítica relacionada à constante de Hubble. III. A constante cosmológica que representa uma força oposta à gravitação. Existem outros modelos cosmológicos, como o de quintessência ou ϕCDM [34], Flat ϕCDM, XCDM, wCDM, FlatwCDM, FlatΛCDM, ΛCDM + nrun + r, ΛCDM+LO e νCDM, dentre outros, que possuem suas próprias características e propósitos, mas apresentam alicerces sobre o MCP, que é o mais consistente com as observações atuais [35]. 3.9 PARÂMETROS COSMOLÓGICOS Esta seção tem como principal objetivo definir alguns dos principais parâmetros cosmo- lógicos que são usados para caracterizar diferentes modelos cosmológicos. 3.9.1 O parâmetro e a constante de Hubble O parâmetro de Hubble, também chamado de lei ou função de Hubble, é a taxa norma- lizada de expansão em um tempo t: 40 H(t) ≡ ˙a(t) a(t), (3.8) onde ˙a é a derivada com relação ao tempo do fator de escala a ≡ a(t) definido em um tempo t. Essencialmente, o parâmetro de Hubble nos fornece a escala do Universo apesar de em si não tratar de movimento. Na verdade o que representa o movimento no Universo é o fator de escala, pois ele me diz como as distâncias variam e com isso eu posso calcular a velocidade e consequentemente a aceleração dessas variações na distância, ou seja, é através do fator de escala que obtemos a velocidade e aceleração do Universo. Graficamente (ver Figura 3.13 feita com base em [36]) podemos interpretar o parâmetro de Hubble como sendo a inclinação em um dado tempo da curva representativa da curvatura do Universo. Esses gráficos que representamos na Figura 3.13 são gerados quando considera- mos a dinâmica de um Universo descrito pela métrica de FLRW e analisamos um dos casos particulares de era do Universo (matéria, por exemplo como representado na figura). O parâmetro de Hubble medido “hoje” (redshift z = 0) é a chamada constante de Hub- ble, denotada por H0, e definida como [29]: H0 = H(t0) = ( ˙a a ) t=t0, (3.9) onde o fator de escala a(t) é medido no tempo hoje t0, também chamado de tempo do observa- dor. Neste caso podemos também adotar a notação a0 ≡ a(t0) para o fator de escala hoje. A constante assim como o parâmetro carregam o nome em homenagem a Edwin Powell Hubble (1889-1953), embora haja uma discussão na comunidade científica acerca disso. Segundo o que consta na história, Georges-Henri Édouard Lemaître (1894-1966) [31] deduziu teoricamente es- ses parâmetros em uma lei mais simples que a forma dada na Eq. 3.8, chamada hoje de lei de Hubble ou Hubble-Lemaître, que discutiremos mais posteriormente, enquanto que Hubble a inferiu a partir de observações [37]. Logo, muitos trabalhos na literatura acabam por se referir a constante e parâmetro de Hubble sob o nome de parâmetro e constante de Hubble-Lemaître. Destacamos a medida obtida pelo grupo responsável pelas análises dos dados da sonda espacial Planck, que tem como objetivo principal o mapeamento da CMB. Neste aspecto o valor teórico da constante de Hubble é obtido com os mapas da CMB que por sua vez são produzidos a partir da modelagem da evolução de sua radiação. A colaboração Planck (2018) mede des- ses mapas da CMB dados referentes aos espectros de anisotropia de temperatura e polarização que são extremamente bem ajustados por um modelo ΛCDM de apenas 6 parâmetros. Embora alguns parâmetros sejam derivados das medições CMB com precisão extremamente alta, tais medidas não nos fornecem uma medida direta de H0. Em vez disso, eles fornecem uma res- trição indireta – com uma incerteza muito pequena – mas apenas sob a hipótese deste modelo cosmológico de 6 parâmetros [38]. Supondo ΛCDM com seções espaciais planas a colaboração 41 Planck infere um valor da constante de Hubble de [1]: H0 = 67.36 ± 0.54 km s −1 Mpc −1. (3.10) Essa medida em particular é apresentada como sendo a melhor do Planck [1] com 68% de confiança, combinada com as verossimilhanças de TT (espectro de potência da temperatura), TE (espectro de potência da temperatura e polarização nos modos E) e EE + LowE + lenteamento (combinação entre o espectro de potência nos modos E com baixos valores de l nos modos E mais lenteamento da CMB) — para mais detalhes sobre os espectros de potência da CMB veja [39, 40, 41] ou acesse o site de autoria de Wayne Hu 17. Uma outra medida foi obtida no trabalho de Brout et al. [42] em uma colaboração do Pantheon+ e da equipe SH0ES para a análise de 1701 curvas de luz de 1550 distintas Supernovas do tipo Ia (SNe Ia) entre os redshifts 0.001 < z < 2.26, considerando um Universo com seções espaciais planas (Ωκ = 0) para o modelo ΛCDM, dada por: H0 = 73.4 ± 1.1 km s −1 Mpc −1. (3.11) Se trata de uma medida dependente de modelo, que vem de um ajuste simultâneo da constante de Hubble junto com parâmetros cosmológicos como w. Foi a primeira vez que o Pantheon+ realizou esse tipo de ajuste [42]. A análise consiste em se utilizar altos redshifts de SNe Ia na criação de um diagrama de Hubble, do qual é extraído a medida da constante de Hubble bem como outros parâmetros cosmológicos. Para mais detalhes sobre os passos envolvidos para a criação desse diagrama veja a parte do webnario apresentada por Dan Scolnic em [43] ou a Ref. [42], e discutiremos mais sobre as análises de SNe Ia na Subseção 4.1.2. As medidas de SNe Ia da equipe SH0ES foram englobadas na medida do Pantheon+ SH0ES, no entanto elas são mais precisas vistas separadamente. A medida de H0 realizada pela equipe SH0ES formada por Riess et al. [44] acabou sendo bem mais precisa que a sua medida anterior realizada em 2019 [45] ao usar observações de Cefeidas em 42 galáxias hospedeiras de SNe Ia na calibração da constante de Hubble. Ambas foram feitas com o Hubble Space Telescope (HST) que obteve para a medida mais recente citada H0 = 73.04 ± 1.04 km s −1 Mpc −1, (3.12) que inclui incertezas sistemáticas e está próximo da mediana de todas as variantes de análise. Essa é uma medida de escada de distância local (do inglês local distance ladder), pois utiliza um sample de 42 galáxias hospedeiras de SNe Ia do Pantheon+ (ver a Tabela 6 da Ref. [44]) — daí o nome “local”. Esse método consiste em se utilizar a geometria do efeito paralaxe 18 na 17http://background.uchicago.edu/~whu/index.html 18O efeito paralaxe surge devido ao movimento da Terra ao redor do Sol, que gera uma mudança aparente na posição de estrelas próximas em comparação com estrelas de fundo que estão em maiores distâncias. Essa mudança dura em torno de um ano e apresenta uma amplitude angular no céu que é dada pela distância Terra-Sol 42 calibração de Cefeidas, que por sua vez são usadas na calibração de SNe Ia na mesma galáxia hospedeira para uma medição mais confiável da sua distância relativa [46] e então pode-se usar as SNe, mais precisamente os seus redshifts para calibrar H0 (ver Figura 3.12). Se trata de uma medida independente de modelo, pois esse método é quase completamente insensível ao modelo cosmológico de fundo, só não é completamente devido ao fato da ancoragem ser feita com Cefeidas em z = 0 [47]. Geometria: Efeito Paralaxe Cefeidas SNe IaH0 1ª Etapa 2ª Etapa 3ª Etapa Figura 3.12: Etapas de inferência da medida de H0 realizada pela equipe SH0ES. Os resultados que usam o método de calibração de Supernovas e o valor teórico obtido com a análise da CMB diferem em cerca de 10%, o que é um grande problema, pois parece que o método de Supernovas prevê que o Universo se expanda a uma taxa 10% mais rápida do que o método da CMB. Consideramos esse um dos maiores problemas da Cosmologia Moderna, o qual chamamos na literatura de tensão de Hubble, pois são valores que não dependem muito de suposições sobre modelos cosmológicos para H0 em torno de 73 km s −1 Mpc −1 [48]. Para mais detalhes sobre a tensão de Hubble veja as referências [49, 50, 51], a referência [52] apresenta na sua tabela 1 valores de |H0 − H 0| > 2.8 obtidos ao longo dos anos até 2021, em que H0 = 68.26 km s −1 Mpc−1. A priori os valores de H0 (o estimado a partir da CMB e o estimado com SNe Ia) não tem que ser coincidentes. Se isso ocorrer então o modelo cosmológico adotado é ainda mais corroborado pois ele sobrevive a um teste extremamente forte. Em contrapartida se eles não coincidem existem três explicações possíveis para essa discrepância nos valores: existem erros nos dados ou erros nas análises19, ou se ambas as medições são precisas, essa diferença está nos revelando algo novo sobre o Universo e existe um erro no modelo [43]. Assim, os cosmó- logos precisam procurar novos métodos para se calcular distâncias no Universo para resolver esse problema. Uma das propostas de solução para a tensão de Hubble consiste no uso de uma energia escura primordial como discutido em [53]. Outra solução se encontra numa nova téc- em razão da distância relativa da estrela [46]. 19Neste caso, o valor de H0 inferido pelo Planck apresenta 5σ (1 em 3.5 milhões) de tensão com a previsão de H0 feita por Riess et al., sem indicação de que a discrepância surge das incertezas de medição ou variações de análise [44]. 43 nica desenvolvida por uma equipe de astrônomos da Universidade de Chicago liderados por Wendy Freedman. Em seu trabalho usam estrelas chamadas de gigantes vermelhas para me- dir distâncias. Essas estrelas estão no final de suas vidas e consequentemente passam por um evento chamado de flash de hélio quando atingem uma certa luminosidade, o que resulta em uma diminuição do brilho da estrela. Como existe um limite para o qual a luminosidade de uma estrela gigante vermelha não pode ficar mais brilhante, os astrônomos conseguem então comparar as gigantes vermelhas da Via Láctea com as de outras galáxias para determinar as suas distâncias. Esse método, chamado de TRGB (do inglês Tip of the Red Giant Branch), é preciso e apresenta uma boa acurácia20, e é paralelo, mas independente da escala de distância de Cefeidas. Apresentou até o momento um valor para a constante de Hubble de [38]: H0 = 69.8 ± 0.8 km s −1 Mpc −1. (3.13) Apesar de não ser um valor próximo para resolver a tensão de Hubble, nos revela uma nova possibilidade de aprender mais sobre o nosso Universo. 3.9.2 Tempo de Hubble Também podemos notar, no gráfico representado na Figura 3.13, o chamado tempo de Hubble, tH, definido pelo inverso da constante de Hubble: tH = 1 H0 , (3.14) que é representado graficamente pelo intervalo delimitado entre o ponto em que a curva tangente intercepta o eixo do tempo t e o ponto t0. O tempo de Hubble pode ser calculado facilmente se lembrarmos que 1 Mpc = 3.086 × 10 19 km. Deste modo a constante de Hubble em termos de unidades temporais é: H0 = 67.4 ± 0.5 3.086 × 1019 ± 0 [ km s −1 km ] = 2.18 × 10 −18s −1, (3.15) em que usamos a medida do Planck para a constante de Hubble. Neste caso a incerteza na medida pela propagação de erro é aproximadamente zero21. Logo o tempo de Hubble é dado por: tH = 1 2.18 × 10−18 = 4.59 × 10 17s. (3.16) 20A acurácia é a soma da exatidão e da precisão. Isso significa que os resultados do experimento não devem ser apenas precisos. Mas também deve estar próximo da referência ou valor real utilizado como base [4]. 21Temos que o Mpc é uma unidade bem estabelecida, portanto não apresenta erro. Mas devemos considerá-lo como zero para o cálculo da propagação do erro dessa divisão. Lembrando que a propagação do erro de uma divisão é dada por: Pdiv = X ± σX Y ± σY = X Y ± X Y ( σX X + σY Y ) . 44 Como 1 ano = 3.154 × 10 7 s, podemos estimar que o tempo de Hubble em anos é de aproxi- madamente 1.45 × 10 10 anos (basta fazermos uma regra de três) que é aproximadamente 14 bilhões de anos, assim como mostra o gráfico da Figura 3.14 baseado na referência [54]. Podemos interpretar o tempo de Hubble como uma estimativa aproximada da medida da idade do universo, assumindo que a taxa de expansão permaneceu constante [4]. Isso fica mais claro se imaginarmos o cenário em que o parâmetro de Hubble está sendo reproduzido como um filme ao contrário. Agora, ao invés das galáxias se afastarem uma das outras com o avanço do tempo, temos que elas iriam se aproximar uma das outras a medida que o tempo vai para trás. Deste modo, no passado, elas estariam muito mais próximas do que estão agora. Extrapolando essa ideia ainda mais teríamos o início do Universo, onde toda a matéria que compõe as galáxias estaria concentrada em apenas um único ponto, num instante de tempo no passado. Se levarmos em conta a observação de que a expansão do Universo está acelerando podemos obter uma determinação precisa da verdadeira idade do Universo. 0 t0 tH k = −1 H0 k = +1 k = 0 a(t) t a0 Figura 3.13: Representações gráficas das possíveis dinâmicas do Universo para o caso de ma- téria e representação da reta tangente a curvatura do Modelo Cosmológico Padrão que se trata do parâmetro de Hubble. No ponto em que t = t0 temos a constante de Hubble. Fonte: Autor. Outra maneira de se determinar essas quantidades é fazendo uso de uma perspectiva da física elementar. Considerando que a distância d percorrida por um corpo em velocidade v constante em um determinado intervalo de tempo é dada pela equação da cinemática: d = vt. (3.17) Analogamente, essa noção pode ser estendida para a constante de Hubble, tendo em vista que se trata de uma quantidade com dimensão inversa ao tempo de acordo com 3.15. Podemos 45 50 H ( km s −1Mpc −1) 60 70 80 90 100 10 8 12 14 16 18 20t(bilhõesdeanos) Figura 3.14: Gráfico do tempo em bilhões de anos em função do parâmetro de Hubble. No ponto em que H = H0 = 67.4 km s −1 Mpc −1 temos a idade estimada do Universo, que é aproximadamente t = 14 bilhões de anos. Fonte: Autor. então expressar a constante de Hubble como [29]: v = H0d. (3.18) Essa expressão é a forma mais conhecida da lei de Hubble ou lei de Hubble-Lemaître. Ela expressa uma correlação direta entre a distância de uma galáxia e sua velocidade de recessão conforme determinado pelo redshift. Dividindo ambos os lados dessa igualdade por H0 obte- mos: v H0 = d. (3.19) E substituindo a Eq. 3.19 em 3.17 temos o tempo de Hubble: v H0 = vt ⇒ tH = 1 H0 . (3.20) Os mesmos procedimentos podem ser feitos considerando-se o parâmetro de Hubble ao invés da constante. 3.9.3 Distância ou comprimento de Hubble Assim como o tempo de Hubble fornece uma escala de tempo natural para a expansão do Universo, a distância ou comprimento de Hubble, definido como [29, 6]: DH = c H0 = 299792458 ± 0 67.4 ± 0.5 = 4448 ± 33 Mpc, (3.21) 46 onde c, a velocidade da luz no vácuo, tem um valor exato [6] e usamos a relação 1 m = 1000 km para a conversão de km para m na sua unidade de medida de H0. DH em unidades de anos-luz é aproximadamente 14 bilhões de anos-luz, o que nos diz que o valor numérica do comprimento de Hubble é, por definição, igual ao tempo de Hubble em anos. A Eq. 3.21 é a distância de Hubble hoje, para z = 0. Em outro caso, temos que DH é dado por uma forma análoga como: DH(z) = c H(z). (3.22) 3.9.4 Redshift (Desvio para o vermelho) O redshift (desvio para o vermelho) é fundamental para determinarmos a constante de Hubble, pois assim conseguimos obter várias informações sobre o Universo, como sua idade, tamanho, velocidade de expansão, se está acelerando, desacelerando, expandindo, contraindo e até seu possível fim. Usamos o redshift por que é uma grandeza de medida de grandes distâncias (acima de 300 milhões de anos-luz). Seu símbolo matemático é dado na literatura por z e ele é medido pela seguinte equação: z = λobs − λe λe , (3.23) onde λobs e λe são os comprimentos de onda observado por um observador e emitido por uma fonte. Sabendo que no Universo o comprimento de onda emitido difere proporcionalmente do comprimento de onda observado por um fator a(t), ou seja: λe = a(t)λobs, (3.24) onde t é o tempo no qual houve a emissão do redshift. Neste caso, substituindo 3.24 em 3.23 obtemos: z = 1 a(t) − 1. (3.25) No tempo hoje, temos que t = t0 (tempo de emissão é equivalente ao tempo do obser- vador) e z = 0, pois a luz chega de forma instantânea neste caso. Logo concluímos que o fator de escala hoje é: a(t0) ≡ a0 = 1. (3.26) Na literatura temos uma forma conveniente de se reescrever o redshift (Eq. 3.25) em termos do fator de escala 22: 22Neste caso podemos escrever tanto a(t) quanto a(z) tendo em vista que z ≡ z(t). 47 1 + z = 1 a(t), (3.27) ou z = a(t0) − a(t) a(t) , (3.28) de acordo com o resultado encontrado em 3.26, que se for substituído em 3.28 nos dá novamente a expressão 3.25. Vamos reescrever esta expressão numa forma mais conveniente dada por: z + 1 = a0 a . (3.29) 3.9.5 Distância comóvel em termos do redshift A distância comóvel é definida pelo comprimento de uma curva de tempo cosmológico constante que conecta dois pontos distantes e arbitrários no Universo. Matematicamente ela é dada por: Dc = a0r. (3.30) Como r é a trajetória dada por r = c ∫ t0 te dt ′ a(t′), (3.31) então, a Eq. 3.30 fica, substituindo a Eq. 3.31, na forma: Dc = a0c ∫ t0 te dt ′ a(t′). (3.32) Vamos agora reescrever esta equação em termos do redshift, pois é a grandeza de nosso interesse para determinar a constante de Hubble, já que esta carrega várias informações a res- peito do Universo. Devemos reescrever a integral em dt ′ em termos de dz′ numa troca de variáveis simples, ou seja devemos trocar dt por dz nesta integração. A princípio vamos partir da Eq. 3.29, derivando ambos os lados desta igualdade com relação a t, obtemos: d dt (z + 1) = d dt ( a0 a ) , (3.33) dz dt = a0 d dt ( 1 a ) , (3.34) 48 onde a0 = a(t0) não depende de t, por isso é tratado como constante. A derivada do lado direito pode ser calculada pela regra da derivada do quociente, obtendo: dz dt = a0 ( d(1) dt a − 1 d(a) dt a2 ) = −a0 a2 da dt . (3.35) Vamos usar a notação de Newton e reescrever a Eq. 3.35 como: ˙z = −a0 a2 ˙a = −a0 a ˙a a. (3.36) Reescrevendo em termos do parâmetro de Hubble: ˙z = −a0 a H(t) ⇒ dz dt = −a0 a H(z). (3.37) onde H(t) ≡ H(z) devido ao fato das quantidades t e z representarem as mesmas coisas, no caso o tempo, só que com proporções diferentes. O mais preciso, mas que raramente faze- mos na física, seria escrever H(t(z)), só que por compactação escrevemos ou H(t) ou H(z), subentendendo que H(t(z)) = H(t) = H(z). Que reescrevemos como: dz = dt ( − a0 a H(z) ) ⇒ dt = − dz a0 a H(z). (3.38) Em termos de linha (não interprete como derivadas) esta equação fica: dt ′ = − dz′ a0 a(t′)H(z′). (3.39) Os limites de integração na Eq. 3.32 também são alterados. Para isso analisamos o redshift (Eq. 3.29) onde: t0 → z0, te → ze. Usando o redshift: z0 + 1 = a0 a0 ⇒ z0 = 0, (3.40) onde a(t) = a(t0) no redshift o que significa que estamos “observando as coisas hoje\"no fator de escala. E, 49 ze + 1 = a0 a(t) ⇒ ze = z = a0 a(t) − 1, (3.41) permanece com a mesma forma, pois o fator de escala a(t) é o observado na emissão. Substituindo as relações 3.39, 3.40 e 3.41 na Eq. 3.32 obtemos: Dc = a0c ∫ t0 te dt ′ a(t′) = −a0c ∫ z0=0 ze=z 1 a(t′) dz′ a0 a(t′)H(z′). (3.42) Simplificando a(t ′) e a0 que sai da integração por ser tratada como constante: Dc = c ∫ z 0 dz′ H(z′), (3.43) onde foi usado o sinal negativo a frente da integral para inverter a ordem de integração23. Mul- tiplicando H(z′) por H0/H0 temos Dc = c ∫ z 0 dz′ H(z′)H0 H0 = c H0 ∫ z 0 dz′ H(z′) H0 . (3.44) Sabendo que, por definição: E(z) := H(z) H0 ⇒ E(z′) := H(z′) H0 (3.45) é o parâmetro de Hubble normalizado, podemos reescrever a Eq. 3.44 como: Dc(z) = c H0 ∫ z 0 dz′ E(z′), (3.46) que se trata da distância comóvel em termos do redshift. Durante este presente trabalho apresentamos outras distâncias cosmológicas, como as distâncias de luminosidade, angular de diâmetro, volumétrica. Entretanto são quantidades que podem ser estimadas em termos de Dc e/ou dados cosmológicos. 3.10 DINÂMICA DO UNIVERSO EM EXPANSÃO: A MÉTRICA FLRW E AS EQUAÇÕES DE FRIEDMANN Os conceitos de homogeneidade e isotropia do Universo aplicados em grande escala ne- cessitam de uma boa interpretação matemática para serem fortemente evidenciados. Para isto a Cosmologia estuda a geometria diferencial que descreve a gravitação. Um dos usos da geome- tria diferencial está no caso em que podemos atribuir uma métrica pseudo-riemanniana a cada ponto do meu espaço-tempo. Na mecânica hamiltoniana e lagrangiana, por exemplo podemos 23Esse sinal surge pelo fato de que z aumenta quando “andamos” pro passado enquanto que t aumenta quando “andamos” para o futuro. Ou seja quanto mais no passado maior será o valor de z. 50 utilizar geometria diferencial para descrever os mesmos problemas e situações envolvendo vín- culos, coordenadas generalizadas e etc. Então a geometria diferencial trata-se basicamente de um novo modo de se expressar a matemática que está envolvida em sistemas geométricos. Em suma a geometria diferencial utiliza técnicas de cálculo em problemas geométricos, ou seja estuda a geometria usando o cálculo. Um dos seus principais usos está na teoria da RG. Podemos equipar o espaço com uma métrica, que seria um objeto matemático que des- creve a distância dAB entre dois pontos A = (xA, yA, zA) e B = (xB, yB, zB) no R3 (como mostra a Figura 3.15) dada pela expressão: dAB = √ (xB − xA)2 + (yB − yA)2 + (zB − zA)2. (3.47) Essa métrica é um caso particular de Universo de tipo plano, pois ela é usada para medir distâncias no R N . No entanto, podemos ter outros tipos de Universos que não são descritos por espaços do tipo R N , que são o hiperbólico e o esférico que também obedecem o PC. A métrica nesses outros casos é um tipo de extensão da métrica no espaço plano. y z x O A B dAB Figura 3.15: Representação de uma medida de distância entre dois pontos, A e B, no espaço tridimensional euclidiano. Fonte: Autor. O que fazemos agora é medir o tamanho de cada vetor velocidade da curva, e ao fazer- mos uma integral de linha somando cada contribuição dos comprimentos dos vetores obtemos então a distância final entre dois pontos, que se trata da minha métrica no espaço em análise. Esta métrica é bem mais complicada, primeiramente por causa da sua integração, que não é trivial. E ainda, por que não sabemos qual o tipo de curva que conecta dois pontos no espaço. No espaço plano usamos uma reta, e sem saber estamos utilizando uma geodésica. Em uma hipersuperfície esférica existem sempre duas geodésicas ligando os dois pontos, a de ca- minho direto e caminho indireto. Já em um toro podemos ter infinitas geodésicas ligando dois 51 pontos na sua superfície. O único conceito que sobra das métricas de estudo no plano são os locais, tanto que a gravitação apresentada por Einstein na RG se trata de um conceito local, ela nos diz o que acontece com a dinâmica da métrica, que intuitivamente se trata da medida do tamanho de vetores que compõem uma curva, em um ponto do meu Universo. A RG, por ser uma teoria métrica, estuda a dinâmica das medidas representadas pelas métricas. No caso de um Universo homogêneo e isotrópico as métricas são restritas, existem apenas algumas compatíveis com as suas seções espaciais. Já demos um exemplo de incom- patibilidade de uma métrica no Universo, quando propusemos a distância entre dois pontos no R3. Precisamos descrever nosso Universo como uma variedade, escolher uma maneira de como fatiar ele (ou seja, fazer uma folheação do Universo), no qual cada fatia terá uma métrica com- patível com a homogeneidade e isotropia e analisar as dinâmicas restantes. Na RG o que sobra de dinâmica destas folhas é representado pelas equações de Friedmann. Na descrição do Universo verdadeiro que é inomogêneo usamos um tipo de suposição: Existem dois Universos, o ideal descrito pela métrica de Friedmann (gµν′) e o real que possui uma métrica (gµν) na qual a diferença entre a métrica real e a ideal (gµν − gµν′) é pequena hipoteticamente. Aplicando teoria de perturbação nessa métrica pequena, isto é um dinâmica sobre ela, na prática se tivéssemos uma instabilidade estaríamos tratando de um Universo ino- mogêneo. Entretanto isso não ocorre, então a instabilidade na métrica pequena não existe, demonstrando que o Universo é homogêneo e isotrópico. Assim, podemos assumir o PC, nos restringindo a métrica de Friedmann-Lemaître- Robertson-Walker (FLRW) dada por: ds 2 = −c2dt 2 + a2(t) [ dr2 1 − κr2 + r2(dθ2 + sen 2θdφ2) ] , (3.48) em que a(t) é o fator de escala que expressa a evolução espacial do Universo. E ainda, κ = {−1, 0, 1} é a constante de curvatura que representa um Universo com seção espacial hiperbólica (κ = −1) ou de geometria aberta, plana (κ = 0) ou de geometria plana e esférica (κ = +1) ou de geometria fechada (ver Figura 3.16). Deste modo, com a suposição de homo- geneidade e isotropia, tudo o que precisamos saber sobre a geometria do nosso Universo está contida no fator de escala a(t), na constante de curvatura κ e no chamado raio de curvatura r. Se κ ̸= 0 então o raio de curvatura é r = R0 (raio de curvatura no momento presente), mas se κ = 0 então r = ∞ [29]. Desses Universos apenas o esférico é finito em extensão. A princípio poderíamos partir de um ponto em uma direção e, com tempo suficiente, se retornar a este mesmo ponto de partida. Em contrapartida, nos outros dois tipos de Universos, hiperbólico e plano, nunca se poderia retornar ao mesmo ponto de partida. Posteriormente introduziremos a chamada densidade de matéria que está intimamente conectada também às geometrias de Universos. Enfim, se o espaço tem curvatura negativa (κ = −1), dizemos que ele é hiperbólico (Figura 3.16(a)) e neste caso, não existiria massa suficiente para interromper a expansão do 52 (a) Geometria hiperbólica (κ = −1) . (b) Geometria plana (κ = 0) . (c) Geometria esférica (κ = +1). Figura 3.16: Representação da geometria dos tipos de Universo. Fonte: Autor. Universo. Assim, o Universo seria ilimitado e se expandiria para sempre como mostrado no gráfico da Figura 3.13. Por outro lado, se o espaço não tem curvatura (κ = 0), estaríamos tratando de um Universo plano (Figura 3.16(b)), onde existiria massa suficiente para interromper a expansão do Universo, mas somente após um tempo infinito. Neste caso, o Universo ainda seria ilimitado e também se expandiria eternamente, mas apresentando uma taxa de expansão que gradualmente decai para zero após t = ∞ como mostra no gráfico da Figura 3.13 E, em último caso, se a curvatura do espaço é positiva (κ = +1), o nosso Universo seria esférico (Figura 3.16(c)), e teria muito mais massa do que este poderia sustentar, de modo que cessaria a expansão do Universo e mais ainda, faria com que esta se transformasse numa contração, devido ao fato deste tipo de Universo ser finito. Assim, eventualmente, as galáxias iriam parar de se afastar umas das outras, bem como outros objetos cosmológicos com massa, e começariam a se aproximar até um ponto no qual o Universo colapsaria sobre si mesmo, no chamado Big Crunch. O gráfico que representa esse tipo de Universo pode também ser visualizado na Figura 3.13. No contexto da RG, a partir das EE (considerando Λ - Eq. 3.7), admitindo o PC que nos dá como métrica do espaço-tempo a métrica de FLRW, temos como caso particular as cha- madas Equações de Friedmann, que neste aspecto representam um conjunto de equações que descrevem a expansão métrica do espaço em modelos governados pelo PC. Essas equações fo- ram apresentadas pelo matemático e cosmológo russo Alexander Friedmann (1988-1925) em 53 1922 as quais foram deduzidas prevendo a existência de um Universo em expansão [31]. Ao assumir uma curvatura positiva para o Universo, Friedmann determinou duas equações diferen- ciais que conectavam a evolução temporal do Universo à densidade de matéria e à constante cosmológica [55]. Devido a isso, as equações carregam o seu nome. Elas são representadas matematicamente por: ( ˙a a )2 = 8πGρ + Λ 3 − κc 2 a2 (3.49) e 3 ¨a a = Λ − 4πG ( ρ + 3p c2 ) . (3.50) Uma boa dedução dessas equações pode ser encontrada na referência [34], onde partindo da métrica de FLRW, que obedece o PC, podemos obter, após um certo trabalho algébrico, as equações de Friedmann como caso particular das EE. Usando a primeira equação, temos que a segunda equação pode ser expressa como (con- siderando Λ = 0): ˙ρ = −3H ( ρ + p c2 ) , (3.51) que é a chamada equação da continuidade ou equação do fluido e representa a conservação da massa-energia. Esse é um dos modos de derivarmos a Eq. 3.51. No entanto, poderíamos partir da definição do tensor energia-momento para o caso de um fluido perfeito, que é um fluido ou gás que (I) se movimenta pelo espaço-tempo com uma quadrivelocidade u = uµ que pode ou não variar de um evento para outro, e (II) exibe uma densidade de massa-energia ρ e uma pressão p do tipo isotrópica 24 em cada ponto de elemento do fluido [24]. Neste caso em particular temos o seguinte tensor energia-momento [24]: Tµν = (ρ + p)uµuν + pgµν. (3.52) Neste caso, qualquer fluido que possa ter sua energia-momento expressadas na forma dada pela Eq. 3.52 é considerado um fluido perfeito. Considerando o caso de um observador em repouso e a conservação de Tµν obtemos como resultado a equação da continuidade. Assim, podemos concluir que ela é uma expressão mais geral. Logo, parece que o nosso modelo é bem mais independente do que parece, devido ao fato de que nem mesmo precisaríamos utilizar as EE para para o seu desenvolvimento, tendo em vista que usamos nesse trabalho apenas a Eq. 3.51, como veremos na Seção 5.5. Porém isso é apenas uma coincidência teórica (jargão usado 24Uma pressão é dita isotrópica quando ela age igualmente em todas as direções de um fluido. Em outras palavras, isso quer dizer que em todas as direções de um dado fluido temos a mesma pressão. 54 pelo autor), já que nosso modelo está interessado mais na independência de modelo no que diz respeito a energia escura. Além do mais para análise dos resultados relacionados ao parâmetro de Hubble do nosso modelo, que veremos posteriormente, usamos uma das Eqs. de Friedmann, logo essa coincidência teórica para por aqui. Supondo que a energia escura é um fluido do tipo barotrópico, i.e. um fluido cuja den- sidade de energia depende apenas da sua pressão, podemos associá-la a uma equação de estado da forma: p = wρ, (3.53) em que w é um parâmetro que representa a equação de estado da energia escura em nosso tra- balho (w = p/ρ). Neste caso, estamos considerando a energia escura como um fluido perfeito e barotrópico que satisfaz a conservação do tensor energia-momento ∇µT µν = 0. Essa é uma de- finição um pouco tola, pois não conseguimos medir a pressão da energia escura, que é negativa, mas o que realmente nos interessa é a forma com que esta evolui com o tempo. A independência de modelo no nosso caso é muito sutil e está restrita apenas a hipótese de que a energia escura é um fluido barotrópico que satisfaz a conservação do tensor energia-momento para um fluido perfeito. A equação 3.53 nos diz que cada componente do Universo possui uma equação de estado que relaciona sua pressão e densidade de energia. Substituindo as Eqs. 3.8 e 3.53 na Eq. 3.51 obtemos: ˙ρ + 3 ˙a aρ(1 + w) = 0. (3.54) Reescrevendo essa expressão e integrando: 1 ρ dρ dt = −3(1 + w) 1 a da dt ∫ t t0 dρ(t ′) ρ(t′) = −3(1 + w) ∫ a a0 da ′ a′ . (3.55) Essas integrais resultam na função logaritmo natural de ambos os lados: ln [ ρ(t) ρ(t0) ] = −3(1 + w){ln[a(t)] − ln[a(t0)]}. (3.56) Temos que ln[a(t0)] = ln[1] = 0 de acordo com 3.26. Podemos então reescrever o lado direito da igualdade em 3.56 usando a propriedade de potenciação da função ln como: ln [ ρ(t) ρ(t0) ] = ln[a−3(1+w)]. (3.57) Elevando ambos os lados à exponencial de Euler obtemos de acordo com a definição da função logaritmo natural o seguinte resultado: 55 ρ(a) = ρ0a−3(1+w), (3.58) onde ρ0 ≡ ρ(t0) é uma constante que possui um valor para cada componente do Universo em questão. Nesse caso o mais correto seria reescrever 3.58 como: ρi(a) = ρi0a−3(1+w), (3.59) em que i representa o tipo de componente do Universo. De acordo com essa expressão temos três tipos de casos de dominação de densidade do Universo, na literatura também chamados de eras, que são:    w = 0 ρM (a) = ρM 0a−3 (Matéria) w = 1/3 ρR(a) = ρR0a−4 (Radiação) w = −1 ρΛ(a) = ρΛ0a0 (Constante cosmológica) (3.60) No nosso trabalho queremos que os dados nos digam qual a melhor representação para a equação de estado da energia escura. Os resultados apresentados em 3.60 são teóricos e nossa abordagem é independente de modelo. Podemos reescrever a primeira equação de Friedmann (Eq. 3.49) dada em termos de uma densidade total ρT como: H 2 = 8πGρT 3 − κc 2 a2 , (3.61) onde consideramos novamente Λ = 0 e: ρT = ∑ i ρi = ρM + ρR + ρDE (3.62) onde ρM (densidade de matéria), ρR (densidade de radiação) são dados pelas duas primeiras expressões em 3.60 e ρDE (densidade da energia escura) é dado pela forma genérica (Eq. 3.58). Então a Eq. 3.61 fica: H 2 = 8πG 3 [ ρM 0a−3 + ρR0a−4 + ρDE0a−3(1+w)] − κc 2 a2 . (3.63) Sabendo que o fator de escala em termos do redshift é dado por: a(z) = (1 + z) −1, (3.64) podemos reescrever 3.63 em função do redshift como: 56 H 2 = 8πG 3 [ ρM 0(1 + z) 3 + ρR0(1 + z) 4 + ρDE0(1 + z)3(1+w)] − κc 2 (1 + z)−2 = 8πG 3 [ ρM 0(1 + z) 3 + ρR0(1 + z) 4 + ρDE0(1 + z)3(1+w) − 3κc 2 8πG (1 + z) 2] . (3.65) Definindo: ρκ0 = −3κc 2 8πG , (3.66) obtemos: H 2 = 8πG 3 [ ρκ0(1 + z)2 + ρM 0(1 + z) 3 + ρR0(1 + z) 4 + ρDE0(1 + z)3(1+w)] . (3.67) Dividindo ambos os lados dessa igualdade por H 2 0 : H 2 H 2 0 = 8πG 3H 2 0 [ ρκ0(1 + z) 2 + ρM 0(1 + z)3 + ρR0(1 + z) 4 + ρDE0(1 + z) 3(1+w)] . (3.68) O termo em fração fora do colchetes se trata do inverso da densidade crítica que é definida como: ρc0 = 3H 2 0 8πG. (3.69) onde o subescrito 0 na densidade crítica se refere a densidade crítica hoje dada em termos da constante de Hubble H0. Logo a Eq. 3.68 pode ser reescrita como: H 2 H 2 0 = ρκ0 ρc0 (1 + z)2 + ρM 0 ρc0 (1 + z) 3 + ρR0 ρc0 (1 + z) 4 + ρDE0 ρc0 (1 + z)3(1+w). (3.70) Por definição temos que as densidades adimensionais Ω são dadas por 25: Ωi = ρi ρc0 e Ωi0 = ρi0 ρc0 , (3.71) onde Ωi são as densidades adimensionais ou parâmetros de densidade, tal que i = κ, M, R, DE 25Alguns autores na literatura definem uma função Ω(t) = ρ/ρc para ρc também como função do tempo, enquanto que em nosso trabalho usamos Ω para representar a densidade em um instante arbitrário dividida pela densidade crítica hoje [56]. Em outras palavras essa densidade se trata de uma escala geométrica, onde a ordem numérica é aproximadamente 1, coisa que não teríamos se fosse usada ordens dimensionais. Por exemplo, algum número muito grande de g/cm 3 de matéria escura não nos dá nenhuma informação interessante que possa ser estudada, mas em termos da densidade crítica podemos ter muitas informações a seu respeito, como a sua saturação, etc. 57 representa a curvatura, matéria, radiação e energia escura, respectivamente. Usando essa expressão e o quadrado da função de Hubble normalizada (Eq. 3.45) temos: E2(z) = Ωκ0(1 + z) 2 + ΩM 0(1 + z) 3 + ΩR0(1 + z) 4 + ΩDE0(1 + z)3(1+w). (3.72) Extraindo a raiz quadrada de ambos os lados: E(z) = √ Ωκ0(1 + z)2 + ΩM 0(1 + z)3 + ΩR0(1 + z)4 + ΩDE0(1 + z)3(1+w). (3.73) Ou em termos de Ωi: E(z) = √ Ωκ(z) + ΩM (z) + ΩR(z) + ΩDE(z), (3.74) onde: Ωi(z) = Ωi0(1 + z)j, (3.75) tal que j = 2, 3, 4, 3(1 + w), respectivamente para i = κ, M, R, DE. Aqui cabe uma discussão mais detalhada a respeito dos parâmetros de densidade. Temos que o parâmetro de densidade de curvatura Ωκ pode ser obtido a partir da relação 3.75: Ωκ(z) = Ωκ0(1 + z) 2 ⇒ Ωκ = ρκ0 ρc0 1 a2 , (3.76) em que Ωκ ≡ Ωκ[a(z)] e foi substituído a segunda relação de 3.71 e a Eq. 3.27. Basta agora substituir as Eqs. 3.66 e 3.69 em 3.76: Ωκ = −3κc 2 8πG 3H 2 0 8πG 1 a2 = − κc 2 a2H 2 0 . (3.77) Podemos definir, analogamente, o parâmetro de densidade de curvatura em um instante de tempo t qualquer como: Ωκ = − κc 2 a2H 2(t), (3.78) em que a única diferença das manipulações algébricas feitas anteriormente é que ρc0 (densidade crítica hoje) é substituída pela densidade crítica em qualquer instante de tempo t denotada por ρc e definida neste caso, analogamente a Eq. 3.69, como: ρc = 3H 2(t) 8πG . (3.79) 58 Em ambas as Eqs. dadas em 3.77 e 3.78 temos que κ = {−1, 0, 1} é a constante de curvatura que representa os tipos de Universos: hiperbólico, plano e esférico como já mencionamos. Assim, de acordo com a Eq. 3.78, temos as seguintes conclusões a respeito do tipo do Universo em termos de Ωκ:    Ωκ = 0 para: Universo plano (κ = 0) Ωκ < 0 para: Universo esférico (κ = +1) Ωκ > 0 para: Universo hiperbólico (κ = −1) (3.80) A densidade de matéria ΩM é definida a partir de 3.75 como: ΩM (z) = ΩM 0(1 + z) 3 ⇒ ΩM = ρM 0 ρc0 1 a3 , (3.81) em que ΩM ≡ ΩM [a(z)] e usamos novamente a segunda relação de 3.71 e a Eq. 3.27. De acordo com 3.60 temos que: ρM 0 = ρM a3. (3.82) Então, substituindo as Eqs. 3.82 e 3.69 em 3.81, obtemos: ΩM = ρM a3 3H 2 0 8πG 1 a3 = 8πG 3H 2 0 ρM . (3.83) Analogamente, para um tempo t qualquer, temos: ΩM = 8πG 3H 2(t)ρM . (3.84) Com a descoberta da existência de mais massa no Universo do que se observava, foi proposta a matéria escura na tentativa de se explicar tal característica sobre a massa no Universo (discutiremos mais sobre isso na Seção 3.11.2). Assim, o parâmetro de densidade de matéria ΩM passou a ser decomposto em dois parâmetros: ΩM = Ωc + Ωb, (3.85) e que Ωc e Ωb são os parâmetros de densidade de matéria escura (o subescrito c vêm do inglês cold — frio) e matéria bariônica, respectivamente. Deste modo podemos usar a Eq.3.75 da mesma forma como: ΩM (z) = ΩM 0(1 + z) 3 ⇒ Ωc(z) + Ωb(z) = Ωc0(1 + z) 3 + Ωb0(1 + z) 3 (3.86) Por comparação é evidente que: Ωc(z) = Ωc0(1 + z) 3 e Ωb(z) = Ωb0(1 + z) 3. (3.87) 59 Usando a segunda relação de 3.71 tal que i = cdm, b para ambas as Eqs. de 3.87, respectiva- mente, bem como a Eq. 3.27 obtemos: Ωc = ρcdm0 ρc0 1 a3 e Ωb = ρb0 ρc0 1 a3 . (3.88) em que Ωc ≡ Ωc[a(z)] Ωb ≡ Ωb[a(z)], e usamos o subescrito cdm de cold dark matter na densidade da matéria escura, apenas para não haver confusão com a densidade crítica em um tempo qualquer, denotada como ρc na Eq. 3.79. Basta agora usarmos a Eq. 3.82 que é dividida em dois termos como: ρcdm0 + ρb0 = ρcdma3 + ρba3, (3.89) de modo que por comparação sabemos que ρcdm0 = ρcdma3 e ρb = ρba3. Logo, as Eqs. dadas em 3.88 ficam na seguinte forma: Ωc = ρcdma3 ρc0 1 a3 = ρcdm ρc0 e Ωb = ρba3 ρc0 1 a3 = ρb ρc0 . (3.90) Substituindo a Eq. 3.69 nas equações dadas em 3.90 obtemos: Ωc = 8πG 3H 2 0 ρcdm e Ωb = 8πG 3H 2 0 ρb. (3.91) Finalmente, substituindo essas relações na Eq. 3.85 obtemos o parâmetro de densidade de matéria decomposto em termos das densidades dos dois parâmetros Ωc e Ωb: ΩM = 8πG 3H 2 0 (ρcdm + ρb), (3.92) De forma análoga, para um tempo t qualquer, temos: ΩM = 8πG 3H 2(t)(ρcdm + ρb). (3.93) Os parâmetros de densidade de matéria escura e densidade de matéria bariônica podem ser calculados se tivermos a constante de Hubble H0 e as suas densidades ρcdm e ρb. Entretanto isso depende de dados observacionais que são coletados com uso de instrumentos de medida bem calibrados para uma boa precisão. Afim de ilustrar os cálculos analiticamente vamos con- siderar os parâmetros cosmológicos do Planck reproduzidos da Ref. [1] na Tabela 3.2. Deste modo, para este caso podemos facilmente calcular Ωc usando-se a seguinte relação: Ωch 2 = 0.1200 ± 0.0012, (3.94) em que: h = H0 (100 km s −1 Mpc−1), (3.95) 60 Tabela 3.2: Alguns dos parâmetros cosmológicos do Planck considerando Ωκ = 0 (Universo plano) com 68% de confiança combinado com TT, TE, EE+lowE+lenteamento. Parâmetro Valor e erro H0[km s −1 Mpc −1] 67.36 ± 0.54 Ωbh 2 0.02237 ± 0.00015 Ωch 2 0.1200 ± 0.0012 ΩΛ 0.6847 ± 0.0073 ΩM 0.3153 ± 0.0073 é o chamado parâmetro h que descreve a incerteza no valor da constante de Hubble [57]. Con- siderando o valor de H0 medido pelo Planck (Tabela 3.2) temos: h = 67.36 ± 0.54 km s −1 Mpc−1 100 ± 0 km s −1 Mpc−1 ≈ 0.674 ± 0.005 (3.96) Podemos calcular h 2 usando esse resultado como: h 2 = [0.674 ± 0.005] 2 = (0.674 ± 0.005)(0.674 ± 0.005) = 0.454276 ± 0.007, (3.97) em que usamos a propagação de erro do produto 26 para o cálculo da incerteza no erro de h 2. Portanto, substituindo esse resultado na Eq. 3.94 obtemos a densidade de matéria escura como aproximadamente: Ωc = 0.1200 ± 0.0012 0.454276 ± 0.007 = 0.2641 ± 0.007 ≈ 0.26. (3.98) Já o parâmetro de densidade de matéria bariônica Ωb é dado pela seguinte relação: Ωbh 2 = 0.0224 ± 0.0001. (3.99) 26A propagação do erro do produto é dada por: Pprod = (X ± σX )(Y ± σY ) = XY ± XY ( σX X + σY Y ). 61 Substituindo 3.97 nessa expressão obtemos: Ωb = 0.0224 ± 0.0001 0.454276 ± 0.007 = 0.049 ± 0.001 ≈ 0.05. (3.100) Deste modo podemos constatar, de acordo com a Eq. 3.85, que o parâmetro de densidade de matéria é dado aproximadamente por: ΩM = (0.2641 ± 0.007) + (0.049 ± 0.001) = 0.3131 ± 0.008 ≈ 0.31, (3.101) em que usamos a propagação de erro da soma 27. Observe que nosso resultado obtido analitica- mente é aproximadamente igual a medida do Planck de acordo com a Tabela 3.2. Podemos desconsiderar a contribuição do parâmetro de densidade de radiação ΩR ≡ ΩR[a(z)] para a descrição atual do Universo, pois o MCP considera ΩR ≈ 0, (3.102) ou seja, desprezível diante à densidade de matéria e energia escura [58] do Universo. E por fim, temos o parâmetro de densidade da energia escura: ΩDE = ΩDE0(1 + z)3(1+w), (3.103) em que w ≡ w(z) é a equação de estado da energia escura em nosso trabalho, que tem por objetivo investigar o comportamento dessa equação com o método de reconstrução e o modelo proposto. Para o MCP (ΛCDM) temos que w = −1 e a Eq. 3.103 é reescrita como: ΩΛ = ΩΛ0(1 + z)3(1−1) = ρΛ0 ρc0 1 a0 , (3.104) em que usamos as Eqs. 3.71 e 3.27. De acordo com as relações dadas em 3.60 temos que ρΛ0 = ρΛ e substituindo a Eq. 3.69 em 3.104 obtemos: ΩΛ = 8πG 3H 2 0 ρΛ. (3.105) Analogamente para um tempo t qualquer: ΩΛ = 8πG 3H 2(t)ρΛ = Λ 3H 2(t), (3.106) 27A propagação de erro de uma soma é dada pela seguinte expressão geral: Psoma = (X ± σX ) + (Y ± σY ) = (X + Y ) ± (σX + σY ). 62 em que definimos [59]: Λ = 8πGρΛ. (3.107) Na literatura, para a colaboração do Planck, temos que ΩΛ tem valor estimado de [1]: ΩΛ = 0.6847 ± 0.0073 ≈ 0.69, (3.108) que é determinado a partir da seguinte relação: ΩΛh 2 = 0.3107 ± 0.0082. (3.109) ou a partir de Λ = (4.24 ± 0.11) × 10 −66eV2 = (2.846 ± 0.076) × 10 −122m 2 P l onde mP l é a massa de Planck, uma unidade natural [1]. Lembrando que o ΛCDM considera um Universo com seções espaciais planas. De acordo com todos esses resultados analisados nessa seção, temos que a Eq. 3.74 é reescrita de acordo com o MCP como: E(z) = √ Ωκ(z) + ΩM (z) + ΩR(z) + ΩΛ(z) (3.110) “Hoje” temos que E(z = 0) = H0/H0 = 1, logo: 1 = Ωκ + ΩM + ΩΛ, (3.111) em que usamos o valor dado em 3.102 para o parâmetro de densidade de radiação. Na literatura as vezes é mais conveniente reescrever essa expressão na seguinte forma: Ωtot = ΩM + ΩΛ = 1 − Ωκ. (3.112) Para um Universo plano Ωκ = 0 de modo que desta forma precisamos requerer que ΩΛ seja diferente de zero para que essa expressão seja consistente. Isso é chamado de problema da constante cosmológica [59]. De fato, pois basta somarmos o valor da literatura de ΩM = 0.31 (Eq. 3.101) com ΩΛ = 0.69 (Eq. 3.108) e verificar que temos como resultado o algarismo 1. Para a constante de Hubble do Pantheon+ SH0ES (dados esses que utilizamos nesse presente trabalho) temos 4 tipos de modelos analisados cujos parâmetros obtidos estão reprodu- zidos na Tabela 3.3. As medidas de H0 são em unidades de [km s−1 Mpc −1]. E, temos 4 tipos de modelos investigados pelo Pantheon+ SHOES: FlatΛCDM que considera como parâmetros livre ΩM e fixos w = −1 e ΩM +ΩΛ = 1 (Universo com seções espaciais planas daí o nome Flat que vêm do inglês, tal que Ωκ = 0), ΛCDM com ΩM e ΩΛ livres e w = −1 fixo, FlatwCDM com w e ΩM livres e ΩM + ΩΛ = 1 fixo e por fim, Flatw0waCDM em que w = w0 + wa(1 + z), ΩM , w0, wa livres e ΩM + ΩΛ = 1 [42]. Sendo que wa é um parâmetro da equação de estado da energia escura — para o modelo ΛCDM temos que wa = 0 [60]. Neste caso não temos as 63 medidas de Ωc para comparação com as nossas estimativas de modo que somaremos os nos- sos resultados, para este parâmetro com Ωb = 0.05, e obteremos ΩM e assim, conseguiremos analisá-los com relação aos resultados do Pantheon+ SH0ES. Tabela 3.3: Modelos cosmológicos e parâmetros determinados pelo Pantheon+ SH0ES com 68% de confiança. Parâmetros Modelos ΩM ΩDE H0 w0 wa FlatΛCDM 0.338 ± 0.018 0.662 ± 0.018 73.4 ± 1.1 −1 ΛCDM 0.277 ± 0.054 0.570 ± 0.080 73.3 ± 1.1 −1 FlatwCDM 0.307 +0.058 −0.063 0.693 +0.063 −0.058 72.86 +0.94 −1.06 −0.89 ± 0.13 Flatw0waCDM 0.386 +0.056 −0.070 0.614 +0.070 −0.056 73.40 +0.99 −1.22 −1.81 +1.71 −0.60 −0.4 +1.0 −1.8 3.11 COMPONENTES DO UNIVERSO De acordo com o modelo ΛCDM o Universo é constituído da seguinte forma aproxi- mada28 considerando as medidas do Planck [1]: • 5% de matéria bariônica. • 26% de matéria escura. • 69% de energia escura. Nesta seção será dissertado um pouco sobre essas componentes do Universo. 3.11.1 Matéria Bariônica Como sabemos o Universo é homogêneo e isotrópico em grandes escalas e dentro dessa modelagem, o conteúdo de energia do Universo é dado em termos de sua densidade. Para definirmos uma densidade precisamos de uma escala de distância e por essa razão, todas as densidades são assumidas como dadas na escala de homogeneidade, ou seja a escala de distância 28Alguns trabalhos na literatura também consideram a radiação como componente do Universo, entretanto ela apresenta um valor aproximadamente zero como já discutimos. 64 quando o Universo pode ser tratado como homogêneo29. Deste modo, todas as densidades presentes no Universo podem ser tratadas como constantes em qualquer tempo t. O Universo é em sua maioria composto por bárions (prótons, nêutrons, etc), que por sua vez formam as estrelas, planetas, galáxias, etc. O conjunto de todos esses objetos celestes é cha- mado de matéria bariônica. De acordo com o que sabemos esses objetos estão em movimento, pois por exemplo, planetas orbitam estrelas e estas orbitam o centro da galáxia. Ao considerarmos apenas o centro de massa dos objetos dentro de um volume teremos a impressão, à medida que avançamos para escalas mais altas, que esses objetos se movem cada vez menos. Ou seja a interpretação do movimento desses objetos acaba se tornando irrelevante em altas escalas. Devido a isso, tratamos a matéria comum na escala de homogeneidade como poeira em queda livre, onde cada grão de poeira representa um volume grande o suficiente para que a região em que ele está possa ser tratada como homogênea e essa característica nos diz que os objetos não se movem mais um em relação ao outro. Se tivermos N objetos com massa total M dentro de um volume V num tempo t0, então num tempo t, teremos os mesmos N objetos com massa total M só que agora em um volume V [a(t)/a(t0)] 3 e isso nos diz que a densidade inicial ρb(t0) = M V , (3.113) se torna num tempo t: ρb(t) = M V [a(t0) a(t) ]3 = ρb(t0) [a(t0) a(t) ]3. (3.114) Essa é uma hipótese que pode ser testada por meio de observações. 3.11.2 Matéria Escura Na Cosmologia a principal ferramenta capaz de corroborar teorias/hipóteses acerca do Universo envolve evidências observacionais e são justamente essas que propuseram a existência da matéria escura, na década de 30, quando foram analisados movimentos orbitais de galáxias em aglomerados30. Esses estudos apontaram numa velocidade da ordem de dez a cem vezes maior do que se esperava para as galáxias. Em décadas posteriores novas evidências como as apontadas na década de 70 pela astrônoma Vera Rubin (1928-2016) et al. em suas pesquisas (ver referências [61, 62] para mais informações) reforçaram a hipótese de que existia mais massa no Universo do que se detectava visivelmente como a de estrelas e planetas, etc. Esse excesso de matéria foi atribuído à matéria escura [63]. Em princípio, o mesmo raciocínio abordado para a matéria bariônica poderia ser apli- cado a matéria escura, porém como não a vemos diretamente, i.e. não conseguimos observá-la pois esta é invisível para nós, não podemos pressupor que este tipo de matéria está em movi- 29As principais discussões e equações das subseções referentes as matérias bariônica e escura foram tiradas do tutorial que pode ser encontrado neste link: https://github.com/NumCosmo/NumCosmo/blob/ master/notebooks/tutorials/DarkEnergyIntro.ipynb 30Neste aspecto um aglomerado se trata de um acúmulo de centenas a milhares de galáxias. 65 mento em altas ou baixas escalas. Felizmente existem outras maneiras de determinarmos se a matéria escura é quente, morna ou fria, sendo estes os principais candidatos teóricos para a sua descrição. • Matéria escura fria (CDM - do inglês cold dark matter): um dos possíveis candidatos para a sua formação são partículas que interagem fracamente chamadas de WIMPS 31 (Weakly Interacting Massive Particles com tradução literal de Partículas Maciças Fracamente In- terativas). Os neutralinos são o exemplo mais canônico de WIMPS e surgiu da Teoria da Supersimetria 32, geralmente abreviada como SUSY. A eventual descoberta de um neutra- lino ajudaria a resolver dois problemas em aberto da Física: caracterizar a composição da matéria escura e comprovar a própria supersimetria, uma teoria já bem desenvolvida, mas ainda em busca de comprovações experimentais. Em contrapartida “isto essencialmente irá nos dizer que há toda uma série de outras coisas novas lá fora apenas esperando para serem descobertas” segundo a física de partículas Mariangela Lisanti da Universidade de Princeton [63, 64]. • Matéria escura morna (WDM - do inglês warm dark matter): Se trata da fase entre a matéria escura fria e quente. Neste caso a matéria escura supostamente seria composta por partículas neutras, como o gravitino a superpartícula do graviton que são os hipo- téticos portadores da força da gravidade, e caso fossem detectados seriam um elemento fundamental para a formulação de uma consistente teoria quântica da gravidade [63, 66]. • Matéria escura quente (HDM - do inglês hot dark matter): Segundo a teoria do Big Bang no início o Universo era denso e quente, e isso favoreceu numa abundância enorme de densidade de neutrinos, que seriam os supostos candidatos à formação da matéria es- cura [63]. Apenas os neutrinos que surgiram do Big Bang seriam os responsáveis pela formação dessa matéria. As evidências observacionais atuais favorecem uma matéria escura fria (CDM) - para uma discussão mais completa ver [67]. Assim, a densidade associada a ela se comportará exa- tamente como foi discutida com os bárions, pois tanto os bárions quanto os WIMPS (supostos candidatos à formação da matéria escura fria) interagem fracamente, além do fato de que ambos 31As WIMPS teriam entre 1 e 1000 vezes a massa de um próton e interagem uma com as outras apenas através da força fraca, que em física de partículas se trata da força responsável pelo decaimento radioativo [64]. 32Segundo a supersimetria cada partícula tem um parceiro “super” (com spins diferentes) que de certo modo ajudaria a preencher algumas lacunas presentes no Modelo Padrão. Algumas dessas superpartículas, como as refe- rentes ao fóton e ao bóson Z, teriam propriedades semelhantes às que poderiam ser calculadas para os constituintes da matéria escura. Logo isso nos daria um forte pressuposto de que esta poderia ser uma mistura dessas partícu- las supersimétricas. Os neutralinos como superpartículas seriam em teoria as mais fáceis de ser encontradas pois as colisões dessas partículas devem produzir um grande número de pósitrons (partículas de elétrons com carga positiva usadas em tomografias, por exemplo) de alta energia [65]. 66 seriam compostos por um fluido de pressão nula 33. Ou seja: ρc(t) = MCDM V [a(t0) a(t) ]3 = ρc(t0) [a(t0) a(t) ]3, (3.115) em que ρc(t) é a densidade da matéria escura fria (o subíndice c vêm do inglês cold cuja tradução literal é fria(o)) e MCDM é a massa total de um objeto N com volume V composto de matéria escura fria. Apesar disso tudo a matéria escura ainda permanece um mistério para a comunidade científica. Deste modo, não podemos afirmar que elas são WIMPS ou qualquer outra coisa, pois ainda não temos evidências plausíveis que nos forneçam uma maior compreensão acerca dessa matéria abundante em nosso Universo. 3.11.3 Energia Escura Segundo o glossário presente no site oficial do telescópio Hubble a energia escura é “uma força misteriosa que parece funcionar de forma oposta à da gravidade e faz com que o universo se expanda em um ritmo mais rápido”. Antes de prosseguirmos vamos fazer um apanhado geral de alguns modelos cosmológicos, sendo que discutimos os principais na Seção 3.8, afim de contextualizar a importância da energia escura. Em 1917, Einstein introduziu uma constante Λ em suas equações relativísticas afim de propor um modelo cosmológico do Universo. Esse foi o primeiro modelo baseado na RG. Nessa mesma época surgiram outros modelos cosmológicos baseados na RG, mas com uma diferença no que diz respeito a estaticidade do Universo. Para W. de Sitter (1872-1934), A. Friedmann (1888-1925) e G. Lemaître (1894-1966), o Universo estava em expansão. O modelo de de Sitter acabou não se saindo muito bem pois a ideia de um Universo totalmente destituído de matéria e radiação não condizia com a tendência atrativa e repulsiva que existia entre a gravitação e a constante cosmológica, respectivamente, para com a matéria existente no Universo [25]. Os outros modelos além de explicarem a expansão formalizavam essa ideia, de tal forma que eram mais consistentes com a RG. Apenas em 1929 que o astrônomo Edwin Hubble (1889-1953) formalizou uma lei (Eq. 3.18), na qual estabelece que o redshift de galáxias e supernovas era causado pela expansão do espaço que era uma propriedade intrínseca deste. Em 1998, observações de Supernovas do tipo Ia validaram a suposição de que o Universo estaria em expansão, e mais ainda, que esta ocorria de forma acelerada. Para explicar esse fenômeno, foi introduzido um componente exótico do Universo com pressão negativa chamado de energia escura. Neste caso a pressão negativa da energia escura age contra a gravidade, ou seja, um corpo de maior massa no Universo tende a atrair um corpo de menor massa de acordo com o que sabemos sobre a gravidade, e neste sentido a energia escura precisaria “puxar” esses 33Isso quer dizer que o fluido seria composto de partículas em repouso, em outras palavras, paradas. Devido a essa característica quando o volume, V , aumenta temos que a densidade, ρ, diminui. No entanto, o número de partículas permanece o mesmo pois estas estão paradas. 67 corpos para que eles não sejam atraídos pela gravidade, logo seria como se estivessem sendo repelidos, mas a verdadeira responsável por isso é a energia escura. Atualmente assume-se o modelo cosmológico ΛCDM onde a energia escura tem equa- ção de estado constante com valor w = −1 que é atribuído a constante cosmológica Λ devido ao fato de que sua densidade é realmente (até onde sabemos) uma constante da Natureza. Em alguns casos considera-se o modelo XCDM para a energia escura onde ela apresenta uma equa- ção de estado arbitrária mas constante. Afim de saber se o MCP está certo, os pesquisadores deixam livre w e o restringem com observações e suposições de modo que se o resultado en- contrado é próximo de w = −1, podemos dizer que o MCP ainda é fortemente corroborado. Em contrapartida se w for um valor negativo menor que este, por exemplo w = −23 (sugestão do autor, poderia ser, por exemplo w = −1.5), então o MCP pode ser suspeito para descrever o Universo [68]. Essa expansão é causada no espaço-tempo. Vamos supor uma analogia: Considere que temos um balão vazio e desenhamos pontos neste. Ao enchermos esse balão pouco a pouco veremos que os pontos desenhados se afastam um dos outros em virtude da expansão do “te- cido” do balão. É exatamente isso que ocorre com o nosso Universo, os objetos astronômicos e cosmológicos estão sempre se afastando uns dos outros, devido a expansão no “tecido” do espaço-tempo [69]. Para entender melhor como ocorre a expansão do Universo considere as figuras apre- sentadas em 3.17 tiradas do video da referência [70]. A grade representa o nosso Universo onde, neste exemplo, cada aresta corresponde a distância de 1 Mpc (Figura 3.17(a)). Suponha galáxias a uma distância de 1Mpc uma das outras em um tempo t = 0s (Figura 3.17(b)). A expansão do Universo não altera a posição das galáxias, e deste modo, quando ela ocorre neste exemplo, as galáxias parecem estar se afastando uma das outras e, de fato a distância entre elas aumenta, embora nenhuma tenha mudado de posição. Na Figura 3.17(d) o tamanho das arestas passa a ser de 1.5 Mpc e neste caso a primeira galáxia a direita pareceu se afastar da galáxia central com uma distância de 1 Mpc enquanto que a segunda galáxia da direita para esquerda aparentemente se afasta da galáxia central a uma distância de 0.5 Mpc. Essa aparência de deslocamento é governada pela lei de Hubble, que neste exemplo, em particular nos diz que a velocidade de afastamento por segundo é proporcional a distância das galáxias por um fator de 0.5 [70]. Matematicamente: v = 0.5 d0, (3.116) onde d0 é a distância inicial da galáxia com relação à outra. Apesar da expansão ser fortemente provada com observações, ainda não sabemos o que é a energia escura. O modelo ΛCDM nos diz que a energia escura é a constante cosmológica, no entanto, é uma afirmação apenas com intuito de validação do modelo. Tanto que existem outros modelos que propõe outras intepretações para essa componente, como o ϕCDM que a 68 (a) Tamanho das arestas na condição inicial. (b) Galáxias na condição inicial t = 0 s. (c) Expansão do Universo em t = 0.5 s. (d) Universo em t = 1 s. Figura 3.17: Representação da expansão do Universo. Fonte: Canal D-Dimensões. considera um campo escalar, também chamado de quintessência. Uma coisa é certa, essa é uma componente que permeia todo o Universo, mas ainda faltam evidências para que possamos tomar a liberdade de definir exatamente o que compõe a energia escura e assim, determinarmos a sua verdadeira natureza. 3.12 RECONSTRUÇÃO DA EQUAÇÃO DE ESTADO DA ENERGIA ESCURA Antes de entrarmos em detalhes sobre a reconstrução da equação de estado da ener- gia escura w(z) vamos entender um pouco sobre o que seria uma equação de estado em seus detalhes, bem como o método de reconstrução, para só então aplicarmos esses conceitos na componente do Universo que estamos interessados, a energia escura. 3.12.1 Equação de estado Quando estudamos a matéria podemos relacionar o estado em que ela se encontra (só- lido, líquido ou gasoso, por exemplo) em termos de quantidades termodinâmicas, como o nú- mero de mols n, a pressão P , o volume V e a temperatura T . Essas quantidades são chamadas de variáveis de estado, pois podem variar de um estado para outro em diferentes sistemas. Deste modo, variáveis que descrevem o estado de um sistema são chamadas de variáveis de estado e as equações que se relacionam com esses estados são chamadas de equações de estado [71]. A matéria, na natureza, pode se apresentar de duas formas: como uma substância pura ou como uma mistura. Estas duas formas, por sua vez, podem ser subdivididas em outras duas 69 classificações principais como mostra o diagrama na Figura 3.18. Figura 3.18: Classificação da matéria. As substâncias puras ou simplesmente substâncias são formadas por apenas um tipo de constituinte (átomo, molécula, etc) e possuem pontos de fusão e ebulição constantes a uma dada pressão, além de uma densidade bem definida, em determinada pressão e temperatura. Por exemplo a água. Podendo ser formadas por átomos de apenas um único elemento (simples) ou por átomos de mais de um tipo de elemento (compostas) [71]. Já as misturas são formadas por duas ou mais substâncias puras de modo que essa ca- racterística faz com que tenham densidades variantes. Deste modo elas podem ser homogêneas, pois apresentam uma única fase, ou heterogêneas apresentando duas ou mais fases [71]. Apesar de em princípio o estado de uma substância pura ser especificado quando sabe- mos os valores de n, V , p e T , foi estabelecido experimentalmente, que basta especificarmos apenas três dessas variáveis para analisarmos o estado de um determinado sistema. Realmente é um fato experimental que a aproximação termodinâmica descreve bem vários fenômenos ma- croscópicos independentemente de suas características microscópicas. Assim, cada substância pode ser descrita por uma equação de estado que relaciona 4 variáveis termodinâmicas. A forma geral de uma equação de estado é do tipo [71]: p = f (T, V, n). (3.117) Essa equação nos diz que se conhecemos os valores de n, T e V (variáveis de estado), então a pressão p tem um valor fixo. No Universo cada substância é descrita por sua própria equação de estado, mas a forma explícita da equação é conhecida em apenas alguns casos. Em princípio, poderíamos fazer isso com qualquer sistema [71]. A equação de estado mais simples que conhecemos é a chamada lei do gás ideal ou equa- ção de Clapeyron, que faz exatamente o que queremos, relacionar a pressão p com o número de 70 mols n, a temperatura T e o volume V : p = f (T, V, n) = nRT V , (3.118) em que R é a constante universal dos gases [71]. Nesse aspecto, se os graus de liberdade relevantes da energia escura podem ser descritos por um fluido barotrópico, então naturalmente podemos fazer essa modelagem associando a ela variáveis de estado e, consequentemente, uma equação de estado, deixando em aberto, no nosso trabalho, apenas o parâmetro w(z) que representa a equação de estado da energia escura. 3.12.2 A Física no método de reconstrução Quando falamos de reconstrução na Física, estamos querendo falar sobre a operação de construir quantidades físicas a partir de dados brutos34 coletados em experimentos e/ou si- mulações. Como um processo de software podemos entender a reconstrução como sendo o procedimento de redução de dados, satisfazendo de certa forma as exigências envolvidas com a análise de dados. A reconstrução também possui grande aplicabilidade em outras áreas. Por exemplo, no trabalho de especialistas em análises de crimes, que usam os dados brutos coletados em cenas de crimes na reconstrução de curvas referentes as taxas de criminalização, como a ocorrência de crimes. Ela também pode ser usada na reconstrução de um acidente, por exemplo, pois assim os especialistas conseguem ter um melhor entendimento sobre como ele ocorreu, quando as evidências não são, por si só, suficientes para esclarecer as causas do acidente (a sequência que ele ocorreu) e assim, determinam os fatos do evento da maneira mais exata possível [72]. Para se ter uma ideia do quanto a reconstrução é aplicável, temos áreas dedicadas apenas a reconstrução de acidentes em grandes empresas de produção de automóveis como a acidento- logia, que surgiu com o objetivo de se caracterizar o acidente no que diz respeito à sua natureza, forma de ocorrência, como, onde e quando ocorreu, entre outros fatores. Temos também a acidentometria. que se preocupa em estudar por meio de modelos matemáticos as variáveis envolvidas nos acidentes como meio de criar métodos de prevenção para estes. A reconstrução de acidentes se faz necessária sempre que não houver um consenso entre as partes envolvidas no evento [72]. No âmbito da Cosmologia, que é o nosso maior interesse, o método de reconstrução serve, por exemplo, na reconstrução do perfil de densidade de um aglomerado que atua como lente gravitacional — o aglomerado que desvia a luz das galáxias de fundo. Deste modo, pode- mos inferir a massa a partir do desvio que esta causa na luz proveniente das galáxias. No nosso caso, empregamos a reconstrução na equação de estado da energia escura w(z) 34Dados brutos são informações brutas obtidas em primeira mão. Em outras palavras, são dados que não estão organizados em sua totalidade e que foram obtidos com o intuito de se estudar os seus sumários estatísticos, responsáveis por nos dizer algo sobre tais conjuntos de dados. 71 no que tange uma abordagem paramétrica e independente de modelo para essa função. Podemos imaginar alguns cenários para entender as possíveis maneiras de se fazer Fí- sica, e assim compreender melhor o que seria a reconstrução. Pegue como exemplo um cientista realizando um experimento em laboratório: ele vai fazer repetições de seu experimento, onde cada uma dessas pode ser representada por pontos em um gráfico. Temos então este gráfico, com eixo de abscissas x, onde se localizam as medidas do experimento, com uma boa precisão, e temos o eixo das ordenadas y, que representa o fenômeno medido. Suponha agora que para este experimento também temos um modelo teórico, de onde conseguimos tirar uma fenomenologia. Ou seja, esse modelo nos diz como que o fenômeno observado vai se comportar. Sendo, por exemplo, representado por uma função quadrática também conhecida por parábola. Um ponto a se observar é que ao realizarmos o experimento as medidas do fenômeno em y são feitas sabendo que nosso instrumento é falho, isto é, sabemos que ele erra e também como e quanto ele erra. Deste modo, ao saber de todas essas características em meu instru- mento, temos capacidade para entender que o erro experimental se dará por uma variável do tipo gaussiana, e que ela possui uma variância fixa. O trabalho do experimentador consiste em se pegar a equação da parábola, apresentada de forma explícita, cujo eixo é paralelo ao eixo Oy, por [73]: y = ax2 + bx + c, (3.119) onde a, b, e c são constantes e x são as medidas experimentais, e neste caso, fitar 35 3 parâmetros representados pelas constantes, temos ainda o modelo teórico que nos restringe à parábola, e isso acarreta em apenas 3 graus de liberdade, que antes eram infinitos. E, o mais importante, sabemos o sumário estatístico dos dados, uma vez que a diferença entre a teoria e os dados tem que ser uma variável gaussiana. Com isso em mente, podemos plotar o gráfico, ajustar o nosso χ2, encontrar os melhores estimadores para os parâmetros a, b e c e assim o nosso problema estará de certa forma resolvido — isso é o que em princípio estamos pressupondo. É importante ressaltar que estamos resolvendo o problema dada todas as nossas escolhas iniciais, teóricas e experimentais. Neste caso será que realmente o erro do nosso instrumento de medida é uma variável do tipo gaussiana com variância fixa? Será que de fato, a classe de modelos de parábolas é suficiente para descrever os dados? Estas são algumas perguntas que podemos nos fazer a mais sobre tais estudos. Também é notável que existem situações mais extrapoladas ainda, onde não temos nem mesmo uma justificativa teórica para afirmarmos que o nosso modelo é uma parábola, uma reta, ou qualquer outra curva, pois muitas vezes não teremos informações suficientes sobre o nosso instrumento que pode não ser um mero aparelho. De fato, na Cosmologia um instrumento é uma sequência enorme de procedimentos que envolvem instrumentos físicos e escolhas teóricas (ainda observacionais). 35Essa palavra vêm da palavra em inglês fit (tradução literal: ajustar), que é um verbo [74], e significa, no nosso caso, ajustar um valor adequado aos parâmetros correspondentes da parábola. 72 Um exemplo disso é o mapa CMB como mostrado na Figura 3.11, que foi observado pela sonda espacial Planck. Muitas vezes, o cosmólogo observacional, olha para esse mapa e pode pensar que de fato este é o mapa que o Planck mediu. Essa afirmação não é totalmente correta, pois o Planck é um aparato que não olha o céu como um todo mas sim um ponto em particular. Deste modo, quando ele aponta em direção a um ponto no céu acaba absorvendo todos os fótons provenientes dessa mesma direção de tal forma que cada detector do Planck irá medir faixas de frequências específicas. Os detectores do Planck são uma espécie de interferômetro 36 que apontam para duas direções distintas no céu, mas para um mesmo ponto, onde em cada uma dessas, teremos fó- tons de diferentes intensidades. Após detectarem essas intensidades elas serão convertidas em escalas de temperatura e assim, o Planck consegue medir a diferença de temperatura entre cada uma das direções. Assim que mede um ponto no céu a sonda faz um giro, pois ela está em uma órbita e por consequência o aparato irá girar de acordo com o movimento orbital. Assim, ele faz uma medida de outro ponto usando o mesmo procedimento e assim ele constrói faixas com uma certa grossura, que vão se complementando para a formação do mapa final da CMB. Claro que essa operação é repetida várias vezes para um acúmulo de sumários estatísticos de cada direção do céu. Além disso temos outros fatores, como a Terra no meio das medidas, pois o Planck terá que realizar um giro sobre esta antes de medir todo o céu. O primeiro experimental recebe a chamada série temporal que se trata da temperatura que o Planck estava medindo em um tempo t quando ele apontava numa determinada direção, levando em consideração todas as correções relativísticas, pelo fato da sonda estar numa órbita fora da Terra, por exemplo. Além disso a direção depende da calibração da sonda na órbita. Ele precisa incorporar todas essas informações de tal modo que elas possam ser traduzidas em um mapa. Nesse momento, temos interesse nos possíveis erros que os instrumentalistas cometeram quando fizeram uma medida, que era de fato, uma medida de duas direções do céu, num certo instante de tempo, num certo ponto da órbita e, a partir disto tentar interpretá-los no erro final, que é representado por um bin onde é acumulado todas as medidas do mesmo ponto do céu. Não é nada fácil mantermos um relatório final sobre todas as influências que a medida sofreu e se conhecêssemos o sumário es- tatístico de todas bastaria considerarmos todas essas estatísticas de forma que poderemos dizer o erro exato obtido em um particular ponto do céu. Toda essa discussão foi exposta como modo de exemplificar que no mundo real, muitas vezes, teremos uma segunda teoria em nosso problema, que é a teoria do erro. Deste modo, precisaremos desenvolver modelos teóricos, fenomenológicos, usufruir muito do teorema do limite central (que entramos em mais detalhes na Seção 5.7) para a análise de dados. Em outras palavras, toda a física do nosso problema, terá uma teoria da física por trás do que de fato queremos entender e uma modelagem dos erros nos instrumentos, ou seja da maneira que 36O High Frequency Instrument, HFI, e o Low Frequency Instrument, LFI, que são bem sensíveis e foram projetados para medir, respectivamente, a radiação difusa que permeia o céu na faixa de frequência de 84 GHz a 1 THz (comprimento de onda entre 3.6 e 0.3 mm), e microondas na faixa de frequência de 27 a 77 GHz (comprimento de onda entre 11.1 e 3.9 mm) [75]. 73 tentamos chegar na física do problema. A partir disso tentamos reconstruir o mais próximo possível, usando técnicas de reconstrução, o que os dados nos dizem a respeito da física. 3.12.3 Abordagem paramétrica, não-paramétrica, dependência e independência de mo- delo Na Cosmologia era/é muito comum fazer uma confusão entre dois termos distintos, que são a independência de modelo e modelo não-paramétrico. O artigo [76] disserta um pouco sobre tais jargões usados na área da Cosmologia, Astrofísica e Estatística. Um modelo pode ter uma abordagem paramétrica, onde impomos uma distribuição de probabilidade para os dados. Em outras palavras, quando usamos um modelo matemático para relacionar um certo valor da variável em estudo com a sua probabilidade de ocorrência estamos adotando uma abordagem paramétrica. Caso contrário, se trata de uma abordagem não-paramétrica, pois usamos os dados para reconstruir sua própria distribuição de probabili- dade. Um exemplo seria dizer que certo conjunto de dados se comporta como uma distribuição gaussiana a priori (abordagem paramétrica). Se não podemos afirmar como tal conjunto se comporta de antemão, então trata-se de uma abordagem não-paramétrica. Consequentemente após adotarmos uma dessas abordagens, o único problema que nos resta é determinar a curva da teoria, que pode ser dependente ou independente de modelo. No primeiro caso, escolhemos uma função a priori, como a função quadrática, para representar os dados e no segundo caso, usamos os dados para determinar a função que melhor os representa. Com o objetivo de entender melhor o uso dessas conotações, vamos analisar em quais situações podemos usar tais termos. Inicialmente vamos supor uma abordagem paramétrica a qual conhecemos os sumários estatísticos do nosso instrumento de medição e por alguma razão sabemos que essas medições são representadas por uma distribuição gaussiana, pois temos maneiras de determinar isso a priori. Em contrapartida à isso, não temos nenhuma ideia acerca do nosso modelo teórico, tendo apenas pontos num gráfico, por exemplo. Neste caso, sabemos onde estão localizados tais pontos e suas respectivas barras de erro. Se não temos uma teoria para explicar os dados o que podemos fazer é “chutar” alguns palpites e analisar qual o melhor que se ajusta aos dados. Basicamente esse seria o pressuposto básico de toda teoria física, pois alguém sempre começa com palpites sobre o que ocorre em fenômenos naturais. Em resumo, se não temos nenhum embasamento teórico, então não temos outra alternativa além de começar “chutando” a teoria. Em outro caso, podemos tentar reconstruir o nosso modelo. Esse é o chamado método de reconstrução. Ao invés de pressupormos que o nosso problema se comporta como uma função quadrática, uma reta ou outra curva, nós estaremos interessados em dizer apenas o mínimo possível sobre os meus dados. Isso parece ser uma extrapolação, pois o mínimo possível pode ser representado por qualquer função. Entretanto se não colocamos nenhuma restrição sobre o termo “qualquer função”, além do fato de que ela é uma função que leva reais em reais, podemos então dizer que qualquer função é uma coleção de infinitos pontos. Se não dizemos nada sobre 74 a função, logo não podemos nem mesmo afirmar que ela é contínua, por exemplo, e os pontos podem ser todos descontínuos de forma que esses infinitos pontos não terão nenhuma conexão plausível entre si graficamente. Para contornar isso, precisamos fazer algumas considerações sobre o nosso problema. Uma delas seria esperar que a função seja contínua. Exceto no caso em que existe alguma característica peculiar no objeto físico em análise, é difícil imaginarmos um fenômeno que não age de forma contínua na natureza. Por exemplo, digamos que uma bolinha seja solta de uma determinada altura h, devido a gravidade, sabemos que ela terá uma aceleração igual a g de modo que sua velocidade deve ser aproximadamente constante em todo o percurso, desprezando-se a resistência do ar. Assim dizendo, a bolinha não terá uma velocidade v2 ̸= v1 em um intervalo particular desta trajetória, representado por b na Figura 3.11. A Física não permite tal particularidade, pois nesse intervalo ainda temos uma aceleração igual a g. v1 g v2 h b v1 b Figura 3.19: Representação de um experimento no qual soltamos uma bolinha de uma certa altura e esta apresenta uma velocidade descontínua em seu percurso no trajeto delimitado por b. Fonte: Autor. De certa forma começamos a fazer uma modelagem, onde tentamos colocar o mínimo possível. Quando assumimos que a função que representa o nosso fenômeno é contínua, es- peramos que não exista grandes variações em pequenas escalas. Ou seja, não esperamos que exista várias oscilações, da ordem de trilhão, por exemplo, entre medidas feitas em um intervalo pequeno. Nesse intervalo não esperamos curvaturas muito altas. Neste aspecto, quando estamos fazendo a modelagem da reconstrução de certo fenô- meno físico, precisamos exigir o mínimo de hipóteses teóricas, que geralmente só irão tangen- ciar a matemática do problema, e a partir disto tentamos encontrar uma estrutura matemática que descreva uma classe enorme de funções que possuam tais pré-requisitos. Essa é a defini- ção de modelagem independente de modelo, como já mencionamos. A grosso modo, este é um termo usado inapropriadamente para este caso, pois de certo modo estamos supondo que a 75 função que representa os meus dados é suave/contínua, que elas não têm curvaturas grandes em pequenas escalas. Mas é isso o que queremos dizer quando falamos em análise independente de modelo. Ao mesmo tempo, existem situações onde temos um escopo teórico muito bem desen- volvido, ou seja, entendemos muito bem a teoria do problema e sobre a curva que o representa. No entanto não podemos dizer nada acerca dos erros e assim, necessitamos de alguma teoria para descrevê-los. Esse procedimento é o que abrange a chamada abordagem não-paramétrica. Este é um termo usado em textos estatísticos. Se de certa forma queremos minimizar a influência humana nos resultados científicos, então estaremos interessados em uma abordagem não-paramétrica e independente de modelo. Neste caso estamos interessados em descobrir a partir dos dados qual a estatística que estes representam e ao mesmo tempo também queremos extrai deles a física por trás dos seus erros. Obviamente isso é muito difícil de se fazer quando não temos dados suficientes para a mode- lagem. Em algumas área da Física isso não é um problema, pois se conseguimos repetir um experimento muitas vezes de diferentes formas, então simplesmente acabamos com esse empe- cilho. A reconstrução, neste caso em particular, fica evidente devido a abundância de dados que deixam as barras de erro o menor possível, o que ocasiona na não necessidade de argumentos matemáticos para reconstruir a física do problema. Basta observarmos a disposição dos dados graficamente para estimar a curva que melhor se ajusta à eles, isso tudo devido as suas pequenas barras de erro 37. Na Cosmologia, como já mencionamos somos limitados experimentalmente de modo que ela é mais observacional do que experimental. Claro que tentamos, ao máximo, fazer Cosmologia da forma mais científica possível. Nesse contexto, sob o PC, sabemos que se observarmos regiões diferentes do Universo, obteremos realizações diferentes de uma mesma estatística. Não conseguimos repetir experimentos devido a variância cósmica. Pois temos uma certa escala do Universo onde não conseguimos realizar uma repetição de modo que a variância é muito grande, podendo ser considerada infinita. De fato, a realização de um dado de uma distribuição é compatível com qualquer distribuição, e isso nos diz pouca coisa sobre o nosso objeto de estudo. Afim de esclarecermos ainda mais essa afirmação considere a Figura 3.20 para a dis- cussão que iremos abordar. Suponha uma certa estatística de distribuição de densidade de gás dentro de uma caixa e que ela apresenta flutuações e variações. Assumindo, por alguma razão, que sabemos que o gás se espalhou de forma a ser estatisticamente igual em todas as porções da caixa vamos supor que somos incapazes de recriar tal caixa, isto é, não conseguimos repetir o experimento. No entanto se supormos que existe uma homogeneidade estatística, um cubo grande corresponde a N realizações de um cubo pequeno que preenchem a mesma escala desse cubo grande. Do ponto de vista estatístico podemos usar o conceito de ensemble com leves al- 37Um exemplo prático provém dos gráficos de barras de erro do Planck, que são tão pequenos que é fácil ver, quase que instantaneamente, a curva física característica do problema. 76 terações. Assim, apesar de sermos incapazes de entender o que ocorre nessa caixa, por que não temos a liberdade de repetição do experimento, podemos separar pequenas porções da mesma, em cubos, por exemplo, onde em cada um desses temos uma representação da estatística da caixa dentro da escala de um cubo pequeno. Assim podemos dizer que esses cubos representam um ensemble de um mesmo cubo. Isso é viável, por que a estatística dentro de cada cubo é igual, por hipótese. Deste modo, todos os cubos serão as repetições do experimento em grande escala. Claro que se quisermos estudar a física no tamanho da caixa em si isso não é possível, pois não poderemos repetir o experimento. O máximo que podemos fazer é dividirmos a caixa em duas outras com mesmas proporções, de modo que teremos uma estatística usando dois pontos. Essa analogia só é possível se supormos que o que acontece dentro do cubo da figura é homogêneo estatisticamente. Essa suposição nos diz que estatisticamente o que acontece em uma das arestas do cubo é equivalente, estatisticamente, ao que acontece em outra de suas arestas. Neste sentido, além do fato de que a média da matéria tende a zero no Universo, para este ser supostamente tratado como homogêneo, temos que, estatisticamente, o que acontece em uma região do Universo é compatível com o que ocorre em outra. Nessa discussão fica claro que o número de repetições de um experimento diminui com o aumento do número de escala deste. Em contrapartida, o número de repetições aumenta quando diminuímos a escala do experimento. Essa constatação estendida para a Cosmologia é a chamada variância cósmica, como já discutimos. Escala maior Escala menor Repetições Diminuem Repetições Aumentam Figura 3.20: Representação de uma caixa dividida em outros 4 subsistemas. O número de repetições aumenta de acordo com a diminuição da escala. Fonte: Autor. O uso mais difundido desse termo se refere ao fato de que as medições são afetadas pela estrutura cósmica em grande escala, de tal modo que uma medida de qualquer ponto do céu pode diferir de uma medida de outro ponto do céu por uma quantidade que pode ser muito maior do que a variância da amostra. Para uma discussão mais detalhada ver [77]. 77 Na Cosmologia, tudo o que está no limite do raio de Hubble, que é até onde se estende o Universo observável, não pode ser repetido. Se de certo modo, nessa região tiver uma ano- malia estatística consequentemente esta não será percebida, pois não conseguimos distinguir física de anomalia estatística em grandes escalas. Suponha por exemplo uma reta perfeita, e no final desse gráfico temos um outlier muito distante da reta, mas com uma barra de erro muito pequena, e que esteja digamos à 3σ de distância, ou seja com menos de 1% de chances de ser apenas uma mera variação estatística. Apesar disso, somos impossibilitados de repetir o expe- rimento, e desta forma podemos concluir infinitas coisas diferentes a respeito desse dado. Essa característica é tão comum e abrangente na comunidade científica que existem áreas inteiras na Cosmologia dedicadas ao estudo de flutuações estatísticas em grandes escalas, supondo que essas flutuações possam representar uma física completamente ou parcialmente diferente da já existente. Em compensação existe toda uma gama enorme de cosmólogos que consideram essas flutuações apenas uma mera flutuação estatística sem interesses práticos ou teóricos para seus estudos em Cosmologia. Essa limitação na Cosmologia nos mostra que nunca seremos capazes, pelo menos não em todas as áreas, de fazer uma análise independente de modelo e não-paramétrica. Claro que isso não significa que essas ferramentas não possam ser úteis em várias áreas específicas da Cosmologia e em certos problemas. 78 4 DADOS OBSERVACIONAIS Nesta seção discutiremos sobre os dados observacionais usados em nossa reconstrução da equação de estado da energia escura. Como a energia escura não é um observável direto precisamos de outras quantidades para estimar w(z). Entre os diversos meios de medirmos os efeitos da energia escura na evolu- ção do Universo, temos as medidas de Supernovas do tipo Ia (SNe Ia), responsáveis pela des- coberta da expansão acelerada do Universo [78, 79], as oscilações acústicas de bárions (BAO) e os cronômetros cosmológicos (CCH). A partir da reconstrução de w(z) obtemos todas as quantidades necessárias para a cons- trução das verossimilhanças para cada caso. A verossimilhança, também chamada de likelihood, é um objeto básico de estatística que nos fornece a probabilidade de um conjunto de dados ob- servados D dado um modelo. Assim ela pode ser considerada uma função do próprio modelo. Quando realizamos um experimento, estamos interessados em usar os dados para uma inferên- cia estatística sobre o modelo físico subjacente. Isso envolve uma hipótese H, a teoria em si, que pode ser por exemplo, um conjunto de parâmetros de um certo modelo, ou a validação do próprio modelo. Se o modelo é definido em termos de um conjunto de parâmetros θ, então temos [80]: L(θ) = p(D|θ), (4.1) em que L é a função de verossimilhança. A grosso modo a verossimilhança pode ser pensada como a comparação entre os resultados observados e os resultados obtidos com a reconstrução. Entramos em mais detalhes a respeito da verossimilhança na Subseção 5.7.1. Dividimos esse capítulo em duas etapas para cada observável cosmológico sendo que houve uma implementação de dados mais recentes e/ou em maior quantidade de uma etapa para outra. Além disso, na 1ª etapa estávamos calibrando a nossa reconstrução de modo que constatamos, com a 2ª etapa, que não tinha uma maneira fácil e consistente de prosseguirmos utilizando os observáveis de distance priors da CMB, devido ao fato de que estes dependem fortemente de modelo. Assim, abandonamos a ideia inicial de usarmos redshifts altos, restrin- gindo o nosso modelo de 0 ≤ z ≤ 1080 para 0 ≤ z ≤ 3.0 da 1ª para a 2ª etapa. Essa restrição só foi possível por que a implementação dos novos dados de SNe Ia, BAO e CCH era por si só suficiente para restringir o nosso modelo em baixos redshifts. Os observáveis cosmológicos da 2ª etapa também são usados na 3ª etapa, bem como o mesmo intervalo de redshift. Os dados observacionais apresentados nesse capítulo foram implementados como obje- tos na biblioteca NumCosmo. Todos os dados supomos como sendo normalmente distribuídos. De tal forma que podemos usufruir do teorema do limite central tal que a distribuição amos- tral da média de uma determinada estimativa também é normalmente distribuída, independente- mente do tamanho da amostra. Ou seja, todas as nossas estimativas também se distribuem como 79 uma gaussiana devido a essa suposição. Retornaremos a essa discussão com mais detalhes na Seção 5.7. Dividimos este Capítulo em duas Subseções para cada observável: 1ª etapa e 2ª e 3ª etapas. 4.1 DADOS DE SUPERNOVAS DO TIPO IA As Supernovas do tipo Ia, abreviadas para SNe Ia, com acrônimo do inglês Supernovae e Ia representando o seu tipo de classificação, são observáveis cosmológicos usados em medidas de distâncias no Universo. Elas desempenharam um papel crucial para o descobrimento do Universo em expansão acelerada, o que acarretou numa mudança de paradigmas na Cosmologia Moderna. Foi justamente as observações de SNe Ia que renderam à Saul Perlmutter, Adam Riess e Brian Schmidt o prêmio Nobel em Física de 2011, apesar dessas terem sido realizadas alguns anos antes, em 1998, validando a suposição de que o Universo estaria em expansão acelerada [78, 79]. A análise de Supernovas teve início com os trabalhos de Hubble. Em 1929 Hubble ob- servou que quanto maior o redshift de um objeto, maior será a sua velocidade de afastamento com relação a um observador na Terra e mais ainda, sua distância também será maior. Ele anali- sou estrelas variáveis, chamadas de Cefeidas, para obter distâncias de luminosidade em função do redshift. Essas estrelas apresentam a peculiaridade de uma luminosidade proporcional ao período de oscilação da própria luminosidade 1, assim pode-se calcular a luminosidade média desses objetos. No entanto, Cefeidas não são observáveis além do Grupo Local de galáxias e para obter informações de luminosidade mais distantes, usando essa mesma abordagem, preci- saríamos de uma classe de objetos ou eventos astronômicos muito mais brilhantes. Para tal ficou evidenciado que Supernovas seriam o principal objeto de análise para medidas de distância de luminosidade [37]. Uma Supernova é uma explosão decorrente do estágio final da evolução de estrelas ou um sistema de estrelas. Uma estrela, como o Sol, não é uma Supernova, mas tenderá a ser em algum momento da evolução do Universo. As Supernovas são classificadas de acordo com os seus espectros e curvas de luz. Em particular estamos interessados nas Supernovas pertencentes ao tipo2 Ia, que apresentam espectro de linha de silício II (uma vez ionizado), devido ao fato de que elas são explosões termonucleares de anãs brancas que apresentam uma massa abundante 3 o que faz com que suas explosões sejam similares entre si, sendo mais facilmente observadas, coisa que não ocorre com outros tipos de Supernovas [37]. Atualmente existem alguns grupos dedicados ao estudo das SNe Ia e o seu empenho, sem sombras de dúvidas, proporcionou um avanço extraordinário para a atual compreensão 1Essa propriedade foi descoberta por Henrietta Leavitt [81, 82]. 2Para mais detalhes sobre os tipos de Supernovas ver [37] e [83]. 3O limite máximo para as massas de anãs brancas (do inglês white dwarfs) é de 1.4 vezes a massa do Sol, ou seja M⋆ = 1.4 × M⊙, em que M⊙ = 1.98844 × 10 30 kg é a massa do Sol [6]. 80 do Universo. A partir desses objetos conseguimos determinar medidas de distância chamadas de escadas de distância cósmica, cuja abordagem tradicional de medidas engloba paralaxes, Cefeidas, SNe, TRGB, etc. Usualmente, na literatura, é comum referenciarmos o termo vela padrão às SNe. Isso se deve ao fato de que supostamente elas explodem sempre com a mesma magnitude, o que não é bem uma verdade. Vamos entender melhor essa afirmação com uma pequena discussão: A SNe explode, e depois de um certo tempo (questão de dias) o brilho tem um auge de sua intensidade e começa a diminuir. Se temos uma vela hipoteticamente perfeita (que não derrete, não perde o tamanho do pavio nem nada, permanecendo a mesma vela sempre) toda vez que a acendemos sabemos que ela irá emitir a mesma quantidade de luz — isso é o que chamamos de vela padrão. A luz emitida a partir de um ponto apresenta uma frente de onda do tipo esférica. É um fato experimental que a intensidade do brilho cai com o quadrado da distância que pode ser interpretado como a área esférica que representa a frente de onda. Em outras palavras quanto mais perto do centro de emissão da luz mais brilhante ela será, pois a intensidade da luz é maior devido a uma área menor e mais ainda, a luz se conserva e por isso em áreas maiores ela parece menos intensa apesar de que nenhuma informação sobre a luz foi perdida no caminho. Deste modo, se sabemos quanta luz existia na emissão e quanta luz é observada ao se analisar parte dessa frente de onda conseguimos estimar o quanto de luz existe na frente de onda inteira, pois supomos que a quantidade de luz é a mesma em todas as direções dessa frente de luz, bem como conceitos de isotropia. E como a luz se conserva, se conseguimos determinar os parâmetros de interesse em uma frente de onda, sabemos que teremos a mesma quantidade em outra frente de onda. Neste caso elas podem ser usadas como velas padrão para medidas de distâncias do Universo pois ao medirmos o quão brilhantes elas aparecem no céu, podemos inferir a que distância estão. Isso é semelhante a uma vela padrão que parece mais fraca a distâncias maiores (ilustração à esquerda). No entanto, o termo vela padrão é bem extrapolado para o que realmente ocorre. Na verdade foi descoberta uma relação empírica, i.e. não deduzida de primeiros princípios, em que existirão circunstâncias em que uma galáxia tem uma Cefeida e uma SNe e podemos constatar, que em escalas cosmológicas, qualquer diferença de distância entre ambas, numa mesma galá- xia, é desprezível, não afetando em nada os nossos cálculos. Assim, se sabemos a distância da Cefeida com relação a um observador na Terra, também saberemos a da SNe. Suponha agora duas SNe nessa mesma galáxia com uma Cefeida, que apresentam magnitudes diferentes, em que cada uma demora em torno de semanas para extinguir toda a sua luminosidade ao passo que a outra leva meses. Neste aspecto basta observarmos a cor e quantos dias demora pra cessar o brilho de uma SNe para inferir a sua magnitude. As Supernovas são ditas velas padronizáveis no sentido de que ao combinar essas informações conseguimos determinar um padrão. 81 4.1.1 1ª etapa - SNe Ia Os dados usados na 1ª etapa deste presente trabalho são referentes a um sample de 740 SNe Ia de Betoule et al. (2014) [84], cuja análise foi nomeada como “JLA” sample (do inglês Joint Light-curve analysis com tradução literal para “análise conjunta luz-curva”) que se trata de uma colaboração entre os grupos SDSS (Sloan Digital Sky Survey) e SNLS (Supernova Legacy Survey). Esses dados são estatísticos e sistemáticos, e apresentam uma matriz de covariância do módulo de distância categorizado [84]. A verossimilhança de supernovas nada mais é do que a comparação entre as observações de supernovas e a nossa previsão teórica de qual seria a distância dessas supernovas da Terra. Dada uma certa teoria, a verossimilhança de se encontrar uma certa distribuição de magnitudes aparentes m de supernovas é: −2 ln LSNe Ia = ∆mT C −1 stat+syst∆m, (4.2) em que a matriz de covariância Cstat+syst dos dados é uma combinação das matrizes representa- tivas dos erros estatísticos Cstat e sistemáticos Csyst dada por: Cstat+syst = Cstat + Csys, (4.3) cuja representação matricial é a chamada matriz de Fisher. A matriz de covariância desses dados se encontra em [84] bem como uma discussão mais aprofundada sobre a sua construção. Entre- tanto para o cálculo da verossimilhança precisamos da inversa dessa matriz de modo que seria um processo deverás difícil se não fosse abordado numericamente — teríamos que inverter uma matriz de ordem 740 × 740. Dentre várias técnicas para inversão de matrizes numericamente, temos a de Cholesky que é definida para a resolução de sistemas lineares (n×n) tal que a matriz do sistema seja simétrica e definida positivamente [85]. Essa é a tecnicalidade numérica correta para a inversão de matrizes. Da Eq. 4.2 temos ∆mi definido como: ∆mi = mB,i − m model B,i , (4.4) em que mB,i corresponde ao pico de magnitude observado da banda B da i-ésima SNe Ia (se trata da magnitude somada em todas as bandas quando foi observada tal Supernova com o uso de um instrumento de medida como um telescópio) definido como: mB,i = µi − αXi + βci + MB, (4.5) em que α e β são os parâmetros de incômodo relacionados ao alongamento e cor, respectiva- mente, da luminosidade da SNe, MB = {M1, M2} é a magnitude fiducial4 de uma SNe Ia que é 4O parâmetro M2 é determinado a partir da relação ∆M = M2 − M1 em que ∆M = −0.070 para JLA com erros estatísticos e sistemáticos [84]. O ajuste de MB depende do modelo cosmológico e neste caso não podemos 82 calibrada definindo uma escala de distância absoluta com âncoras de distância primária, como Cefeidas (os dados referentes a esses parâmetros estão dispostos na Tabela 4.1 [84]), Xi é o o parâmetro de alongamento correspondente à largura da curva de luz, c é a cor máxima da SNe Ia que inclui contribuições da cor intrínseca da SNe e poeira estelar e, µi é o chamado módulo de distância definido como o logaritmo na base 10 da razão entre a correção da distância de luminosidade e 10 unidades de pc (parsec) 5 [84, 42]: µi = 5 log10 [ DL 10pc ]. (4.6) Tabela 4.1: Parâmetros de SNe Ia de Betoule et al. (2014). Parâmetro α β M1 M2 Valor 0.14 3.15 −19.05 −19.12 O outro termo da Equação 4.4 é definido como: mmodel B,i = µmodel i − αXi + βci + MB (4.7) que neste caso representa a magnitude esperada na banda B do nosso modelo em que [76, 86]: µmodel i = 5 log10(DL(zhel i , zcmb i )) + 5 log10 [ c H0 ] + 25, (4.8) é o módulo de distância do modelo, em que DL(zhel i , zcmb i ), uma função dos parâmetros cosmo- lógicos de nosso interesse, é definido a partir da distância de luminosidade DL como: DL(zhel i , zcmb i ) = H0 c DL(zhel i , zcmb i ). (4.9) Neste sentido DL(zhel i , zcmb i ) é interpretado como uma correção para a distância de lumi- nosidade devido ao fato de que as medidas de SNe Ia do nosso modelo são estimadas no frame heliocêntrico, em que consideramos o redshift heliocêntrico 6 zhel i , enquanto que os cálculos dos dados observacionais foram realizados para o referencial em queda livre que corresponde ao quadro da CMB dado pelo redshift zcmb i [86]. ajustar esse parâmetro usando w(z) diretamente, uma vez que ele não depende de modelo. Para tal, usamos âncoras de distância primária e indiretamente fixamos o valor de MB e por isso usamos os dados de Cefeidas na fixação de MB, pois se não ele seria totalmente degenerado. 5O parsec é uma unidade de distância definida como a distância na qual 1 unidade astronômica (AU do inglês Astronomical Unit) subtende um ângulo de 1 segundo de arco. Temos que 1 pc = 206264.806 AU ou 3.085678 × 1016 m [6]. 6O redshift heliocêntrico é aquele no qual a correção é feita no referencial do Sol [86]. Neste sentido seria o redshift da SNe corrigido com relação ao do Sol. Analogamente para zcmb que é a correção do redshift da SNe com relação à CMB. 83 Da definição de distância de luminosidade em termos da distância de movimento próprio ou distância comóvel transversa DM , uma variável relacionada à variação da distância própria entre duas fontes no referencial das mesmas, temos para o nosso caso [76, 87]: DL(zhel, zcmb) = (1 + zhel)DM (zcmb). (4.10) em que: DM (zcmb) = c H0 Sκ(x), (4.11) tal que: Sκ(x) =    sin (√ −Ωκx)/ √ −Ωκ Ωκ < 0 x Ωκ = 0 sinh (√Ωκx)/ √ Ωκ Ωκ > 0 (4.12) Sendo que neste caso x é uma nova variável que é dada pela distância comóvel (3.46) multiplicada pelo fator H0/c: x ≡ H0 c Dc(zcmb) = H0 c c H0 ∫ zcmb 0 dz′ E(z′) = ∫ zcmb 0 dz′ E(z′). (4.13) Logo a distância de luminosidade corrigida (Equação 4.9) pode ser reescrita usando as Eqs. 4.10 e 4.11 como: DL(zhel, zcmb) = H0 c (1 + zhel)DM (zcmb) = H0 c (1 + zhel) c H0 Sκ(x) = (1 + zhel)Sκ(x). (4.14) Considerando um Universo de seções espaciais planas temos que Ωκ = 0 então, de acordo com 4.12, temos Sκ(x) = x. Logo a Eq. 4.14 pode ser reescrita usando 4.13 como: DL(zhel, zcmb) = (1 + zhel) ∫ zcmb 0 dz′ E(z′). (4.15) Por conveniência vamos multiplicar ambos os lados dessa expressão por c/H0: c H0 DL(zhel, zcmb) = (1 + zhel) c H0 ∫ zcmb 0 dz′ E(z′) = (1 + zhel)Dc(zcmb). (4.16) 84 Isolando DL obtemos por fim: DL(zhel, zcmb) = H0 c (1 + zhel)Dc(zcmb). (4.17) Assim, para este caso em particular, se calculamos Dc(zcmb) podemos calcular facil- mente DL(zhel, zcmb) tendo em vista que a distância de luminosidade corrigida é um funcional da distância comóvel. Para outros tipos de curvaturas do Universo basta usarmos as outras re- lações dadas em 4.12 que também são dependentes de x e assim, podem ser reescritas também em termos da distância comóvel. Em resumo, temos uma série de ingredientes teóricos e observacionais necessários para o cálculo de ∆mi que é a variável assumimos se distribuir por uma distribuição do tipo gaus- siana multivariada com covariância dadas pelas matrizes de Fisher. Com isso é fácil observar que existem muitos detalhes que se desdobram quando trabalhamos com esses tipos de coisas na prática. Ou seja, precisamos saber sobre Cosmologia para o cálculo das distâncias cosmo- lógicas, fenomenologia de supernovas para o cálculo das correções fenomenológicas e técnicas de inversão de matriz. Além disso, podem existir outros tipos de complicações, pois podemos ter casos em que o estudo da verossimilhança completa é viável e neste caso em particular precisaremos calcular o determinante da matriz. 4.1.2 2ª e 3ª etapas - SNe Ia Na 2ª e 3ª etapas deste trabalho usamos um sample de 1701 SNe Ia de Brout et al. (2022) [42] sendo que 1550 foram espectroscopicamente confirmadas, variando de z = 0.001 a z = 2.26. Na nova era de grandes amostras, o progresso de análise depende de esforços no que diz respeito a calibração rigorosa dessas velas padronizáveis e a caracterização de suas incertezas, que inclui erros sistemáticos e uma sutil covariância de erros. Tendo como foco a análise de SNe Ia, foi realizado um estudo conjunto pelos projetos nomeados como Pantheon+ (lê-se o sinal de + na língua inglesa como plus) e SH0ES (do inglês Supernovae and H0 for the Equation of State of dark energy). Como resultado dessa parceria obteve-se um conjunto de medições de escada de distância e redshifts e, mais ainda, suas incertezas, que podem ser utilizadas para uma ampla gama de investigações [42, 43]. O projeto SH0ES começou no ano de 2005 como um esforço na tentativa de abordar a precisão de nível percentual de medição da constante de Hubble, para tal foi alavancado novas câmeras no Telescópio Espacial Hubble (HST do inglês Hubble Space Telescope) com o objetivo de se alcançar Cefeidas em um número maior de galáxias hospedeiras de SNe Ia, que estão a distâncias maiores. Devido a isso, os erros sistemáticos foram reduzidos de tal forma que o projeto progrediu para alcançar uma atual incerteza de 1.4% na constante de Hubble [44]. O elemento-chave desse projeto é a inclusão das Cefeidas da equipe SH0ES como calibradores das SNe Ia [42, 43]. A análise Pantheon+ é uma sucessora direta da análise Pantheon (1048 SNe) realizada 85 por Scolnic et al. 2018 [88] que sucede a JLA por Betoule et al. 2014 [84] e tem como principal objetivo medir a aceleração do Universo, matéria escura e energia escura com dados de SNe Ia. De fato, os dados analisados na primeira análise [88] aumentou o número de SNe Ia distintas em aproximadamente 50% com trabalhos apresentando inúmeras melhorias em erros sistemá- ticos, que dificultaram a análise Pantheon [42]. A análise do Pantheon+ inclui atribuições de galáxias hospedeiras, correções da CMB e correções da velocidade peculiar 7 como reportado na Ref. [89]. Para redshifts 0.01 < z < 0.1 as medidas de SNe Ia são bem sensíveis as veloci- dades peculiares de tal modo que foi realizado um estudo mais aprofundado nesse intervalo em particular que pode ser visto em mais detalhes na Ref. [90]. Uma melhoria importante adicio- nal do Pantheon+ é o vínculo formal com a análise SH0ES, contabilizando a covariância entre as medições de SNe Ia nas amostras de dados usadas por SH0ES e Pantheon+. Pela primeira vez, o Pantheon+ realiza um ajuste simultâneo da constante de Hubble junto com parâmetros cosmológicos como w. A análise mostra que qualquer incerteza sistemática relacionada a SNe não pode afetar significativamente a inferência de H0 da análise SH0ES. As expressões que apresentaremos a seguir são reproduções das que podem ser encon- tradas na Ref. [42]. A verossimilhança desses observáveis é: −2 ln LSNe Ia = ∆DT C −1 stat+syst∆D, (4.18) em que D é o vetor da distância modular residual das 1701 SNe numericamente implementado como: ∆Di = µi − µmodel(zi), (4.19) em que µi é o chamado módulo de distância definido como: µi = mB,i + αXi − βci − MB − δbias + δhost, (4.20) em que temos [44]: MB = −19.253 ± 0.027 mag, (4.21) e: mB,i = −2.5 log10(x0) (4.22) em que x0 é a amplitude da curva de luz da SNe. Em 4.20 temos os parâmetros α, β e MB = {M1} para este caso (representam as mesmas propriedades de SNe Ia discutidas na Subseção 4.1.1) — mais adiante iremos discutir melhor acerca desses parâmetros —, δbias que é a correção de primeiro termo que leva em conta os viés de seleção determinados por Popovic et al. (2021) [91] descrito em detalhes no Apêndice A de [42], e δhost que é a correção da luminosidade, chamada de “passo”, para correlações residuais entre o brilho padronizado de um SNe Ia e a 7Velocidades peculiares representam o movimento físico de uma galáxia em relação ao referencial de repouso cosmológico. 86 massa da galáxia hospedeira [42]. Neste sentido, cada distância de SNe (µi) é comparada com o valor previsto da distância do modelo dado o redshift medido (µmodel(zi)). As distâncias do modelo são definidas como [44]: µmodel(zi) = 5 log [cH −1 0 (1 + z) ∫ z 0 dz′ E(z′) ], (4.23) tal que satisfaz: mmodel B,i = µmodel(zi) + 25 + MB. (4.24) A inversa da matriz de covariância C −1 stat+syst desses dados é definida a partir da matriz de covariância como: Cstat+syst = Cstat + Csyst, (4.25) em que Cstat e Csyst são as matrizes de erros estatísticos e sistemáticos, respectivamente mais detalhadas em [42]. Nestas análises foi preciso inverter uma matriz de ordem 1701 × 1701 para satisfazer a Eq. 4.18. O parâmetro MB = {M1} da Eq. 4.20 é degenerado quando analisamos a SNe sozinha. Para contornar isso usamos as restrições impostas por SH0ES nesses dados de tal modo que: ∆D′ i =   µi − µCepheid i i ∈ Cefeida hospedeira µi − µmodel(zi) em outro caso, (4.26) em que µCepheid i é a Cefeida calibrada na mesma galáxia hospedeira de uma SNe. Assim, precisa- mos incluir também a matriz de covariância da Cefeida hospedeira C Cepheid stat+syst na verossimilhança como: −2 ln L′ SNe Ia = ∆D′T (C SNe Ia stat+syst + C Cepheid stat+syst) −1∆D′, (4.27) em que C SNe Ia stat+syst é a matriz de covariância da SNe. A principal diferença entre as duas etapas de SNe Ia está no fato de que as técnicas empregadas nos anos 2000-2015, como a de [84], necessitavam de análises dos parâmetros de incômodo α e β. Agora, eles já vêm com a correção imposta da calibração do SH0ES. Em outras palavras, os valores de mB,i na banda B já vem corrigidos para α e β, como se os três primeiros termos da Eq. 4.20 fossem compactados em apenas um termo que neste caso é dado em x0 (Eq. 4.22). Em contrapartida, ainda precisamos ajustar MB = {M1} diferentemente da 1ª etapa, em que era ajustado MB = {M1, M2} pois na análise JLA pensava-se existir dois samples com magnitudes absolutas diferentes de tal forma que dois sub-samples da magnitude de SNe apresentavam magnitudes absolutas diferentes. Os samples mais recentes abrangem uma técnica estatística que calcula as magnitudes na banda B mB,i já com as correções de α e β. Desta forma não precisamos mais fitar α e β em nosso modelo, o que é bom pois quando esses dois parâmetros estão livres a covariância é muito mais difícil de ser calculada numericamente. Isto é, α = β = 0 na 2ª e 3ª etapas, 87 devido ao fato de que não precisamos mais nos preocupar com o alongamento Xi nem a cor ci das SNe na determinação de µi na banda B. Contudo, ainda precisamos fitar uma magnitude MB = {M1} de tal forma que a outra M2 = −19.25, pode ser mantida fixa, o que não afeta em nada os nossos resultados na 2ª e 3ª etapas, tendo em mente que essa era uma característica da JLA usada na 1ª etapa. De fato, a equipe do SH0ES mede a magnitude do pico absoluto, MB, das SNe Ia, calibrando as distâncias das galáxias hospedeiras de SNe com âncoras de distância geométrica local através da relação de período de luminosidade de uma Cefeida. Os dados implementados na NumCosmo foram extraidos do Data Release do Pantheon+ SH0ES disponível na plataforma GitHub 8. 4.2 OSCILAÇÕES ACÚSTICAS DE BÁRIONS - BAO No Universo as distribuições de galáxias e aglomerados contém várias informações so- bre a sua evolução. Tanto que um dos principais objetivos dos levantamentos astronômicos está no mapeamento desses objetos. Graças a esses mapas podemos por exemplo, compreender melhor as flutuações primordiais na densidade de matéria e assim analisar como as estruturas se formam. Com isso podemos analisar como a matéria se organiza na formação de galáxias e aglomerados, e também entender como o Universo e suas componentes evoluem [55]. As estruturas cósmicas, como chamamos a estrutura do Universo em expansão, podem ser estudadas utilizando-se uma teoria de expansão linear do Universo, em que os estágios pri- mordiais podem ser estudados analiticamente e os mais avançados podem ser determinados numericamente. Deste modo, para que essas estruturas cósmicas existam, necessariamente, a densidade média do Universo deve sofrer algumas perturbações para que as estruturas cresçam através do fenômeno de atração gravitacional [55]. No início do século XXI, foram detectados picos acústicos, i.e. ondas sonoras, no espectro de potência de anisotropia da CMB que cons- tataram para a comunidade científica que tais ondas eram geradas por perturbações cósmicas no plasma primordial formado por bárions, sendo essas perturbações para z >> 1000, e mais ainda, que elas são predominantemente adiabáticas [92]. Essas perturbações são da era da in- flação, quando o Universo se expandiu suficientemente para que a maioria dos fótons tivesse espaço para se propagar livremente. Os fótons que forneciam a pressão necessária para condu- zir as ondas sonoras no plasma primordial, que se propagaram de forma esférica, i.e. em todas as direções, pelo plasma à partir de pontos com excesso de densidade de matéria, tratado como o ponto central de propagação. Portanto, se não houvesse pressão a propagação dessas ondas deveria cessar [93]. Logo após o desacoplamento, quando os bárions se desacoplam dos fótons, ocorreu o último espalhamento dessas ondas sonoras que continuaram por um curto período de tempo (chamado de época de arrasto — redshift: zd ≈ 1059 com subescrito d referente ao inglês drag epoch) pois os fótons ainda eram muito mais numerosos que os bárions (cerca de um bilhão 8https://github.com/PantheonPlusSH0ES/DataRelease. 88 para um). A partir do momento em que cessaram a sua propagação deixaram na distribuição bariônica sobredensidades que eram separadas por uma escala de comprimento chamada de comprimento comóvel do horizonte sonoro ou horizonte sonoro denotada por rs(zd). A subse- quente evolução em grande escala da matéria do Universo seria então dominada pela gravidade que deixaria uma separação preferencial na distribuição das galáxias cuja detecção é chamada de oscilações acústicas de bárions, abreviadas para BAO do inglês Baryon Acoustic Oscilla- tions. Esse fenômeno pode ser usado como uma régua padrão para medidas de distância no Universo [93, 94, 95]. Para mais detalhes sobre a formação de BAO acesse o website de autoria de Dan Eisenstein 9. Neste caso, podemos estimar as distâncias de galáxias com o uso de réguas padrão em um método baseado na preferência por pares de galáxias a uma distância de aproxi- madamente 490 milhões de anos-luz. Essa separação parece diminuir a medida que as galáxias se afastam entre si, como uma régua de comprimento fixo, daí o nome régua padrão. A homogeneidade nos diz que as propriedades estatísticas do Universo, como a sua densidade, são as mesmas em todas as regiões. Para testá-la podemos usar BAO, que neste caso são consideradas réguas padrão, como são chamados os objetos ou fenômenos cujo tamanho real é sempre o mesmo, para qualquer objeto ou fenômeno com mesmas propriedades, o que demonstra a homogeneidade do Universo. Ao comparar esse tamanho real com o tamanho aparente, obtemos uma medida do quão longe tais objetos estão de um observador na Terra. A escala acústica de BAO apresenta duas contribuições, como mostra a Figura 4.1, que podem ser observadas para perturbações ao longo (direção radial r||) [55], r||(z) = cz H(z) (4.28) e ortogonal (direção transversal r⊥) à linha de visada das galáxias, sendo esta definida em termos da distância de diâmetro angular DA como [55]: r⊥(z) = (1 + z)DA(z). (4.29) Sendo que DA pode ser determinado se soubermos a extensão do objeto estudado utilizando a seguinte expressão [96]: DA(z) = (1 + z)−1 c H0K Sκ(Kx), (4.30) em que K ≡ c/(a0H0) é o inverso do fator de escala hoje em unidades de H0/c e x é definido de acordo com 4.13. As duas contribuições de escala acústica são iguais para BAO em um Universo descrito pela métrica de FLRW, pois a expansão é isotrópica em todos os pontos. Qualquer desacordo significativo entre as escalas acústicas de BAO radial e transversal sinalizaria uma quebra da isotropia remota e, portanto, da homogeneidade [97]. Deste modo, se 9https://lweb.cfa.harvard.edu/~deisenst/acousticpeak/acoustic_physics. html 89 r|| r⊥ − 1 ̸= 0, (4.31) for significativamente diferente de 0 então estamos trabalhando com um Universo que não obe- dece a métrica de FLRW [97]. r|| r⊥ DA(z) ∆θ z z + ∆z Figura 4.1: Representação trigonométrica para as duas contribuições da escala acústica: a radial r|| e a transversal r⊥. Como podemos analisar a escala acústica em duas direções, o levantamento feito é tri- dimensional. Para tal definimos uma distância híbrida que combina ambas as contribuições chamada de distância volumétrica ou distância média do volume DV dada na seguinte forma [55]: DV (z) = (r2 ⊥r||) 1 3 . (4.32) Substituindo as Eqs. 4.28 e 4.29 em 4.32 obtemos: DV (z) = [ (1 + z) 2D2 A(z) cz H(z) ] 1 3 (4.33) Elevando a Eq. 4.30 ao quadrado e substituindo K ≡ c(a0H0) sendo que a0 = 1 obtemos: D2 A(z) = (1 + z) −2 c2 H 2 0 K2 S2 κ(Kx) = (1 + z) −2 c2 H 2 0 a2 0H 2 0 c2 S2 κ ( c a0H0 x ) = (1 + z) −2S2 κ ( c H0 x ) . (4.34) Considerando um Universo com seções espaciais planas temos que Sκ(x) = x de acordo com 4.12. Para o nosso caso é fácil notar que S2 κ ( c H0 x ) = c2 H 2 0 x2, (4.35) 90 é equivalente a: c2 H 2 0 S2 κ(x) = c2 H 2 0 x2. (4.36) Deste modo podemos reescrever 4.34 como: D2 A(z) = (1 + z)−2 c2 H 2 0 S2 κ(x) = (1 + z)−2D2 M (z) , (4.37) em que usamos a Eq. 4.11 ao quadrado para o nosso caso. Logo, substituindo 4.37 em 4.33 obtemos: DV (z) = [ (1 + z) 2(1 + z)−2D2 M (z) cz H(z) ] 1 3 = [ D2 M (z) cz H(z) ] 1 3 . (4.38) Observe que essa expressão se trata de uma média geométrica no volume, que nada mais é do que a multiplicação de dados nos quais extraímos a raiz de acordo com a quantidade de números multiplicados. Esse é o melhor sumário estatístico que conseguimos extrair de BAO. Isso se deve muito ao fato de que dados de BAO são difíceis de se medir, mas o cálculo das médias de suas distâncias nos fornece o menor erro e de certa forma teremos mais clareza acerca dos erros sistemáticos e estatísticos envolvidos nas medidas. Usualmente na literatura as medidas de BAO são dadas em unidades da escala de rs(zd) [95], por isso comumente medimos o seu pico acústico pela seguinte relação de distância: DV (z) rs(zd) . (4.39) Podemos também medir outras quantidades de forma análoga dadas por: DM (z) rs(zd) e DH(z) rs(zd) . (4.40) E, calculamos o horizonte do som rs(zd) usando: rs ≡ rs(zd) = 1 H0 ∫ ∞ zd cs(z) E(z) dz, (4.41) em que cs(z) é a velocidade do som. 91 4.2.1 1ª etapa - BAO Na 1ª etapa usamos os dados de Beutler et. al (2011) [98] (6dFGS — do inglês Six Degree Field Galaxy Survey), Padmanabhan et al. (2012) [99] (SDSS DR7 — do inglês Sloan Digital Sky Survey Data Release Seven), Anderson et al. (2012) [100] (SDSS DR9 — mesma informação do anterior com Data Release Nine) e Blake et al. (2012) [101] (WiggleZ — do inglês “the baryon wiggles” — tradução literal: “o bárion se mexe”, uma característica da medição de BAO em distribuições de galáxias). Os levantamentos de dados de BAO desses grupos são apresentados na Tabela 4.2, os quais foram usados na reconstrução de w(z). Tabela 4.2: Dados de BAO usados na 1ª etapa deste trabalho. A medida 1 não apresenta cova- riância. Redshift (z) Quantidade medida Valor Grupo Referência 0.106 rs/DV (z) 0.336 ± 0.015 6dFGS [98] 0.35 DV (z)/rs 8.88 ± 0.17 SDSS DR7 [99] 0.57 DV (z)/rs 13.67 ± 0.22 SDSS DR9 [100] 0.44 rs/DV (z) 0.0916 ± 0.0071 WiggleZ [101] 0.60 rs/DV (z) 0.0726 ± 0.0034 WiggleZ [101] 0.73 rs/DV (z) 0.0592 ± 0.0032 WiggleZ [101] Observe que são dados um tanto quanto ultrapassados, apesar de serem bem consisten- tes. Devido a este fator optou-se por uma busca na literatura de dados de BAO mais recentes para uso neste trabalho os quais realizamos uma análise mais rigorosa. Entretanto, mencionar os resultados obtidos com esses dados antigos em nossa reconstrução é algo que não pode ser menosprezado, tendo em vista que eles apenas corroboram ainda mais os novos resultados. A verossimilhança de BAO é dada por [102]: −2 ln LBAO = (x − d) T C −1(x − d), (4.42) em que x são os parâmetros estimados do nosso modelo e d dos dados teóricos, cuja diferença entre ambos é determinada por: 92 x − d = [ rs DV (0.1) − 0.336, DV (0.35) rs − 8.88, DV (0.57) rs − 13.67, rs DV (0.44) − 0.0916, rs DV (0.60) − 0.0726, rs DV (0.73) − 0.0592 ] . (4.43) E a inversa da matriz de covariância, C −1, é dada pela Eq. 4 do artigo de referência [102]. 4.2.2 2ª e 3ª etapas - BAO Na 2ª e 3ª etapas usamos novamente os dados de Beutler et. al (2011) [98] (6dFGS) e novos dados: Ross et al. (2015) [103] (SDSS DR7 MGS, com terceiro acrônimo do inglês main galaxy sample que se refere a galáxias com baixos redshifts), Alam et al. (2016) [104] (SDSS DR12 BOSS final consensus, em que o acrônimo BOSS vêm do inglês Baryon Oscillation Spec- troscopic Survey) 10, Alam et al. (2021) [105] (SDSS DR16 eBOSS LRG, eBOSS ELG, eBOSS Quasar, Lyα-lyα e Lyα-quasar — com acrônimos pós DR16 do inglês extended representa a letra “e” no acrônimo eBOSS luminous red galaxies e eBOSS emission line galaxies) 11. Os dados de Alam et al. estão disponíveis em um repositório aberto na rede 12. Os levantamentos de dados de BAO desses grupos são apresentados na Tabela 4.3, os quais foram usados na reconstrução de w(z). A verossimilhança para os dados de BAO dessas etapas é dada pela mesma forma apre- sentada na Eq. 4.42 com os novos dados implementados na NumCosmo. Apenas as medidas de 6dFGS, MGS, eBOSS ELG e Lyα (Lyα e QUASAR) não apre- sentam covariância. Enquanto que para BOSS, eBOSS LRG e eBOSS quasar temos covariância. Detalhes sobre a matriz de covariância desses dados podem ser encontrados em [94] e [105]. 4.3 MEDIDAS DO PARÂMETRO DE HUBBLE: CRONÔMETROS COSMOLÓGICOS Os observáveis chamados de cronômetros cosmológicos ou cósmicos são medidas do parâmetro de Hubble H(z) para certos valores de z fixos. 10Não usamos os dados de BAO referente ao redshift z = 0.61 apresentados na referência [104] devido ao fato de que estes já estão incorporados nos dados do SDSS DR16 LRG da referência [105]. 11A palavra quasar não possui tradução, são um tipo de objeto celeste massivo e extremamente remoto que emitem quantidades excepcionalmente grandes de energia e, normalmente, apresentam uma imagem estelar em um telescópio. Foi sugerido que os quasares contêm buracos negros maciços e podem representar um estágio na evolução de algumas galáxias. E Lyα se refere ao conjunto de raios resultantes da emissão do átomo de hidrogênio quando o elétron transita de n = 2 até n = 1. Neste aspecto o primeiro caso mede apenas as absorções de Lyα e o segundo caso usa quasares nas medições de tais absorções [106]. 12Link de acesso ao repositório: https://svn.sdss.org/public/data/eboss/DR16cosmo/ tags/v1_0_0/likelihoods/. 93 Tabela 4.3: Dados de BAO usados na 2ª e 3ª etapas deste trabalho. O valor apresentado em Ross et al. (2015) é de 664 ± 25 Mpc da quantidade DV (z)rs,fid rs , entretanto aqui apresentamos ele com base na referência Alam et al. (2021). Redshift (z) Quantidade medida Valor Grupo Referência 0.106 rs/DV (z) 0.336 ± 0.015 6dFGS [98] 0.15 DV (z)/rs 4.47 ± 0.17 MGS [103] 0.38 DM (z)/rs 10.23 ± 0.17 BOSS [105] 0.38 DH(z)/rs 25.00 ± 0.76 BOSS [105] 0.51 DM (z)/rs 13.36 ± 0.21 BOSS [105] 0.51 DH(z)/rs 22.33 ± 0.58 BOSS [105] 0.70 DM (z)/rs 17.86 ± 0.33 eBOSS LRG [105] 0.70 DH(z)/rs 19.33 ± 0.53 eBOSS LRG [105] 0.85 DV (z)/rs 18.33 +0.57 −0.62 eBOSS ELG [105] 1.48 DM (z)/rs 30.69 ± 0.80 eBOSS QUASAR [105] 1.48 DH(z)/rs 13.26 ± 0.55 eBOSS QUASAR [105] 2.33 DM (z)/rs 37.6 ± 1.9 Lyα-Lyα [105] 2.33 DH(z)/rs 8.93 ± 0.28 Lyα-Lyα [105] 2.33 DM (z)/rs 37.3 ± 1.7 Lyα-QUASAR [105] 2.33 DH(z)/rs 9.08 ± 0.34 Lyα-QUASAR [105] 94 As medidas de H(z) podem ser obtidas de duas maneiras. Uma envolve o cálculo das idades diferenciais de evolução passiva de galáxias que recebe o nome de Método da Idade Di- ferencial (do inglês Differential Age Method), e a outra é baseada na detecção de características radiais de BAO chamada de Método Radial de BAO (do inglês The Radial BAO Method). A primeira é a mais usada devido ao fato de que ela nos fornece informações diretas de H(z) — comumente na literatura quando nos referimos aos CCH estamos nos referindo a este método, ao passo que o outro método é chamado por BAOH [107, 108]. No nosso trabalho, estamos mais interessados nos observáveis medidos a partir do pri- meiro método, tendo em vista que não existem muitos dados na literatura que utilizam o segundo método e se trata de um método dependente de modelo [108]. Existe uma expressão matemática na literatura para os CCH. Vamos deduzi-la a partir da definição da distância de luminosidade dada por [87]: DL(z) = (1 + z)DM (z). (4.44) Para um Universo com seções espaciais planas temos, de acordo com 4.11, 4.12 e 4.13, para o nosso caso que DM (z) = c H0 =x ︷ ︸︸ ︷ Sκ(x) = c H0 H0 c Dc(z) = Dc(z). (4.45) Assim, podemos substituir a Eq. 3.46 em 4.44 obtendo: DL(z) = (1 + z) c H0 ∫ z 0 dz′ E(z′). (4.46) De acordo com as Eqs. 3.45, 3.8 e 3.64 podemos reescrever o parâmetro de Hubble normalizado E(z′)numa forma mais conveniente como: E(z′) = H(z′) H0 = 1 H0 1 a(z′) da(z′) dt = 1 H0 1 (1 + z′)−1 d[(1 + z′) −1] dt , (4.47) cuja derivada, pela regra da cadeia, é: E(z′) = −(1 + z′) H0 (1 + z′) −2 dz′ dt = −(1 + z′)−1 H0 dz′ dt . (4.48) Substituindo 4.48 em 4.46 obtemos: DL(z) = (1 + z) c H0 ∫ z 0 [H0(1 + z′) dt dz′ ]dz′ = (1 + z) ∫ z 0 [ − (1 + z′) dt dz′ ]dz′, (4.49) 95 em que simplificamos H0 pois ele é constante perante a integral. O termo entre colchetes dessa expressão é o inverso do parâmetro de Hubble em termos do redshift: H(z) = − 1 (1 + z) dz dt . (4.50) Deste modo podemos constatar que qualquer método de medida de H(z) deve contar com um relógio que data a variação da idade do Universo com relação redshift. Os CCH é um método cujo relógio é fornecido pela técnica de datação espectroscópica das idades das galáxias. Com base nas medições de diferença de idades ∆t, entre duas galáxias de evolução passiva13 que se formaram ao mesmo tempo, mas que estão separadas por um pequeno intervalo de redshift14 ∆z, pode-se inferir a derivada dz/dt da razão dada por ∆z/∆t. A exigência por serem galáxias passivas provém do fato de que, para calular H(z), a idade média de suas estrelas deve exceder em muito a diferença de idade ∆t entre as duas amostras de galáxias [109]. Temos também como outras características desse método que as medições são indepen- dentes da escala de distância de Cefeidas nas suas galáxias hospedeiras e não dependem de nenhum modelo cosmológico particular, embora naturalmente fiquem sujeitas a incertezas sis- temáticas, como as associadas com a técnica de síntese da população estelar (SPS - do inglês stellar population synthesis) que serve para o cálculo da luminosidade das galáxias proveniente de suas estrelas, que podem ter idades diferentes [108, 111]. Para os CCH, temos a seguinte verossimilhança: LCCH = ∏ i exp {−(H(zi) − H obs i )2 2σ2 i } , (4.51) em que H obs i e σi são pontos de dados observados para o parâmetro de Hubble e seus erros, res- pectivamente, e H(zi) é o parâmetro de Hubble medido do modelo, que depende indiretamente de w(zi). Precisamos ressaltar que teríamos mais um termo do lado direito dessa igualdade que é um termo de normalização. No entanto ele é um termo constante que pode ser tratado como zero. Se aplicarmos o ln em ambos os lados dessa igualdade obtemos, de acordo com a pro- priedade do produto e a definição da função ln, o seguinte resultado: 13São galáxias com estrelas antigas (metalicidades semelhantes) e baixas taxas de formação de estrelas, o que na prática seria dizer que as galáxias apresentam uma cor avermelhada. Essas galáxias acabaram se tornando bons objetos de estudo na obtenção de valores observacionais do parâmetro de Hubble em redshifts z ≲ 2 [108, 109]. 14Esse é um modo de se medir o redshift de uma galáxia: pelo aumento do espaço entre as galáxias e não pela velocidade das galáxias no espaço [110]. 96 ln LCCH = ln [ ∏ i exp { −(H(zi) − H obs i ) 2 2σ2 i }] = ∑ i ln [ exp {−(H(zi) − H obs i )2 2σ2 i }] = −1 2 ∑ i (H(zi) − H obs i ) 2 σ2 i . (4.52) Reescrevendo numa forma mais conveniente: −2 ln LCCH = ∑ i (H(zi) − H obs i ) 2 σ2 i = χ 2 i (4.53) que se trata da chamada distribuição χ2 (qui-quadrado) que é um modo quantitativo de deter- minação da concordância entre a distribuição experimental e teórica. Em outras palavras ele serve para avaliar quantitativamente a relação entre as medidas de um experimento e a distri- buição esperada para o fenômeno em estudo. Para o caso em que os erros são descritos por variáveis gaussianas não correlacionadas as expressões 4.51 e 4.53 são equivalentes a menos de uma constante de normalização que pode ser tratada como zero. Neste aspecto, minimizar a Eq. 4.53 é equivalente à maximizar a Eq. 4.51 (retornaremos a esse assunto na Seção 5.7.1). Observe também que a verossimilhança de CCH é bem mais simples do que a dos outros observáveis, pois não temos a inversa da matriz de covariância. Isso se deve ao fato de que como são medidas diretas de H(z) não precisamos determinar a matriz de covariância. 4.3.1 1ª etapa - CCH Na 1ª etapa trabalhamos com 21 medidas de H(z): 1 medida de Riess et al. (2011) [112] que estimou a constante de Hubble H0 usando observações ópticas e infravermelhas de 600 Cefeidas; outras 8 medidas são providas por Moresco et al. (2013) [113] na Tabela 4 dessa referência correspondentes ao intervalo de redshift 0.1791 ≤ z ≤ 1.037 do modelo SPS BC03; 11 medidas estão presentes na Tabela 2 do trabalho de Stern et al. (2009) [114] no intervalo de redshift 0.1 ≤ z ≤ 1.75 que também usa o método de evolução de galáxias passivas; E o último observável dado por Busca et al. (2013) [115] é: H obs r = 224 ± 8 km s −1 Mpc −1, (4.54) que se trata de uma estimativa de H(z)rs/(1 + z) em z = 2.3, e é derivado da fração de fluxo transmitida das florestas Lyα de mais de 48000 quasares combinados com observações da CMB [76]. Para os CCH da 1ª etapa, assumindo que H(z) assume uma distribuição gaussiana e dado que o erro em cada medida é independente, temos que verossimilhança é dada por [76]: 97 −2 ln LCCH = N =20∑ i (H(zi) − H obs i ) 2 σ2 i + [H(z)rs 1 + z − H obs r ]∣ ∣ ∣ ∣z=z21 σ2 21 , (4.55) em que H obs i e σi são pontos de dados dos observáveis e seus respectivos erros. H(zi) são as estimativas do nosso modelo para o parâmetro de Hubble. As 20 medidas de H(z) estão dis- postas na Tabela 4.4 e a 21ª é dada em 4.54. Enfatizando que os valores H0 desses observáveis são tratados como H(z) em outros modelos, como o nosso. Tabela 4.4: Dados de cronômetros cosmológicos H(z) em unidades de km s−1 Mpc −1 usados na 1ª etapa. A 21ª medida foi dada em 4.54. redshift (z) H(z) ± σi Referência redshift (z) H(z) ± σi Referência 0.0 73.8 ± 2.4 [112] 0.17 83 ± 8 [114] 0.18 75 ± 4 [113] 0.27 77 ± 14 [114] 0.20 75 ± 5 [113] 0.4 95 ± 17 [114] 0.35 83 ± 14 [113] 0.48 97 ± 60 [114] 0.59 104 ± 13 [113] 0.88 90 ± 40 [114] 0.68 92 ± 8 [113] 0.9 117 ± 23 [114] 0.78 105 ± 12 [113] 1.3 168 ± 17 [114] 0.88 125 ± 17 [113] 1.43 177 ± 18 [114] 1.04 154 ± 20 [113] 1.53 140 ± 14 [114] 0.1 69 ± 12 [114] 1.75 202 ± 40 [114] No segundo termo de 4.55 temos que o termo referente ao nosso modelo é descrito como: H(z)rs 1 + z = H(z21)rs 1 + z21 , (4.56) em que z = 2.3 para o observável dado em 4.54. 98 4.3.2 2ª e 3ª etapas - CCH Na 2ª e 3ª etapas usamos 31 medidas de H(z) que estão dispostas na Tabela 4.5 e são uma reprodução direta da Tabela 1 de Gomez-Valent e Luca Amendola (2018) [108]. Apesar de não serem dados recentes são 10 a mais do que os dados da 1ª etapa. Além disso, são 31 dados em um intervalo pequeno de redshift 0.07 ≤ z ≤ 1.965, o que gera uma melhor precisão nessa região. Ressaltamos que 8 dessas medidas já tinham sido usadas na 1ª etapa providas por Moresco et al. [113]. A verossimilhança desses observáveis é dada pela forma simples que deduzimos na Eq. 4.53 devido ao fato de que são observáveis deduzidos a partir de uma mesma técnica. Não usamos o valor de H0 do trabalho de Gómez-Valent, pois a tecnicalidade que este usa abrange muitos detalhes a respeito dos tipos de kernel para a sua obtenção, coisa que teríamos que adicionar a verossimilhança e alteraria a sua forma simples. 4.4 MEDIDAS DA CMB PELO WMAP Dados de CMB requerem um cálculo deverás complexo devido a equação de Boltzmann inserida em sua teoria. Para contornar isso temos uma redução desses dados chamada de shift parameters (SP — tradução literal: parâmetros de mudança). Nos SP os pesquisadores extraem parte da informação referente à CMB de forma que podemos usá-los em modelos do tipo cine- máticos, como o deste presente trabalho. Com esses dados conseguimos fazer restrições mais fortes acerca dos modelos. 4.4.1 1ª etapa - Distance priors Os SP usados na 1ª etapa são dados coletados pela sonda espacial WMAP (do inglês Wilkinson Microwave Anisotrpy Probe), mais precisamente da 9ª missão, que foi concebida com o objetivo de se estudar o espaço profundo, medindo as diferenças de temperatura da CMB. Neste caso os priors de distância da CMB (como chamamos um conjunto de SP) são compostos por dois SP que são o parâmetro de deslocamento R, que é independente dos mo- delos subjacentes se estes tiverem os mesmos componentes bariônicos, de matéria escura e de espectro de flutuação primordial e la que se trata da escala acústica da CMB e é praticamente não correlacionada com R. Assim, esses dois parâmetros podem ser usados como uma com- pressão numericamente econômica dos dados completos da CMB [35]. Neste caso a verossimilhança é dada por: −2 ln Ldpriors = (x − d) T C −1(x − d) (4.57) em que x = (lA, R, z∗) são parâmetros do modelo proposto (z∗ é o redshift na época da disso- ciação) e o vetor d tem componentes: 99 Tabela 4.5: Dados de cronômetros cosmológicos H(z) em unidades de km s−1 Mpc −1 usados na 2ª e 3ª etapas. São uma reprodução direta da Tabela 1 de Gomez-Valent e Luca Amendola (2018). redshift (z) H(z) ± σi redshift (z) H(z) ± σi redshift (z) H(z) ± σi 0.07 69.0 ± 19.6 0.4 95.0 ± 17.0 0.88 90.0 ± 40.0 0.09 69.0 ± 12.0 0.4004 77.0 ± 10.2 0.9 117.0 ± 23.0 0.12 68.6 ± 26.2 0.4247 87.1 ± 11.2 1.037 154.0 ± 20.0 0.17 83.0 ± 8.0 0.4497 92.8 ± 12.9 1.3 168.0 ± 17.0 0.1791 75.0 ± 4.0 0.47 89.0 ± 49.6 1.363 160.0 ± 33.6 0.1993 75.0 ± 5.0 0.4783 80.9 ± 9.0 1.43 177.0 ± 18.0 0.2 72.9 ± 29.6 0.48 97.0 ± 62.0 1.53 140.0 ± 14.0 0.27 77.0 ± 14.0 0.5929 104.0 ± 13.0 1.75 202.0 ± 40.0 0.28 88.8 ± 36.6 0.6797 92.0 ± 8.0 1.965 186.5 ± 50.4 0.3519 83.0 ± 14.0 0.7812 105.0 ± 12.0 0.3802 83.0 ± 13.5 0.8754 125.0 ± 17.0 d1 = lWMAP A = 302.40 (4.58) d2 = RWMAP = 1.7246 (4.59) d3 = zWMAP ∗ = 1090.88. (4.60) Estes são os valores da máxima verossimilhança obtidos da 9ª missão do WMAP assumindo uma equação de estado da energia escura constante com curvatura espacial não-nula (modelo cosmológico ‘0wCDM’). Os elementos da matriz de covariância inversa, C −1 são dados na 100 Tabela 11 do artigo 15 [102]. No nosso trabalho x é estimado a partir dos dados no modelo. Essas estimativas não saem no melhor ajuste da curva, precisamos mandar imprimir posteriormente caso seja de nosso interesse, devido ao fato de que não são parâmetros, mas sim observáveis/medidas que infeliz- mente dependem de modelo e por isso não são tão robustas, pois não sabemos ao certo o quanto estamos restringindo nossa análise. Como discutiremos mais a respeito, no Capítulo 6, esses dados são excluídos de uma etapa para outra, por deixarem o nosso modelo muito dependente. 15Neste artigo vemos que a tabela nos fornece a matriz inversa de covariância da distância posterior do WMAP9. Devemos enfatizar que da posterior do WMAP extraímos a prior para ser usada em outros modelos. i.e., usa-se a posterior do WMAP como prior em outras análises. 101 5 METODOLOGIA Neste capítulo encontra-se todo o embasamento teórico matemático desenvolvido para a produção deste presente trabalho de mestrado. 5.1 ABORDAGEM ADOTADA Este presente trabalho adotou uma abordagem paramétrica e independente de modelo para a reconstrução da equação de estado da energia escura w(z) em função do redshift z. Neste caso, supomos que a nossa distribuição de probabilidade é normalmente distribuída, i.e. uma gaussiana, de modo que podemos usufruir do teorema do limite central tal que a distri- buição amostral da média de uma determinada estimativa também é normalmente distribuída, independentemente do tamanho da amostra. Além disso a reconstrução é independente de mo- delo no que se concerne apenas à energia escura, que será reconstruída a partir dos dados. Desta forma todo o restante do arcabouço teórico ainda é dependente de modelo. A nossa abordagem é a mesma adotada em [76] na reconstrução da função de desacele- ração e vai mais além, no quesito de sermos mais cautelosos. Neste aspecto, vamos supor que sabemos sobre algumas constituintes do Universo, como a matéria escura e bárions. Quantida- des essas já existentes no estudo em Cosmologia. Em contrapartida não vamos supor nada a respeito da energia escura. É nesse contexto em particular que pretendemos reconstruir w(z). Agora a energia escura pode ser qualquer fluido barotrópico e como sabemos qualquer fluido deste tipo pode ser totalmente descrito por uma equação de estado da forma w = p/ρ. A princípio usamos a técnica de spline cúbica que entraremos em mais detalhes na subseção de- dicada à ela. Caso se mostre viável, para futuros trabalhos, pode-se abordar outras técnicas de reconstrução como a de funções radiais de base e de processo gaussiano. 5.2 DESCRIÇÃO DO FATOR DE ESCALA COMO UM POLINÔMIO O fator de escala como uma função pode ser descrito por um polinômio. Esse tipo de aproximação de funções por polinômio é um conceito bem antigo e ainda muito usado da análise numérica na chamada interpolação polinomial. Isso se deve muito ao fato de que os polinômios podem ser facilmente implementados em códigos computacionais. Além do mais suas derivadas e integrais não são outra coisa além de polinômios e podemos encontrar com certa facilidade as suas raízes, dentre outras características que torna compreensível o uso de descrição de funções por polinômios em simulações computacionais [116]. Neste caso, considerando o fator de escala a(t) como uma função polinomial depen- dente do tempo, podemos usar uma série de Taylor e expandir a(t) em torno de t0 (tempo hoje ou do observador), pois estamos interessados em determinar o fator de escala num tempo t, 102 logo o único referencial que podemos sem sombra de dúvidas fixar, se trata do nosso próprio referencial. Assim temos: a(t) = ∞∑ n=0 a(n)(t0) n! (t − t0) n = a(t0) + a′(t0) 1! (t − t0) + a′′(t0) 2! (t − t0) 2 + a′′′(t0) 3! (t − t0)3 + · · · , (5.1) em que a derivada é com relação à t. Comumente, quando a derivada é com relação ao tempo, utilizamos a notação de Newton. Então a expansão em série dada em 5.1 pode ser reescrita como: a(t) = a(t0) + ˙a(t0)(t − t0) + ¨a(t0) 2 (t − t0)2, (5.2) em que trabalharemos até o termo de 2ª ordem de potências, pois ele já nos dá uma boa estima- tiva 1. Um ponto a se ressaltar é que fazemos a expansão em série de Taylor em torno do tempo do observador, t0, pois estamos interessados em determinar o fator de escala num tempo t, logo o único referencial que podemos sem sombra de dúvidas fixar, se trata do nosso próprio referencial. Dividindo ambos os lados da igualdade na Equação 5.2 por a(t0) (fator de escala hoje) obtemos: a(t) a(t0) = a(t0) a(t0) + ˙a(t0) a(t0)(t − t0) + 1 2 ¨a(t0) a(to) (t − to) 2. (5.3) O termo ˙a(t0) a(t0) = ( ˙a a ) t0 = H0 é a função de Hubble no tempo t0. Então a Equação 5.3 fica: a(t) a(t0) = 1 + H0(t − t0) + 1 2 ¨a(t0) a(t0)(t − t0)2. (5.4) Já o termo ¨a(t0) a(t0) = ( ¨a a ) t0 exigirá um pouco mais de álgebra para ser reescrito numa forma mais conveniente. Vamos multiplicar [ − a(t0) a(t0) ( − ˙a 2(t0) ˙a2(t0) )] = 1 apenas nesse termo, de modo que temos: ¨a(t0) a(t0) = ¨a(t0) a(t0) [ − a(t0) a(t0) ( − ˙a2(t0) ˙a2(t0) )] . (5.5) O lado direito desta igualdade pode ser reescrito de tal forma que obtemos: 1Lembrando também que 1! = 1 e 2! = 2.1 = 2, são operações fatoriais. 103 ¨a(t0) a(t0) = ¨a(t0) a(t0) [ − a(t0) a(t0) ( − ˙a2(t0) ˙a2(t0) )] = − [( − ¨a(t0)a(t0) ˙a2(t0) ) ˙a2(t0) a2(t0) ] = − q0H 2 0 , (5.6) em que q0 é a função de desaceleração no tempo t0, e H 2 0 é a função de Hubble ao quadrado no tempo t0 definidas respectivamente por: q0 ≡ − ¨a(t0)a(t0) ˙a2(t0) = − ( ¨aa ˙a2 ) t0 e H 2 0 = ˙a2(t0) a2(t0) = [( ˙a a ) t0 ]2. (5.7) Substituindo a Equação 5.6 na Equação 5.4 obtemos: a(t) a(t0) = 1 + H0(t − t0) − q0H 2 0 2 (t − t0)2. (5.8) Multiplicando ambos os lados desta igualdade pelo fator de escala hoje a(t0) = a0 obtemos por fim uma forma conveniente para a expansão do fator de escala a(t): a(t) = a0 [ 1 + H0(t − t0) − q0H 2 0 2 (t − t0)2] . (5.9) 5.3 TÉCNICA DE spline PARA INTERPOLAÇÃO POLINOMIAL O fator de escala na forma dada em 5.9 nos diz que bastaríamos fitar os três coeficientes em aberto que são o a0, H0 e q0 afim de obter uma boa análise do problema em estudo. No entanto essa é uma das piores maneiras de se pensar sobre reconstrução, a não ser que tenhamos um intervalo bem pequeno que justifique o uso de uma expansão em série de Taylor. Alguns pontos a se ressaltar é que a série de Taylor possui um raio de convergência e se não temos a mínima noção do que tentamos reconstruir então não sabemos qual seria esse raio. Em outras palavras se não sabemos qual função estamos expandindo então também não sabemos o seu raio de convergência. Além disso, uma análise em série de Taylor com apenas três termos em todo o seu intervalo, implica em uma reconstrução de todas as funções que sejam expansíveis nesse tipo de série com raio de convergência razoável para uma boa descrição. Diante disso essa descrição pode ser muito extrapolada, tanto que este tipo de análise em particular geralmente é descartada em estudos de vários problemas envolvendo reconstrução na Física, pois muitas vezes ao observarmos a disposição dos pontos em um gráfico fica claro que este não será descrito de forma adequada por um polinômio de 3ª ordem. Uma alternativa seria pensar que se aumentássemos a ordem da expansão isso seria re- solvido. Entretanto, acabamos caindo no chamado fenômeno de Runge. Esse fenômeno ocorre 104 quando tentamos ajustar uma interpolação polinomial de um polinômio de ordem elevada e nas bordas do intervalo em análise temos um problema de oscilação (ver Figura 5.1). Isso prejudica muito o ajuste da curva que melhor se adapta aos dados e consequentemente a reconstrução. Figura 5.1: Representação gráfica do fenômeno de Runge. A curva vermelha é a chamada fun- ção de Runge. A curva azul é uma interpolação polinomial em 5ª ordem dessa mesma função, usando 6 pontos de interpolação igualmente espaçados. E a curva verde é uma interpolação, também da mesma função, em 9ª ordem com 10 pontos de interpolação igualmente espaçados. Fonte: Wikimedia. Como estamos interessados em uma função contínua, o aumento da ordem do polinômio seria prejudicial a tal característica. Justamente por que essa oscilação registrada com o aumento da ordem polinomial é resultante da técnica do ajuste polinomial e não um certo fenômeno que estamos extraindo da natureza. Devido a isso a comunidade científica acabou desenvolvendo várias outras técnicas de interpolação, tendo em vista que aumentar a ordem do polinômio ocasionaria esse fenômeno. Uma alternativa é interpolar a função f (x) em grupos de poucos pontos, o que restringe nossa função a um polinômio de grau menor. Para tal devemos impor algumas condições para que a função aproximada seja contínua e também possua derivadas contínuas até uma certa ordem de interesse. Esse tipo de interpolação polinomial é conhecido na literatura como spline2. Existe uma definição formal para esta técnica que aqui será exposta com base em [85]. Definição 5.1. Função Spline: Seja f (x) uma função tabelada nos pontos x0 < x1 < ... < xn. Uma função Sp(x) é denominada spline interpolante de grau p com nós nos pontos xi, i = 0, 1, ..., n, se satisfaz as seguintes condições: 2Não possui tradução para o português. 105 (I) em cada subintervalo [xi, xi+1], i = 0, 1, ..., n − 1, Sp(x) é um polinômio de grau p : sp(x). (II) Sp(x) é contínua e tem derivada contínua até ordem p − 1 no intervalo. (III) Sp(xi) = f (xi), i = 0, 1, ..., n. A origem do nome spline vem de uma régua elástica usada em desenhos de engenharia, que pode ser curvada de forma que passe por um dado conjunto de pontos (xi, yi) que é chamado de spline. Sob certas hipóteses a curva formada pela régua ode ser aproximada por uma função por partes, de modo que cada uma representa um polinômio cúbico, tal que essa função e suas primeiras duas derivadas sejam contínuas sempre (a terceira derivada pode ser descontínua nos pontos xi). Segundo a definição de spline podemos então dizer que tal função nada mais é do que uma spline cúbica interpolante com nós nos pontos xi [85]. A técnica de spline pode ser linear S1(x), quadrática S2(x) ou cúbica S3(x). A des- vantagem de se usar a spline linear, S1(x) vêm do fato de que esta apresenta uma derivada primeira descontínua nos nós. Em contrapartida splines quadráticas apresentam polinômios com primeiras derivadas contínuas e, portanto, a curvatura do polinômio pode trocar nos nós, ou seja a curva que antes era de um jeito passa a ser de outro abruptamente. Devido a esses fatores as splines cúbicas são as mais usadas, pois elas são funções polinomiais por partes, con- tínuas em que cada uma de suas partes Pk(x), é um polinômio de grau 3 no intervalo dado por [xk−1, xk], k = 1, 2, ..., n. E ainda, splines cúbicas apresentam primeira e segunda derivadas contínuas nos nós, portanto todos os parâmetros (valor da função em cada nó) se relacionam por meio dessas condições, tendo em vista que essa característica nos diz que as splines cúbicas não apresentam picos e nem trocam abruptamente de curvatura nos nós [85, 76]. Graficamente uma spline consiste no seguinte cenário: Suponha pontos observados dispostos em um gráfico e suas respectivas barras de erro. Considerando o método de spline teremos nós onde entre cada um desses temos um tipo de polinômio cúbico particular para cada caso. Veja a Figura 5.2 para melhor compreensão. No nó representado na posição (x1, y1), nessa mesma figura, vamos impor a condição de que: P0(x1) = P1(x1). (5.10) Além disso temos interesse em manter a continuidade pelo menos até a segunda ordem de derivada desse polinômio de forma que temos no nó que estamos usando como referencial: P ′ 0(x1) = P ′ 1(x1) e P ′′ 0 (x1) = P ′′ 1 (x1). (5.11) Entretanto ao fazermos tal consideração acabamos por perder a liberdade total dos po- linômios, de modo que agora eles tem que existir em cada intervalo e satisfazer a continuidade 106 0 y x Dados e erros NÓ Pi Polinômios f (x) P0 P1 P2 P3 P4 x1 y1 x2 y2 Figura 5.2: Representação gráfica de uma função usando a técnica de spline. Fonte: Autor. de Pi(xi) bem como a sua primeira e segunda derivadas. Podemos aplicar as condições 5.10 e 5.11 para os outros nós da minha interpolação tal que em cada caso teremos um vínculo de ligação entre cada polinômio. Em outras palavras, o polinômio P1 tem um vínculo com o po- linômio P2 no ponto x2, por exemplo, assim como também possui um vínculo com o polinômio P0 no ponto x1. Podemos estender essa mesma característica aos demais polinômios. Existe uma gama enorme de literatura sobre essa técnica, que é muito conhecida e usada pela comunidade científica. Por exemplo o livro de autoria de Carl de Boor [2] é um dos melho- res acerca dessa técnica, em que ele demonstra que ela é uma das melhores coisas que podemos fazer ao interpolar uma função. Isso devido ao fato de que a spline minimiza o erro médio quadrático (MSE - do inglês mean squared error) de interpolação entre a curva verdadeira e a que estamos trabalhando. Essa característica vêm também do fato da spline ser cúbica. Para entender melhor, vamos supor que conhecemos a curva f (x) que representa um dado conjunto de dados graficamente, representados por pontos e seus respectivos erros. Pode- mos extrair alguns pontos dessa curva verdadeira e reconstruir outra denotada por ˆf (x), em que o símbolo de chapéu representa que se trata de um estimador. O MSE é uma medida geral do tamanho do erro de medição que é frequentemente usada. Ele é definido como [117]: MSE = E[( ˆX − X) 2], (5.12) em que nessa notação temos que E( ˆX) é o valor esperado do estimador ˆX, ou seja da medida e, X é o valor verdadeiro da quantidade medida. No nosso caso temos: MSE = E[( ˆf (x) − f (x)) 2]. (5.13) Em outro aspecto, o MSE nada mais é do que o desvio quadrático esperado de X e x0 e pode ser decomposto em contribuições de variância e viés como: 107 MSE = σ2 + β2. (5.14) Isso é claro pois se pensarmos que ˆf (x) é um estimador da curva, então de certo modo, ele apresenta uma distribuição estatística que vai ter uma certa estimativa de variância e viés. Neste aspecto, a variância representa o quanto que a estimativa da curva real varia pelo fato de que os dados contém erro. Ao passo que o viés representa a diferença que ocorre entre a curva estimada e a curva verdadeira, uma vez realizada a análise, ou seja se tentamos ajustar uma parábola usando uma reta, de certa forma estamos introduzindo um viés, pois ambas as curvas possuem uma diferença. Dessa forma quando medimos o MSE, estamos medindo o quanto tem de viés, que seria o quanto a gente diferiu em nossa estimativa da curva real, e o quanto tem de variância por que estamos reconstruindo uma estimativa em cima de algo que por si só já possui erro. Do ponto de vista que mais nos interessa, da interpolação: se conhecemos os pontos verdadeiros nos nós, a melhor curva que interpola esses pontos, i.e. que é capaz de calcular o valor entre os nós, de forma que o MSE de interpolação é mínimo, é uma spline cúbica. MSE → mínimo As splines são amplamente utilizadas em tudo o que fazemos, principalmente na Cosmo- logia. Toda vez que vamos descrever uma função numericamente, por exemplo, a não ser que a função seja simples e conhecemos os seus coeficientes, não temos outra maneira de descrevê-la a não ser usando o método de interpolação. Então no nosso caso usaremos as splines cúbicas para descrever o nosso Universo. 5.4 EQUAÇÃO DE ESTADO DA ENERGIA ESCURA POR PARTES Além do que discutimos na subseção anterior, existe uma outra técnica matemática por trás disso tudo. Na reconstrução de uma função, as derivadas dela terão erros cada vez maiores. Do ponto de vista numérico derivar é muitas vezes pior do que integrar, pois se integramos uma função com muito ruído ela tende a ficar suave. Ao passo que se derivamos uma função com muito ruído ela tende a ficar com mais ruído ainda. Então ao invés de descrever o próprio fator de escala a(t) usando splines cúbicas, descrevemos a maior derivada de nosso interesse, de 2ª ordem, que representará indiretamente a equação de estado da energia escura, uma vez que a segunda derivada com relação ao tempo t do fator de escala dado na Eq. 5.9 expressa uma expansão acelerada do Universo. Do ponto de vista que mais estamos interessados, temos o modelo cosmológico CDM (Cold Dark Matter) mais uma certa função w(z) que nos fornece a equação de estado da energia 108 escura em um dado redshift que matematicamente é dada por: w(z) = p DE ρDE (5.15) em que pDE e ρDE são a pressão e a densidade da energia escura (o subescrito DE vêm do inglês dark energy). Neste trabalho evitamos fazer escolhas arbitrárias da forma w(z) e consequentemente, a priori restringindo-o a formas funcionais específicas, aproximando w(z) por partes até uma função polinomial de 3ª ordem, ou seja, um spline cúbico. O primeiro passo para construir um estimador de w(z) (denotado por ˆw(z)) é especificar o intervalo de redshift (domínio D) no qual a função é definida. Este intervalo é D = [zmín, zmáx], em que zmín e zmáx são os desvios para o vermelho mínimo e máximo dos dados observacionais usados em nosso trabalho. A próxima etapa consiste em partirmos o domínio D em n sub- intervalos nos quais definimos ˆw(z) como uma função cúbica, a saber, ˆw(z) =    z0 ≤ z ≤ z1 P0(z) P0(z0) = w0 e P0(z1) = w1 z1 ≤ z ≤ z2 P1(z) P1(z1) = w1 e P1(z2) = w2 ... ... ... zn−1 ≤ z ≤ zn Pn−1(z) Pn−1(zn−1) = wn−1 e Pn−1(zn) = wn (5.16) em que Pk(z) = ak(z − zk) 3 + bk(z − zk) 2 + ck(z − zk) + dk, z ∈ [zk, zk+1), (5.17) com intervalo fechado quando k = n − 1. Neste caso z0 = zmín e zn = zmáx e Pk são polinômios cúbicos definidos para o i-ésimo sub-intervalo. Temos que w0, w1, w2, ..., wN são os valores da equação de estado da energia escura, w(z), nos nós. Por exemplo, w0 representa o valor da função w(z) no nó zero3 ou no primeiro redshift. Note ainda que cada polinômio Pk(z) no segmento [zk, zk+1) depende de 4 parâmetros (ak, bk, ck e dk) e, consequentemente, precisamos estimar 4n parâmetros para definir ˆw(z) de acordo com o domínio D. No entanto, impomos as seguintes condições de continuidade, Pk+1(zk+1) = Pk(zk+1), (5.18) P ′ k+1(zk+1) = P ′ k(zk+1), (5.19) P ′′ k+1(zk+1) = P ′′ k (zk+1), (5.20) nos n − 1 nós internos k ∈ (1, n − 1) e o símbolo de plica ( ′) denota derivada em relação a z. Isso restringe a nossa reconstrução de 4n parâmetros para n + 2 a serem determinados. No 3Também podemos denotá-lo como o primeiro nó. Lembrando que qualquer área do conhecimento humano que não seja a computação, começa a contar parâmetros a partir de 1, ao passo que na computação a contagem começa em 0. 109 entanto, ainda temos dois parâmetros em aberto a se determinar. Além disso, nas bordas, a curva reconstruída tende a apresentar um MSE maior em comparação com o seu “interior”. Para fixar os parâmetros e manter o MSE homogêneo em toda a curva vamos usar a técnica de Carl de Boor [2], chamada em inglês como not-a-knot, com tradução literal para “não é um nó” ou “não nós”, que é representada por duas condições de contorno de dois “não nós” dadas por: w′′′ 0 (z1) = w′′′ 1 (z1) e w′′′ n−2(zn−1) = w′′′ n−1(zn−1) (5.21) De acordo com essa técnica estamos dizendo que a derivada terceira também é contínua somente em dois casos particulares: entre o 1º e 2º polinômios, ou seja no segundo nó e entre o penúltimo e último polinômios, ou seja no penúltimo nó. Temos que z0 se trata do redshift representado no primeiro nó da spline cúbica, ou seja cada z que o nosso software escolhe é um nó em nossa reconstrução. Devido a isso como todo o intervalo de redshift vai até n, acabamos então com n + 1 nós que são os parâmetros a se determinar W = {d0, ..., dn}. Como temos uma relação de um para um entre a função para cada nó ˆwk ≡ ˆw(zk) e o parâmetro dk de cada polinômio, a partir de agora renomearemos o conjunto como W = { ˆwk} por uma questão de simplicidade notacional. Assim, a nossa função fica totalmente descrita se conhecemos os n + 1 nós e os n + 1 valores dessa função. Com isso estamos parametrizando a função interpolada com esses n + 1 valores de w(z) ao obter estimativas da equação de estado da energia escura. Ao escrevermos a equação de estado usando splines cúbicas obteremos o parâmetro de Hubble e o fator de escala, quando e se necessário através de integração. Deste modo, qualquer ruído que possamos obter na nossa reconstrução da equação de estado da energia escura vai ser minimizado ou diminuído ao encontrarmos observáveis que provém a partir da sua integração. 5.5 IMPLEMENTAÇÃO DO SOFTWARE WSPLINE Em Tecnologia de Informação temos a chamada implementação de software (programa computacional) que se trata da fase de criação de um software no contexto de um sistema de informação, ou seja corresponde à elaboração e preparação dos módulos que serão necessários para a sua produção. Essa é uma atividade que faz parte de um processo de software e que na prática são atividades complexas, que incluem diversas etapas para que atendam as necessidades de cliente e/ou usuário [118]. Esta seção é dedicada aos cálculos envolvidos na implementação do software Wspline. Como em teoria a energia escura é considerada um fluido, podemos usar a equação de continuidade de um fluido para analisá-la. Esta equação é dada matematicamente por 3.51 que aqui consideramos com c = 1: ˙ρ + 3H(ρ + p) = 0, (5.22) 110 em que ρ ≡ ρ(t) é a densidade e p ≡ p(t) a pressão isotrópica, ambas da energia escura e H é o parâmetro de Hubble definido na Eq. 3.8, que aqui reescrevemos novamente como: H ≡ H(t) = ˙a(t) a(t) = ˙a a. (5.23) No entanto, vamos reescrever essa expressão em termos do redshift, z, que se trata da principal grandeza de medida em nossos cálculos. Matematicamente sabemos que: 1 + z = a0 a ⇒ a = a0 1 + z , (5.24) em que a0 ≡ a(t0) é o fator de escala hoje. Substituindo 5.24 em 5.23: H = 1 a da dt = 1 a0 1 + z d dt [ a0 1 + z ] = (1 + z) a0 a0 d dt[(1 + z) −1] . (5.25) O fator de escala hoje é tratado como constante na derivação com relação à t, e dessa forma podemos simplificá-lo, pois o temos no numerador e denominador, no entanto z ≡ z(t), então, de acordo com a “regra da cadeia”: H = −(1 + z)(1 + z) −1−1 d dt [ 1 + z] = − 1 + z (1 + z)2 dz dt , (5.26) usando a notação de Newton: H = − ˙z (1 + z). (5.27) Podemos reescrever a Eq. 5.22 substituindo 5.27 na forma: dρ dt − 3 ˙z (1 + z)(ρ + p) = 0. (5.28) Por conveniência vamos fazer uma troca de variável no primeiro termo do lado esquerdo dessa igualdade. A partir da “regra da cadeia” podemos reescrever 5.28 como: dρ dz dz dt − 3 ˙z (1 + z)(ρ + p) = 0. (5.29) Dividindo ambos os lados dessa igualdade por ˙z obtemos: dρ dz − 3(ρ + p) (1 + z) = 0, (5.30) uma forma em termos de z diretamente. Reescrevendo 5.30: dρ dz = 3(ρ + p) (1 + z) . (5.31) 111 Em ciência da computação, geralmente, precisamos distinguir variáveis lineares de va- riáveis exponenciais. No nosso caso o redshift de objetos próximos, da fase recente do Uni- verso, como mostra a Tabela 5.1 apresentam uma variação que pode ser considerada do tipo linear, no entanto em um Universo mais antigo, como também mostra a tabela temos valores que se distanciam de forma não-linear [1]. Tabela 5.1: Valores aproximados de redshift para as principais fases do Universo e objeto ou fenômeno descrito de acordo com z. Fase do Universo Objeto/Fenômeno redshift z Recente Planetas e estrelas 0.1 Primeiras Galáxias (Reionização) 10 Antiga CMB 1080 Igualdade entre radiação e matéria 10000 Inicial Nucleossíntese 10 10 Neste aspecto, se quisermos criar um modelo fenomenológico que descreva a energia escura, e que seja capaz de descrever as coisas acontecendo com ela desde as mais antigas até as mais recentes fases do Universo é evidente que a variável z não se trata de uma boa variável computacional, pois ela varia muito de escala. Supondo que em uma grade dividida em 1000 partes iguais de valores de z como mostra a Figura 5.3, temos como 0 o menor valor e 10 10 o maior valor de z. Então neste caso, teremos 10 10/1000 = 107 valores de z para cada intervalo e isso na descrição da nossa função Wspline significa dizer que no nosso método not-a-knot [76], o primeiro nó é z = 0 e o segundo nó é z = 107, o que engloba muitas fases do Universo. Essa peculiaridade nos dá uma simplici- dade para a função, o que acarreta em uma falta de nível de detalhe nos cálculos numéricos e consequentemente numa péssima descrição do modelo. Esse tipo de problema acontece bastante na literatura e uma maneira de lidarmos com ele é trabalharmos com uma variável que seja o logaritmo natural, ln, da variável em estudo. No nosso caso, de imediato pensamos que ao invés de trabalharmos com z devemos usar ln[z], ou seja a nova variável seria do tipo: α = ln[z]. (5.32) 112 Figura 5.3: Grade de representação dos valores de z com divisão em 1000 partes iguais. No entanto, essa variável α ainda não é boa para os cálculos, como veremos. Quando z = 10 10 obtemos, de acordo com uma propriedade básica do logaritmo natural, o seguinte resultado para α: α = 10 ln[10] ≈ 23.02585. (5.33) Esse não é um número tão grande, então a princípio seria mais fácil de se trabalhar. Quando z = 1 obtemos α = 10 ln[1] = 0. (5.34) É fato que a variação de escala é bem menor que antes, pois de z = 1 até z = 10 10 tere- mos uma variação pequena de escala em α com relação a variável z, no caso, a diferença entre os dois resultados que seria de aproximadamente 23.02585. Porém teremos uma discrepância quando z = 0, que se trata do redshift hoje, pois neste caso: α = 10 ln[0] → ∞, (5.35) e isso nos diz que no intervalo de z = 0 até z = 1 teríamos uma escala muito grande de valores e de certo modo teríamos o mesmo problema já discutido anteriormente. Efetivamente, a melhor variável para trabalharmos deve ter a seguinte forma: α = ln[1 + z]. (5.36) Ou se quisermos escrever z em termos dessa nova variável basta elevarmos ambos os lados da igualdade em 5.36 à exponencial, e usarmos a definição da função exponencial, de tal modo que obtemos: eα = eln[1+z] ⇒ e α = 1 + z ⇒ z = eα − 1. (5.37) Agora, quando z = 0 (redshift hoje, de acordo com 5.36, obtemos como resultado para a nova variável α: αi = ln[1 + 0] = 0. (5.38) E quando z = 10 10 (redshift final), temos que: αf = ln[1 + 1010] ≈ 23.02585, (5.39) 113 o mesmo resultado encontrado em 5.33. Para fins de comparação temos, quando z = 1: α = ln[1 + 1] = ln[2] ≈ 0.69315, (5.40) que se trata de um valor pequeno de variação com relação ao valor encontrado em 5.34. Então, podemos constatar que a nossa nova variável α, definida em 5.36, varia de 0 até 10 ln[10]. Com- putacionalmente essa é uma variação de escala pequena, o que facilita os cálculos numéricos e o particionamento no método not-a-knot em nossa função, e ainda por cima, engloba todas as fases do Universo, que é o nosso principal interesse. Como foi feito no artigo de referência [76] na reconstrução da função de desaceleração, vamos fazer um particionamento em 12 nós como mostra a Figura 5.4. Neste caso teremos um intervalo com valor de: Intgrade = αf Nnós = ln[1 + 1010] 12 ≈ 1.91882 (5.41) entre cada divisão da grade. Isso significa dizer que o primeiro nó z na posição αi = 0 numeri- camente é, de acordo com 5.40, dado por: z = e 0 − 1 = 0, (5.42) e o próximo nó será dado por: z = e 1.91882 − 1 ≈ 5.81292. (5.43) Esse ainda é um valor muito grande. αi αf 1.91882 Figura 5.4: Grade ilustrativa dos valores de α com divisão em 12 partes iguais no intervalo de redshift 0 ≤ z ≤ 10 10. Precisamos pensar numa maneira de discretizarmos o espaço para representarmos a fun- ção de forma que os nós estejam em pontos representativos do fenômeno físico em estudo. Se diminuirmos o nosso intervalo de redshift de z = 0 até z = 10 4, provavelmente teríamos mais valores entre o αi e αi+1. Além disso, incluímos o redshift da CMB que estamos mais interes- sados em uma primeira calibração do nosso trabalho. Assim, de acordo com 5.36 temos que a 114 minha grade terá um intervalo da grade de αi = 0 até: αf = ln[1 + 104] ≈ 9.21044. (5.44) Assim, se dividirmos a grade dos valores de α em 12 partes (nós) teremos que cada sub intervalo compreenderá um valor de aproximadamente 0.76754 entre cada nó (ver Figura 5.5). αi αf 0.76754 Figura 5.5: Grade ilustrativa dos valores de α com divisão em 12 partes iguais no intervalo de redshift 0 ≤ z ≤ 10 4. Neste caso temos que o primeiro nó é dado em z = 0 para α = 0 e o segundo é dado em: z = e0.76754 − 1 ≈ 1.1545, (5.45) para α = 0.76754. E o terceiro nó, para α = 2 × 0.76754, é z = e 2×0.76754 − 1 ≈ 3.6417. (5.46) Para fins de visualização o último nó em α = 9.21044 é dado para o redshift: z = e 12×0.76754 − 1 ≈ 10 4. (5.47) Deste modo, os intervalos serão mais próximos inicialmente e se distanciam mais rapidamente de acordo com os valores de redshift para as eras do Universo. No modelo o último valor de z que dita o intervalo em α e assim, realiza todos os procedimentos que discutimos de forma ilus- trativa para selecionar os valores de z, ou seja os nós da spline cúbica, em todo o intervalo de z0 até zf . Para visualizar esse procedimento ilustrativo de forma mais precisa vá até o Apêndice B, em que é apresentado um script que foi produzido com o objetivo de ilustrar o comportamento de α com relação à z para os dois casos (foi plotado até um gráfico representativo da função 5.36 em escala linear - no caso, uma função logarítmica do tipo crescente). Considerando isso precisamos agora calcular a Equação 5.31 nessa variável. De acordo com a regra da cadeia temos que: dρ dα = dρ dz dz dα . (5.48) A derivada de z com relação a α é fácil de ser calculada. Logo: dz dα = d dα [e α − 1 ] = e α, (5.49) 115 de acordo com a derivada da função exponencial. Substituindo 5.31 e 5.49 em 5.48 obtemos: dρ dα = 3(ρ + p) (1 + z) e α. (5.50) Como 1 + z = eα, de acordo com a segunda passagem em 5.37, podemos simplificar o termo eα, pois o teremos no numerador e denominador restando apenas: dρ dα = 3(ρ + p). (5.51) O nosso maior interesse é o da modelagem da equação de estado com relação à pressão. Pri- meiramente vamos dividir ambos os lados dessa igualdade por ρ, o que nos dá a seguinte forma: 1 ρ dρ dα = 3 ( 1 + p ρ ) . (5.52) Por definição temos que: w = p ρ , (5.53) em que w é a equação de estado para fluidos barotrópicos e que pode ser usada para a energia escura como discutimos ao apresentar a Eq. 3.52. E, assim como ρ e p dependem de t temos que w ≡ w(t). No nosso caso, estamos trabalhando com a variável α que é escrita em termos de z, logo w ≡ w[α(z)] ≡ w(α) por compactação. Então a Eq. 5.52 pode ser escrita como: 1 ρ dρ dα = 3[1 + w(α)]. (5.54) E ainda, o lado esquerdo dessa igualdade pode ser reescrito fazendo uso da derivada da função logarítmica, que no nosso caso resulta em: d ln[ρ] dα = 3[1 + w(α)]. (5.55) Integrando 5.55 com relação à variável α em ambos os lados da igualdade obtemos: ∫ α α0 d ln ρ(α′) dα′ dα′ = ∫ α α0 3[1 + w(α′)] dα′, (5.56) em que α′ é usado apenas como notação para os termos de integração, já que temos a variável α nos limites desta. E o limite inferior, α0, nada mais é do que a variável α hoje. Neste aspecto, temos que hoje z = 0 então de acordo com a Equação 5.36: α = ln[1 + 0] = 0. (5.57) 116 Logo, o limite inferior em 5.56 pode ser reescrito usando 5.57: ∫ α 0 d ln ρ(α′) dα′ dα′ = ∫ α 0 3[1 + w(α′)] dα′. (5.58) Usando o 2º teorema fundamental do cálculo ou regra de Barrow podemos calcular facilmente a integração do lado esquerdo dessa igualdade: ln[ρ(α)] − ln[ρ(0)] = ∫ α 0 3[1 + w(α′)] dα′. (5.59) E usando uma propriedade do logaritmo natural, que nos diz que a subtração dos logaritmos é o logaritmo da divisão [119], no lado esquerdo desta igualdade obtemos a seguinte forma: ln [ ρ ρ0 ] = ∫ α 0 3[1 + w(α′)] dα′, (5.60) em que ρ ≡ ρ(α) e ρ0 ≡ ρ(0). Elevando ambos os lados desta expressão à exponencial temos: exp { ln [ ρ ρ0 ]} = exp { ∫ α 0 3[1 + w(α′)] dα′}. (5.61) Usando a definição da função exponencial e que a integral de uma soma é a soma das integrais: ρ ρ0 = exp { ∫ α 0 3[1 + w(α′)] dα′} = exp { 3 ∫ α 0 dα′ + 3 ∫ α 0 w(α′) dα′} . (5.62) A primeira integração é trivial, já a segunda manteremos da mesma forma, o que nos dá, iso- lando ρ: ρ = ρ0 exp {3 [ α + ∫ α 0 w(α′) dα′]} . (5.63) Essa é a densidade de energia escura, explicitamente ρ ≡ ρDE(α). Definindo os parâmetros de densidades para a energia escura como ΩDE = ρ ρc0 e ΩDE0 = ρ0 ρc0 , podemos reescrever 5.63 usando-as, se dividirmos ambos os lados da igualdade pela densidade crítica hoje ρc0, ou seja: ρ ρc0 = ρ0 ρc0 exp { 3[α + ∫ α 0 w(α′) dα′]} ⇒ ΩDE = ΩDE0 exp { 3[α + ∫ α 0 w(α′) dα′]} . (5.64) Substituindo a Equação 5.36 em 5.64: ΩDE =ΩDE0 exp {3 ( ln[1 + z] + ∫ α 0 w(α′) dα′)} =ΩDE0 exp {3 ln[1 + z] } exp { 3 ∫ α 0 w(α′) dα′}, (5.65) em que usamos uma propriedade da função exponencial. Usando primeiramente uma proprie- 117 dade da função logarítmica no primeiro termo entre chaves: ΩDE = ΩDE0 exp { ln[(1 + z)3]} exp {3 ∫ α 0 w(α′) dα′} , (5.66) e aplicando a definição da função exponencial: ΩDE = ΩDE0(1 + z) 3 exp { 3 ∫ α 0 w(α′) dα′}. (5.67) Substituindo 5.37 em 5.67 temos ΩDE em termos de α escrito como: ΩDE = ΩDE0 exp { 3 [α + ∫ α 0 w(α′) dα′]} . (5.68) A densidade adimensional ΩDE depende do redshift z logo a derivada de 5.68 com relação à z é dada pela regra da cadeia como: dΩDE dz = dΩDE dα dα dz (5.69) Em 5.49 verificamos que dz dα = eα então: dα dz = 1 eα = e−α, (5.70) no entanto, de acordo com 5.37, temos que eα = 1 + z logo: dα dz = 1 1 + z . (5.71) Substituindo este resultado em 5.69: dΩDE dz = dΩDE dα [ 1 1 + z ]. (5.72) E a outra derivação é mais fácil de ser calculada se usarmos a forma dada em 5.63, para tal temos temos a seguinte forma: dΩDE dα = 1 ρc0 dρ dα = ρ0 ρc0 d dα [ exp {3[α + ∫ α 0 w(α′) dα′] }] , (5.73) em que a derivada não atua sobre ρc0 e nem sobre ρ0. De acordo com a derivada da função exponencial temos: dΩDE dα = ΩDE0 exp {3 [α + ∫ α 0 w(α′) dα′]} ︸ ︷︷ ︸ ΩDE:Equação 5.68 ( d dα [ 3α] ︸ ︷︷ ︸ 3 + d dα [3 ∫ α 0 w(α′) dα′]) , (5.74) 118 A segunda derivação nessa expressão pode ser calculada fazendo uso do 1º teorema fundamental do cálculo, o qual nos dá o seguinte resultado no nosso caso: d dα [3 ∫ α 0 w(α′) dα′] = 3w(α) − 3w(0) = 3w(α), (5.75) em que w(0) = 0 pois de acordo com 5.36 para α ser nulo z tem que ser igual a zero, o que é dado apenas quando a = a0 em 5.24 e isso só ocorre em t = 0 quando a pressão é nula para satisfazer 5.53. Substituindo 5.75 em 5.74: dΩDE dα = ΩDE3[1 + w(α)]. (5.76) E então a Eq. 5.72 pode ser reescrita usando essa expressão como: dΩDE dz = ΩDE3[1 + w(α)] [ 1 1 + z ] . (5.77) Em termos de α basta usarmos 1 + z = e α: dΩDE dz = 3e −α[1 + w(α)]ΩDE. (5.78) Essa expressão representa a derivada primeira do parâmetro de densidade de energia escura. A derivada segunda segue o mesmo raciocínio para o seu cálculo analítico. Temos que: d2ΩDE dz2 = d dz [dΩDE dz ] = dα dz d dα [3e −α[1+w(α)]ΩDE ] = 3e−α d dα [e −α[1+w(α)]ΩDE ]. (5.79) A derivação atua nos três termos nessa expressão então usando a regra de Leibniz: d2ΩDE dz2 = 3e −α{de−α dα [1 + w(α)]ΩDE + e−α d dα [ [1 + w(α)]ΩDE] }, (5.80) Novamente: d2ΩDE dz2 =3e −α{ (−e−α)[1 + w(α)]ΩDE + e−α[( d dα [ [1 + w(α)] ]) ΩDE + [1 + w(α)]dΩDE dα ]} =3e −α{ (−e−α)[1 + w(α)]ΩDE + e−α[w′(α)ΩDE + [1 + w(α)]dΩDE dα ]} , (5.81) em que w′(α) = dw(α) dα . Substituindo 5.76 nessa expressão: d2ΩDE dz2 = 3e −α{ (−e−α)[1+w(α)]ΩDE+e −α[ w′(α)ΩDE+[1+w(α)]ΩDE3[1+w(α)] ]} . (5.82) 119 Deixando e −α e ΩDE em evidência e organizando os termos: d 2ΩDE dz2 =e−2α{ − 3[1 + w(α)] + 3w′(α) + 9[1 + w(α)] 2}ΩDE =e−2α{ − 3[1 + w(α)] + 3w′(α) + 9[1 + 2w(α) + w2(α) }ΩDE =e−2α{ − 3 − 3w(α) + 3w′(α) + 9 + 18w(α) + 9w2(α) }ΩDE =e−2α{6 + 15w(α) + 3w′(α) + 9w2(α)} ΩDE. (5.83) Por fim: d2ΩDE dz2 = 3{2 + w(α)[5 + 3w(α)] + w′(α)}e−2αΩDE. (5.84) Neste caso temos que w(α) ≡ w(z) será uma spline cúbica como vimos na subseção anterior, a qual conheceremos os coeficientes de seus polinômios cúbicos. 5.6 PROCEDIMENTO DE ESTIMAÇÃO NUMÉRICO A partir de um conjunto de dados observacionais D = {SNe Ia, BAO, CCH}, determi- namos o intervalo de redshift z necessário para a reconstrução de w(z). Assim, podemos fazer uma reconstrução pelo método de splines cúbicas usando uma nova variável implementada para reconstruir w(z), que nada mais é do que os parâmetros que irão definir a curva do nosso mo- delo. Portanto em princípio w(z) é a nossa hipótese estatística reconstruída indiretamente a partir dos dados observacionais. Se fizermos uma hipótese sobre w(z) então obtemos uma certa curva. Dada uma spline cúbica podemos calcular quantidades de nosso interesse como o pa- râmetro de Hubble H(z), distâncias cosmológicas D(z), dentre outros observáveis. A nossa hipótese nos diz que a junção de CDM, com outras componentes de matéria como bárions e neutrinos (parâmetros fixos), mais w(z) (parâmetro livre) nos fornece a determinação da Cos- mologia em nosso trabalho. Nesse aspecto, o nosso modelo envolve uma parte fixa que é depen- dente de modelo, em que temos alguns parâmetros fixos e uma parte livre, que é independente de modelo, no que diz respeito a energia escura como apresentado na Figura 5.6. Com w(z) reconstruído podemos estimar a densidade da energia escura ρDE(z) e poste- riormente o parâmetro de densidade da energia escura ΩDE(z) do nosso modelo, dados respec- tivamente pelas Eqs. 5.63 e 5.68, que foram implementadas numericamente usando uma nova variável computacional, α (Equação 5.36). Com o parâmetro ΩDE podemos estimar o parâmetro de Hubble H(z) usando a Equação 3.74, que aqui escrevemos novamente para melhor visualização: H(z) = H0√Ωκ(z) + ΩM (z) + ΩR(z) + ΩDE(z), (5.85) 120 Modelo Parâmetro livre: w(z) Parâmetros fixos: CDM Neutrinos Bárions, dentre outros. Figura 5.6: Parâmetros fixos e livre do nosso modelo. No nosso trabalho consideramos H0 (Eq. 3.9) como livre ou fixo, T (True) ou F (False) em notação de dados booleanos. Quanto aos outros parâmetros temos que Ωκ pode ser T ou F, assim como Ωc definido no nosso trabalho, a partir do parâmetro de densidade de matéria ΩM , como: ΩM = Ωc + 0.05. (5.86) O valor 0.05 que aparece como acréscimo na Eq. 5.86 se refere ao valor encontrado na literatura [1] para o parâmetro de densidade de matéria bariônica Ωb (Eq. 3.100). E consideramos ΩR ≈ 0 de acordo com 3.102. Assim, H(z) é estimado em nosso trabalho de acordo com essas relações como: H(z) = H0√ Ωκ(z) + Ωc(z) + 0.05 + ΩDE(z). (5.87) Determinar H(z) em diferentes pontos da evolução do Universo pode nos ajudar a entender melhor a dinâmica de sua expansão, ademais, tem se mostrado útil na realização de análises e testes cosmológicos, como o desenvolvido neste trabalho. Devemos salientar que os parâmetros fixos do nosso modelo dados na Figura 5.6 se referem especificamente às densidades de matéria escura (ρcdm) e bariônica (ρb) e não aos pa- râmetros de densidade de ambos. Logo Ωc pode ser T ou F para se adequar ao nosso modelo, tal que ainda assim mantemos o principal aspecto deste, que é a componente de matéria escura fixa. Os valores de partida para os três parâmetros principais adotados neste trabalho são: H0 = 70.0 km s −1 Mpc −1 Ωc = 0.25 e Ωk = 0. (5.88) Ou seja, quando são livres, os parâmetros são ajustados pelos dados e têm como ponto de partida os valores dados em 5.88. Neste caso, a análise independente de modelo nos diz que não podemos deixá-los fixos, ao passo que se o forem estaremos dentro de uma abordagem 121 mais dependente de modelo. Neste aspecto, quando fixos, eles representam o MCP (ΛCDM). Por exemplo, na configuração TFT temos que H0 e Ωκ são livres e apenas Ωc é fixo. Vamos analisar o termo correspondente à matéria escura da expressão 3.92: Ωc︸︷︷︸ F = 8πG 3 H 2 0︸︷︷︸ T ρcdm. (5.89) Apesar de H0 ser livre para variar ele tentará se adequar ao valor fixo de Ωc. Desta forma ele é indiretamente um parâmetro fixo também. Neste caso a melhor configuração do nosso trabalho para analisarmos é TTT para os parâmetros H0, Ωc e Ωκ, pois mantemos as componentes da Figura 5.6 fixas e variamos apenas w(z), sendo que os parâmetros do nosso modelo são ajustados de acordo com os dados. Com H(z) estimado podemos determinar E(z), pois se trata do parâmetro de Hubble estimado em razão da constante de Hubble que pode ser T ou F, como já mencionamos. Assim podemos estimar a distância comóvel Dc em termos do redshift que é escrita como: Dc(z) = a0r = c H0 ∫ z 0 dz′ E(z′), (5.90) como deduzimos em 3.46. Finalmente, podemos então determinar a nossa previsão teórica para os observáveis cos- mológicos, e assim calcular as verossimilhanças para cada um desses casos, de modo que a verossimilhança final nos fornece a probabilidade do conjunto de dados observacionais D = {SNe Ia, BAO, CCH} dado o nosso modelo da reconstrução de w(z). Apenas após estimarmos w(z) que podemos calcular os observáveis que por sua vez são comparados com as observações. Neste sentido vamos construir a verossimilhança da seguinte forma: L(D| ˆθ) = L(D| ˆH0, { ˆwk(z)}, ˆΩc, ˆΩκ, ˆα, ˆβ, ˆMB), (5.91) em que ˆθ é o espaço paramétrico de dados do nosso modelo. Após a execução do software obtemos o best-fit4 referente ao conjunto de parâmetros que maximiza a verossimilhança. Os valores obtidos são as estimativas calculadas para um certo conjunto de dados, por exemplo: ˆθ . = ˆH0, ˆΩc, ˆΩκ, { ˆwk}, ˆα, ˆβ, ˆM1, ˆM2. Neste aspecto, os estimadores representam os parâmetros que uma vez livres e minimizados numericamente nos fornecem as estimativas para o modelo. A questão é que um estimador é uma função indireta dos dados. Por exemplo, { ˆwk} ≡ { ˆwk(D)} se pensarmos que o ponto dessa função que a maximiza é implicitamente uma função de D. Uma vez que sabemos que os estimadores são função dos dados, então podemos usar a própria distribuição destes para calcular a média, variância, valor esperado, região de confiança 5 4Melhor ajuste das estimativas, ou seja os melhores resultados obtidos com a execução do software. 5Podemos interpretar a região de confiança como sendo a probabilidade de dado um conjunto de dados esti- 122 e etc, desses estimadores, pois a inferência estatística é feita sobre eles. Para uma melhor compreensão da estimação numérica para o leitor, um mapa conceitual a respeito deste procedimento pode ser visualizado na Figura 5.7 no qual temos o procedimento sobre as 1ª e 2ª etapas, em que executamos o software para todas as configurações possíveis dos parâmetros principais do nosso modelo, pois queríamos ter um panorama geral sobre todas as possibilidades. Posteriormente, para a 3ª etapa, fixamos apenas Ωκ mantendo H0 e Ωc como parâmetros livres, pois essas configurações em particular já nos forneciam uma boa estimativa para a reconstrução de w(z). As Eqs. 5.78 e 5.84 também foram implementadas numericamente, entretanto não foram usadas na estimação de nenhum parâmetro nesse trabalho. Essa implementação ocorreu por que o código computacional foi escrito com o intuito de ser reutilizado em uma série de possíveis aplicações. Por exemplo, caso seja de nosso interesse estimarmos o parâmetro de desaceleração q(z) a partir da reconstrução de w(z), sabemos que q(z) depende dos seguintes parâmetros: q(z) = {E′(z), E(z)}. (5.92) Assim como o jerk, outro parâmetro cinemático do Universo, que é dado por: j(z) = {E′′(z), E′(z), E(z)}. (5.93) Estes são parâmetros que dependem das derivadas de E(z). Quando implementamos um objeto na biblioteca NumCosmo, o fazemos implementando todas as quantidades necessárias para todos os cálculos possíveis, afim de usar em futuros trabalhos. 5.7 MÉTODOS ESTATÍSTICOS EMPREGADOS NAS ANÁLISES Nesta seção abordaremos algumas ferramentas de estatística que foram utilizadas ao longo deste trabalho para as análises. Como, por exemplo, na verificação das regiões de confi- ança dos dados obtidos. 5.7.1 Método da máxima verossimilhança Supondo que temos uma função teórica f (x) para ajuste de um conjunto de dados {xi, fi(xi| ˆθ)} (veja a Figura 5.8) em que f (xi| ˆθ representa a função do nosso modelo escrita em termos dos dados D = xi e dos estimadores ˆθ. Se não sabemos nada a respeito dos pontos que representam esses dados, além das suas barras de erros, a única coisa que podemos fazer é esperar uma função f1 que passe entre cada ponto e que esta descreva fielmente os dados. Essa função pode ser determinada matematicamente por: marmos um valor próximo de um outro valor que descreva realmente o fenômeno em estudo. Em outras palavras a região de confiança nos diz o quão próximo o nosso valor estimado está do valor verdadeiro. 123 D = {SNe Ia, BAO, CCH} w(z) ΩDE(z) H(z) ΩM (z) = Ωc + 0.05 e ΩR(z) ≈ 0 H0=T ou F Intervalo de z Método de Reconstrução ρDE(z) α = ln[1 + z] w(z) = p/ρ Ωκ=T ou F Ωc=T ou F E(z) Dc(z) −2 ln L L = ∏ LD Figura 5.7: Mapa conceitual referente ao procedimento de estimação numérico deste presente trabalho para as 1ª e 2ª Etapas. χ2(xi| ˆθ) ≡ f1(xi| ˆθ) = N∑ i=1 [f t i − fi(xi| ˆθ)] 2 σ2 i , (5.94) em que σ2 i é a variância, uma medida que mostra o quão distante cada valor de um conjunto de dados está da média [117]. Quanto menor for a variância, mais próximos os valores estão da média, caso contrário quanto maior for a variância, mais os valores estão distantes da média. A Eq. 5.94 é a chamada distribuição χ2 ou o método dos mínimos quadrados [120], em que ˆθ nesse contexto são os parâmetros livres/estimadores 6 do modelo. É um método muito usado em Regressão Linear 7 para estimar os parâmetros desconhecidos criando um modelo que minimizará a soma dos erros quadrados entre os dados observados e os previstos por um certo modelo. 6Neste aspecto, se a melhor curva que descreve os dados é uma reta então ˆθ são parâmetros/estimadores que representas a inclinação da reta e o valor que corta o eixo, por exemplo. Se for uma parábola teremos mais parâmetros/estimadores. 7A Regressão Linear é um modelo que tem por objetivo resumir o relacionamento entre duas ou mais variáveis por meio de uma linha, e assim usar o resultado da função dessa linha para estimar valores, quando conhecendo as variáveis que a afetam [120]. 124 0 y x f1 f2 Figura 5.8: Representação gráfica de um ajuste de curvas usado na discussão sobre a distribui- ção χ 2 ser chamada de verossimilhança. No entanto, se quisermos algo mais rebuscado estatisticamente fazemos a seguinte per- gunta: quem é fi(xi| ˆθ)? Ele não pode ser o valor da minha função, pois se for ela teria que passar exatamente nos pontos como mostra a curva vermelha na Figura 5.8. O que fazemos é supor que fi seja um verdadeiro que denotamos como f t i mais um acréscimo de erro δi devido ao processo de medição: fi(xi| ˆθ) = f t i + δi(xi| ˆθ). (5.95) O processo chamado de mínimos quadrados dado em 5.94 é bem robusto, pois a curva é praticamente colocada a mão no seu ajuste com os pontos. Entretanto com a Eq. 5.95 temos algo mais consistente estatisticamente. Vamos considerar δi uma variável do tipo gaussiana cujos valores não estão correlacionados entre si. Se agora estendermos esse conceito para os dados atribuindo a estes a ideia de que são variáveis gaussianas não correlacionadas entre si, a distribuição de probabilidade desses δi é dada por: L(xi| ˆθ) = N∏ i=1 1 √ 2πσi e −δ2 i (xi| ˆθ) 2σ2 i , (5.96) em que L é a verossimilhança. Ela nos diz qual a distribuição estatística dos erros cometidos ao se observar os dados. Substituindo 5.95 nessa expressão obtemos: L(xi| ˆθ) = N∏ i=1 1 √2πσi exp { −1 2 [f t i − fi(xi| ˆθ)] 2 σ2 i } (5.97) Muitas vezes é mais conveniente utilizarmos a função logaritmo natural, ln, da verossimilhança. 125 Os valores dos parâmetros que maximizam a L ˆθmax L(xi) = arg max[L(xi| ˆθ)] (5.98) também maximizam ln L, pois o logaritmo é uma função monotonicamente crescente [121]. Lembrando que no nosso caso os estimadores são uma função indireta dos dados xi. Acontece que no máximo, a derivada da verossimilhança L é zero com relação aos parâmetros ˆθ = θj, pois supomos que o seu máximo não se encontra nos limites dos domínios desses parâmetros, ou seja [121]: ∂L ∂θj = 0, j = 1, ..., m. (5.99) Neste caso se a 2ª derivada for negativa teremos um máximo de L, o chamado método da máxima verossimilhança. Dessa forma podemos reescrever a Eq. 5.97 como: ln L = ln [ N∏ i=1 1 √ 2πσi exp {−1 2 (f t i − fi) 2 σ2 i }] , (5.100) em que L ≡ L(xi| ˆθ) e fi ≡ fi(xi| ˆθ) por compactação. Reescrevendo: ln L = N∑ i=1 ln [ 1 √2πσi ] + N∑ i=1 ln [ exp{ −1 2 (f t i − fi) 2 σ2 i }] = N∑ i=1 { ln[1] − ln[ √2πσi]} + N∑ i=1 [ − 1 2 (f t i − fi) 2 σ2 i ] = N∑ i=1 [ − 1 2 (f t i − fi)2 σ2 i ] − N∑ i=1 ln[ √ 2πσi], (5.101) em que usamos a propriedade do ln de um produto ser a soma dos ln, bem como as propriedades ln[1] = 0 e o ln da divisão ser a diferença dos ln, assim como a definição da função ln.8 Observe que a dependência nos parâmetros do modelo está toda no primeiro termo do lado direito dessa igualdade, de modo que podemos considerar o segundo termo constante, o chamado logaritmo da normalização. Deste modo para qualquer valor do meu espaço paramétrico, tal que f t i ≡ f (xi|θ), este segundo termo sempre será constante. Assim, podemos reescrever 5.101 como: −2 ln L = N∑ i=1 (f t i − fi) 2 σ2 i . (5.102) Essa é a verossimilhança construída supondo que os erros das medidas são variáveis gaussianas não correlacionadas. Se o fossem teríamos correlações entre os valores de δi e os de σi e não seria uma distribuição tão simples assim. Considerando que σi não depende dos parâmetros ˆθ 8Para mais detalhes sobre as propriedades e a definição da função ln mencionadas aqui, bem como em outras partes da dissertação, ver [119]. 126 essa equação é maximizada encontrando-se os valores de ˆθ que a minimizam. Neste caso, o método da máxima verossimilhança coincide com o método dos mínimos quadrados (Eq. 5.94): −2 ln L = χ2. (5.103) Deste modo, a partir desse resultado, podemos constatar que fazer mínimos quadrados é equiva- lente a fazer uma análise estatística de máxima verossimilhança (Eq. 5.96) supondo que nossos erros são variáveis gaussianas não correlacionadas. Em outras palavras minimizar χ 2 é equiva- lente a maximizar L. Isso é fácil de ser verificado. Basta retornamos alguns passos anteriores à Eq. 5.102 considerando o termos de normalização nulo que se refere ao caso que estamos discutindo: ln L = N∑ i=1 −1 2 (f t i − fi) 2 σ2 i ⇒ L = exp { N∑ i=1 [ − 1 2 (f t i − fi)2 σ2 i ]} . (5.104) Como a exponencial de uma soma é o produto das exponenciais temos: L = N∏ i=1 exp { −1 2 (f t i − fi) 2 σ2 i } . (5.105) Supondo como exemplo f t i = 2 e fi = 1 para σi = 1 tal que N = 1. Substituindo esses valores em 5.105 obtemos: L = exp { −1 2 (2 − 1)2 12 } ≈ 0.606. Agora vamos supor o caso em que a verossimilhança deve ser máxima, isso ocorre quando fi = f t i . Considerando os mesmos valores de N e σi anteriores, e fi = 2 = f t i por exemplo, obtemos: L = exp {−1 2 (2 − 2) 2 12 } = 1. (5.106) Logo concluímos que a máxima verossimilhança de L será dada quando: fi = f t i . (5.107) No nosso trabalho queremos fazer essa mesma afirmação quando por exemplo: ˆw(z) = w(z). (5.108) Evidentemente, pois teremos um valor máximo para L neste caso, do mesmo modo que teremos um mínimo 9 em −2 ln L (Eq. 5.102). Na prática isso tudo quer dizer que a nossa verossimi- 9É fácil verificar essa afirmação, pois para qualquer valor que substituirmos, tal que f t i ̸= fi, teremos −2 ln L > 0 sempre. Sendo seu menor valor dado de acordo com a Eq. 5.107. 127 lhança, do ponto de vista de uma função dos parâmetros fica gaussiana. Nem sempre a apro- ximação gaussiana é boa para análises, mas como um primeiro método, conseguimos obter a matriz de covariância e com esta calcular as regiões de confiança do modelo que terão formas elípticas devido ao fato de que maximizar L ou minimizar −2 ln L nos fornece a forma assintó- tica para a verossimilhança do modelo, ou seja aquela na qual temos um limite para ela, mesmo que seja aumentado o número de parâmetros. Sabemos que os dados observacionais, exceto os de CCH, são correlacionados separadamente, pois temos matrizes de covariância entre eles. Em contrapartida não existe correlação entre as verossimilhanças de cada caso, tendo em vista que os dados de SNe Ia, BAO e CCH não são correlacionados entre si. Assim, basta calcularmos o produto das verossimilhanças L de cada caso ou a soma entre cada −2 ln L dos dados. No nosso caso optamos por calcular a soma −2 ln L, pois sabemos explicitamente a sua forma para cada observável de acordo com o que apresentamos no Capítulo 4. Portanto, quando nos referimos a −2 ln L como sendo a verossimilhança final, basta ter em mente que seria equivalente a calcular a verossimilhança L final, já que são dados com erros gaussianos e não correlacionados. 5.7.2 Método da matriz de Fisher e de Markov Chain Monte Carlo Quando estimamos muitos parâmetros, temos um gasto computacional muito alto para a determinação das regiões de confiança do nosso modelo, ou seja as suas incertezas das esti- mativas. Para contornar isso usamos o método da matriz de Fisher que nos fornece uma boa aproximação da matriz de covariância dos estimadores ˆθ do modelo [121]. Em outro caso, se ainda procuramos por uma maior confiança a respeito das estimativas, quando meu problema tem altas dimensões, i.e. muitos parâmetros livres, utiliza-se o método de Markov Chain Monte Carlo (MCMC) que assim como a matriz de Fisher também nos fornece uma boa aproximação da matriz de covariância dos estimadores, só que com mais consistência. Nesse contexto, o método da matriz de Fisher não é um bom método para determinação das regiões de confiança de modelos degenerados, como é o do nosso trabalho. Deste modo precisamos de um método mais robusto para determinarmos as regiões de confiança numericamente e é nesse caso que entra o método de MCMC. A matriz de covariância quando determinada pode ser construída a partir das estimativas do meu modelo, sendo que ela contém as variâncias: Var(ˆx) = 1 N N∑ i=1 (ˆxi − ⟨ˆx⟩) 2, (5.109) para todas as estimativas do modelo mais as suas covariâncias, as quais medem o quanto todos os pares de variáveis estão relacionados pela seguinte expressão [122]: Cov({ˆxi}, {ˆyi}) = 1 N − 1 N∑ i=1 (ˆxi − ⟨ˆx⟩)(ˆyi − ⟨ˆy⟩). (5.110) 128 Essa covariância é uma medida da extensão para a qual existe um relacionamento linear entre xi e yi. Se ela for positiva, nos indica que grandes desses parâmetros tendem a ocorrer juntos, e um valor negativo nos diz que valores grandes de uma variável tendem a ocorrer juntos de valores pequenos para outra variável [122]. A partir de seu valor podemos obter o chamado coeficiente de correlação ou correlação de Pearson comum entre duas variáveis, que é definido como: Corr({ˆxi}, {ˆyi}) = Cov({ˆxi}, {ˆyi}) σˆxσˆy . (5.111) Ela nos fornece uma medida da direção e do grau com que duas variáveis se associam linear- mente em uma amostra. Temos que a correlação nunca pode ser maior do que 1 ou menor do que −1. Uma correlação próxima a zero indica que as duas variáveis não estão relacionadas entre si. Nesse contexto, uma correlação positiva indica que as duas variáveis movem-se juntas, e a relação é forte quanto mais a correlação se aproxima de 1. Enquanto que uma correlação ne- gativa indica que as duas variáveis movem-se em direções opostas, e que a relação também fica mais forte quanto mais próxima de −1 a correlação permanecer. Deste modo, duas variáveis que estão perfeitamente correlacionadas positivamente movem-se essencialmente em perfeita proporção na mesma direção, ao passo que dois conjuntos que estão perfeitamente correlaci- onados negativamente movem-se em perfeita proporção em direções opostas. Essa correlação pode ser medida de diferentes maneiras e ser considerada um tipo de média, variância, cosseno de um ângulo, entre outras coisas (para mais informações veja [123, 124]). Nos casos em que não conseguimos calcular a matriz de covariância do nosso modelo, temos que −2 ln L = X T C −1X nos dá uma boa aproximação para a inversa da matriz de co- variância C. Acontece que no mínimo a derivada de −2 ln L = χ2 com relação aos parâmetros θ = θj do modelo é zero, ou seja: d[−2 ln L] dθj = dχ 2 dθj = 0. (5.112) Um método de determinação dessa matriz de confiança é através do uso da matriz de Fisher Fij que é definida como o valor esperado da matriz Hessiana, Fij = 〈 ∂2 ln L(θ) ∂θi∂θj ∣ ∣ ∣ ∣ ∣θ0 〉 , (5.113) em que θ0 são os valores verdadeiros dos estimadores. Na prática não calculamos essa expres- são. O que fazemos é supor um conjunto de dados suficientemente grande de tal forma que podemos calcular se derivada segunda de ln L no melhor ajuste de ˆθ, C(ˆθi, ˆθj) = − (∂2 ln L(θ) ∂θi∂θj ∣ ∣ ∣ ∣ ∣ ˆθ )−1. (5.114) No limite assintótico (N → ∞), o inverso dessa equação é um estimador da matriz de Fisher, 129 ou seja, lim N →∞ ∂2 ln L(θ) ∂θi∂θj ∣ ∣ ∣ ∣ ∣ ˆθ = Fij. (5.115) Podemos então constatar, de acordo com as Eqs. 5.114 e 5.115, que a inversa da matriz de Fisher é a matriz de covariância: [Fij]−1 = [Cij]. (5.116) Para uma discussão mais detalhada sobre todo o processo ver o apêndice A.4 da Ref. [121]. No entanto como mencionado o método da matriz de Fisher não é tão confiante para as determinações de regiões de confiança de modelos degenerados, de modo que para tal usamos o método de MCMC, que é muito popular e usado quando pretendemos amostrar distribuições de probabilidade de altas dimensões. Esse é um método que foi desenvolvido na mesma época do advento dos primeiros computadores e foi muito usado na área de física de partículas como parte do projeto Manhattan na criação da bomba atômica [125]. O método de MCMC é uma junção das técnicas de Cadeias de Markov e de Monte Carlo. A Cadeia de Markov é um método sistemático em que se gera uma sequência de variá- veis randômicas onde o seu valor atual e probabilisticamente dependente do seu valor anterior. Em particular, a seleção da próxima variável depende apenas da última variável, gerando uma espécie de ciclo ou cadeia. Em contrapartida o método ou algoritmo de Monte Carlo serve para amostrar randomicamente (aleatoriamente) uma distribuição de probabilidade e aproximar uma quantidade desejada sendo muito utilizada na estimação de quantidades que são difíceis de se- rem calculadas com exatidão. A combinação entre esses dois métodos é chamado de Cadeia de Markov Monte Carlo, que na prática é essencialmente uma integração de Monte Carlo usando Cadeias de Markov. A integração de Monte Carlo extrai amostras da distribuição e, em seguida, forma médias de amostra para aproximar as expectativas. Neste aspecto, o MCMC analisa essas amostras executando uma Cadeia de Markov que exige um longo tempo para sua construção. Para mais detalhes ver a Ref. [126]. 5.7.3 Teorema do limite central e Regra 68-95-99 Com a discussão sobre o método da matriz de Fisher e de MCMC, não ficou muito ex- plícito, mas para obter a região de confiança de alguns parâmetros do modelo mantendo todos os outros fixos, basta calcular a equação paramétrica da elipse, sendo que a região de confiança obtida pelo método da Matriz de Fisher e de MCMC se aproxima assintoticamente da região de confiança do perfil da verossimilhança, da mesma forma que a distribuição dos estimadores é assintoticamente gaussiana [121]. Nesse contexto, estamos usufruindo do teorema do limite 130 central que nos diz o seguinte: se a população10 ou processo do qual uma amostra11 é reti- rada for normalmente distribuída, i.e. uma gaussiana, então a distribuição amostral da média também será normalmente distribuída, independentemente do tamanho da amostra. Em outras palavras, o teorema do limite central nos diz que contribuições estatísticas de origens distintas não-correlacionadas quando somadas tendem a uma distribuição gaussiana. Ou seja, a mistura de diferentes distribuições de variáveis de um sistema, como a distribuição de Poisson, multino- mial, dentre outras, nos diz que a sua estatística tende a uma distribuição gaussiana [117]. Como um primeiro método, essa é uma boa aproximação para dados experimentais como discutimos no final da Subseção 5.7.1. Em nosso caso a região de confiança se aplica por exemplo no seguinte cenário (ver Figura 5.9): Suponha que w0 e w1 são pontos verdadeiros, ou seja que descrevem o fenômeno como ele é realmente. Dado o meu modelo a pergunta que fazemos é: qual a probabilidade de estimarmos ˆw0 e ˆw1 se w0 e w1 são verdadeiros? Em outras palavras, qual a probabilidade de obtermos a hipótese alternativa, que contém os estimadores, sendo que, por exemplo, temos a ocorrência da hipótese verdadeira com 1σ de confiança ou em 68.3% das vezes? Esse é um teste de hipótese estatística, que se trata de um método de inferência estatística usado para decidir se os dados disponíveis suportam suficientemente uma hipótese alternativa, sendo que podemos rejeitar ou aceitar a hipótese alternativa dependendo do quão precisa ela é com relação a hipótese verdadeira [117]. É exatamente essa pergunta que tentamos responder quando determinamos a matriz de covariância que em nosso trabalho foi determinada numericamente. 0 w(z) z Região de confiança w0 w1 ˆw1 ˆw0 Figura 5.9: Representação da região de confiança entre valores verdadeiros w0, w1 (hipótese) e valores estimados ˆw1 e ˆw1 (hipótese alternativa) para 1σ de confiança cuja forma geométrica é dada pela equação paramétrica da elipse. Fonte: Autor. 10O conjunto de valores de uma característica (observável) associada a uma coleção de indivíduos ou objetos de interesse é dito ser uma população [127]. 11Uma sequência X1, ..., XN de N variáveis aleatórias independentes e identicamente distribuídas (i.i.d.) com função de densidade (f.d.p.) ou, no caso discreto, função de probabilidade (f.p.) f (x|θ) é dita ser uma amostra aleatória de tamanho N da distribuição de X [127]. 131 Para uma análise mais estatística das estimativas, as regiões de confiança do modelo podem ser representadas pela chamada regra 68-95-99, também chamada de regra empírica ou três sigma (3σ) que é muito usada em análises estatísticas. Essa regra afirma que, para uma distribuição normal (gaussiana), quase todos os dados observados cairão dentro de três desvios padrão (denotados por σ) da média (indicada por µ). Em particular, ela prevê que 68.3% das observações estão dentro do primeiro desvio padrão (µ ± σ), 95.4% dentro dos primeiros dois desvios padrão (µ ± 2σ) e 99.7% dentro dos três primeiros desvios padrão (µ ± 3σ). Graficamente as regiões de confiança de acordo com essa regra são representadas por áreas embaixo de uma curva gaussiana simétrica de modo que quanto maior a sua área, mais precisa é a nossa estimativa na rejeição de hipóteses, como mostra a Figura 5.10. Melhor dizendo, a região crítica, que seria a área que sobra quando consideramos uma hipótese verdadeira com um determinado intervalo de confiança, acaba ficando cada vez menor de modo que o poder de previsão da rejeição de uma hipótese acaba não ocorrendo. Assim sendo, por exemplo, se temos 1σ de confiança numa hipótese que supomos como verdadeira, então 68.3% das vezes obtemos ela, ao passo que se tivermos uma hipótese alternativa numa área que não seja a compreendida por 1σ então podemos rejeitá-la com 1σ de confiança. Figura 5.10: Regiões de confiança usando a regra 68-95-99 para 1σ, 2σ e 3σ de confiança. Fonte: Autor. Vejamos alguns exemplos simples reproduzidos da Ref. [128] para compreender melhor como funciona essa regra: Vamos supor que uma população de animais em um zoológico seja normalmente distribuída. Cada animal vive em média 13.1 anos (média µ), e o desvio padrão σ da expectativa de vida é de 1.5 anos. Se quisermos saber a probabilidade de um animal viver mais de 14.6 anos, pode-se usar a regra 68-95-99. Sabendo que a média da distribuição é de 13.1 anos, para cada desvio padrão ocorrem as seguintes faixas etárias: • 1σ (µ ± σ): (13.1 − 1.5) a (13.1 + 1.5) ou 11.6 a 14.6. 132 • 2σ (µ ± 2σ): 13.1 − (2 × 1.5) a 13.1 + (2 × 1.5) ou 10.1 a 16.1. • 3σ (µ ± 3σ): 13.1 − (3 × 1.5) a 13.1 + (3 × 1.5), ou 8.6 a 17.6. Esses dados estão representados na Figura 5.11. Figura 5.11: Regiões de confiança usando a regra 68-95-99 para 1σ, 2σ e 3σ de confiança para o primeiro exemplo discutido. Fonte: Autor. Assim a regra aplicada a esse exemplo nos mostra que 68.3% da distribuição está englo- bado/contido em 1σ, neste caso de 11.6 a 14.6. Os 31.7% restantes da distribuição total dada por 100% estão fora dessa faixa de modo que uma metade (15.85%) fica acima de 14.6 e a outra (15.85%) abaixo de 11.6. A minha hipótese alternativa é: qual a probabilidade do animal viver mais de 14.6? E a resposta é 15.85% pois essa hipótese está contida na metade acima de 14.6. Dessa forma estamos rejeitando com 1σ de confiança na hipótese verdadeira a hipótese alter- nativa de que o animal vive mais de 14.6 anos. Em contrapartida podemos aceitar uma outra hipótese alternativa dele viver dos 12 aos 14 anos com 1σ de confiança na hipótese verdadeira. Devemos ficar atentos a uma característica peculiar da ocorrência de eventos. Por exem- plo: Quando consideramos uma distribuição gaussiana para a minha hipótese verdadeira de cada animal viver em média 13.1 ± 1.5 anos de vida com 1σ de confiança, estamos na verdade dizendo que 68.3% das vezes podemos medir exatamente 13.1 bem como as outras medidas contidas na região de 1σ (11.6 a 14.6). Ou seja, 68.3% das vezes pode ocorrer qualquer va- lor de 11.6 a 14.6. Dependendo do grau de precisão de algarismos significativos temos muitas ocorrências nessa região de 1σ. Podemos então afirmar que para uma distribuição gaussiana todos as hipóteses/eventos que ocorrem dentro de 1σ de confiança são equiprováveis, i.e. todas as hipóteses alternativas dentro de 1σ de confiança da hipótese verdadeira ocorrem 68.3% das vezes. Neste contexto, não devemos de forma alguma cair na crença incorreta de que, se um determinado evento ocorre com mais frequência do que o normal no passado, é menos provável que aconteça no futuro ou vice-versa, quando estabelecemos que a probabilidade de tais eventos 133 não depende do que aconteceu no passado. Neste caso por exemplo, se jogarmos uma moeda honesta 3 vezes seguidas e ela cair uma vez cara e duas coroas e, logo em seguida, jogarmos essa mesma moeda mais 3 vezes e ela cair cara em todos os lançamentos, não devemos crer que se jogarmos mais 3 vezes essa moeda, ela não possa cair cara 3 vezes novamente. Pelo contrário tais eventos (cair cara ou coroa) são independentes do que ocorreu no passado. Caso acredi- temos que ela não possa cair cara 3 vezes novamente, então, estaríamos crendo na chamada falácia do jogador também conhecida por falácia de Monte Carlo ou falácia da maturidade das chances, que mais formalmente nos diz que uma única observação de um evento raro (no exemplo, cair cara 3 vezes seguidas) não contradiz que o evento é de fato raro, ou seja, é uma crença de que uma sequência tem mais probabilidade de terminar do que continuar [129]. Es- tendendo esse conceito para o nosso estudo podemos dizer que as medidas em 1σ de confiança são equiprováveis de tal forma que mesmo que uma delas ocorra 5 vezes seguidas, por exemplo, continuará tendo a mesma probabilidade de ocorrência nessa região. Essa probabilidade não cai com um evento raro ocorrendo, ainda permanece equiprovável. Como outro exemplo de uso da regra 68-95-99, suponha que a média de vida de uma espécie de um animal no zoológico é 10 anos de idade, com um desvio padrão de 1.4 anos. Se quisermos saber qual a probabilidade desse animal em particular viver por mais de 7.2 anos basta fazermos as mesmas análises do caso anterior: • 1σ (µ ± σ): (10 − 1.4) a (10 + 1.4) ou 8.6 a 11.4. • 2σ (µ ± 2σ): 10 − (2 × 1.4) a 10 + (2 × 1.4) ou 7.2 a 12.8. • 3σ (µ ± 3σ): 10 − (3 × 1.4) a 10 + (3 × 1.4), ou 5.8 a 14.2. Esses dados estão representados na Figura 5.12. Figura 5.12: Regiões de confiança usando a regra 68-95-99 para 1σ, 2σ e 3σ de confiança para o segundo exemplo discutido. Fonte: Autor. 134 Assim a regra aplicada a esse exemplo nos mostra que 95.4% da distribuição está englo- bado/contido em 2σ, neste caso de 7.2 a 12.8. Em outras palavras a minha hipótese verdadeira é que o animal viverá em média 10 ± 1.4 anos de idade com 2σ de confiança ou 95.4% das vezes. Os 4.6% restantes da distribuição total dada por 100% estão fora dessa faixa de modo que uma metade (2.3%) fica acima de 12.8 e a outra (2.3%) abaixo de 7.2. A minha hipótese alternativa é: qual a probabilidade do animal viver mais de 7.2 anos? A resposta não é tão trivial pois temos que a partir de 7.2 o animal pode viver todos os anos da região em 2σ e viver também mais a metade (2.3%) que fica acima de 12.8. Assim temos que somar toda a região de confiança de 2σ mais essa outra metade, de modo que temos então: 95.4% + 2.3% = 97.7%. Ou seja 97.7% das vezes esse animal sobrevive mais de 7.2 anos. Em contrapartida podemos rejeitar uma outra hipótese alternativa dele viver menos de 7.2 anos 2.3% das vezes com 2σ de confiança na hipótese verdadeira de 10 ± 1.4 anos de idade. Observe que temos sempre atrelado a região de confiança a seguinte pergunta: Consi- derando uma hipótese inicial como verdadeira com, por exemplo, 1σ de confiança ou 68.3% de ocorrência, rejeitamos ou aceitamos uma hipótese alternativa? Respondemos essa pergunta verificando se a hipótese alternativa está contida ou não na região de confiança em 1σ da nossa hipótese inicial. Faremos essas mesmas indagações nas análises dos nossos resultados no Capí- tulo 6. De modo que cada estimativa é descrita por uma distribuição gaussiana. Salientamos também que quanto maior a região de confiança (3σ ou mais) da hipótese verdadeira mais precisa ela é. Em contrapartida, para as nossas análises, o nosso modelo pode- ria acabar rejeitando poucas hipóteses alternativas nessa região de 3σ, i.e. ele aceitaria muitas hipóteses alternativas, o que nos diria que ele corrobora ainda mais outros modelos cosmológi- cos. Caso contrário, se o nosso modelo rejeita muitas hipóteses alternativas nessa região de 3σ, i.e. se ele aceita poucas hipóteses alternativas ou nenhuma, então de duas uma, ou ele tem uma boa acurácia ou ele é falho em algum atributo. 135 6 RESULTADOS E DISCUSSÕES Neste capítulo abordamos os principais resultados da reconstrução de modo que a partir deles discutimos sobre o quão consistente são as estimativas. Dividimos esse presente capítulo em três etapas. Na 1ª e 2ª etapas obtivemos os best-fits do nosso modelo com domínio para w(z) de 0 ≤ z ≤ 1080 e 0 ≤ z ≤ 3.0, respectivamente, em cada etapa. E, na 3ª etapa, usamos o mesmo domínio da 2ª etapa para uma modelagem utili- zando o método de Markov Chain Monte Carlo (MCMC) na obtenção das regiões de confiança da nossa reconstrução. 6.1 DISCUSSÃO SOBRE A RECONSTRUÇÃO O software nomeado como wspline pode ser encontrado na biblioteca numérica Num- Cosmo disponível na plataforma GitHub. O w no nome do software representa o parâmetro w da equação de estado da energia escura e o termo spline referencia o método polinomial usado na interpolação numérica 1. O processo de testagem de diferentes hipóteses ao fixar e/ ou manter livre os principais parâmetros na reconstrução foi todo organizado em um script em python que automatiza a chamada das funções na linguagem C que foram implementadas na NumCosmo. De acordo com as configurações temos que para cada reconstrução de w, considerando um número de nós em particular, o software executou um total de 8 vezes na 1ª e 2ª etapas. Ou seja, por exemplo, executamos a reconstrução para 5 nós e com H0 livre (podendo variar), Ωc fixo (não podendo variar) e Ωκ livre (configuração: TFT) obtemos as estimativas para w(z) em 5 redshifts diferentes de acordo com a variável α que implementamos numericamente. Ainda para 5 nós executamos o software para as demais configurações possíveis (Tabela 6.1) para cada um dos 3 parâmetros. Fizemos todas as configurações possíveis pois queríamos ter um panorama geral de todas as possibilidades nessas etapas. Para a 3ª etapa devido a alta demanda de tempo na execução do método de MCMC bem como a filtragem na 2ª etapa ficou constatado que a configuração TTT já nos fornecia bons resultados para análises. Desta forma optamos por executar apenas as configurações em que os parâmetros principais do nosso modelo são livres, ou seja, apenas a configuração TTT para 5, 6, 7 e 8 nós bem como o modelo XCDM que entraremos em mais detalhes na Seção 6.4. Para a reconstrução precisamos determinar a localização dos nós e o número destes que é viável na reconstrução de w(z), pois de certa forma podemos pensar que dependendo do nú- mero de nós poderíamos ter algo que afete as nossas medidas. Sabemos que essa incerteza acerca dos nós é ruim no sentido de que ela é discreta. Digamos, por exemplo, que usemos 3 nós na reconstrução: teremos uma curva muito enviesada de modo que essa vai se parecer muito 1Na biblioteca NumCosmo podemos encontrar o programa principal com o nome de nc_hicosmo_de_wspline.c e neste caso, na dissertação, quando mencionamos wspline estamos nos referindo ao conjunto de todos os códigos necessários para a execução do software. 136 Tabela 6.1: Tipos de dados booleanos para os parâmetros da reconstrução em que T (true) e F (False) se referem ao parâmetro livre e fixo, respectivamente. Parâmetros H0 Ωc Ωκ T T T T T F T F T T F F F T T F T F F F T F F F com um polinômio cúbico. E se aumentássemos o número de nós para 23? Bom, apesar de que agora a nossa curva não é mais enviesada, teremos outro problema, que reside no aumento da variância, pois quanto maior o número de nós, maior será o número de parâmetros que teremos de ajustar. Ou seja, a mesma quantidade de dados agora está tentando ajustar 23 parâmetros. Esses são os casos em que temos muitos parâmetros livres na reconstrução, de modo que o pro- grama tende a ficar mais lento e com barras de erro maiores. Logo a precisão para as estimativas desses parâmetros será menor, pois os estimadores acabam acumulando erros que por sua vez fazem com que as demais quantidades estimadas tenham erros maiores em comparação com as execuções nas quais temos um menor número de parâmetros livres. Consideramos −2 ln L = χ2 como sendo a verossimilhança do nosso problema. An- tigamente não tínhamos muita intuição a respeito da verossimilhança e isso acabou mudando com o passar dos anos. Isso se deve a algumas razões específicas que abordamos em detalhes na Subseção 5.7.1. Quanto menor for o valor da verossimilhança −2 ln L mais consistente será o nosso modelo. Entretanto quando o sistema é muito degenerado a execução do software acaba caindo nos chamados falsos mínimos da verossimilhança de modo que precisamos incluir um 137 processo de repetição da determinação do best-fit na tentativa de refinar a busca por um mínimo verdadeiro ou algo próximo. Para controlar esse processo de refinamento incluímos os erros absoluto e relativos definidos como: σabs ≤ 1.0 × 10 −3 e σrel ≤ 0.0. (6.1) Assim, o processo de minimização ocorre até que se atinja a tolerância máxima desses erros. Ou seja, quando as condições em 6.1 são satisfeitas o software considera uma boa preci- são para as estimativas e não executa novamente, gerando o best-fit. Neste caso, pode ocorrer de uma das condições ser satisfeita antes da outra, de modo que ela continua ficando menor ainda até que a outra seja também satisfeita. Os dados observacionais tiveram um avanço considerável de uma etapa para outra. Na 1ª etapa trabalhamos com os mesmos dados usados na reconstrução de q(z) em [76] enquanto que na 2ª etapa houve todo um trabalho de busca na literatura e implementação na biblioteca NumCosmo de dados mais recentes e/ou em maior quantidade. Com destaque para os dados de SNe Ia que apresenta uma grande diferença de dados na 2ª etapa (1701 SNe Ia de Brout et al. (2022) [42]) em comparação com os dados usados na 1ª etapa (740 SNe Ia de Betoule et al. (2014) [84]). Quanto aos dados relacionados ao distance priors da CMB temos que só usamos esses observáveis na 1ª etapa, pois a sua presença na 2ª etapa nos mostrou que, basicamente, eles serviam apenas para fixar w(z) próximo de zero, além de que esses observáveis são obtidos no contexto de um modelo específico, como por exemplo, o ΛCDM, e assim, a nossa análise seria bem mais dependente de modelo, o que não é de nosso interesse, uma vez que w(z) necessari- amente tem que ser independente de modelo em nosso trabalho, e isso acabaria poluindo a sua reconstrução. Particularmente, na 1ª etapa, em alguns best-fits do modelo, devido ao fato de estarmos trabalhando com um sistema muito degenerado, não conseguimos minimizar a nossa verossi- milhança e/ou temos algum parâmetro do modelo muito degenerado de tal forma que essas características geram uma matriz que não pode ser invertida. Caso seja insistido na inversão da matriz vamos obter os chamados nan (do inglês not a number) para as incertezas das es- timativas, que surgem quando temos por exemplo, alguma divisão de um parâmetro por zero. Deste modo a matriz não é definida positivamente e os erros não são confiáveis nesses casos. Uma alternativa seria diminuirmos o intervalo de redshift. De fato, foi o que fizemos para as 2ª e 3ª etapas, onde restringimos o nosso modelo no intervalo de redshift de z = 0 até z = 3.0. Isso ocorreu pois a calibração do modelo de 0 ≤ z ≤ 1080 na 1ª etapa nos permitiu excluir os observáveis cosmológicos distance priors para as etapas posteriores, pois estes deixavam o nosso modelo muito dependente em altos redshifts, gerando resultados mais enviesados em zf = 1080. Além disso, essa restrição só foi possível também devido ao grande número de dados implementados no intervalo de 0 ≤ z ≤ 3.0. 138 Na 3ª etapa, fizemos uso do método de MCMC para baixos redshifts afim de obtermos uma melhor noção sobre as incertezas do modelo com uma determinação mais robusta das regiões de confiança das estimativas e assim da distribuição para visualizações gráficas. Para uma melhor análise vamos reproduzir numa tabela as principais medidas dos parâ- metros cosmológicos encontrados na literatura que usaremos para comparação com os nossos resultados. As duas primeiras colunas de medidas correspondem a medidas determinadas a par- tir de uma análise dependentes de modelo e as duas últimas colunas de análises independentes de modelo apenas no que diz respeito a energia escura. A medida da constante de Hubble é dada em unidades de km s−1 Mpc−1 e é dependente de um modelo particular que pode ou não usar quantidades do MCP. Neste caso, a dependência e independência de modelo se refere aos mé- todos empregados em cada caso, como o tipo de medida ou considerações com relação a algum parâmetro. Os modelos FlatΛCDM e FlatwCDM são do Pantheon+ SH0ES para um Universo com seções espaciais planas em que o segundo modelo foi usado apenas os dados de SNe Ia na determinação de w0 [42]. O valor de Ωc determinado para o Planck foi calculado analiticamente na Eq. 3.98. Todas as medidas são dadas com 1σ de confiança, ou seja elas ocorrem 68.3% das vezes. Tabela 6.2: Principais medidas dos parâmetros cosmológicos que usaremos para as análises de alguns resultados em particular do nosso modelo. Dependente de modelo Independente de modelo Parâmetro Planck [1] FlatΛCDM [42] FlatwCDM [42] Riess [44] H0 67.4 ± 0.5 73.4 ± 1.1 72.86 +0.94 −1.06 73.04 ± 1.04 ΩM 0.3153 ± 0.0073 0.338 ± 0.018 0.307 +0.058 −0.063 Ωκ 0 0 0 Ωc 0.2641 ± 0.007 w0 −1.03 ± 0.03 −1 −0.89 ± 0.13 Vamos agora discutir alguns resultados obtidos em cada etapa. 139 6.2 1ª ETAPA - RESULTADOS E DISCUSSÕES Na 1ª etapa o intervalo de redshift foi analisado de 0 ≤ z ≤ 1080 em que z0 = 0 (redshift hoje) e zf = 1080 (redshift da CMB2). Maiores valores de z não foram analisados, pois não temos dados suficientes para uma boa estimativa da equação de estado em eras anteriores à CMB, bem como outros parâmetros. Além disso, próximo de zf , temos poucos dados obser- vacionais. Isso acarreta em resultados mais enviesados para w(z) no último nó, que representa esse redshift. E mais ainda, é visível nos resultados que nós próximos a esse também tendem a mudar abruptamente em comparação com os demais. Como executamos a reconstrução na 1ª etapa de 3 a 10 nós, temos um total de 8 conjuntos de estimativas (deve-se contar o 3), em que cada uma terá um total de 8 best-fits de acordo com as configurações apresentadas na Tabela 6.1. No total temos 8 × 8 = 64 best-fits nessa etapa. Nessa etapa usamos os dados observacionais que apresentamos nas subseções 4.1.1, 4.2.1, 4.3.1 e 4.4.1. Ela serviu como calibração da nossa reconstrução de modo que excluímos os dados de distance priors da CMB para a 2ª e 3ª etapas, pois constatamos que estes deixavam a nossa curva de w(z) muito dependente de modelo. Vamos analisar 2 best-fits em particular: a) 5 nós para w(z) com H0, Ωc e Ωκ na configuração TTT. Com espaço paramétrico de dados sendo: ˆθ . = ( ˆH0, Ωc, Ωκ, { ˆwk}, ˆα, ˆβ, ˆM1, ˆM2). (6.2) b) 8 nós para w(z) com H0, Ωc e Ωκ na configuração TTF. Com espaço paramétrico de dados sendo: ˆθ . = ( ˆH0, ˆΩc, { ˆwk}, ˆα, ˆβ, ˆM1, ˆM2), (6.3) em que { ˆwk} é o conjunto de todos as estimativas de w(z) no nó zk tal que k = 0, 1, ..., N representa a posição numérica do redshift no nó da interpolação. Os best-fits de a) e b) nessa 1ª etapa com as estimativas para cada parâmetro de seu respectivo espaço paramétrico podem ser visualizados na Tabela 6.3. Lembrando que o best-ft é gerado apenas quando as relações na Eq. 6.1 são satisfeitas. Como mencionado não podemos confiar cegamente nas regiões de confiança dessa etapa devido ao fato que essas foram determinadas pelo método da matriz de Fisher, que não é apro- priado para sistemas degenerados, como é o do nosso trabalho. Deste modo a discussão dos casos a) e b) se limitará apenas a analisar qual o valor mais próximo em teoria para cada um. Cada caso discutido separadamente (veja a Tabela 6.2 para as comparações que faremos em cada caso — se não consta nessa tabela a comparação que faremos, então a citamos na discus- são): 2Esse redshift não é medido, mas sim calculado a partir da consideração de que o espectro da CMB é um corpo negro quase perfeito. Essa descoberta foi feita pelo satélite COBE do inglês Cosmic Background Explorer [130]. 140 Tabela 6.3: Best-fit da configuração de 5 nós (TTT) e 8 nós (TTF) na 1ª etapa com 0 ≤ z ≤ 1080. A verossimilhança total é −2 ln L ≈ 701.33 e −2 ln L ≈ 701.89, respectivamente. Parâmetro Valor e erro (5 - TTT) z (5 nós) Valor e erro (8 - TTF) z (8 nós) ˆH0 69.2 ± 1.484 — 68.06 ± 1.085 — ˆΩc 0.2512 ± 0.01707 — 0.2401 ± 0.01935 — ˆΩκ 0.00897 ± 0.05842 — — — ˆw0 −0.8981 ± 0.2399 0 −1.697 ± 0.761 0 ˆw1 −0.9841 ± 3.697 × 10 −8 0.2 −0.828 ± 0.234 0.2 ˆw2 −0.9879 ± 0.3317 0.849 −1.333 ± 0.3531 0.3976 ˆw3 −0.0416 ± 0.5322 6.942 −0.134 ± 0.6325 0.849 ˆw4 −1.21 × 10 −5 ± 6.71 × 10 −11 1080 −2.352 ± 0.3772 2.091 ˆw5 — — −6.9 × 10 −6 ± 0.0216 6.942 ˆw6 — — −0.0002665 ± 0.7182 43.21 ˆw7 — — −5 ± 223.6 1080 ˆα 0.1409 ± 0.009322 — 0.1413 ± 0.0093 — ˆβ 3.096 ± 0.1137 — 3.097 ± 0.1138 — ˆM1 −19.06 ± 0.05892 — −19.13 ± 0.04208 — ˆM2 −19.13 ± 0.06078 — −19.2 ± 0.04511 — 141 a) 5 nós (TTT): A estimativa de ˆH0 é levemente maior que sua medida encontrada na li- teratura para a colaboração do Planck (lembrando que essa é uma medida proveniente de uma hipótese dependente de modelo, mais precisamente o ΛCDM). Em contrapartida ela está abaixo das medidas do Pantheon+ SH0ES e do Riess como mostradas na Tabela 6.2. A medida mais próxima de nossa estimativa para a constante de Hubble ˆH0 é dada por Freedman et al. [38] (H0 = 69.8 ± 0.8 km s −1 Mpc−1) que também é obtida com hipóteses independentes de modelo. Temos que ˆΩc e ˆΩκ são livres, ou seja eles são ajustados, assim como ˆH0, de acordo com os dados observacionais usados nessa etapa. Tanto ˆΩc quanto ˆΩκ são próximos de seus valores na literatura para a colaboração do Planck. Os estimadores de ˆw(z) foram determinados nos nós, que representam os redshifts, da interpolação em splines cúbicas dados na 3ª coluna da Tabela 6.3. As estimativas de ˆw0 e ˆw2 são bem próximas de w = −1 o que nos diz que a energia escura se comporta como constante cosmológica. No entanto ˆw1 apresenta um valor para a equação de estado −1 < w < 0 sendo que um dos candidatos que satisfaz esse intervalo para w é a chamada quintessência ou componente Q [131, 132]. O seu nome vêm do fato dela ser considerada uma quinta contribuição 3 para a densidade de energia cósmica, sendo amplamente definida, permitindo várias possibilidades, incluindo uma equação de estado que é constante, evoluindo uniformemente ou oscilando. Exemplos de um componente Q são campos fundamentais (escalar, vetorial ou tensor) ou objetos macroscópicos, como uma rede de luz ou cordas cósmicas emaranhadas. O trabalho de Caldwell et al. [131], por exemplo, se aplica a qualquer componente cujas propriedades hidrodinâmicas possam ser imitadas por um campo escalar evoluindo em um potencial que se acopla à matéria apenas por meio da gravidade. Nesses modelos, chamados de QCDM, a matéria é uma mistura de matéria escura fria não interativa (CDM), ou seja, poeira e um campo escalar [133]. Por sua vez ˆw3 e ˆw4 apresentam valores estimados próximos de w = 0 que se referem em teoria a um comportamento de matéria para a energia escura, de acordo com 3.60. Nesse aspecto ˆw4 é dado em z = 1080 e isso nos diz que a expansão do Universo era totalmente dominada por matéria na época da CMB. Devido a estimativas como essas que constatamos que os observáveis distance priors acabavam deixando nosso modelo muito dependente. Os parâmetros de SNe Ia estimados no nosso trabalho são valores próximos dos encon- trados no trabalho de Betoule et al. (Tabela 4.1). b) 8 nós (TTF): Para este caso consideramos apenas ˆΩκ = 0 fixo dos parâmetros principais (Universo plano). O valor de ˆH0 obtido com o nosso modelo para este caso é bem mais próximo do valor na 3As outras 4 contribuições são de bárions, neutrinos, matéria escura e radiação [131]. 142 literatura a colaboração do Planck que a estimativa do caso a). Nessa perspectiva o nosso modelo está nos fornecendo uma medida para a constante de Hubble mais dependente de modelo quando fixamos ˆΩκ = 0. Nesta configuração (TTF) ˆΩc era ajustado de acordo com os dados observacionais. A soma ˆΩM = ˆΩc + 0.05 = 0.2901 é bem próxima do valor para a medida do Planck e mais próxima ainda do modelo FlatwCDM do Pantheon+ SH0ES que se trata de uma distribuição assimétrica. Os estimadores de ˆw(z) foram determinados nos nós, que representam os redshifts, da in- terpolação em splines cúbicas dados na 5ª coluna da Tabela 6.3. Nesse caso em particular não obtivemos nenhum valor próximo o bastante que condiz em um comportamento de constante cosmológica para a energia escura, de modo que ˆw0, ˆw2, ˆw4 e ˆw7 são estimati- vas com valores de w < −1, que na literatura temos como possível candidato a chamada energia fantasma que satisfaz essa equação de estado da energia escura. Existem traba- lhos na literatura que estudam a possibilidade da equação de estado da energia escura ser menor que −1 como os de Carrol et al. [134] e Brett McInnes [135]. Um dos possíveis candidatos que satisfaz a equação de estado com w < −1 é a chamada energia fantasma que apresenta energia cinética negativa e prevê uma expansão em excesso do Universo, o que levaria ao cenário chamado de Big Rip (tradução literal: grande dilaceração), onde tudo o que existe no Universo, em seu estágio final, seria progressivamente dilacerado pela expansão, até que as distâncias entre as partículas se tornem infinitas [136]. Observe também, em particular, que o erro de ˆw5 é exageradamente grande de modo que os dados observacionais não estão nos dizendo nada a respeito de w(z) no redshift da CMB e não podemos concluir nada a respeito dessa estimativa em particular. Devido a isso que não podemos confiar muito no método da matriz de Fisher para sistemas degenerados. Obti- vemos novamente valores de −1 < w < 0 em ˆw1 e ˆw3. E um comportamento próximo de matéria para a energia escura (w = 0) em ˆw5 e ˆw6. Os parâmetros de SNe Ia estimados no nosso trabalho são valores próximos dos encon- trados no trabalho de Betoule et al. (Tabela 4.1). 6.3 2ª ETAPA - RESULTADOS E DISCUSSÕES Na 2ª etapa, devido a exclusão dos observáveis distance priors da CMB, restringimos o nosso modelo no intervalo de redshift de 0 ≤ z ≤ 3.0 em que z0 = 0 (redshift hoje) e z = 3.0 (redshifts de galáxias na sua maioria formadoras de estrelas [137]). Restrição essa que só foi possível também por que a implementação de dados mais recentes e/ou em maior quantidade, que apresentamos nas Subseções 4.1.2, 4.2.2 e 4.3.2, era por si só suficiente para restringir nosso modelo em baixos redshifts. A reconstrução foi feita de 3 a 12 nós na interpolação polinimial de w(z). Como temos 8 configurações possíveis de acordo com a Tabela 6.1 então obtemos um total de 8 × 10 = 80 143 best-fits. Lembrando que a contagem sempre começa do zero na programação, deste modo de 3 a 12 nós temos um intervalo de 10 nós. Em outras palavras temos 10 best-fits para cada configuração da Tabela 6.1. Vamos analisar 2 best-fits em particular: a) 5 nós para w(z) com H0, Ωc e Ωκ na configuração TTT. Com espaço paramétrico de dados sendo: ˆθ . = ( ˆH0, ˆΩc, ˆΩκ, { ˆwk}, ˆM1). (6.4) b) 8 nós para w(z) com H0, Ωc e Ωκ na configuração TTF. Com espaço paramétrico de dados sendo: ˆθ . = ( ˆH0, ˆΩc, { ˆwk}, ˆM1). (6.5) Os best-fits de a) e b) nessa 2ª etapa com as estimativas para cada parâmetro de seu respectivo espaço paramétrico podem ser visualizados na Tabela 6.4. Lembrando, novamente, que o best-fit é gerado apenas quando as relações na Eq. 6.1 são satisfeitas. Nesta etapa temos que os parâmetros de incômodo das SNe IA (α, β e M2) são fixos, apenas M1 é livre e estimado (no nosso modelo adotamos α = β = 0 e M2 = −19.25 sendo que esse valor para M2 não interfere nas nossas estimativas, poderia ser qualquer valor). Discutimos no final da Subseção 4.1.2 acerca desse aspecto. Novamente, pelo fato de ser um sistema muito degenerado acabamos por não confiar nas regiões de confiança obtidas pelo método da matriz de Fisher de modo que discutiremos os resultados dessa etapa verificando o quão próximos são de valores encontrados em teoria e na literatura, sem muita confiança. Cada caso discutido separadamente (veja a Tabela 6.2 para as comparações que faremos em cada caso — se não consta nessa tabela a comparação que faremos, então a citamos na discussão): a) 5 nós (TTT): Nesta configuração em particular da 2ª etapa a nossa estimativa para a constante de Hubble é um pouco maior que a medida do Planck. Em contrapartida ela é bem mais próxima das medidas do Pantheon+ SH0ES para ambos os modelos da Tabela 6.2 e do Riess. Temos que ˆΩκ é próximo de zero que condiz com um Universo plano. A soma ˆΩM = ˆΩc + 0.05 = 0.3326 é mais próxima do modelo FlatΛCDM do Pantheon+ SH0ES. Isso se deve ao fato de que ˆΩc é maior em comparação com a medida do Planck de modo que parece que temos mais matéria escura no Universo do que o observado. Os estimadores de ˆw(z) foram determinados nos nós (redshifts) da interpolação em spli- nes cúbicas dados na 3ª coluna da Tabela 6.4. Temos que ˆw0, ˆw1 e ˆw2 são bem próximos de w = −1 que corresponde à constante cosmológica. Já ˆw3 apresenta um valor de w < −1. Por outro lado ˆw4 dado no redshift z = 3.0 nos diz que a energia escura não se comporta nem como radiação nem como matéria de acordo com 3.60, de modo que temos um valor estimado de 0 < w < 1/3. 144 Tabela 6.4: Best-fit da configuração de 5 nós (TTT) e 8 nós (TTF) na 2ª etapa com 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1482.402 e −2 ln L ≈ 1481.436, respectivamente. Parâmetro Valor e erro (5 - TTT) z (5 nós) Valor e erro (8 - TTF) z (8 nós) ˆH0 71.7 ± 0.9676 — 71.44 ± 1.102 — ˆΩc 0.2826 ± 0.02952 — 0.2693 ± 0.01555 — ˆΩκ −0.0699 ± 0.08767 — — — ˆw0 −0.5991 ± 0.2766 0 −0.4472 ± 0.7867 0 ˆw1 −1.034 ± 0.1187 0.2 −1.174 ± 0.1737 0.2 ˆw2 −0.8412 ± 0.1036 0.4312 −0.95 ± 0.2697 0.2913 ˆw3 −1.297 ± 0.7539 1.0238 −0.5634 ± 0.3953 0.4312 ˆw4 0.1537 ± 0.1697 3.0 −1.426 ± 0.67 0.6533 ˆw5 — — −0.4944 ± 2.055 1.0238 ˆw6 — — −4.999 ± 10.08 1.688 ˆw7 — — 0.3671 ± 0.7068 3.0 ˆM1 −19.29 ± 0.02554 — −19.29 ± 0.02529 — 145 Quanto ao parâmetro de SNe Ia temos uma estimativa bem próxima de seu resultado pelo SH0ES (Eq. 4.21). b) 8 nós (TTF): Nesta configuração temos ˆΩκ fixo (Universo plano) de tal modo que obte- mos uma estimativa para a constante de Hubble muito similar com a do caso a) da mesma etapa, só que mais próxima do Pantheon+ SH0ES para ambos os modelos da Tabela 6.2 bem como da medida do Riess. A estimativa de ˆΩc é menor neste caso sendo que temos ˆΩM = ˆΩc + 0.05 = 0.3193 bem mais próximo da medida do Planck e do modelo FlatwCDM. Os estimadores de ˆw(z) foram determinados nos nós (redshifts) da interpolação em spli- nes cúbicas dados na 5ª coluna da Tabela 6.4. Assim sendo, temos que ˆw0, ˆw3, ˆw5 apresen- tam valores de −1 < w < 0. Enquanto que ˆw1, ˆw2 são valores estimados bem próximos de um comportamento de constante cosmológica para a energia escura. Novamente ob- tivemos também w < −1 em ˆw4 e ˆw6. O outro resultado, dado em ˆw7, nos fornece uma estimativa de 0 < w < 1/3, que nos diz que a energia escura ao invés de ter pressão ne- gativa, teria uma pressão positiva. Isso iria contra tudo o que observamos sobre expansão do Universo, pois ao invés da energia escura funcionar de forma contrária a gravidade, ela estaria empurrando os objetos, que colapsariam entre si. Esse seria o possível cená- rio do Big Crunch, modelo descartado devido a constatação de que o Universo está em expansão acelerada. No entanto, essa estimativa também apresenta uma incerteza grande em comparação com as demais de modo que, novamente, parece que os dados não estão nos dizendo nada a respeito de w(z) nesse redshift em particular. Tanto na 1ª quanto na 2ª etapa analisamos o conjunto de dados de { ˆwk} em que k = 0, 1, ..., N para cada redshift devido ao fato de que não temos como afirmar quais os melhores resultados para w(z) em todo o domínio de redshifts, pois não temos muita confiança na matriz de Fisher para sistemas degenerados. No entanto, na 3ª etapa, com o uso do método de MCMC podemos realizar tal análise de modo que temos mais robustez em nossos resultados. 6.4 3ª ETAPA - RESULTADOS E DISCUSSÕES Para reduzir as incertezas dos parâmetros em nossa reconstrução ao máximo aplicamos o método de MCMC que é baseado em um algoritmo que se move aleatoriamente no espaço dos parâmetros do nosso modelo. Executamos um total de 2400 walkers (tradução literal: ca- minhantes) no conjunto para 5, 6, 7 e 8 nós na interpolação polinomial de w(z), bem como para o modelo XCDM que considera a equação de estado da energia escura constante em todo o intervalo de redshift, assim obtemos apenas uma estimativa para w(z). O programa executou os best-fits apenas na configuração TTT para os parâmetros principais H0, Ωc e Ωκ com valores de partida dados de acordo com a Eq. 5.88. 146 Inicialmente, com o método de MCMC, a minha verossimilhança começa com um va- lor muito grande (perto da casa de 1 milhão) devido ao fato de que sempre começamos com uma distribuição aleatória, que dificilmente terá alguma relação com a distribuição certa para o modelo. Deste modo o acceptance ratio do método é alto, em torno de 40% estabilizando mais próximo de 5% e em alguns casos 11% que seria a melhor região do modelo e quando a verossimilhança minimiza. Para se obter um número considerável de pontos da distribuição o programa tem que ser executado por horas, em torno de 20h—40h em computadores com 4 núcleos de processadores, de modo que cabe ao usuário interromper a sua execução, senão ele pode continuar por muito mais tempo. No entanto com esse tempo de execução mencionado ele gera em torno de 2 milhões de pontos, que quando usados já geram uma boa estimativa da distribuição. A estabilidade ocorre numa assintota inferior de modo que sempre vemos o acceptance ratio caindo até um dígito que não estamos plotando mais. No entanto isso significa que a nossa reconstrução convergiu. Afim de diagnosticar o catálogo de amostras gerado usamos o mcat_analyze (ferra- menta disponível na NumCosmo) para excluir os pontos da distribuição que não são relevantes, e assim obter os best-fits do modelo e plotar os gráficos usando scripts implementados que auto- matizam o nosso trabalho. Assim sendo, analisaremos 2 casos em particular: 5 nós e o modelo XCDM. Para ambos os casos temos a mesma configuração TTT para os parâmetros principais e o mesmo espaço paramétrico de dados: ˆθ . = ( ˆH0, ˆΩc, ˆΩκ, { ˆwk}, ˆM1). (6.6) Sendo que nessa 3ª etapa usamos os mesmos dados observacionais da 2ª etapa que apresentamos nas Subseções 4.1.2, 4.2.2 e 4.3.2, bem como o mesmo intervalo de redshift de 0 ≤ z ≤ 3.0. Cada caso discutido separadamente (veja a Tabela 6.2 para as comparações que faremos em cada caso — se não consta nessa tabela a comparação que faremos, então a citamos na discussão): a) 5 nós (TTT): Nessa configuração temos uma spline cúbica de 5 nós na interpolação polinomial de w(z). O best-fit dessa configuração pode ser visualizado na Tabela 6.5. Nesse ponto vale fazer uma breve explicação de como interpretar esses resultados. O va- lor do melhor ajuste (best-fit) para H0 é aquele que maximiza a probabilidade dos dados dado o nosso modelo. Contudo, é natural esperar que o mesmo modelo seja compatível com outros valores de H0. Assim a região de 1σ nos fornece todos os valores de H0 tal que a probabilidade do conjunto de dados não seja muito pequena. Por exemplo, 1σ corresponde a valores de H0 para quais os dados tem uma chance de pelo 68.3% de ocor- rência. Naturalmente, fazendo essa restrição aceitamos um erro do tipo um de 31.7%, ou seja, valores fora do intervalo de 1σ ainda podem gerar o mesmo conjunto de dados porém com uma probabilidade de 31.7%. Aqui fica evidente o balanço que precisamos fazer, se escolhermos regiões maiores, minimizamos o erro do tipo um, mas aumentamos também a incerteza da nossa determinação. Em outras palavras, se aumentarmos o inter- 147 Tabela 6.5: Best-fit da configuração de 5 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossimi- lhança total é −2 ln L ≈ 1484. Parâmetro Valor e erro redshift z ˆH0 71.55 ± 0.9684 ˆΩc 0.2732 ± 0.01665 ˆΩκ 0.0002968 ± 0.01011 ˆw0 −0.6263 ± 0.338 0 ˆw1 −1.072 ± 0.1141 0.2 ˆw2 −0.842 ± 0.1676 0.431 ˆw3 −1.435 ± 1.1 1.024 ˆw4 0.2152 ± 1.835 3.0 ˆM1 −19.29 ± 0.0257 valo de 1σ para 2σ ou 3σ, por exemplo, então a chance de excluirmos boas hipóteses é menor, pois estamos obtendo mais confiança sob a medida de H0, entretanto diminuímos a capacidade de prever o seu valor verdadeiro. Dado o nosso modelo (com suas hipóteses e conjunto de dados) supostamente verdadeiro obtemos uma estimativa da constante de Hubble com valor ˆH0 = 71.55 ± 0.9684 km s−1 Mpc−1. Dessa forma, como nosso modelo é tratado como verdadeiro, consequentemente nossas estimativas são bem descritas pelo nosso ferramental estatístico de análise. Vamos agora estimar a constante de Hubble usando um outro modelo (com suas hipóteses e con- junto de dados). Primeiramente vamos associar 1σ = 68.3% de confiança ao intervalo de confiança da nossa estimativa dado de 71.55 − 0.9684 = 70.58 a 71.55 + 0.9684 = 72.52 em unidades de km s−1 Mpc −1 como mostra a Figura 6.1. Salientando que o intervalo de confiança é um funcional dos dados. Em outras palavras estamos dizendo que com a confiança de 1σ ou 68.3% das vezes aceitamos as hipóteses de que ˆH0 está entre esse intervalo de confiança. No entanto, as hipóteses descartadas ainda podem ocorrer, logo 31.7% das vezes estamos errando ao 148 Figura 6.1: Região de confiança de ˆH0 obtida da configuração de 5 nós (TTT) na 3ª etapa associada a 1σ = 68.3% de confiança. fazer tal consideração. O intervalo de confiança da medida da constante de Hubble do Planck [1] associado a 1σ de confiança é dado de 67.36 − 0.54 = 66.82 a 67.36 + 0.54 = 67.90 em unidades de km s−1 Mpc−1. Evidentemente essa região não está contida na região de confiança do nosso modelo como mostra a Figura 6.2, de modo que podemos afirmar que não existe nenhum valor de H0 no qual conseguimos satisfazer a região de confiança de 1σ tanto para os da- dos do Planck quanto para os nossos dados. Podemos questionar vários pontos a respeito dessa discrepância, pois saber que existe uma inconsistência no nosso modelo, quando este observa dados diferentes de outro modelo, não nos diz aonde que se encontra essa inconsistência. A nossa estimativa tem 3.8σ de tensão/discrepância com o maior valor da região de 1σ do Planck (H P+ 0 = 67.90 km s −1 Mpc−1). Essa tensão é calculada quando a região de confiança em 1σ das hipóteses e conjunto de dados em análise está contida pela primeira vez na região de confiança das hipóteses e conjunto de dados do nosso mo- delo. Em outras palavras estamos dizendo o quão longe o valor da nossa estimativa está estatisticamente da medida do Planck, sendo que estamos considerando apenas uma casa decimal de precisão. De fato poderia ser uma discrepância mais precisa com mais casas decimais. A estimativa de ˆH0 usando os modelos do Pantheon+ SH0ES e do Riess, cujas medidas da constante de Hubble são dadas na Tabela 6.2, apresentam intervalos de confiança em 1σ contidos na região de 1σ do nosso modelo. Todos esses casos eram esperados, pois usamos os mesmos conjuntos de dados de SNe Ia do Pantheon+ SH0ES, que por sua vez usou os mesmos conjuntos de dados do Riess, o que corrobora ainda mais estes trabalhos. Lembrando que os dados de SNe Ia são a maior quantidade de dados em nosso trabalho. 149 Figura 6.2: Região de confiança de ˆH0 e da medida do Planck H P 0 , ambas associadas a 1σ = 68.3% de confiança. Deste modo podemos afirmar que existe pelo menos um valor de H0 no qual conseguimos satisfazer a região de confiança de 1σ tanto para os dados do Pantheon+ SH0ES e do Riess quanto para os nossos dados. Assim sendo nosso modelo de hipóteses e conjunto de dados é consistente com esses outros modelos de hipóteses e conjunto de dados com 1σ de confiança. Quanto à ˆΩc temos que a soma ˆΩM = ˆΩc + 0.05 = 0.3232 ± 0.01665 apresenta um intervalo de confiança associado com 1σ de confiança de 0.3232 − 0.01665 = 0.30655 a 0.3232 + 0.01665 = 0.33985. Neste caso temos que a região de 1σ de confiança para ΩM do Planck e Pantheon+ SH0ES (Modelos FlatΛCDM e FlatwCDM) estão contidas na região de confiança da nossa estimativa. Em outras palavras nossas hipóteses e conjunto de dados são consistentes com as hipóteses e conjunto de dados desses outros modelos com 1σ de confiança para a estimativa de ˆΩM . Temos que ˆΩκ ≈ 0, resultado esse que é consistente com um Universo do tipo plano com κ = 0 de acordo com 3.80. Ou seja, os dados e nossas hipóteses estão restringindo o nosso modelo para um Universo com seções espaciais planas. Finalmente, vamos analisar os resultados obtidos para a reconstrução de w(z). Diferente- mente de como fizemos na 1ª e 2ª etapas, neste momento pretendemos analisar o conjunto de valores {wk} em que k = 0, 1, 2, 3, 4 como um só. Pois agora temos um gráfico de densidade de probabilidade apresentado na Figura 6.3 que nos mostra que apesar de al- guns valores de w(z) serem diferentes de −1 em um redshift particular, como em z = 2 ainda temos valores que passam por −1, devido ao fato de que w(z) é degenerado. Esse gráfico de densidade de probabilidade engloba tanto as médias quanto as variâncias da reconstrução de w(z) no intervalo de redshift de 0 ≤ z ≤ 3.0. A Figura 6.3 nos mostra 150 que a maioria dos valores de w(z) passam em torno do valor −1 que é consistente com constante cosmológica para redshifts em que a cor amarelada é mais concentrada no iní- cio da curva. Entretanto próximo de z = 3.0 (em torno do nó ˆw4) temos também uma alta concentração de pontos passando em valores de w ̸= −1, que não condizem nem com um modelo de matéria (w = 0) e/ou de radiação (w = 1/3). Ressalto que apesar do grande intervalo de confiança da medida em ˆw4, os dados tendem a gerar mais curvas de w(z) que passam mais concentradamente em torno de sua estimativa central ˆw4 = 0.2152 e neste caso temos valores de 0 < w < 1/3. Assim, parece que os dados estão propondo um novo comportamento para a energia escura em altos redshifts. Lembrando que agora temos uma grande quantidade de dados nesse intervalo em particular, e o uso do método de MCMC nos permite diagnosticar com maior consistência tais resultados. Logo, em todo o domínio da função w(z) temos um comportamento de distribuição bimodal para a equação de estado da energia escura devido a presença de dois picos de alta densidade. Figura 6.3: Gráfico de densidade de probabilidade para o best-fit de 5 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. Quanto à ˆM1, ele contém com 1σ de confiança associado ao seu intervalo de confiança, o valor da magnitude absoluta na banda B MB dada em 4.21. Na Figura 6.4 temos as regiões de confiança estatística de alguns parâmetros do modelo representadas graficamente. Neste caso, com a confiança de 1σ sabemos que os valores verdadeiros de cada par de parâmetros estão dentro da região central (cor mais escura). 151 Se os valores verdadeiros de cada par de parâmetros estiver na região externa (cor mais clara) então temos uma confiança de 2σ. Quando a forma elíptica é na diagonal como a do quadro de H0 com M1 significa que os parâmetros são correlacionados entre si, ou seja quando um aumenta o outro aumenta também. Em outro caso, se a forma elíptica é na diagonal contrária como a do quadro de w0 com w1 dizemos que são anti correlacionados, ou seja se um parâmetro diminui o outro aumenta. E, quando a forma é aparentemente circular os parâmetros não apresentam correlação forte entre si. Para os casos que são formas diferentes de elipses temos regiões com altas concentrações de w(z) com 1σ, isso significa que apesar do parâmetro preferir uma região como a mostrada no quadro, também existe outras regiões em que ele prefere permanecer. Nestes corner plots de MCMC, como são chamados, temos alguns parâmetros que não se encontram no best-fit da Tabela 6.5 como ΩX0 ≡ ΩDE0, Ωκ0 e Ωc0 que são, respectivamente, os parâmetros de densidade de energia escura, curvatura e matéria escura “hoje”4, devido ao fato de que são parâmetros mais ajustáveis para essa análise em particular. A distribuição marginal de w3 nos diz que essa estimativa tem uma região de confiança associada com 1σ que não abrange w = −1 de modo que parece que os dados estão nos dizendo que w < −1 nesse redshift em particular, no entanto ainda temos 31.7% de possibilidade da ocorrência de um comportamento de constante cosmológica. De fato é o que ocorre quando observamos esse redshift em particular na Figura 6.3, pois temos valores de w que passamos por w = −1 e outros que passam por w < −1. Destacamos nessa discussão o quadro que representa a distribuição marginal do último nó, em w4, o que é tratado como novidade em nosso trabalho, pois neste caso em particular, os da- dos observacionais estão nos dizendo que a energia escura apresenta um comportamento diferente de constante cosmológica, matéria e/ou radiação em altos redshifts. Isso é evi- dente pois temos uma região de 1σ que nos diz que a maioria dos dados estão altamente concentrados em valores de 0 < w < 1/3. b) Modelo XCDM (w constante): Para este caso em particular temos w(z) constante em todo o intervalo de redshift de 0 ≤ z ≤ 3.0. O best-fit dessa configuração pode ser visualizado na Tabela 6.6. Vamos fazer as mesmas análises iniciais do caso anterior. Portanto, dado o nosso modelo (com suas hipóteses e conjunto de dados) supostamente verdadeiro, obtemos para a cons- tante de Hubble a seguinte estimativa: ˆH0 = 71.75 ± 0.8774 km s−1 Mpc−1. Usando a medida do Planck pretendemos agora estimar ˆH0 associando 1σ ao seu intervalo de con- fiança dado de 71.75 − 0.8774 = 70.8726 a 71.75 + 0.8774 = 72.6274 em unidades de km s−1 Mpc−1. Da mesma forma que o caso anterior, estamos dizendo com a confiança de 1σ ou 68.3% das vezes aceitamos as hipóteses de ˆH0 está entre esse intervalo, porém 4Esses parâmetros se ajustam com os dados observacionais para obtenção de ΩDE, Ωκ e Ωc de acordo com a Eq. 3.75. 152 69.0 70.5 72.0 73.5 75.0H0 0.20 0.24 0.28 0.32c0 0.04 0.02 0.00 0.02k0 1.5 1.0 0.5 0.0 0.5w0 1.4 1.2 1.0 0.8w1 1.50 1.25 1.00 0.75 0.50w2 4 3 2 1w3 4.5 3.0 1.5 0.0w4 0.64 0.68 0.72 0.76 x0 19.36 19.32 19.28 19.24 19.201 69.0 70.5 72.0 73.5 75.0 H0 0.20 0.24 0.28 0.32 c0 0.04 0.02 0.00 0.02 k0 1.5 1.0 0.5 0.0 0.5 w0 1.4 1.2 1.0 0.8 w1 1.50 1.25 1.00 0.75 0.50 w2 4 3 2 1 w3 4.5 3.0 1.5 0.0 w4 19.36 19.32 19.28 19.24 19.20 1 Figura 6.4: Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (externa/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4 e M1 (configuração de 5 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. estamos cometendo um erro de tipo um de tal forma que as hipóteses descartadas ainda podem ocorrer. Neste caso 31.7% das vezes estamos errando ao fazer tal consideração. Como sabemos o intervalo de confiança da medida do Planck associado a 1σ é dado de 66.82 a 67.90 também em unidades de km s−1 Mpc−1. Esse intervalo não está contido na região de confiança da nossa estimativa, de tal forma que podemos novamente afirmar que não existe nenhum valor da constante de Hubble que satisfaça a região de 1σ tanto para os dados do Planck quanto para os nossos dados. Nessa discussão, destacamos que a nossa estimativa tem 4.4σ de tensão/discrepância com relação a medida do Planck. Coincidentemente se trata da mesma tensão encontrada entre a medida do Riess (2019) e 153 Tabela 6.6: Best-fit do modelo XCDM (w constante) na configuração TTT da 3ª etapa no inter- valo de redshift de 0 ≤ z ≤ 3.0. A verossimilhança total é −2 ln L ≈ 1503. Parâmetro Valor e erro ˆH0 71.75 ± 0.8774 ˆΩc 0.2829 ± 0.01131 ˆΩκ −0.005662 ± 0.009833 ˆw −1.087 ± 0.03592 ˆM1 −19.31 ± 0.02476 do Planck [45]. A estimativa de ˆH0 usando os modelos do Pantheon+ SH0ES e do Riess cujas medidas da constante de Hubble são dadas na Tabela 6.2, apresentam regiões de confiança em 1σ contidos na região de 1σ de ˆH0. Novamente, são casos esperados devido ao fato de usarmos os mesmos dados observacionais de SNe Ia destes trabalhos. Deste modo podemos afirmar que existe pelo menos um valor de H0 no qual conseguimos satisfazer a região de confiança de 1σ tanto para os dados do Pantheon+ SH0ES e do Riess quanto para os nossos dados. Em outras palavras, nosso modelo de hipóteses e conjunto de dados é consistente com esses outros modelos de hipóteses e conjunto de dados com 1σ de confiança. Quanto à ˆΩc temos que a soma ˆΩM = ˆΩc + 0.05 = 0.3329 ± 0.01131 apresenta um intervalo de confiança associado com 1σ de 0.3329 − 0.01131 = 0.32159 a 0.3329 + 0.01131 = 0.34421. Neste caso temos que a região de 1σ de confiança para ΩM do Planck e Pantheon+ SH0ES (Modelos FlatΛCDM e FlatwCDM) estão contidas na região de confiança da nossa estimativa. Ou seja, nossas hipóteses e conjunto de dados são consistentes com as hipóteses e conjunto de dados desses outros modelos com 1σ de confiança para a estimativa de ˆΩM . Temos que ˆΩκ ≈ 0, resultado esse que é novamente consistente com um Universo do tipo plano com κ = 0 de acordo com 3.80. Por fim, temos que ˆw em todo o intervalo de redshift é constante com região de confi- ança associada com 3σ = 99.7% de −1.087 − 3 × 0.03592 = −1.19476 a −1.087 + 3 × 0.03592 = −0.97924 consistente com um comportamento de constante cosmológica 154 (w = −1) para a energia escura. Neste caso aumentando o intervalo de confiança es- tamos sendo mais conservadores pois estamos obtendo mais confiança sob a medida de w. Em contrapartida, diminuímos a capacidade de prever o seu valor verdadeiro. Nesse caso um comportamento de constante cosmológica se encontra próximo da borda direita da distribuição gaussiana desse parâmetro como mostrado na Figura 6.5. Figura 6.5: Região de confiança de ˆw para o modelo XCDM na 3ª etapa associada com 3σ = 99.7% de confiança. Usando o modelo do Planck (com suas hipóteses e conjunto de dados) observamos que a região de confiança em 1σ de w0 dado na Tabela 6.2 está contida na região de confiança de 1σ da nossa estimativa (−1.087−0.03592 = −1.12292 a −1.087+0.03592 = −1.05108). Assim dizendo, ambos os modelos de hipóteses e conjunto de dados concordam em 1σ de confiança. Podemos comparar nossa estimativa com as medidas de Betoule et. al [84] que também trabalhou com uma equação de estado da energia escura constante. Este trabalho mediu w = −1.018 ± 0.057 (com erros estatísticos e sistemáticos quando combinado com me- didas da CMB) e w = −1.027 ± 0.055 (com a adição de BAO). Do mesmo modo que a comparação anterior feita com o Planck é evidente que a região de confiança em 1σ des- sas medidas está contida na região de confiança de 1σ da nossa estimativa. Deste modo, ambos os modelos de hipóteses e conjunto de dados coincidem com 1σ de confiança. Quanto à ˆM1, ele contém com 2σ de confiança associado ao seu intervalo de confiança 5, 5Com 1σ de confiança temos uma região de confiança de −19.33476 a −19.28524 para ˆM1 de modo que M1 de 4.21 não está contido em tal intervalo de 1σ com duas casas decimais de precisão. Mesmo assim ainda é um valor próximo. Se tivermos 2σ associado a região de confiança da nossa estimativa, então a região de confiança de 1σ de M1 está contida, de modo que estamos sendo mais conservadores nesse aspecto. 155 o valor da magnitude absoluta na banda B MB dada em 4.21. Na Figura 6.6 temos as regiões de confiança estatística de alguns parâmetros do modelo XCDM representadas graficamente. Com a confiança de 1σ = 68.3% e 2σ = 95.4% sabemos que os valores verdadeiros de cada par de parâmetros estão, respectivamente, na região central (cor mais escura) e na região externa (cor mais clara). Nos quadros podemos visualizar formas de correlação positiva e negativa entre cada par de parâmetros, bem como formas circulares, em que o valor de um parâmetro não influencia no valor de seu par de forma significativa. As distribuições marginais são representadas por curvas gaussianas simétricas e neste caso, não fica evidente um comportamento bimodal para a equação de estado da energia escura como constatamos para w variando com relação à z. Ou seja, se mantivermos w(z) constante em todo o intervalo de redshift perdemos muita informação a respeito do comportamento da energia escura em um determinado ponto do domínio em z, pois nesse caso ela prefere ser constante com valores dados pela sua estimativa ˆw = −1.087 ± 0.03592 de acordo com seu intervalo de confiança em análise. Observe que no quadro da distribuição marginal de w na Figura 6.6 o comportamento de constante cosmológica não se encontra na região sombreada de 1σ = 68.3%, pois w = −1 está mais próximo da sua borda direita. Destacamos, quanto aos nossos resultados obtidos, que outros trabalhos na literatura que analisaram a equação de estado da energia escura independente de modelo também encontraram valores próximos de w = −1 consistente com constante cosmológica. Como referência posso citar Betoule et al. [84] que trabalhou com uma equação de estado da energia escura cons- tante, bem como os trabalhos do Pantheon+ SH0ES [42]. Além disso outros trabalhos mediram w ̸= −1 como o de Capozziello et al. (2019) [138] para baixos redshifts que constatou que diferentes comportamentos de energia escura são permitidos no intervalo de 0 ≤ z ≤ 2.0, con- sistentes com uma energia fantasma ou quintessência, por exemplo. O trabalho de Tripathi et al. (2016) [139] consistiu no uso de uma parametrização de expansão em séries de Taylor para w(z) analisando a equação de estado da energia escura para dois modelos, um com w constante e outro com w em função de z, ambos para baixos redshifts, e também constatou variações em suas estimativas que correspondem a valores de w < −1 e −1 < w < 0, como as que encon- tramos em nosso trabalho nas 1ª e 2ª etapas que apesar de não serem estimativas confiantes são interessantes para se analisar separadamente. O destaque mesmo ficou para a 3ª etapa, a qual temos resultados mais robustos e confiáveis pelo método de MCMC e onde constatamos um comportamento bimodal para a distribuição de probabilidade da equação de estado da energia escura. 156 69.0 70.5 72.0 73.5 75.0H0 0.26 0.28 0.30 0.32c0 0.030 0.015 0.000 0.015 0.030k0 1.20 1.14 1.08 1.02w 0.625 0.650 0.675 0.700 0.725 x0 19.40 19.36 19.32 19.28 19.241 69.0 70.5 72.0 73.5 75.0 H0 0.26 0.28 0.30 0.32 c0 0.030 0.015 0.000 0.015 0.030 k0 1.20 1.14 1.08 1.02 w 19.40 19.36 19.32 19.28 19.24 1 Figura 6.6: Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (externa/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w, e M1 (Modelo XCDM (w constante)). A área sombreada na distribuição marginal representa 1σ. 157 7 CONSIDERAÇÕES FINAIS Neste trabalho apresentamos uma abordagem paramétrica e independente de modelo na reconstrução da equação de estado da energia escura usando o método de splines cúbicas na sua interpolação polinomial. A primeira e mais extensa parte deste trabalho englobou um estudo mais aprofundado sobre o método de reconstrução bem como a implementação dos códigos computacionais ne- cessários para as estimativas da equação de estado da energia escura. Todos os resultados ana- lisados neste presente trabalho foram obtidos através de cálculo numérico e as estimativas para ˆw(z), que era o nosso principal parâmetro em análise, foram devidamente analisadas em com- paração com resultados existentes na literatura. Os resultados obtidos na 1ª e 2ª etapas nos dizem que a equação de estado da energia escura tende a ser constante para baixos redshifts em ambos os intervalos 0 ≤ z ≤ 1080 e 0 ≤ z ≤ 3, com valor estimado de ˆw(z) ≈ −1, consistente com a constante cosmológica Λ, um tipo de constante que foi introduzida por Einstein em sua teoria da Relatividade Geral, que apresenta densidade constante e age de forma oposta a gravidade. Na 1ª etapa, em redshifts próximos da CMB (zf = 1080) temos que ˆw(z) apresenta valores muito enviesados. Isso se deve ao fato de que este é um ponto que depende fortemente de modelo. Com a inclusão de dados observacionais (Supernovas do tipo Ia, oscilações acústicas de bárions e cronômetros cosmológicos) mais recentes e/ou em maior quantidade na 2ª etapa, constatamos que podíamos restringir o nosso modelo no intervalo de redshifts de 0 ≤ z ≤ 1080 para 0 ≤ z ≤ 3.0 nessa etapa, deixando nosso modelo mais independente com a exclusão dos dados observacionais de distance priors. Neste caso para redshifts próximos de z = 3.0 obtivemos resultados em que w ̸= −1. No entanto, o nosso sistema é muito degenerado de modo que não podemos confiar nas incertezas obtidas pelo método da matriz de Fisher em tais etapas. Afim de obtermos resultados mais robustos executamos uma 3ª etapa que consistiu no uso do método de MCMC para os mesmos dados observacionais e intervalo de redshifts da 2ª etapa. Essa foi a etapa final desse presente trabalho e foi onde tivemos a constatação de uma distribuição bimodal para a equação de estado da energia escura em que w(z) = −1 para redshifts iniciais do domínio compreendido entre 0 ≤ z ≤ 3.0 e 0 < w(z) < 1/3 em redshifts próximos de z = 3.0. Pretendemos submeter um artigo ao Journal of Cosmology and Astroparticle Physics (JCAP) afim de divulgar nossos resultados que aparentam ser bem promissores a respeito da energia escura. Ressaltamos que em nenhum momento queremos obter os valores na literatura para as nossas estimativas de w(z). Pelo contrário, estamos realizando uma ciência do tipo investi- gativa na tentativa de obtenção de novos resultados. Por mais que de fato encontremos esses novos resultados, como ocorreu na 3ª etapa em que 0 < w(z) < 1/3 em redshifts próximos de z = 3.0, o nosso modelo não foge da descrição atual do Universo pois também conseguimos 158 obter resultados próximos da literatura, como estimativas consistentes com constante cosmo- lógica (w(z) = −1) para redshifts mais baixos como mostrado na Figura 6.3 e discutido no Capítulo 6.4. Desta forma, apesar dos novos resultados, chegamos a consideração final de que estes não são conflitantes com o que já se conhece sobre Cosmologia. Demonstrando assim, a consistência do método e modelo discutidos bem como do código desenvolvido. 159 A TABELAS DE BEST-FITS E GRÁFICOS Neste apêndice se encontram os demais best-fits que não foram analisados no Capí- tulo 6. Você pode visualizar os best-fits, gráficos e outros arquivos gerados de cada etapa na seguinte pasta compartilhada do Google Drive: https://drive.google.com/drive/ folders/16RwO4PJdGrw-re-JCKdpyiG4nFToyZ2n?usp=sharing. A.1 3ª ETAPA Tabela A.1: Best-fit da configuração de 6 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossi- milhança total é −2 ln L ≈ 1483. Parâmetro Valor e erro redshift z ˆH0 71.13 ± 1.009 ˆΩc 0.2177 ± 0.0199 ˆΩκ 0.002131 ± 0.01015 ˆw0 −0.2352 ± 0.5182 0 ˆw1 −0.9897 ± 0.1169 0.2 ˆw2 −0.6019 ± 0.2251 0.354 ˆw3 −0.9146 ± 0.5567 0.653 ˆw4 −0.7563 ± 1.072 1.304 ˆw5 0.02574 ± 1.727 3.0 ˆM1 −19.3 ± 0.02583 160 Tabela A.2: Best-fit da configuração de 7 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossi- milhança total é −2 ln L ≈ 1483. Parâmetro Valor e erro redshift z ˆH0 71.51 ± 1.058 ˆΩc 0.2777 ± 0.02151 ˆΩκ −0.0006252 ± 0.01005 ˆw0 −0.09391 ± 0.6597 0 ˆw1 −1.079 ± 0.1367 0.2 ˆw2 −0.7397 ± 0.2655 0.315 ˆw3 −0.8817 ± 0.3702 0.508 ˆw4 −1.242 ± 1.272 0.851 ˆw5 −3.855 ± 1.16 1.52 ˆw6 0.3068 ± 1.741 3.0 ˆM1 −19.28 ± 0.02558 161 Tabela A.3: Best-fit da configuração de 8 nós e TTT na 3ª etapa com 0 ≤ z ≤ 3.0. A verossi- milhança total é −2 ln L ≈ 1483. Parâmetro Valor e erro redshift z ˆH0 71.83 ± 1.097 ˆΩc 0.2722 ± 0.0183 ˆΩκ 0.005229 ± 0.01005 ˆw0 −0.2066 ± 0.7724 0 ˆw1 −1.125 ± 0.1856 0.2 ˆw2 −0.8397 ± 0.2853 0.291 ˆw3 −0.6842 ± 0.41 0.431 ˆw4 −1.551 ± 0.8081 0.653 ˆw5 −0.7082 ± 1.38 1.024 ˆw6 −4.073 ± 1.397 1.688 ˆw7 0.3066 ± 1.744 3.0 ˆM1 −19.27 ± 0.02546 162 A.1.1 Gráficos de densidade de probabilidade e corner plots de MCMC Figura A.1: Gráfico de densidade de probabilidade para o best-fit de 6 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. Figura A.2: Gráfico de densidade de probabilidade para o best-fit de 7 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. 163 Figura A.3: Gráfico de densidade de probabilidade para o best-fit de 8 nós na interpolação polinomial de w(z) para o intervalo de redshift de 0 ≤ z ≤ 3.0. 164 69.0 70.5 72.0 73.5 75.0H0 0.16 0.20 0.24 0.28 0.32c0 0.04 0.02 0.00 0.02k0 1.6 0.8 0.0 0.8w0 1.4 1.2 1.0 0.8w1 1.6 1.2 0.8 0.4w2 4 3 2 1 0w3 4 3 2 1w4 4.5 3.0 1.5 0.0w5 0.60 0.64 0.68 0.72 0.76 x0 19.36 19.32 19.28 19.24 19.201 69.0 70.5 72.0 73.5 75.0 H0 0.16 0.20 0.24 0.28 0.32 c0 0.04 0.02 0.00 0.02 k0 1.6 0.8 0.0 0.8 w0 1.4 1.2 1.0 0.8 w1 1.6 1.2 0.8 0.4 w2 4 3 2 1 0 w3 4 3 2 1 w4 4.5 3.0 1.5 0.0 w5 19.36 19.32 19.28 19.24 19.20 1 Figura A.4: Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (externa/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4, w5 e M1 (configuração de 6 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. 165 68 70 72 74H0 0.12 0.18 0.24 0.30c0 0.030 0.015 0.000 0.015 0.030k0 3 2 1 0w0 1.50 1.25 1.00 0.75w1 2.0 1.6 1.2 0.8 0.4w2 1.8 1.2 0.6 0.0w3 4.5 3.0 1.5 0.0w4 4 3 2 1 0w5 4.5 3.0 1.5 0.0w6 0.60 0.66 0.72 0.78 0.84 x0 19.36 19.32 19.28 19.24 19.201 68 70 72 74 H0 0.12 0.18 0.24 0.30 c0 0.030 0.015 0.000 0.015 0.030 k0 3 2 1 0 w0 1.50 1.25 1.00 0.75 w1 2.0 1.6 1.2 0.8 0.4 w2 1.8 1.2 0.6 0.0 w3 4.5 3.0 1.5 0.0 w4 4 3 2 1 0 w5 4.5 3.0 1.5 0.0 w6 19.36 19.32 19.28 19.24 19.20 1 Figura A.5: Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (externa/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4, w5, w6 e M1 (configu- ração de 7 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. 166 68 70 72 74H0 0.20 0.24 0.28 0.32c0 0.02 0.00 0.02k0 3 2 1 0w0 1.8 1.5 1.2 0.9w1 2.0 1.5 1.0 0.5w2 1.6 0.8 0.0 0.8w3 4.5 3.0 1.5 0.0w4 4.5 3.0 1.5 0.0w5 4.5 3.0 1.5 0.0w6 4.5 3.0 1.5 0.0w7 0.60 0.65 0.70 0.75 0.80 x0 19.36 19.32 19.28 19.241 68 70 72 74 H0 0.20 0.24 0.28 0.32 c0 0.02 0.00 0.02 k0 3 2 1 0 w0 1.8 1.5 1.2 0.9 w1 2.0 1.5 1.0 0.5 w2 1.6 0.8 0.0 0.8 w3 4.5 3.0 1.5 0.0 w4 4.5 3.0 1.5 0.0 w5 4.5 3.0 1.5 0.0 w6 4.5 3.0 1.5 0.0 w7 19.36 19.32 19.28 19.24 1 Figura A.6: Regiões de confiança de 1σ = 68.3% (central/escura) e 2σ = 95.4% (externa/clara) para os parâmetros cosmológicos H0, ΩDE0, Ωc0, Ωκ0, w0, w1, w2, w3, w4, w5, w6, w7 e M1 (configuração de 8 nós na interpolação polinomial de w(z)). A área sombreada na distribuição marginal representa 1σ. 167 B ANÁLISE ILUSTRATIVA DOS VALORES DA VARIÁVEL IMPLEMENTADA COM RELAÇÃO AO REDSHIFT Esse script foi desenvolvido apenas para fins demonstrativos dos valores de α imple- mentado em termos do redshift. Ele determina em qual valor de z é estimado w(z). Para reprodução dos códigos copie-os em uma IDE de sua preferência tal que seja possível compilar e executar tal script com o uso de linguagem Python. Os mesmos códigos estão disponíveis no meu repositório do GitHub (Link de acesso direto desse arquivo: https://github.com/ Sander23Car/UEL_Sander/blob/main/AlphaNumerico.ipynb). Caso esteja al- terado o caminho para os códigos basta procurar por pelo nome do script no repositório (Link: https://github.com/Sander23Car). AlphaNumerico November 2, 2022 1 Análise ilustrativa Este notebook tem como principal objetivo mostrar os valores de 𝛼 (variável implementada) e de 𝑧 (redshift) para um intervalo dividido por um número de nós em particular. 𝛼 = ln[1 + 𝑧]. (1) Apenas para efeitos de ilustração de como se comporta a variável implementada no intervalo de redshift especificado. 1.1 Códigos [13]: #bibliotecas import numpy as np import math as mt import matplotlib.pyplot as plt import pandas as pdb [14]: #função: alpha = ln[1+z] #informações de entrada zi = 0.0 #redshift inicial zf = 10**(10) #redshift final nos = 12 #divisões da grade (número de nós) #função que calcula o último valor de alpha na grade alphai = mt.log(1+zi) #Valor do primeiro alpha correspondente a zi alphaf = mt.log(1+zf) #Valor do último alpha correspondente a zf #divisões da grade divgrade = alphaf/nos #função que retorna números com espaçamento uniforme em um intervalo␣ ↪especificado. intervgrade = np.linspace(alphai, alphaf, num=nos, endpoint=False) #escala␣ ↪linear 1 intervgrade = np.append(intervgrade,alphaf) #adiciona o último valor de alpha␣ ↪a lista #intervgrade = list(intervgrade) #nº de valores em cada lista (O nº de valores em alpha é igual ao nº de valores␣ ↪em z.) compalpha=len(intervgrade) #função que retorna os valores de z em cada nó znos = np.expm1(intervgrade) #escala linear #znos = list(znos) #informações de saída print(\"Primeiro valor de alpha é:\",alphai,\"para zi=\",zi,\"\\n\") print(\"Último valor de alpha é:\",alphaf, \"para zf=\",zf,\"\\n\") print(\"Valor de alpha entre cada intervalo da grade:\",divgrade,\"\\n\") print(\"Valores de alpha em\",nos,\"nós =\",intervgrade,\"\\n\") print(\"Valores de z em cada um dos\", nos,\"nós=\",znos,\"\\n\") print(\"*OBS*: Lembrando que na computação a contagem sempre começa do zero.\") print(\"Ou seja, na posição 0 temos o nó em alpha =\",alphai, \"que corresponde a␣ ↪z=\", zi) print(\"E na posição\", compalpha, \"temos o nó em alpha =\",alphaf, \"que␣ ↪corresponde a z=\", zf) print(\"Por isso as listas de alpha e z apresentam\", compalpha, \"valores cada,␣ ↪pois contamos o 0 também.\\n\") Primeiro valor de alpha é: 0.0 para zi= 0.0 Último valor de alpha é: 23.025850930040455 para zf= 10000000000 Valor de alpha entre cada intervalo da grade: 1.9188209108367047 Valores de alpha em 12 nós = [ 0. 1.91882091 3.83764182 5.75646273 7.67528364 9.59410455 11.51292547 13.43174638 15.35056729 17.2693882 19.18820911 21.10703002 23.02585093] Valores de z em cada um dos 12 nós= [0.00000000e+00 5.81292069e+00 4.54158883e+01 3.15227766e+02 2.15343469e+03 1.46769927e+04 9.99990000e+04 6.81291069e+05 4.64158783e+06 3.16227756e+07 2.15443468e+08 1.46779927e+09 1.00000000e+10] 2 *OBS*: Lembrando que na computação a contagem sempre começa do zero. Ou seja, na posição 0 temos o nó em alpha = 0.0 que corresponde a z= 0.0 E na posição 13 temos o nó em alpha = 23.025850930040455 que corresponde a z= 10000000000 Por isso as listas de alpha e z apresentam 13 valores cada, pois contamos o 0 também. 1.1.1 Gráfico Ajuste linear dos dados. Também poderia ser feito um ajuste em escala logaritmica. No entanto, apenas para efeitos de ilustração gráfica plotamos os dados em escala linear. [15]: #Gráfico em escala linear de alpha(z) =ln(1+z) plt.figure(figsize=(14, 7)) #tamanho graph = plt.plot(znos, intervgrade) #plot plt.ylabel('alpha (z)', fontsize=14) #valor no eixo das abscissas plt.xlabel('z', fontsize=14) #valor no eixo das ordenadas plt.legend(['alpha =ln[1+z]'],fontsize=14) #legenda da curva #Mostra o gráfico pro usuário plt.show(graph) [16]: #função: alpha = ln[1+z] #informações de entrada zi = 0.0 #redshift inicial zf = 10**(4) #redshift final nos = 12 #divisões da grade (número de nós) 3 #função que calcula o último valor de alpha na grade alphai = mt.log(1+zi) #Valor do primeiro alpha correspondente a zi alphaf = mt.log(1+zf) #Valor do último alpha correspondente a zf #divisões da grade divgrade = alphaf/nos #função que retorna números com espaçamento uniforme em um intervalo␣ ↪especificado. intervgrade = np.linspace(alphai, alphaf, num=nos, endpoint=False) #escala␣ ↪linear intervgrade = np.append(intervgrade,alphaf) #adiciona o último valor de alpha␣ ↪a lista #intervgrade = list(intervgrade) #nº de valores em cada lista (O nº de valores em alpha é igual ao nº de valores␣ ↪em z.) compalpha=len(intervgrade) #função que retorna os valores de z em cada nó znos = np.expm1(intervgrade) #escala linear #znos = list(znos) #informações de saída print(\"Primeiro valor de alpha é:\",alphai,\"para zi=\",zi,\"\\n\") print(\"Último valor de alpha é:\",alphaf, \"para zf=\",zf,\"\\n\") print(\"Valor de alpha entre cada intervalo da grade:\",divgrade,\"\\n\") print(\"Valores de alpha em\",nos,\"nós =\",intervgrade,\"\\n\") print(\"Valores de z em cada um dos\", nos,\"nós=\",znos,\"\\n\") print(\"*OBS*: Lembrando que na computação a contagem sempre começa do zero.\") print(\"Ou seja, na posição 0 temos o nó em alpha =\",alphai, \"que corresponde a␣ ↪z=\", zi) print(\"E na posição\", compalpha, \"temos o nó em alpha =\",alphaf, \"que␣ ↪corresponde a z=\", zf) print(\"Por isso as listas de alpha e z apresentam\", compalpha, \"valores cada,␣ ↪pois contamos o 0 também.\\n\") Primeiro valor de alpha é: 0.0 para zi= 0.0 Último valor de alpha é: 9.210440366976517 para zf= 10000 Valor de alpha entre cada intervalo da grade: 0.7675366972480431 4 Valores de alpha em 12 nós = [0. 0.7675367 1.53507339 2.30261009 3.07014679 3.83768349 4.60522018 5.37275688 6.14029358 6.90783028 7.67536697 8.44290367 9.21044037] Valores de z em cada um dos 12 nós= [0.00000000e+00 1.15445264e+00 3.64166619e+00 9.00024999e+00 2.05450650e+01 4.54178223e+01 9.90049999e+01 2.14456036e+02 4.63189827e+02 9.99074999e+02 2.15361422e+03 4.64101431e+03 1.00000000e+04] *OBS*: Lembrando que na computação a contagem sempre começa do zero. Ou seja, na posição 0 temos o nó em alpha = 0.0 que corresponde a z= 0.0 E na posição 13 temos o nó em alpha = 9.210440366976517 que corresponde a z= 10000 Por isso as listas de alpha e z apresentam 13 valores cada, pois contamos o 0 também. 1.1.2 Gráfico Ajuste linear dos dados. Também poderia ser feito um ajuste em escala logaritmica. No entanto, apenas para efeitos de ilustração gráfica plotamos os dados em escala linear. [17]: #Gráfico em escala linear de alpha(z) =ln(1+z) plt.figure(figsize=(14, 7)) #tamanho graph = plt.plot(znos, intervgrade) #plot plt.ylabel('alpha (z)', fontsize=14) #valor no eixo das abscissas plt.xlabel('z', fontsize=14) #valor no eixo das ordenadas plt.legend(['alpha =ln[1+z]'],fontsize=14) #legenda da curva #Mostra o gráfico pro usuário plt.show(graph) 56 174 REFERÊNCIAS [1] N. Aghanim et al. Planck 2018 results. VI. Cosmological parameters. Astron. Astrophys., 641:A6, 2020. [Erratum: Astron.Astrophys. 652, C4 (2021)]. [2] Carl de Boor. A Practical Guide to Splines: Revised Edition. New York: Springer, 2001. [3] Marilena Chaui. Convite à Filosofia. São Paulo: Ática, 13 edition, 2004. [4] Richard Rennie and Jonathan Law. Dictionary of Physics. USA: Oxford University Press, 8 edition, 2019. [5] Rogério Rosenfeld. A cosmologia. Física na Escola, 6(1):31–37, jan 2005. [6] W. N. Haynes. CRC Handbook Of Chemistry And Physics. New York: CRC Press, 97 edition, 2017. [7] Colin Stuart. A História do Universo para quem tem pressa. Rio de Janeiro: Valentina, 1 edition, 2018. [8] WIKILIVROS. Astronomia/Astronomia na Pré-história. Disponível em: https://pt.wikibooks.org/wiki/Astronomia/Astronomia_na_ Pr%C3%A9-hist%C3%B3ria com acesso em: 23 junho de 2022. [9] Richard Bevins and Rob Ixer. Retracing the footsteps of H.H. Thomas: a review of his Stonehenge bluestone provenancing study. Antiquity Publications , 92:12, 2018. [10] Kellen N. Skolimoski and João Zanetic. Mitos de Criação: Modelos Cosmogônicos de Diferentes Povos e suas Semelhanças. In II Simpósio Nacional de Educação em Astronomia - II SNEA 2012 - São Paulo, SP, pages 405–415, julho 2012. [11] Moustafa Gadalla. A Cosmologia Egípcia: O Universo Animado. Greensboro, USA: Fundação de Pesquisa Tehuti, 3 edition, 2017. [12] Cibelle Celestino Silva. Estudos de História e Filosofia das Ciências. São Paulo: Editora Livraria da Física, 1 edition, 2006. [13] Penha Maria Cardoso Dias, Wilma Machado Soares Santos, and Mariana Thomé Mar- ques de Souza. A Gravitação Universal: Um texto para o Ensino Médio. Revista Brasi- leira de Ensino de Física, 26(3):257–271, Agosto 2004. 175 [14] Kaushik Patowary. Newton’s Apple Tree in Lincolnshire. Dis- ponível em: https://www.amusingplanet.com/2017/11/ newtons-apple-tree-in-lincolnshire.html com acesso em: 24 ju- lho de 2022. [15] Mario Novello et al. Programa Mínimo de Cosmologia. Rio de Janeiro: Editora Jauá, 1 edition, 2010. [16] Paul A. Tipler and Ralph A. Llewellyn. Física Moderna. Rio de Janeiro: LTC, 6 edition, 2014. [17] R. R. Machado, A. C. Tort, and C. A. D. Zarro. O princípio da equivalência: Uma introdução à relatividade geral. Física na Escola, 19(2):3–14, 2021. [18] NASA. Tudo Sobre o James Webb. Space TODAY, 2021. [19] David J. Griffiths. Eletrodinâmica. São Paulo: Editora Pearson, 3 edition, 2010. [20] David Halliday, Robert Resnick, and Jearl Walker. Fundamentos de Física: Gravitação, Ondas e Termodinâmica, volume 2. Rio de Janeiro: LTC, 10 edition, 2016. [21] Matheus Pinheiro Ramos and Roberto Vinhaes Maluf. Sobre a teoria de Einstein para ondas gravitacionais e sua aplicação no estudo da radiação emitida por um pulsar binário. Revista Brasileira de Ensino de Física, 40(2), 2018. [22] Martin Ress. Enciclopédia Ilustrada do Universo: O Fascínio do Cosmos. São Paulo: Duetto Editorial, 2 edition, 2012. [23] Ville Hirvonen. The Ricci Tensor: A Complete Guide With Examples. Disponível em: https://profoundphysics.com/the-ricci-tensor/ com acesso em: 4 novembro de 2022. [24] Charles W. Misner, Kip S. Thorne, and John Archibal Wheeler. Gravitation. USA: W. H. freeman and Company, 1973. [25] Domingos Soares. O universo estático de Einstein. Revista Brasileira de Ensino de Física, 34(1), 2012. [26] Leandro L. S. Guedes. Princípio cosmológico: Homogeneidade e isotropia. Disponível em: http://astronomia.blog.br/ principio-cosmologico-homogeneidade-e-isotropia/ com acesso em: 24 julho de 2022. [27] James Binney and Scott Tremaine. Galactic Dynamics. Princeton: Princeton University Press, 2 edition, 2008. 176 [28] Jaswant Yadav, Somnath Bharadwaj, Biswajit Pandey, and T. R. Seshadri. Testing ho- mogeneity on large scales in the Sloan Digital Sky Survey Data Release One. Mon. Not. Roy. Astron. Soc., 364:601–606, 2005. [29] Barbara Ryden. Introduction to Cosmology. New York: Cambridge University Press, 2 edition, 2017. [30] ESA. PLANCK IMAGE GALLERY. Disponível em: https://www.cosmos. esa.int/web/planck/picture-gallery com acesso em: 08 setembro de 2022. [31] John Farrell. The Day Without Yesterday: Lemaître, Einstein and the Birth of Modern Cosmology. Basic Books, 2005. [32] OAC: Online Archive of California. Finding Aid for the H. P. Robertson Papers 1922- 1980. Disponível em: http://www.oac.cdlib.org/findaid/ark:/13030/ kt3s2026qn/ com acesso em: 18 novembro de 2022. [33] Nigel J. Hitchin. Arthur geoffrey walker. 17 july 1909 — 31 march 2001. Biographical Memoirs of Fellows of the Royal Society, 52:413–421, 2006. [34] Thais Soares de Andrade Lima. Introdução ao Modelo Cosmológico ϕ-CDM com dinâminca taquiônica. Dissertação de mestrado, UFCG: Universidade Federal de Cam- pina Grande. Unidade Acadêmica de Física, Campina Grande, Paraíba, 2021. [35] Zhongxu Zhai, Chan-Gyung Park, Yun Wang, and Bharat Ratra. CMB distance priors revisited: effects of dark energy dynamics, spatial curvature, primordial power spectrum, and neutrino parameters. JCAP, 07:009, 2020. [36] Robert M. Wald. General Relativity. USA: The University of Chicago Press, 1984. [37] Ribamar R. R. Reis and Beatriz B. Siffert. Supernovas do tipo Ia e a expansão do Uni- verso. Cadernos de Astronomia, 3(1):21–28, 2022. [38] Wendy L. Freedman et al. The Carnegie-Chicago Hubble Program. VIII. An Independent Determination of the Hubble Constant Based on the Tip of the Red Giant Branch, 7 2019. [39] L. Page et al. First year Wilkinson Microwave Anisotropy Probe (WMAP) observations: Interpretation of the TT and TE angular power spectrum peaks. Astrophys. J. Suppl., 148:233, 2003. [40] D. N. et al. Spergel. Three-Year Wilkinson Microwave Anisotropy Probe (WMAP) Ob- servations: Implications for Cosmology. apjs, 170(2):377–408, June 2007. 177 [41] D. Larson et al. Seven-year wilkinson microwave anisotropy probe (wmap*) observa- tions: Power spectra and wmap-derived parameters. The Astrophysical Journal Supple- ment Series, 192(2):16, jan 2011. [42] Dillon Brout et al. The Pantheon+ Analysis: Cosmological Constraints. Astrophys. J., 938(2):110, 2022. [43] Adam G. Riess, Dan Scolnic, and Dillon Brout. Pantheon+, SH0ES e H0 com ±1 km/s/Mpc. 1 vídeo (70 min:25 seg). [Webinar]. Disponível em: https:// pantheonplussh0es.github.io/ com acesso em: 30 novembro de 2022. [44] Adam G. Riess et al. A Comprehensive Measurement of the Local Value of the Hubble Constant with 1 km /s Mpc Uncertainty from the Hubble Space Telescope and the SH0ES Team. Astrophys. J. Lett., 934(1):L7, 2022. [45] Adam G. Riess, Stefano Casertano, Wenlong Yuan, Lucas M. Macri, and Dan Scolnic. Large Magellanic Cloud Cepheid Standards Provide a 1% Foundation for the Determi- nation of the Hubble Constant and Stronger Evidence for Physics beyond ΛCDM. As- trophys. J., 876(1):85, 2019. [46] Neal Jackson. The Hubble Constant. Living Rev. Rel., 10:4, 2007. [47] Taubenberger, S., Suyu, S. H., Komatsu, E., Jee, I., Birrer, S., Bonvin, V., Courbin, F., Rusu, C. E., Shajib, A. J., and Wong, K. C. The hubble constant determined through an inverse distance ladder including quasar time delays and type ia supernovae. A&A, 628:L7, 2019. [48] Gagandeep S. Anand, Zachary R. Claytor, and Ryan Dungee. Worry no more, the hubble tension is relieved: A truly direct measurement of the hubble constant from mooniversal expansion, 2022. [49] W. L. Freedman et al. Final results from the Hubble Space Telescope key project to measure the Hubble constant. Astrophys. J., 553:47–72, 2001. [50] Wendy L. Freedman. Cosmology at a Crossroads. Nature Astron., 1:0121, 2017. [51] Wendy L. Freedman. Measurements of the Hubble Constant: Tensions in Perspective. Astrophys. J., 919(1):16, 2021. [52] Martín López-Corredoira. Hubble tensions: a historical statistical analysis. Monthly Notices of the Royal Astronomical Society, 517(4):5805–5809, nov 2022. [53] Marc Kamionkowski and Adam G. Riess. The hubble tension and early dark energy, 2022. 178 [54] Kim Coble, Kevin McLin, and Lynn Cominsky. Big Ideas in Cosmology. Dis- ponível em: https://phys.libretexts.org/Bookshelves/Astronomy_ _Cosmology/Big_Ideas_in_Cosmology_(Coble_et_al.) com acesso em: 4 novembro de 2022. [55] Celso Oviedo Da Silva Lopes. Oscilações Acústicas de Barións e sua importância, 2020. TCC (Especialização em Teoria da Relatividade) - Instituto de Ciências Ambien- tais, Químicas e Farmacêuticas, Universidade Federal de São Paulo - Campus Diadema, Diadema. [56] Sandro Dias Pinto Vitenti. Estudo das Perturbações em Universos com Ricochete. Tese de doutorado, CBPF: Centro Brasileiro de Pesquisas Físicas. Instituto de Cosmologia, Relatividade e Astrofísica - ICRA, Rio de Janeiro, Outubro 2011. [57] Lucas Collis Olivari. Influência da Transferência de Momento-Energia na Interação entre Matéria e Energia Escura. Dissertação de mestrado, USP: Universidade de São Paulo. Instituto de Física. Depto. de Física Matemática., São Paulo - SP, 2014. [58] Gival Pordeus da Silva Neto. SEstimando parâmetros cosmologicos a partir de dados ob- servacionais. Revista Brasileira de Ensino de Física, 40(2):e2318–1 – e2318–12, 2018. [59] Sean M. Carroll, William H. Press, and Edwin L. Turner. The cosmological constant. Annual Review of Astronomy and Astrophysics, 30(1):499–542, 1992. [60] Ayan Mitra, Jurgen Mifsud, David F. Mota, and David Parkinson. Cosmology with the Einstein Telescope: No Slip Gravity Model and Redshift Specifications. Mon. Not. Roy. Astron. Soc., 502(4):5563–5575, 2021. [61] Vera C. Rubin and Jr. Ford W. Kent. Rotation of the Andromeda Nebula from a Spectros- copic Survey of Emission Regions. The Astrophysical Journal, 159:379, 1970. [62] V. C. Rubin, M. S. Roberts, J. A. Graham, Jr. Ford W. K., and N. Thonnard. Motion of the Galaxy and the local group determined from the velocity anisotropy of distant SC I galaxies. I - The data. The Astrophysical Journal, 81:687, 1976. [63] Heydson Henrique Brito da Silva. Setor escuro do universo: Uma análise termodinâmica. Dissertação de mestrado, Universidade Federal do Rio Grande do Norte, 2014. [64] Laura Dattaro. What could dark matter be? Disponível em: https://www. symmetrymagazine.org/article/what-could-dark-matter-be com acesso em: 24 julho de 2022. [65] Site Inovação Tecnológica. Especial Matéria Escura: Neutri- nos estéreis e neutralinos. Disponível em: https://www. 179 inovacaotecnologica.com.br/noticias/noticia.php?artigo= neutrinos-estereis-neutralinos#Imprimir com acesso em: 24 julho de 2022. [66] Site Inovação Tecnológica. Especial Matéria Escura: Átomos de matéria escura. Disponível em: www.inovacaotecnologica.com.br/noticias/noticia. php?artigo=atomos-materia-escura com acesso em: 24 julho de 2022. [67] Edward W.Kolb and Michael S. Turner. The Early Universe. CRC Press: Taylor & Francis Group, 1990. [68] Kai Liao and Zong-Hong Zhu. A unification of rde model and xcdm model. Physics Letters B, 718(4):1155–1161, 2013. [69] Orfeu Bertolami and Cláudio Gomes. Energia Escura. Revista de Ciência Elementar, 5(4), 2017. [70] D-Dimensões. A Teoria do Big-Bang - História e Evidências. Youtube. Disponí- vel em: https://www.youtube.com/watch?v=ZaVhQJg5j0c&t=150s com acesso em: 18 outubro de 2022. [71] Peter Atkins, Julio de Paula, and James Keeler. Physical Chemistry. Oxford: University Press, 11 edition, 2018. [72] Walter J. Paucar Casas, Tiago Ch. Mello, and Régis A. Peruzzo. Dinâmica Veicular na Reconstrução de Acidentes. Mecánica Computacional, XXIX, Nov 2010. [73] Paulo Winterle and Alfredo Steinbruch. Geometria Analítica. São Paulo: Pearson Ma- kron Books, 1 edition, 1995. [74] Dilys Parkinson. Oxford Phrasal Verbs: Dictionary for Learners of English. New York: Oxford University Press, 8 edition, 2001. [75] ESA. Instruments. Disponível em: https://www.esa.int/Science_ Exploration/Space_Science/Planck/Instruments com acesso em: 08 setembro de 2022. [76] S.D.P. Vitenti and M. Penna-Lima. A general reconstruction of the recent expansion history of the universe. J. Cosmol. Astropart. Phys., 2015(09):045–045, sep 2015. [77] Rachel S. Somerville, Kyoungsoo Lee, Henry C. Ferguson, Jonathan P. Gardner, Leo- nidas A. Moustakas, and Mauro Giavalisco. Cosmic variance in the great observatories origins deep survey. Astrophys. J. Lett., 600:L171, 2004. 180 [78] Adam G. Riess et al. Observational evidence from supernovae for an accelerating uni- verse and a cosmological constant. Astron. J., 116:1009–1038, 1998. [79] S. Perlmutter et al. Measurements of ω and λ from 42 high-redshift supernovae. The Astrophysical Journal, 517(2):565, jun 1999. [80] Martina Gerbino, Massimiliano Lattanzi, Marina Migliaccio, Luca Pagano, Laura Sal- vati, Loris Colombo, Alessandro Gruppuso, Paolo Natoli, and Gianluca Polenta. Like- lihood methods for CMB experiments. Front. in Phys., 8:15, 2020. [81] Henrietta S. Leavitt. 1777 variables in the Magellanic Clouds. Harvard Obs. Annals, 60:87–108, 1908. [82] Henrietta S. Leavitt and Edward C. Pickering. Periods of 25 Variable Stars in the Small Magellanic Cloud. Harvard Obs. Circ., 173:1–3, 1912. [83] Marcelo Módolo, Lamartine Nogueira Frutuoso Guimarães, and Reinaldo R. Rosa. An expert supernova spectral classification using artificial neural networks. In Pan-American Associacion of Computational Interdisciplinary Sciences, 2016. [84] M. Betoule et al. Improved cosmological constraints from a joint analysis of the SDSS-II and SNLS supernova samples. Astron. Astrophys., 568:A22, 2014. [85] Márcia A. Gomes Ruggiero and Vera Lúcia da Rocha Lopes. Cálculo Numérico: As- pectos Teóricos e Computacionais. São Paulo: Pearson Makron Books, 2 edition, 1996. [86] Josh Calcino and Tamara Davis. The need for accurate redshifts in supernova cosmology. JCAP, 01:038, 2017. [87] Andrea Gabrielli, Francesco Sylos Labini, Michael Joyce, and Luciano Pietronero. Sta- tistical Physics for Cosmic Structures. Germany: Springer, 1 edition, 2004. [88] D. M. Scolnic et al. The Complete Light-curve Sample of Spectroscopically Confirmed SNe Ia from Pan-STARRS1 and Cosmological Constraints from the Combined Pantheon Sample. Astrophys. J., 859(2):101, 2018. [89] Anthony Carr, Tamara M. Davis, Dan Scolnic, Daniel Scolnic, Khaled Said, Dillon Brout, Erik R. Peterson, and Richard Kessler. The Pantheon+ analysis: Improving the redshifts and peculiar velocities of Type Ia supernovae used in cosmological analyses. Publ. Astron. Soc. Austral., 39:e046, 2022. [90] Erik R. Peterson, W. D’Arcy Kenworthy, Daniel Scolnic, Adam G. Riess, Dillon Brout, Anthony Carr, Hélène Courtois, Tamara Davis, Arianna Dwomoh, David O. Jones, Bro- die Popovic, Benjamin M. Rose, and Khaled Said. The pantheon+ analysis: Evaluating 181 peculiar velocity corrections in cosmological analyses with nearby type ia supernovae. The Astrophysical Journal, 938(2):112, oct 2022. [91] Brodie Popovic, Dillon Brout, Richard Kessler, Dan Scolnic, and Lisa Lu. Improved Tre- atment of Host-Galaxy Correlations in Cosmological Analyses With Type Ia Supernovae. Astrophys. J., 913(1):49, 2021. [92] Daniel J. Eisenstein et al. Detection of the Baryon Acoustic Peak in the Large-Scale Correlation Function of SDSS Luminous Red Galaxies. Astrophys. J., 633:560–574, 2005. [93] Tamara M. Davis. Cosmological constraints on dark energy. Gen. Rel. Grav., 46:1731, 2014. [94] Maria Giovanna Dainotti, Giuseppe Sarracino, and Salvatore Capozziello. Gamma-Ray Bursts, Supernovae Ia and Baryon Acoustic Oscillations: a binned cosmological analysis. Astronomical Society of Japan, 6 2022. [95] Márcio O’Dwyer, Stefano Anselmi, Glenn D. Starkman, Pier-Stefano Corasaniti, Ravi K. Sheth, and Idit Zehavi. Linear point and sound horizon as purely geometric standard rulers. Phys. Rev. D, 101:083517, Apr 2020. [96] Alan Heavens, Raul Jimenez, and Licia Verde. Standard rulers, candles, and clocks from the low-redshift universe. Phys. Rev. Lett., 113:241302, Dec 2014. [97] Roy Maartens. Is the Universe homogeneous? Phil. Trans. Roy. Soc. Lond. A, 369:5115– 5137, 2011. [98] Florian Beutler, Chris Blake, Matthew Colless, D. Heath Jones, Lister Staveley-Smith, Lachlan A. Campbell, Quentin A. Parker, Will Saunders, and Fred G. Watson. The 6df galaxy survey: baryon acoustic oscillations and the local hubble constant. Monthly Notices of the Royal Astronomical Society, 416:3017–3032, 2011. [99] Nikhil Padmanabhan, Xiaoying Xu, Daniel J. Eisenstein, Richard A. Scalzo, Antonio J. Cuesta, Kushal T. Mehta, and Eyal A. Kazin. A 2 per cent distance to $z$=0.35 by re- constructing baryon acoustic oscillations - i. methods and application to the sloan digital sky survey. Monthly Notices of the Royal Astronomical Society, 427:2132–2145, 2012. [100] Lauren Anderson et al. The clustering of galaxies in the sdss-iii baryon oscillation spec- troscopic survey: Baryon acoustic oscillations in the data release 9 spectroscopic galaxy sample. Monthly Notices of the Royal Astronomical Society, 427:3435–3467, 2012. [101] Chris Blake et al. The wigglez dark energy survey: joint measurements of the expansion and growth history at z < 1. Monthly Notices of the Royal Astronomical Society, 425:405– 414, 2012. 182 [102] Gary F. Hinshaw et al. Nine-year wilkinson microwave anisotropy probe (wmap) obser- vations: Cosmological parameter results. The Astrophysical Journal Supplement Series, 208, 2013. [103] Ashley J. Ross, Lado Samushia, Cullan Howlett, Will J. Percival, Angela Burden, and Marc Manera. The clustering of the SDSS DR7 main Galaxy sample – I. A 4 per cent distance measure at z = 0.15. Monthly Notices of the Royal Astronomical Society, 449(1):835–847, 03 2015. [104] Shadab Alam et al. The clustering of galaxies in the completed SDSS-III Baryon Oscil- lation Spectroscopic Survey: cosmological analysis of the DR12 galaxy sample. Mon. Not. Roy. Astron. Soc., 470(3):2617–2652, 2017. [105] Alam et al. Completed sdss-iv extended baryon oscillation spectroscopic survey: Cos- mological implications from two decades of spectroscopic surveys at the apache point observatory. Phys. Rev. D, 103:083533, Apr 2021. [106] Hélion du Mas des Bourboux et al. The completed sdss-iv extended baryon oscillation spectroscopic survey: Baryon acoustic oscillations with lyα forests. The Astrophysical Journal, 901(2):153, oct 2020. [107] Jun-Jie Wei and Xue-Feng Wu. An Improved Method to Measure the Cosmic Curvature. Astrophys. J., 838(2):160, 2017. [108] Adrià Gómez-Valent and Luca Amendola. H0 from cosmic chronometers and Type Ia supernovae, with Gaussian Processes and the novel Weighted Polynomial Regression method. JCAP, 04:051, 2018. [109] Raul Jimenez and Abraham Loeb. Constraining cosmological parameters based on rela- tive galaxy ages. Astrophys. J., 573:37–42, 2002. [110] Maria de Fátima Oliveira Saraiva, Kepler de Souza Oliveira Filho, and Alexei Machado Müller. Aula 26: Expansão do Universo. Notas de Aula: Fundamentos de Astronomia e Astrofísica, IF UFRGS, Porto Alegre - RS. [111] V. Gonzalez-Perez, C. G. Lacey, C. M. Baugh, C. D. P. Lagos, J. Helly, D. J. R. Campbell, and P. D. Mitchell. How sensitive are predicted galaxy luminosities to the choice of stellar population synthesis model? Mon. Not. Roy. Astron. Soc., 439(1):264–283, 2014. [112] Adam G. Riess, Lucas Macri, Stefano Casertano, Hubert Lampeitl, Henry C. Ferguson, Alexei V. Filippenko, Saurabh W. Jha, Weidong Li, and Ryan Chornock. A 3% solution: Determination of the hubble constant with the hubble space telescope and wide field camera 3. The Astrophysical Journal, 730(2):119, mar 2011. 183 [113] M. Moresco et al. Improved constraints on the expansion rate of the universe up to z ∼ 1.1 from the spectroscopic evolution of cosmic chronometers. Journal of Cosmology and Astroparticle Physics, 2012:006–006, 2012. [114] Daniel Stern, Raul Jimenez, Licia Verde, Marc Kamionkowski, and S. Adam Stanford. Cosmic chronometers: constraining the equation of state of dark energy. i: h(z) me- asurements. Journal of Cosmology and Astroparticle Physics, 2010(02):008–008, feb 2010. [115] N. G. Busca et al. Baryon acoustic oscillations in the Lyα forest of BOSS quasars. aap, 552:A96, April 2013. [116] Anderson Luis Aimi. Interpolação Polinomial, 2008. TCC (Graduação) - Curso de Matemática Licenciatura, UFSC (Universidade Federal de Santa Catarina), Florianópo- lis, Brasil. [117] John A. Rice. Mathematical Statistics And Data Analysis. USA: Thomson Brooks/Cole, 3 edition, 2007. [118] I. Sommerville. Engenharia de Software. São Paulo: Pearson, 9 edition, 2011. [119] Elon Lages Lima. Mathematical Statistics And Data Analysis. Rio de Janeiro: Socie- dade Brasileira de Matemática, 2 edition, 1996. [120] Larry Gonick and Wollcott Smith. The Cartoon Guide To Statistics. New York: HarperPe- rennial, 1993. [121] Mariana Penna Lima. Abundância de Aglomerados de Galáxias como Observável Cos- mológico: Aplicações aos Levantamentos Fotométricos DES e SDSS. Tese de douto- rado, CBPF: Centro Brasileiro de Pesquisas Físicas. Instituto de Cosmologia, Relativi- dade e Astrofísica - ICRA, Rio de Janeiro, Outubro 2010. [122] Bryan F. J. Manly and Jorge A. Navarro. Métodos Estatísticos Multivariados: Uma Introdução. Porto Alegre: Bookman Editora, 3 edition, 2008. [123] Joseph Lee Rodgers and W. Alan Nicewander. Thirteen ways to look at the correlation coefficient. The American Statistician, 42(1):59–66, 1988. [124] L. A. Bertolo. Um Manual de Estatística. Disponível em: http://www.bertolo. pro.br/AdminFin/StatFile/Manual_Estatistica.htm com acesso em: 7 dezembro de 2022. [125] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. London: The MIT Press, 2012. 184 [126] Jason Brownlee. A Gentle Introduction to Markov Chain Monte Carlo for Probability . Disponível em: https://machinelearningmastery.com/ markov-chain-monte-carlo-for-probability/ com acesso em: 27 no- vembro de 2022. [127] Murray R. Spiegel. Probabilidade e Estatística. Coleção Schaum. São Paulo: McGraw- Hill do Brasil, 1978. [128] Adam Heiss, Thomas Brock, and Pete Rathburn. Empirical Rule: Definition, Formula, Example, How It’s Used. Disponível em: https://www.investopedia.com/ terms/e/empirical-rule.asp com acesso em: 3 dezembro de 2022. [129] Daniel M. Oppenheimer and Benoît Monin. The retrospective gambler’s fallacy: Unli- kely events, constructing the past, and multiple universes. Judgment and Decision Ma- king, 4(5):326–334, 2009. [130] Fulvio Melia. Reassessing dust’s role in forming the CMB. Eur. Phys. J. Plus, 135(6):511, 2020. [131] R. R. Caldwell, Rahul Dave, and Paul J. Steinhardt. Cosmological imprint of an energy component with general equation of state. Phys. Rev. Lett., 80:1582–1585, 1998. [132] Hassan Amirhashchi. Constraints on The Dark Energy Equation of State And The Dece- leration Parameter From Recent Cosmic Observations, 3 2015. [133] Christopher A. Clarkson and Richard Barrett. Does the isotropy of the CMB imply a ho- mogeneous universe? Some generalized EGS theorems. Class. Quant. Grav., 16:3781– 3794, 1999. [134] Sean M. Carroll, Mark Hoffman, and Mark Trodden. Can the dark energy equation of state parameter w be less than −1? Phys. Rev. D, 68:023509, 2003. [135] Brett McInnes. What if w < −1 ? In 18th IAP Colloquium on the Nature of Dark Energy: Observational and Theoretical Results on the Accelerating Universe, 10 2002. [136] F. Briscese, E. Elizalde, S. Nojiri, and S.D. Odintsov. Phantom scalar dark energy as modified gravity: Understanding the origin of the big rip singularity. Physics Letters B, 646(2):105–111, 2007. [137] Charles C. Steidel et al. A large structure of galaxies at redshift z ∼ 3 and its cosmolo- gical implications*. The Astrophysical Journal, 492(2):428, jan 1998. [138] Salvatore Capozziello, Ruchika, and Anjan A Sen. Model-independent constraints on dark energy evolution from low-redshift observations. Monthly Notices of the Royal Astronomical Society, 484(4):4484–4494, 01 2019. 185 [139] Ashutosh Tripathi, Archana Sangwan, and H. K. Jassal. Dark energy equation of state parameter and its evolution at low redshift. JCAP, 06:012, 2017.","libVersion":"0.3.2","langs":""}
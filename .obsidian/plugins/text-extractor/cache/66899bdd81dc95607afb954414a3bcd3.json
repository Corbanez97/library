{"path":"Books and Papers/Probability and Statistics/David Williams - Weighing the Odds_ A Course in Probability and Statistics  -Cambridge University Press (2001).pdf","text":"Weighing the Odds Weighing the Odds A Course in Probability and Statistics David Williams CAMBRIDGE UNIVERSITY PRESS CAMBRIDGE UNIVERSITY PRESS Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, Sao Paulo, Delhi, Dubai, Tokyo Cambridge University Press The Edinburgh Building, Cambridge CB2 8RU, UK Published in the United States of America by Cambridge University Press, New York www.cambridge.org Information on this title: www.cambridge.org/9780521006187 Cambridge University Press 2001 This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press. First published 2001 Reprinted 2004 A catalogue record for this publication is available from the British Library ISBN 978-0-521-80356-4 Hardback ISBN 978-0-521-00618-7 Paperback Transferred to digital printing 2010 Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to in this publication, and does not guarantee that any content on such websites is, or will remain, accurate or appropriate. Information regarding prices, travel timetables and other factual information given in this work are correct at the time of first printing but Cambridge University Press does not guarantee the accuracy of such information thereafter. For Sheila, for Jan and Ben, for Mags and Jeff; and, of course, for Emma and Sam and awaited 'Bump' from Gump, the Grumpy Grandpa (I can 'put those silly sums away' now, Emma.) Contents Preface \u0000 xi 1. Introduction \u0000 1 1.1. Conditioning: an intuitive first look \u0000 1 1.2. Sharpening our intuition \u0000 18 1.3. Probability as Pure Maths \u0000 23 1.4. Probability as Applied Maths \u0000 25 1.5. First remarks on Statistics \u0000 26 1.6. Use of Computers \u0000 31 2. Events and Probabilities \u0000 35 2.1. Possible outcome w, actual outcome wart; and Events \u0000 36 2.2. Probabilities \u0000 39 2.3. Probability and Measure \u0000 42 3. Random Variables, Means and Variances \u0000 47 3.1. Random Variables \u0000 47 3.2. DFs, pmfs and pdfs \u0000 50 3.3. Means in the case when 12 is finite \u0000 56 3.4. Means in general \u0000 59 3.5. Variances and Covariances \u0000 65 vii viii 4. Conditioning and Independence \u0000 73 4.1. Conditional probabilities \u0000 73 4.2. Independence \u0000 96 4.3. Laws of large numbers \u0000 103 4.4. Random Walks: a first look \u0000 116 4.5. A simple 'strong Markov principle' \u0000 127 4.6. Simulation of IID sequences \u0000 130 5. Generating Functions; and the Central Limit Theorem \u0000 141 5.1. General comments on the use of Generating Functions (GFs) \u0000 142 5.2. Probability generating functions (pgfs) \u0000 143 5.3. Moment Generating Functions (MGFs) \u0000 146 5.4. The Central Limit Theorem (CLT) \u0000 156 5.5. Characteristic Functions (CFs) \u0000 166 6. Confidence Intervals for one-parameter models \u0000 169 6.1. Introduction \u0000 169 6.2. Some commonsense Frequentist CIs \u0000 173 6.3. Likelihood; sufficiency; exponential family \u0000 181 6.4. Brief notes on Point Estimation \u0000 187 6.5. Maximum-Likelihood Estimators (MLEs) and associated CIs . . \u0000 192 6.6. Bayesian Confidence Intervals \u0000 199 6.7. Hypothesis Testing — if you must \u0000 222 ix 7. Conditional pdfs and multi-parameter Bayesian Statistics \u0000 240 7.1. Joint and conditional pmfs \u0000 240 7.2. Jacobians \u0000 243 7.3. Joint pdfs; transformations \u0000 246 7.4. Conditional pdfs \u0000 258 7.5. Multi-parameter Bayesian Statistics \u0000 265 8. Linear Models, ANOVA, etc \u0000 283 8.1. Overview and motivation \u0000 283 8.2. The Orthonormality Principle and the F-test \u0000 295 8.3. Five basic models: the Mathematics \u0000 304 8.4. Goodness of fit; robustness; hierarchical models \u0000 339 8.5. Multivariate Normal (MVN) Distributions \u0000 365 9. Some further Probability \u0000 383 9.1. Conditional Expectation \u0000 385 9.2. Martingales \u0000 406 9.3. Poisson Processes (PPs) \u0000 426 10. Quantum Probability and Quantum Computing \u0000 440 10.1. Quantum Computing: a first look \u0000 441 10.2. Foundations of Quantum Probability \u0000 448 10.3. Quantum computing: a closer look \u0000 463 10.4. Spin and Entanglement \u0000 472 10.5. Spin and the Dirac equation \u0000 485 10.6. Epilogue \u0000 491 x Appendix A. Some Prerequisites and Addenda \u0000 495 Appendix Al. '0' notation \u0000 495 Appendix A2. Results of 'Taylor' type \u0000 495 Appendix A3. 'o' notation \u0000 496 Appendix A4. Countable and uncountable sets \u0000 496 Appendix A5. The Axiom of Choice \u0000 498 Appendix A6. A non-Borel subset of [0,1] \u0000 499 Appendix A7. Static variables in 'C' \u0000 499 Appendix A8. A 'non-uniqueness' example for moments \u0000 500 Appendix A9. Proof of a 'Two-Envelopes' result \u0000 500 Appendix B. Discussion of some Selected Exercises \u0000 502 Appendix C. Tables \u0000 514 Table of the normal distribution function \u0000 515 Upper percentage points for t \u0000 516 Upper percentage points for X2 \u0000 517 Upper 5% percentage points for F \u0000 518 Appendix D. A small Sample of the Literature \u0000 519 Bibliography \u0000 525 Index \u0000 539 Preface Probability and Statistics used to be married; then they separated; then they got divorced; now they hardly ever see each other. (That's a mischievous first sentence, but there is more than an element of truth in it, what with the subjects' having different journals and with Theoretical Probability's sadly being regarded by many — both mathematicians and statisticians — as merely a branch of Analysis.) In part, this book is a move towards much-needed reconciliation. It is written at a difficult time when Statistics is being profoundly changed by the computer revolution, Probability much less so. This book has many unusual features, as you will see later and as you may suspect already! Above all, it is meant to be fun. I hope that in the Probability you will enjoy the challenge of some 'almost paradoxical' things and the important applications to Genetics, to Bayesian filtering, to Poisson processes, etc. The real-world importance of Statistics is self-evident, and I hope to convey to you that subject's very considerable intrinsic interest, something too often underrated by mathematicians. The challenges of Statistics are somewhat different in kind from those in Probability, but are just as substantial. (a) For whom is the book written? There are different answers to this question. The book derives from courses given at Bath and at Cambridge, and an explanation of the situation at those universities identifies two of the possible (and quite different) types of reader. At Bath, students first have a year-long gentle introduction to Probability and Statistics; and before they start on the type of Statistical theory presented here, they do a course in Applied Statistics. They are therefore familiar with the mechanics of elementary statistical methods and with some computer packages. Students with this type of background (for whom I provide reminders of some Linear Algebra, etc) will find in this book the unifying ideas which fit the separate Statistical methods into a coherent picture, and also ideas which allow those methods to be extended via modern computer techniques. At Cambridge, students are introduced to Statistics only when they have a thorough grounding in Mathematics; and they (or at least, many of them) like to see how Linear Algebra, etc, may be applied to real-world problems. I believe it very important to 'sell' Probability and Statistics to students who see themselves as mathematicians; and even to try to 'convert' some of them. xii \u0000 Preface As someone who sees himself as a mathematician (who has always worked in Probability theory and) who very much wishes he knew more Statistics and had greater wisdom in that subject, I hope that I am equipped to sell it to those who would find medians, modes, square-root formulae for standard deviation, and the like, a huge turn-off. (I know that meeting Statistics through these topics came very close to putting me off the subject for life.) You will see that throughout this book I am going to be honest. Mathematics students should know that Probability is just as axiomatic and rigorous as (say) Group Theory, to which it is connected in remarkable ways (see the very end of Chapter 9). (b) Not only is it the case then that we look at a lot of important topics, but we also look at them seriously — which does not detract from our enjoyment. So let me convince you just how serious this book is. (The previous sentence was written with a twinkle, of course. But seriously ... .) I see great value in both Frequentist and Bayesian approaches to Statistics; and the last thing I want to do is to dwell to too great an extent on old controversies. However, it is undeniable that there is a profound difference between the two philosophies, even in cases where there is universal agreement about the 'answers% and, for the sake of clarity, I usually present the two approaches separately. Where there seems to remain controversy (for example in connection with sharp hypotheses in Bayesian theory), I say very clearly what I think. Where I am uneasy about some aspect of the subject, and I am about several, I say that too. In regard to Probability, I explain why the 'definition' of probability in terms of long-term relative frequency is fatally flawed from the point of view of logic (not just impracticality). I also explain very fully why Probability (the subject) only works if we do not attempt to define what probability means in the real world. This gives Mathematics a great advantage over the approach of Philosophy, a seemingly unreasonable advantage since the 'definition' approach seems at first more honest. It is worth quoting Wigner's famous (if rather convoluted) statement: The language of mathematics reveals itself [to be] unreasonably effective in the natural sciences ... a wonderful gift which we neither understand nor deserve. We should be grateful for it and hope that it will remain valid in future research and that it will extend, for better or for worse, to our pleasure even though perhaps to our bafflement, to wide branches of learning. That for the quantum world we use a completely different Quantum Probability calculus will also be explained, the Mathematics of Analysis of Variance (ANOVA) having prepared the ground. It is important to realize that the Preface \u0000 xiii real world follows which Mathematics it chooses: we cannot insist that Nature follow rules which we think inevitable. (c) I said that, as far as Statistics is concerned, this book is being written at a difficult time. Statistics in this new century will be somewhat different in kind from the Statistics of most of last century in that a significant part of the everyday practice of Statistics (I am not talking about developments in research) will consist of applying Bayes' formula via MCMC (Monte-Carlo-Markov-Chain) packages, of which WinBUGS is a very impressive example. A package MLwiN is a more user-friendly package which does quite a lot of classical methods as well as some important MCMC work. These packages allow us to study more realistic models; and they can deal more-or-less exactly with any sample size in many situations where classical methods can provide only approximate answers for large samples. (However, they can run into serious difficulty, sometimes on very simple problems.) Many ideas from classical Frequentist Statistics — 'deviance', 'relative entropy', etc — continue to play a key role even in `MCMC' Statistics. Moreover, classical results often serve to cross-check 'modern' ones, and those large-sample results do guarantee the desired consistency if sample sizes were increased. A broad background in Statistics culture remains as essential as ever. And the classical Principle of Parsimony (Always use the simplest acceptable model') warns us not to be seduced by the availability of remarkable computer packages into using over-'sophisticated' (and, in consequence, possibly non-robust) models with many parameters. Of course, if Science says that our model requires many parameters, so be it. In part, this book is laying foundations on which your Applied Statistics work can build. It does contain a significant amount of numerical work: to illustrate topics and to show how methods and packages work — or fail to work. It discusses some aspects of how to decide if one's model provides an acceptably good `fit'; and indicates how to test one's model for robustness. It takes enough of a look to get you started at the computer study of non- classical models. I hope that from such material you will learn useful methods and principles for real applications. The fact that I often 'play devil's advocate', and encourage you to think things out for yourself, should help. But the book is already much longer than originally intended; and because I think that each real example should be taken seriously (and not seen as a trite exercise in the arithmetic of the t-test or ANOVA, for example), I leave the study of real examples to parallel courses of study. That parallel study of real-world examples should form the main set of 'Exercises' for the Statistics part of this book. Mathematical exercises are xiv \u0000 Preface less important; and this book contains very few exercises in which (for example) integration masquerades as Statistics. In regard to Statistics, common sense and scientific insight remain, as they have always been, more important than Mathematics. But this must never be made an excuse for not using the right Mathematics when it is available. (d) The book covers a very limited area. It is meant only to provide sufficient of a link between Probability and Statistics to enable you to read more advanced books on Statistics written by wiser people, and to show also that `Probability in its own right' (the large part of that subject which is 'separate' from Statistics) is both fascinating and very important. Sadly, to read more advanced Probability written by wiser people, you will need first to study a lot more Analysis. However, there is much other Probability at this level. See Appendix D. This textbook is meant to teach. I therefore always try to provide the most intuitive explanations for theorems, etc, not necessarily the neatest or cleverest ones. The book is not a work of scholarship, so that it does not contain extensive references to the literature, though many of the references it mentions do, so you can follow up the literature with their help. Appendix D, 'A small sample of the literature', is one of the key sections in this book. Keep referring to it. The book is 'modem' in that it recognizes the uses and limitations of computers. I have tried, as far as possible, to explain everything in the limited area covered by the book, including, for example, giving 'C' programs showing how random- number generators work, how statistical tables are calculated, how the MCMC `Gibbs sampler' operates, etc. I like to know these things myself, so some of you readers might too. But that is not the main reason for giving 'C' details. However good a package is, there will be cases to which it does not apply; one sometimes needs to know how one can program things oneself. In regard to 'C', I think that you will be able to follow the logic of the programs even if you are not familiar with the language. You can easily spot what things are peculiar to 'C' and what really matters. The same applies to WinBUGS . I have always believed that the best way to learn how to program is to read programs, not books on programming languages! I spare you both pointers and Object-Oriented Programs, so my 'C' in this book is rather basic. There is a brief note in Appendix A about static variables which can to some extent be used to obtain some advantages of OOP without the clumsiness. Apology: I do omit explaining why the Gibbs sampler works. Since I have left the treatment of Markov chains to James Norris's fine recent book (which has a brief section on the Gibbs sampler), there is nothing else I can do. Preface \u0000 xv (e) It is possible that in future quantum computers will achieve things far beyond the scope of computers of traditional design. At the time of writing, however, only very primitive quantum computers have actually been made. It seems that (in addition to doing other important things) quantum computers could speed up some of the complex simulations done in some areas of Statistics, perhaps most notably those of Ising-model' type done in image reconstruction. In Quantum Computing, the elegant Mathematics which underlies both ANOVA and Quantum Theory may - and probably will - find very spectacular application. See Chapter 10 for a brief introduction to Quantum Computing and other (more interesting) things in Quantum Probability. (f) My account of Statistics is more-or-less totally free from statements of technical conditions under which theorems hold; this is because it would generally be extremely clumsy even to state those conditions. I take the usual view that the results will hold in most practical situations. In Probability, however, we generally know both exactly what conditions are necessary for a result to hold and exactly what goes wrong when those conditions fail. In acknowledgement of this, I have stated results precisely. The very concrete Two-Envelopes Problem, discussed at several stages of the book, spectacularly illustrates the need for clear understanding, as does the matter of when one can apply the so-useful Stopping-Time Principle. On a first reading, you can (perhaps should) play down 'conditions'. And you should be told now that every set and every function you will ever see outside of research in Mathematics will be what is called 'Borer (definition at 45L) - except of course for the non-Borel set at Appendix A6, p499! So, you can - on a first reading - always ignore 'Borer if you wish; I usually say 'for a nice (Borel) function', which you can interpret as 'for any function'. (But the `Borel' qualification is necessary for full rigour. See the Banach-Tarski Paradox at 43B for the most mind-blowing example of what can go wrong.) (g) Packages. The only package used extensively in the book is WinBUGS. See also the remark on MLwiN above. For Frequentist work, a few uses of Minitab are made. Minitab is widely used for teaching purposes. The favourite Frequentist package amongst academic statisticians is S-PLUS. The wonderful Free Software Foundation has made available a package R with several of the features of S-PLUS at [190]. (h) Note on the book's organization. Things in this-size text are more important than things in small text. All subsections indicated by highlighting are to be read, and all exercises similarly indicated are to be done. You should, of course, xvi \u0000 Preface do absolutely every exercise. A ► adds extra emphasis to something important, a ►► to something very important, while a ►►► ! Do remember that 'the next section' refers to the next section not the next subsection. (A section is on average ten pages long.) I wanted to avoid the 'decimal' numbering of theorems in the 'Theorem 10.2.14' style. So, in this book, 'equation 77(F1)' refers to equation Fl on page 77. If we were on page 77, that equation would be referred to merely as Fl ; but because of the way that DTEX deals with pages, an equation without a page number could be on any page within one page of the current one. It's easier for you to cope with that than for me to tinker further with the DTEX! Sometimes, the '0' symbol signifies a natural break-point other than the end of a proof. Forgive my writing 'etc' rather than 'etc.' throughout. (i) Thanks. Much of this book was written at Bath, which is why that glorious city features in some exercises. My thanks to the Statistics (and Probability) group there for many helpful comments, and in particular to Chris Chatfield, Simon Harris, David Hobson, Chris Rogers and Andy Wood (now at Nottingham). Special thanks to Bill Browne (now at University College, London) and David Draper, both of whom, while MCMC enthusiasts, know a lot more than I do about Statistics generally. A large part of the book was written at Swansea, where the countryside around is even more glorious than that around Bath, and where Shaun Andrews, Roger Hindley and (especially) Aubrey Truman were of real help with the book. Two initially-anonymous referees chosen by Cambridge University Press took their job very conscientiously indeed and submitted many very helpful comments which have improved the book. It is a pleasure to thank those I now know to be Nick Bingham and Michael Stein. Anyone with any knowledge of Statistics will know the great debt I owe to Sir David Cox for his careful reading of the manuscript of the Statistics chapters and for his commenting on them with unsurpassed authority. Richard Dawkins and Richard Tilney—Bassett prevented my misleading you on some topics in Genetics. Colin Evans, Chris Isham and Basil Hiley made helpful comments on Chapter 10. In the light of all the expert advice, this really is an occasion where I really must stress that any remaining errors are, of course, mine. In particular, I must state that the book continued to evolve until after I felt that no further demands could be made on referees. Preface \u0000 xvii I typed the book in BTEX: my thanks to Leslie Lamport and, especially, Donald Knuth. I am most grateful for willing help from Francis Burstall, Ryan Cheal and Andrew Swann (at Bath) and from Francis Clarke at Swansea in the cases where my BTEX skill couldn't achieve the layout I wanted. Most of the diagrams I did in raw Adobe Postscript, some others with a 'C' -to-Postscript converter I wrote. David Tranah of C.U.P. helped ensure that the book was actually written, suggested numerous improvements, and generally earned himself my strong recommendation to prospective authors. My thanks too to visionary artists, copy editors and other C.U.P. staff. Malcolm and Pat, and Alun and Mair, helped me keep some semblance of sanity, and persuaded me that it was about time (for the sake of long-suffering Sheila — so worthy a chief dedicatee) that this book was finished. A big Thankyou to them. For a fine rescue of my computer when all seemed to be lost, I thank Mike Beer, Robin O'Leary and, especially, Alun Evans. For extremely skilled repair in 1992 on a machine in an even more desperate state, me, my most special thankyou of all to the team of miracle workers led by Dr Baglin at Addenbrooke's Hospital, Cambridge. It really is true that, but for them, this book would never have been written. Have a great new millennium, Addenbrooke's! I am sad that the late great Henry Daniels will not see this attempt to make more widely known my lifelong interest in Statistics. We often discussed the subject when not engaged in the more important business of playing music by Beethoven and Brahms. David Williams Swansea, 2001 Please note that I use analysts', rather than algebraists', conventions, so Z+ := {0, 1, 2, ...}, \u0000 N := {1, 2, 3, ...}. (R+ usually denotes [0, co), after all, and notations.) 11: ++ denotes (0, oo) in sensible 1 INTRODUCTION Please, do read the Preface first! Note. Capitalized 'Probability' and 'Statistics' (when they do not start a sentence) here refer to the subjects. Thus, Probability is the study of probabilities, and Statistics that of statistics. (Later, we shall also use 'Statistics' as opposed to 'statistics' in a different, technical, way.) 'Maths' is English for 'Math', `Mathematics' made friendly. 1.1 Conditioning: an intuitive first look One of the main aims of this book is to teach you to 'condition': to use conditional probabilities and conditional expectations effectively. Conditioning is an extremely powerful and versatile technique. In this section and (more especially) the next, I want you to give your intuition free rein, using common sense and not worrying over much about rigour. All of the ideas in these sections will appear later in a more formal setting. Indeed, because fudging issues is not an aid to clear understanding, we shall study things with a level of precision very rare for books at this level; but that is for later. For now, we use common sense, and though we would later regard some of the things in these first two sections as 'too vague', 'non-rigorous', etc, I think it is right to begin the book as I do. Intuition is much more important than rigour, though we do need to know how to back up intuition with rigour. In a subject in which it is easy to be misled by arguments which initially look plausible, our intuition needs constant honing. One is much more likely to make a mistake in elementary Probability than in (say) elementary Group Theory. We all make mistakes; and you should check out carefully everything I say in this book. 2 \u0000 1: Introduction Always try to find counter-arguments to what I claim, shooting them down if I am indeed correct. Some notes on the need for care with intuition occupy the next subsection. Since I shall later be developing the subject rather carefully, it is important that you know from the beginning that it is one of the most enjoyable branches of mathematics. I want you to get into the subject a little before we start building it up systematically. These first two sections contain some exercises for you — some of which (in the next section particularly) are a little challenging. In this connection, do remember that this is a 'first run through' this material: we shall return to it all later; and the later exercises when the book starts properly will give you more practice. If you peep ahead at the very extensive Section 4.1 for example, you will see one of the places where you get more practice with conditional probability. Note. Several of our examples derive from games of chance. Probability did originate with the study of such games, and their study is still of great value for developing one's intuition. It also remains the case that techniques developed for these 'frivolous' purposes continue to have great real-world benefit when applied to more important areas. This is Wigner's 'unreasonable effectiveness of Mathematics' again. A. Notes on the need for care with intuition. I mention some of the reasons why our intuition needs sharpening. Aa. Misuse of the 'Law of Averages' in real-world discussions. All humans, even probabilists in moments of weakness, tend to misuse the Taw of Averages' in everyday life. National Lotteries thrive on the fact that a person will think \"I haven't won anything for a year, so I am more likely to win next week\", which is, of course, nonsense. The (Un)Holy Lottery Machines behave independently on different weeks: they do not `remember what they have done previously and try to balance out'. A distinguished British newspaper even gave advice to people on how to choose a `good' set of six numbers from the set {1, 2, .. . , 49} for the British National Lottery. (To win, you have to choose the same six numbers as the Lottery Machine.) It was said that the six chosen numbers should be 'randomly spread out' and should have an average close to 25. It is clear that the writer thought that Choice A of (say) the six numbers 8, 11, 19, 21, 37, 46 is more likely to win than Choice B of the six numbers 1, 2, 3, 4, 5, 6. Of course, on every week, these two choices have exactly the same chance of winning. Yet the average of all numbers chosen by the Lottery Machine over a year is very likely to be very close to 25; and this tends to 'throw' people. One hears misuse of the law of Averages' from sports commentators every week. Some people tend to think that a new coin which has been tossed 6 times and landed 6 Heads, is more likely to land Tails on the next toss 'to balance things out'. They do not realize that what happens in future will swamp the past: if the coin is tossed a further 1.1. Conditioning: an intuitive first look \u0000 3 1000 times, what happened in the 6 tosses already done will essentially be irrelevant to the proportion of Heads in all 1006 tosses. Now I know that You know all this. But (Exercise!) what do you say to the writer about the Lottery who says, \"Since the average of all the numbers produced by the Lottery Machine over the year is very likely to be close to 25, then, surely, I would be better to stick with Choice A throughout a year than to stick with Choice B throughout the year.\" The British are of course celebrated for being 'bloody-minded', and the reality is that a surprisingly large number stick to Choice B! Ab. Some common errors made in studying the subject. Paradoxically, the most common mistake when it comes to actually doing calculations is to assume `independence' when it is not present, the 'opposite' of the common 'real-world' mistake mentioned earlier. We shall see a number of examples of this. Another common error is to assume that things are equally likely when they are not. (The infamous 'Car and Goats' problem 15P even tripped up a very distinguished Math Faculty.) Indeed, I very deliberately avoid concentration on the 'equally likely' approach to Probability: it is an invitation to disaster. Our intuition finds it very hard to cope with the sometimes perverse behaviour of ratios. The discussion at 179Gb will illustrate this spectacularly. B. A first example on conditional probability. Suppose that 1 in 100 people has a certain disease. A test for the disease has 90% accuracy, which here means that 90% of those who do have the disease will test positively (suggesting that they have the disease) and 10% of those who do not have the disease will test positively. One person is chosen at random from the population, tested for the disease, and the test gives a positive result. That person might be inclined to think: \"I have been tested and found 'positive' by a test which is accurate 90% of the time, so there is a 90% chance that I have the disease.\" However, it is much more likely that the randomly chosen person does not have the disease and the test is in error than that he or she does have the disease and the test is correct. Indeed, we can reason as follows, using 'K.' to signify 'thousand' (1000, not 1024) and 'NC for 'million'. Let us suppose that there are 1M people in the population. Suppose that they are all tested. Then, amongst the 1M people, about 1M x 99% = 990K would not have the disease, of whom about (1M x 99%) x 10% = 99K would test positively; and 1M x 1% = 10K would have the disease, of whom about (1M x 1%) x 90% = 9K would test positively. So, a total of about 99K + 9K = 108K would test positively, of whom only 9K would actually have the disease. In other words, only 1/12 of those who would test positively actually have the disease. 4 \u0000 1: Introduction Because of this, we say that the conditional probability that a randomly chosen person does have the disease given that that person is tested with a positive result, is 1/12. [[Note. At 6G below, we shall modify the interpretation of conditional probability just given, in which we have imagined sampling without replacement of the entire population, to one valid in all situations where we are forced to imagine instead 'sampling with replacement' to ensure 'independence'. The numerical value of the conditional probability is here unaffected.]] I do not want to get involved in 'philosophical' questions at this stage, but, provided you understand that this book contains such things only to the minimal extent consistent with clarity, I mention briefly now a point which will occur in other forms much later on. C. Discussion: Continuation of Example 3B. Now consider the situation where the experiment has actually been performed: a real person with an actual name — let's say it is Homer Simpson — has been chosen, and tested with a positive result. Can we tell Homer that the probability that he has the disease is 1/12? Do note that we are assuming that Homer is the person chosen at random; and that all we know about him is that his test proved positive. It is not the case (for example) that Homer is consulting his doctor because he fears he may have caught a sexually transmitted disease. A Possible Frequency-School View. The problem is that there is no randomness in whether or not Homer has the disease: either he does have it, in which case the probability that he has it (conditional on any information) is 1; or he does not have it, in which case the probability that he has it (conditional on any information) is 0. All that we can say to Homer is that if every person were tested, then the fraction of those with positive results who would have the disease is 1/12; and in this sense he can be 11/12 'confident' that he does not have the disease. [The Note above on sampling with replacement applies here too, but does not really concern us now.] It is not very helpful to tell Homer only that the probability that he has the disease is either 0 or 1 but we don't know which. The Bayesian-School View. If we take the contrasting view of the Bayesian School of Statistics, then we can interpret 'probability that a statement is true' as meaning 'degree of belief in that statement'; and then we can tell Homer that the probability (in this new sense) that he has the disease is 1/12. Remarks. The extent to which the difference between the schools in this case is a matter of Tittle-endians versus Big-endians' is up to you to decide for yourself later. (The dispute in Gulliver's Travels was over which way up to put an egg in an eggcup. If your primary concern is with eating the egg, ... .) In this book, I am certainly happy to tell Homer that the conditional probability that he has the disease is 1/12. I shall sell him a copy of the book so that he can decide what that statement means to him. The logic which we used in Example 3B is the uncontroversial and incontrovertible logic of Bayes' Theorem — a theorem — in Probability. We shall 1.1. Conditioning: an intuitive first look \u0000 5 study the theorem as part of the full mathematical theory in Chapter 4. However, we shall develop many of its key ideas in this section. D. Orientation: The Rules of Probability. Probability, the mathematical theory, is based on two simple rules: an Addition Rule and a Rule for Combining Conditional Probabilities, both of which we have in effect seen in our discussion of Example 3B. The Addition Rule is taken as axiomatic in the mathematical theory; the Rule for Combining Conditional Probabilities is really just a matter of definition. The subject is developed by application of logic to these rules. Especially, no attempt is made to define 'probability' in the real world. The 'long- term relative frequency' (LTRF) idea is the key motivation for Probability, though, as we shall see, it is impossible to make it into a rigorous definition of probability. The LTRF idea motivates the axioms; but once we have the axioms, we forget about the LTRF until its reappearance as a theorem, part of the Strong Law of Large Numbers. ► E. LTRF motivation for Probability. Let A be an event associated with some experiment E, so that A might, or might not, occur when E is performed. Now consider a Super-experiment E\" which consists of an infinite number of independent performances of E, 'independent' in that no performance is allowed to influence others. Write N(A, n) for the number of occurrences of A in the first n performances of E within the Super- experiment. Then the LTRF idea is that N(A, n) \u0000 converges to P(A) \u0000 (El) in some sense, where P(A) is the probability of A, that is, the probability that A occurs within experiment E. If our individual experiment E consists of tossing a coin with probability p of Heads once, then the LTRF idea is that if the coin is thrown repeatedly, then Number of tosses \u0000 p \u0000 (E2) in some sense. Here, A is the event that 'the coin falls Heads' in our individual experiment E, and p = P(A). Since the coin has no memory, we believe (and postulate in mathematical modelling) that it behaves independently on different tosses. The certain event Q (Greek Omega), the event that 'something happens', occurs on every performance of experiment E. The impossible event 0 (`nothing happens') never occurs. The LTRF idea suggests that P(11) = 1, P(0) 0. Note that in order to formulate and prove the Strong Law, we have to set up a model for the Super-experiment Se** , and we have to be precise about 'in some sense'. Number of Heads 6 \u0000 1: Introduction ■ b. F. Addition Rule for Two Events. If A and B are events associated with our experiment E, and these events are disjoint (or exclusive) in that it is impossible for A and B to occur simultaneously on any performance of the experiment, and if A U B is the event that ' A happens or B happens', then, of course, N (A U B, n) = N (A, n) N (B , n). The appropriateness of the set-theoretic 'union' notation will become clear later. If we 'divide by n and let n tend to oo' we obtain LTRF motivation — but not proof — of the Addition Rule for Two Events: if A and B are disjoint, then \u0000 u B) \u0000 (F1) Later, we take (F1) as an axiom. [[For example, if our individual experiment is that of tossing a coin twice, then IP(1 Head in all) = P(HT) TP(TH), \u0000 (F2) where, of course, HT signifies 'Heads on the 1st toss, Tails on the 2nd'.]] If A is any event, we write Ac for the event 'A does not occur'. Then A and AC are disjoint, and A U AC = SZ: precisely one of A and AC occurs within our experiment E. Thus, 1 = P(C2) = IP(A) P(Ac), so that P(Ae) = 1 — P(A). [[Hence, for our coin, lP(it falls Tails) = q := 1 — p.]1 ► G. The LTRF motivation for conditional probability. \u0000 Let A and B be events associated with our experiment E, with IP(A) # 0. \u0000 The LTRF motivation is that we regard the conditional probability JP(B I A) that B occurs given that A occurs as follows. \u0000 Suppose again that our experiment is performed 'independently' infinitely often. \u0000 Then (the LTRF idea is that) IP(B A) is the long-term proportion of those experiments on which A occurs that B (also) occurs, in other words, that both A and B occur: In other words, if A n B is the event that 'A and B occur simultaneously', 1.1. Conditioning: an intuitive first look \u0000 7 then we should have IP(B I A) = limit in some sense of N(A n B, n) N(A,n) N(A n B,n)In \u0000 P(A n B) limit in some sense of N(A,n)In \u0000 P(A) in our experiment E. In the mathematical theory, we define IP(BI A) \u0000 P(A n B) P(A) \u0000 (G1) Suppose that our experiment E has actually been performed in the real world, and that we are told only that event A has occurred. Bayesians would say that IP(B I A) is then our 'probability as degree of belief that B (also) occurred'. Once the experiment has been performed, whether or not B has occurred involves no randomness from the Frequentist standpoint. A Frequentist would have to quote: 'the long-term proportion of those experiments on which A occurs that B (also) occurs is (whatever is the numerical value of) IP(B A)'; and in this sense IP(B I A) represents our 'confidence' that B occurred in an actual experiment on which we are told that A occurred. With this Frequentist view of probability, we should explain to Homer that if the experiment 'Pick a person at random and test him or her for the disease' were performed independently a very large number of times, then on a proportion 11/12 of those occasions on which a person tested positively, he or she would not have the disease. To guarantee the independence of the performances of the experiment, we would have to pick each person from the entire population, so that the same person might be chosen many times. It is of course assumed that if the person is chosen many times, no record of the results of any previous tests is kept. This is an example of sampling with replacement. We shall on a number of occasions compare and contrast sampling with, and sampling without, replacement when we begin on the book proper. ►► H. General Multiplication Rule. We have for any 2 events A and B, n B) = P(A)F(B I A), \u0000 (HI) this being merely a rearrangement of (G1). For any 3 events A, B and C, we have, for the event AnBnC that all of A, B and C occur simultaneously within our experiment E, P(A n B n C) = P((A n B) n \u0000 = P(A n B)P(C I A n B), whence P(A n B n C) = P(A)P(B I A)P(C I A n B). \u0000 (H2) The extension to 4 or more events is now obvious. 8 \u0000 1: Introduction I. A decomposition result. Let A and B be any two events. The events G := A n B and H := AC n B are disjoint, and G U H = B. (Clarification. We are decomposing B according to whether or not A occurs. If B occurs, then either 'A occurs and B occurs' or 'A does not occur and B occurs'.) We have P(B) = P(G) P(H) = P(A)P(B I A) + IP(Ac)P(B Ac). \u0000 (II) This, the simplest decomposition, is extremely useful. Ia. Example 3B revisited. In that example, let B be 'chosen person has the disease' A be 'chosen person tests positively'. We want to find F(B I A). We are given that P(B) = 1%, P(Bc) = 99%, ]PA B) = 90%, P(A I BC) = 10%. We have, keeping the calculation in the same order as before, P(A) = P(Bc n A) + P(B n A) = P(Bc)P(A I BC) + P(B)P(A B) = (0.99 x 0.10) + (0.01 x 0.90) = 0.108 (= 108K/1M). We now know ]PA n B) and P(A), so we can find F(B I A). ►► J. 'Independence means Multiply'. If A and B are two events, then we say that A and B are independent if P(A n B) = P(A)P(B), \u0000 (JI) one of several assertions which we shall meet that 'Independence means Multiply'. If IP(A) = 0, no comment is necessary. If P(A) 0, then we may rearrange (J1) as n PP \u0000 1[1(A P(A) B) I A) = \u0000 = P(B), which says that the information that A occurs on some performance of E does not affect 'our degree of belief that B occurs' on that same performance. If we consider the experiment 'Toss a coin (with probability p of Heads) twice', then, we believe that, since the coin has no memory, the results of the two tosses will be independent. (The laws of physics would be very different if they are not!) Hence we have P(HT) = P(Heads on first toss) x P(Tails on second) = pq, where q, the probability of Tails, is 1 — p; and, using the Addition Rule as at 6(F2), we get the familiar answer that the probability of 'exactly one Head in all' is pq + qp = 2pq. The Multiplication Rules for n independent events follow from the General Multiplication Rules at 7H similarly. If we toss a coin 3 times, the chance of getting HTT is, of course, pqq. 1.1. Conditioning: an intuitive first look \u0000 9 K. Counting. I now begin a discussion (continued in the next section) of various 'counting' and 'conditioning' aspects of the famous binomial-distribution result for coin tossing. The 'counting' approach may well be familiar to you; but, in the main, I want you to condition rather than to count. ► Ka. Lemma. For non-negative integers r and n with 0 < r < r 1,, the number (7), also denoted by 'Cr, of subsets of {1,2, \u0000 , n} of size r is nl n r) where, as usual, n! := n(n - 1)(n - 2) ... 3.2.1, \u0000 0! := 1. If r < 0 or r > n, we define nCr := (nr) := 0. Remarks. Here and everywhere, we adopt the standard convention in Maths that `set' means 'unordered set': {1, 2, 3} = 1, 2}. There are indeed 4C2 = 6 subsets of size two of {1, 2, 3, 4}, namely, {1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}. The empty set is the only subset of {1, 2, ... , n} of size 0, even if n = 0. The official rigorous proof would take one through the steps of the following lemma. Part (a) is not actually relevant for this purpose, but is crucial for other results. ► Kb. Lemma. (a) The number of ordered r-tuples (i1, i2, \u0000 ,i„) where each ik is chosen from {1, 2, ... , n} is nr. (You will probably know from Set Theory that the set of all such r-tuples is the Cartesian product {1, 2, ... , Or.) (b) For 0 < r < n, the number 'Pr of ordered r-tuples (i1, i2, \u0000 , ir ) where each ik is chosen from {1, 2, ... , n} and \u0000 ,i, are distinct is given by (c) The number of permutations of {1, 2, ... , n} is n!. (d) Lemma Ka is true. Proof. In Part (a), there are n ways of choosing i1, and, for each of these choices, n ways of choosing i2, making n x n = n2 ways of choosing the ordered pair (i1, i2). For each of these n2 choices of the ordered pair (i1, i2), there are n ways of choosing i3; and so on. In Part (b), there are n ways of choosing i1, and, for each of these choices, n -1 ways of choosing i2 (because we are now not allowed to choose it again), making n x (n - 1) 10 \u0000 1: Introduction ways of choosing the ordered pair (i1, i2). For each of these n(n — 1) choices of the ordered pair (i1, i2), there are n — 2 ways of choosing i3; and so on. Part (c) is just the special case of Part (b) when r = n. Now for Part (d). By Part (c), each subset of size r of {1, 2, ... , n} gives rise to r! ordered r-tuples (i1, i2, , i r ) where i1, i2, , ir are the distinct elements of the set in some order. So it must be the case that r! x 'C r = n137.; and this leads to our previous formula for n C r L. 'National Lottery' Proof of Lemma 9Ka. One can however obtain clearer intuitive understanding of Lemma 9Ka by using conditioning rather than counting as follows. Yes, a certain amount of intuition goes into the argument too. A British gambler (who clearly knows no Probability) pays 1 pound to choose a subset of size r of the set {1, 2, ... , n}. (In Britain, n = 49, and r = 6.) The Lottery Machine later chooses 'at random' a subset of size r of the set {1, 2, ... , n}. If the machine chooses exactly the same subset as our gambler, then our gambler wins the 'jackpot'. It is clear that our gambler wins the 'jackpot' with probability 1/ (7), and we can find (nr ) from this probability. Now, the probability that the first number chosen by the machine is one of the numbers in our gambler's set is clearly r In. The conditional probability that the second number chosen by the machine is in our gambler's set given that the first is, is clearly (r — 1)/(n — 1), because, given this information about the first, at the time the machine chooses its second number, there are n —1 'remaining' numbers, r — 1 of which are in our gambler's set. By Multiplication Rule 7(H1), the probability that the first two numbers chosen by the machine are in our gambler's set is ✓ r — 1 n x n — 1 . By extending the idea, we see that the probability that the machine chooses exactly the same set as our gambler is ✓ r — 1 \u0000 r — 2 \u0000 1 \u0000 r! \u0000 r!(n — r)! n x n — 1 x n — 2 x xn — r + 1 \u0000 n Pr \u0000 n! (In Britain, then, the probability of winning the jackpot is very roughly 1 in 14 million.) We have proved Lemma 9Ka. For more on our gambler, see Exercise 17Rb below. M. Binomial(n, p) distribution. Now, we can combine the ideas of 8J with Lemma 9Ka. ► Ma. Lemma. If a coin with probability p of Heads is tossed n times, and we write Y for the total number of Heads obtained, then Y has the probability mass function of the binomial(n, p) distribution: P(Y = r) = lr(n, p; r) nT ,)P r (1 P)n—r 1.1. Conditioning: an intuitive first look \u0000 11 Proof Because of the independence of the coin's behaviour on different tosses, we have for any outcome such as HTHHHTTHH ... with exactly r Heads and n — r Tails, P(HTHHHTTRH . ) = pqpppqqpp \u0000 = pr qn—r where q = 1 — p. Now the typical result with r Heads in all is a sequence such as HTHHHTTHH ... in which the set of positions where we have H is a subset of {1, 2, ... , n} of size r. Every one of these (nr) subsets contributes prq' to ]P(Y = r), whence the result follows. ❑ ►► N. Stirling's Formula. This remarkable formula, proved in 1730, states that ( n) n e as n — + oo, (N1) with the precise meaning that, as n oo, the ratio of the two sides of (N1) converges to 1. For n = 10, LHS/RHS = 1.0084. Much more accurate approximations may be found in Subsection 148D. Na. Exercise. Use (N1) to show that, the probability b(2n, 27 n) that we would get n Heads and n Tails in 2n tosses of a fair coin satisfies b2n, 21n)ti rn, • Check that when n = 10, the left-hand side is 0.1762 (to 4 places) and the right-hand side is 0.1784 (to 4 places). Historically, approximation (N2) was a vital step on the route which eventually led to the general Central Limit Theorem of Chapter 5. Nb. Heuristic explanation for Stirling's formula. (I promised that, as far as possible, I would not ask you to take things entirely on trust.) The Probability corresponding to this heuristic explanation can be found in Subsection 162F. We have, with x = n co \u0000 co n! = f xne' dx = f o \u0000 —,F, \u0000 co \u0000 rt = C ITN/T2, f (1+ \u0000 Y ) e —YV7' dy. e \u0000 ---VT-L \u0000 T/ Now (see Appendix Al, p495, and equation 496(ApA 2.4)), for any fixed y, we have, as n \u0000 oo, \u0000 In{ (1 \u0000 Y \u0000 n In (1+ \u0000 ) yVT-t Nrn JJJ = n N/T -2, y \u0000 1 2 n V + 0 ( 'rx 3 / ) — 32 1 (N2) (n y vTone—(n+y,rn) \\/T1 dy y2, 12 \u0000 1: Introduction (see Appendix Al, p495 for (X•) notation and also 496(ApA 2.4)). Hence for every y E Tl,, and this suggests (but does not prove) that n! \u0000 An :- \u0000 /e \u0000 f efrdy = \u0000 (N3) (n)n \\F-t the famous last integral being evaluated at 146B. It is significantly more difficult to make the above clear explanation into a rigorous proof than to provide the following rigorous proof (optional!) which is not an explanation. Nc. Proof of Stirling's formula (optional!). This proof begins with an idea of H. Robbins, quoted in the books by Feller and by Norris. We have for -1 < t < 1, t \u0000 1 \u0000 ln(1 + t) = \u0000 s ds = f {1 - s + s2 - s3 + • • -} ds whence, for 0 < t < 1, \u0000 ln(1 + t) = +t - \u0000 + it3 - • • • , ln(1 - t) = -t - zt2 - 3t3 - Hence, for 0 < t < 1, t 11 In ( 1 ±t ) 1= - 3 1t \u0000 5 t21 t4 2 \u0000 1 t \u0000 t +•• . (so is non-negative) - 1 \u0000 1 \u0000 t 2 < \u0000 + \u0000 ± • 3 \u0000 3 \u0000 3(1 - t2) • Hence (with y = 1/t), we have for y > 1, 1 \u0000 1 0 < c(y) := 2 In y - 1 ( Y + 1) 1 < \u0000 3(y2 _ 1) But if an := In A n , where An is as at (N3), then (check!) 0 < a n - an+1 = c(2n + 1) < \u0000 \u0000 12n \u0000 1 n 12(n + 1) ' whence (an) is a decreasing sequence and (an - (12n)-1) is an increasing sequence. Hence, for some a, a n -4- a, An -4 A := ea , and we have Stirling's formula in the form n 1 + \u0000 e- Y\"`/' AFL (N4) 1.1. Conditioning: an intuitive first look \u0000 13 It only remains to prove that A = For an integer k > 1, integration by parts with u = cosk 0, v = sin 0, shows that ilc+1 \u0000 Z r COS k+1 B do = J udy = [uv] — f v du = f 1- k cosk-1 0 sin2 0 dO = \u0000 — klk+1 (using sin2 = 1 — cos2), whence, for k > 1, k 4+1 = \u0000 k + 1 i k —1 • It is immediately checked that h = z 71- and Il = 1. Hence, using also 12(N4), 2n \u0000 2n 2n — 2 2 /2n-1-1 — 2n + 1 -12n-1 \u0000 2n + 1 2n — 1 \u0000 3 (n!2n)2 \u0000 Ae \u0000 A = (2n + \u0000 202n + 1) (1 + 2n+1 \u0000 2VY7, since (1 + 1/m)m \u0000 e. Similarly, (2n)! 1 \u0000 it ( , 702n 27r \u0000 AN/Y-i • But since 0 < cos 0 < 1 for 0 E [0, Or], the definition of Ik implies that 1-2n —1 > -T2n > 1-2n-F1 - Hence, A 2 \u0000 /2n \u0000 27r 1 > \u0000 27r , 1 > \u0000 1-2n \u0000 1-2n— 1 \u0000 A 2 The only possible conclusion is that A = \u0000 as required. ► 0. The Birthdays Problem. Part (a) of the following problem is well known. Oa. Exercise. Suppose that we have r people labelled 1,2, ... , r in a room. Assume that their birthdays, correspondingly labelled, are equally likely to form any ordered r-tuple with numbers chosen from {1,2,3, \u0000 , n}, where n = 365. We ignore leap years. In regard to the assumption, see Appendix B. (a) Explain by 'counting' why the probability that no two have the same birthday is n — 1 n — 2 \u0000 n — r + 1 \u0000 x \u0000 x • x \u0000 nr Explain also via the use of conditional probabilities. Check that the probability that no two have the same birthday is less than z if r = 23. Two methods of doing this last step quickly with just a pocket calculator are given later in this subsection. 14 \u0000 1: Introduction (b) Suppose that r = 3. There are three possibilities: (i) no two have the same birthday, (ii) some two have the same birthday and the third has a different birthday, (iii) all three have the same birthday. Calculate directly the probabilities of these possibilities, and check that they sum to 1. (c) For general r, calculate the probability of the event that precisely one day in the year is the birthday of two of the people and no day is the birthday of three or more. Your answer should check with that to (b)(ii) when r = 3. Ob. A very useful inequality. 1— \u0000 < \u0000 for x > 0. \u0000 (01) To prove this, integrate 1 > e —Y (y > 0) from 0 to x, getting x > 1 — e' \u0000 > 0). Note that therefore, the probability that no two of our 23 people have the same birthday is at most ( 1 + 2 +•-•+ 22) \u0000 ( 253) exp \u0000 365 \u0000 = exp \u0000 365 \u0000 = 0.499998. Oc. Beware of the 'independence' trap. Continue to ignore leap years, assuming that every year has 365 days. One might be tempted to argue that the probability that two people have different birthdays is 364/365, that there are (223) = 253 pairs of people amongst 23 people, whence the chance that all 23 people have different birthdays is (364/365)253 = 0.4995. You can see that this argument is not correct by supposing that instead of 23 people, one had (say) 400. You should always check out extreme cases to see if your argument might be flawed. The argument is essentially assuming that if, for a pair P of people, Ep denotes the event that the two people in the pair P have different birthdays, then the events Ep, where P ranges over the 253 pairs, are independent. But of course, if persons 1 and 2 have the same birthday and persons 2 and 3 have the same birthday, then persons 1 and 3 must have the same birthday. You have been told already that the most common mistake made in the subject is to assume independence when it is not valid. The 'independence' argument does produce a good approximation for 23 people because (by the Binomial Theorem) ( 364 k 365 ) 1 \\k 365 ) 1 k 365 for k = 1, 2, ... , 22, whence (with the correct probability that no two have the same birthday on the left-hand side) 364 363 \u0000 343 N ( 364) 1+2+• • • +22 \u0000 ( 364 ) 253 365 365 \u0000 365 \u0000 365 \u0000 365 1.1. Conditioning: an intuitive first look \u0000 15 Indeed, mathematical induction shows that k \u0000 ( 364 \\ k 1 365 C 365) whence the probability that no two have the same birthday is at most the 'independence' answer 0.4995. P. The Tar and Goats' Problem. \u0000 This has become notorious, and it is instructive. There are many other versions of essentially the same problem, which was much discussed long before its 'Monty Hall game show' incarnation around 1990. Pa. The Problem. At the end of that American game show, a contestant is shown three closed doors. Behind one of the doors is a car; behind each of the other two is a goat. The contestant chooses one of the three doors. The show's host, who knows which door conceals the car, opens one of the remaining two doors which he knows will definitely reveal a goat. He then asks the contestant whether or not she wishes to switch her choice to the remaining closed door. Should she switch or stick to her original choice? Wrong solution. There remain two closed doors, so the contestant has probability 1/2 of winning whether she switches or not. Correct solution. A contestant who decides before the show definitely to stick with her original choice, wins the car if and only if her original choice was correct, that is, with probability 1/3. A contestant who decides before the show definitely to switch wins the car if and only if her original choice was wrong, that is, with probability 2/3. The fact that switching here doubles the chance of winning can be decided irrespective of what happens during the show. Discussion. What is wrong with the Wrong solution? Well, just because there are only two possibilities, there is no reason at all to assign probability 1/2 to each of them. In this example, the host's knowledge and strategy tilt the odds from the 50 : 50 situation. Remember that the host's strategy precludes the possibility that the contestant first chooses a door with a goat and he then reveals a car; this causes the imbalance by shifting the weight of this possibility to the situation where the contestant chooses a door with a goat and the host reveals a goat. The following exercises clarify this. Pb. Exercise: Having more goats sorts out our intuition. Suppose that there had been 1000 doors, 1 car and 999 goats. After the contestant's choice the host opens 998 doors each of which he knows will reveal a goat. By how much does switching now increase the contestant's chance of winning? Note that the '50 : 50 answer' now seems ridiculous to our intuition. Pc. Exercise. Consider the following modification of the original problem. At the end of an American game show, a contestant is shown three closed doors. Behind one of the doors is a car; behind each of the other two is a goat. The contestant chooses one of the three doors. The show's host, who does not know which door conceals the car, opens one of the remaining two doors and happens to reveal a goat. He then asks the contestant whether or not she wishes to switch her choice to the remaining closed door. Should she 16 \u0000 1: Introduction switch or stick to her original choice? Show that for this problem, the probability of winning is 1/2 irrespective of whether she switches or not. Generalize to 1000 doors. Note. Mathematical models for the cases at Pa and Pc will be described at 73Ab. ►► Q. The 'Two-Envelopes Paradox'. \u0000 This is a much more important (apparent) paradox than the 'Car and Goats' one. Do not dismiss it as trivial. We shall discuss it seriously much later (399M), when it has important lessons for us. Here is a very preliminary version of the problem, one which is not precisely posed. In this version, we shall assume that money is (non-negative and) real-valued rather than integer-valued in some units. In our later well-posed version with the same 'paradoxical' features, money will be integer-valued. Someone shows you two envelopes and says that one contains twice as much money as the other. You choose one of the envelopes, and are then asked if you wish to swap it for the other. (a) 'Conditioning' argument. You might think as follows. \"Suppose that I were to open the envelope I now have to reveal an amount x. Then the other envelope must contain either lx or 2x, so that on average (over many plays at this game), I would have l/x after a swap. So it is best for me to swap.\" (Would you then swap a second time if you had the chance?!) (b) Symmetry argument. Or you might think in this way. \"Suppose that someone chose a number Z at random, and put Z in one envelope, 2Z in the other. If my first envelope were to contain Z, then, by swapping, I would gain Z; if my first envelope were to contain 2Z, then I would lose Z by swapping. So, on average, it does not matter whether I swap.\" That gives you the rough idea of what a properly-stated problem of 'two-envelopes' type will entail. At Subsection 399M, I give a precise statement of a problem with the `paradoxical' features of this one, but one for which the issues are much clearer. I then give a rather extensive discussion of that problem because I cannot let you imagine that Probability has real paradoxes: in spite of what may be claimed on the Internet, it doesn't. In that subsection, I present the discussion in such a way that you will almost certainly be able to get the flavour of the correct explanation if you read that subsection now. That discussion will explain any type of two-envelope 'paradox'. R. Miscellaneous Exercises ► Ra. Exercise: Pooling blood samples. This exercise, which is due to Dorfman and which has been used to great effect in practice, is to be found in Feller's masterpiece. A certain disease, affecting only a small fraction p of the population, can be detected by an error-free blood test. To discover which individuals in a large group (of unrelated people from different regions) have the disease, the following strategy is adopted. The group is divided into blocks of x people. Samples from all people in a block are pooled 1.1. Conditioning: an intuitive first look \u0000 17 (mixed together). If the pooled sample from a block is clear, then the x people in that block are clear; otherwise, every person in the block is tested separately. Show that the expected total number of tests required per person is T = 1 — x + 1 — (1 — pr (Hint. You need only consider a group consisting of x people, since each block situation is repeated. You certainly do one test on a block, and x more if and only if it is not the case that all people in the block are OK). The Binomial Theorem allows us to approximate T by T \u0000 1 — x + 1 — (1 — xp). Deduce that the best choice of x is about 1/ \\//), yielding a minimum number of tests per person of about 2 \\ffi. If p = 1/100, use a pocket calculator to find the exact best value of x, and show that the strategy then leads to just over an 80% saving in number of tests compared with that of testing every individual separately from the beginning. Rb. Exercise: Hypergeometric distributions (and the Lottery). Suppose that John chooses a subset S of size s (0 < s < n) at random from the set {1, 2, ... , n}, each of the (:) subsets of size s having probability 1/(S) of being chosen. Suppose that, independently of John, Jane chooses a subset T of size t (0 < t < n) at random from the set {1, 2, ... , n}, each of the (nt) subsets of size t having probability 1/(nt) of being chosen. Strictly speaking, the independence means that, for every subset So of size s and every subset To of size t, ID(John chooses subset So and Jane chooses subset To) = IED(John chooses subset So) x P(Jane chooses subset To) 1 (7)(7) Let IS n TI denote the number of elements in the set S n T. Prove (using the very substantial Hints only as a last resort) that H(n, s,t; k) := Pas n Ti = k) may be expressed in the three equivalent ways: (s )( n—s) k t—k \u0000 t(n—t kk/ ) ks—k1 ) (i) \u0000 (ii) \u0000 ( (7) \u0000 (7) (nk)cikk)( ntiks ) (iii) \u0000 (7) (7) We say that IS n TI has a hypergeometric distribution. [Hints. For (i), we decide that we can pretend that John has chosen the subset {1, 2, ... , s}; then 'we want' Jane's subset to 18 \u0000 1: Introduction consist of a subset of size k {1, 2, ... , s} and a subset of size t — k in Is +1, s + 2, ... , In (iii), we imagine 'first choosing K := S fl T, then S \\ K, then T \\ S' .] Deduce that the probability that the (un)Holy Lottery Machine of Discussion 10L picks a total of k of our gambler's numbers is rrio (7) \u0000 • It is always a good idea to think up several different methods in these combinatorial questions as a cross-check. The next exercise gives yet another method. Rc. Exercise: A conditioning approach to the hypergeometric distribution. For the experiment of tossing a coin (with probability q of Tails) rt times, calculate P(k Tails in first s tosses I t Tails in all); and explain why this leads to the hypergeometric distribution. (Note. The fact that the answer does not involve q relates to the fact that t is a 'sufficient statistic' for q, as will be explained much later on.) Rd. Exercise. If our answer (i) in Exercise 17Rb is correct, then sAt ks (nt ks ) \u0000 ( nt) k=0 (R1) where we have used the standard notation s A t := min(s, t), the minimum of s and t. The equation corresponds to E k H(n, s, t; k) = 1. Prove equation (R1) by considering the coefficient of xt in + x)s(1 + x)71—s = (1 + xr. There is not really any need to specify the upper and lower limits for the sum in equation (R1) because the binomial coefficients are defined to be 0 for 'impossible' values. Even so, please answer the question: for what values of k is H(n, s, t; k) strictly positive, and why? 1.2 Sharpening our intuition Shortly, I shall look at some 'conditioning' approaches to Lemma 10Ma, the binomial-distribution result. First, I mention three puzzles, the first requiring only common sense, the next two some use of conditional probability. Stars indicate somewhat tricky things. All three puzzles are solved later in the main text. 1.2. Sharpening our intuition \u0000 19 A. Puzzle. Suppose that I keep throwing a fair die (perfectly cubical die) with scores 1, 2, ... , 6 on its faces. Give a very accurate estimate that my accumulated score will at one stage be exactly 10000. Hint. What is the average gap between successive accumulated totals? . B. Puzzle: The 'Waiting for HH' problem. Suppose that I keep tossing a fair coin. (a) How many tosses are there on average before I get the first Head? [Solution. You can do this by 'reversing' the way in which (I hope) you did Exercise A. But here's the way I now want you to do it. Let the answer be x. Then we have the equation x = 1 ± (1 x 0) ±(2 x x) , explained as follows: the 1 is the first toss, which we have to make; with probability 2 , the first toss produces Heads and we have 0 further tosses to make; with probability A, the first toss produces Tails, in which case the whole process starts afresh and so, on average, I have to wait a further x tosses. Here we used intuitively the decomposition result for average values (or 'expectations' or `means') corresponding to 8(11).] (b) How many tosses on average do I make (from the beginning) until the first appearance of the HT pattern (Head immediately followed by a Tail)? (Hint. Using the result of part (a), you can do this immediately, without any further calculation.) (c)* How many tosses on average do I make (from the beginning) until the first appearance of the HH pattern? C. Puzzle: Eddington's 'Four Liars' Problem*. \u0000 The great English astronomer Sir Arthur Eddington posed this problem. He got it wrong. See if you can do better by getting the correct answer 13/41. \"Whenever A, B, C, or D makes a statement, he lies with probability 2/3, tells the truth with probability 1/3. D made a statement, C commented on D's statement, B on C's, and A on B's. What is the conditional probability that D spoke the truth given that A affirms that B denies that C affirms that D lied?\" Hint. Why not start with the 'Two Liars' version, that of finding TP(D spoke the truth I C affirms that D lied), the answer to which is ? 20 \u0000 1: Introduction r 0 1 \u0000 2 3 4 5 6 0 1 1 \u0000 1 \u0000 1 2 1 2 \u0000 1 n 3 1 3 \u0000 3 \u0000 1 4 1 4 6 4 \u0000 1 5 1 5 10 10 \u0000 5 1 6 1 6 15 20 15 6 1 Table D(i): Pascal's triangle for ( nr ) D. Thinking about the binomial-distribution result 10Ma via conditioning. I want to explain two useful methods for doing this. You will recall Pascal's triangle for calculating binomial coefficients. The 'triangle' illustrates the recurrence relation (n) \u0000 (n — 1) + (n — 1) = Cr / \u0000 r — 1) \u0000 r ) . (DI) Suppose that a coin with probability p of Heads (and q = 1 — p of Tails) is tossed n times. Let A be the event 'Heads on first toss', B be the event `r Heads in all' and let bi (n,p;r) := P(B). We want to prove that bi (n, r;p) equals the b(n, r;p) of Lemma 10Ma. Method 1. We have, 'obviously', P(A) = p; P(B I A) = bi (n — 1,p; r — 1); P(A') = q; P(B I AC) = bi (n — 1, p; r). Using the decomposition result 8(11), we therefore find that bi (n,p;r) = pbi (n — 1,p; r — 1) + q bi (n — 1,p; r). \u0000 (D2) This tallies exactly with recurrence relation (D1); and we could now deduce the required result by induction: let Sm be the statement that bi (m,r;p) = b(m,r;p) for 0 < r < m. Then Sn_i implies Sn, as we see by comparing (D2) with (D1). But So is true ... .' ❑ We shall see later a more illuminating connection between Lemma 10Ma, Pascal's Triangle and the Binomial Theorem. 1.2. Sharpening our intuition \u0000 21 Method 2. We have P(A 1B) = because 'it is the chance that the first toss is one of the r which produce Heads out of the n tosses'. Since P(A n B) = P(A)IP(B I A) = P(B)P(A B), we have n(n — \u0000 bi (n,p;r) = p r—i bi (n — 1,p;r — 1) = p2 r \u0000 r (r — 1) 1) bi(n — 2,p; r — 2) n(n — 1) - • • (n — r + 1) \u0000 pr \u0000 bi(n — r, p; 0). r(r — 1) . — 1 But bi(n — r, p; 0), the probability of all Tails in n — r tosses, is qn —r; and we have the desired formula bi (n,p;r) = b(n,p;r). Da. Exercise. For the 'hypergeometric' situation at Exercise 17Rb, let A and B be the events: A:'1ESnT', B:`1,5nTl=k'. By considering P(A), IP(B I A), IP(B), P(A B), prove that t — n x — n x H(n — 1, s — 1, t — 1; k — 1) = H(n, s, t; x — k . E. Solution to Problem 19B. We have already seen that the answer to Part (a) is x = 2. The answer to Part (b) is therefore as easy as 2 + 2 = 4. The reason is that if I wait for the first H (on average, 2 tosses) and then wait for the next T to occur (on average, a further 2 tosses), then I will have for the first time the pattern HT (after on average a total of 4 tosses). Note that the time of the next H after the first H is not necessarily the time of the first HH. This explains the difference between the 'HT' and `HH' cases. Let x = 2 be the average wait for the first Head. Let y be the average total wait for HH. Then y = x 1 +(z x 0) +(2 x y) , whence y = 6. This is because I have to wait on average x tosses for the first Head. I certainly have to toss the coin 1 more time. With probability 2 , it lands Heads, in which case I have the HH pattern and 0 further tosses are needed; with probability 2 it lands Tails and, conditionally on this information, I have to wait on average just as long (y) as I did from the beginning. For a simulation study of this problem, see the last section of this chapter. 22 \u0000 1: Introduction F. Important Note. The correct intuition on which we relied in the above solution implicitly used ideas about the process 'starting afresh' after certain random times such as the time when the first Head appears. This is a trivial case of something called the 'Strong Markov Property'. We shall continue in blissful ignorance of the fact that we are using this intuitively obvious result until we prove it in Subsection 129C. G. Miscellaneous Exercises ► Ga. Exercise. Let y77, be the average number of tosses before the first time I get n Heads in succession if I repeatedly toss a coin with probability p of Heads. Prove that yn = yn- 1 + 1 ± (p x 0) + (q x yn) whence P Yn = Yn- 1 + 1, yo = 0. Deduce, by induction or otherwise, that 1 \u0000 1 yn = -q ( p—n - 1) . ► Gb. Exercise: Unexpected application to gaps in Poisson processes. Shopkeeper Mr Arkwright has observed that people arrive completely randomly at his shop, with, on average, A = 1 person(s) arriving per minute. He will close the shop on the first occasion that c = 6 minutes have elapsed since the last customer arrived. Show that, on average, he keeps the shop open for approximately 6 hours and 42 minutes. Indeed, prove that the exact answer is (eAc - 1)/A minutes. Hint. Use the previous exercise. God takes a very large integer N, and divides time into little intervals of length 6 = 1/N minutes. At times 6,26,36, ..., God tosses a coin with probability p of Heads; and if the coin falls Tails, causes a customer to arrive at the shop at that instant. What is the value of q = 1 - p (with negligible error compared to 1/N?) For some value of it which you must find, Mr Arkwright will close the shop the first time that God gets n Heads in a row. Apply Exercise Ga, and then let N -> co to get the exact answer, using the well-known fact that (1 + - \\ -> ev as k 00. y k k ) ► Gc. Exercise: Which pattern first?: easy case. If I keep tossing a fair coin, what is the chance that I get (a) Pattern HH before Pattern HT, (b) Pattern TH before Pattern HH? ► Gd. Exercise*: Which pattern first?: weird case. Consider the 8 possible patterns of length 3: HHH, HHT, HTH, HTT, THH, THT, TTH, TTT. 1.3. Probability as Pure Maths \u0000 23 Show that if I name any of these patterns, then you can name another one of them such that the chance that the pattern you name appears before the one I name in a sequence of tosses of a fair coin is strictly greater than 1. Most people find this very counter-intuitive. This is an excellent example to illustrate that sometimes you have to live with a problem before you can solve it. 1.3 Probability as Pure Maths The way it is played these days, Probability is a branch of Pure Maths, derived from certain axioms in the same way as (say) Group Theory is. The fundamental role of the Addition Rule makes Probability part of a subject called Measure Theory which studies length, area, volume, and various generalizations. The 'Independence means Multiply' Rule relates to the 'product-measure' construction which allows us to construct area, volume, etc, from length. ∎∎ A. Important discussion: What to do about Measure Theory? I hope that it is already apparent that I believe that intuition is much more important than rigour; but, as I have said earlier, I also believe in clarity and have never believed that fudging adds to clarity. I therefore have a problem here because the first stages of Measure Theory are too difficult for inclusion in a book at this level. But does that mean that we should completely deny ourselves the great advantages of having some understanding of that subject? Let us consider an analogous case. What to do about real numbers in Analysis? Real Analysis makes great claims to be so rigorous: after all, doesn't it have the dreaded E-6 method?! Real numbers are the foundation of Analysis. Yet, increasingly, students are taught the e-S method in considerable detail without being shown that the real numbers exist! Students are asked to take for granted that the set R of real numbers has certain properties: that it is a complete ordered field. The question 'How do we know that such a complete ordered field exists?' is ducked. But that approach — as long as it is made honestly — is valid: the construction of the real numbers (either as Tedekind cuts' or (much better) as 'equivalence classes of Cauchy sequences of rationals') is too difficult at that stage. Measure Theory is the foundation of Probability. Borel, Caratheodory, Fubini and (especially) Lebesgue constructed Measure Theory very early last century; and, substantially extending ideas of Borel, Wiener and others, Kolmogorov used it to build the rigorous foundations of modern Probability Theory in 1932. Are we to insult those great mathematicians by ignoring their work completely and regarding Probability as the shambles it was prior to that work? I think not, especially since the results of Measure Theory are easy to understand. And I 24 \u0000 1: Introduction am determined not to tell you things which you would have to unlearn later if you take the subject further. Several references are made to my book [W], [235], for proofs of measure-theoretic results. It has been traditional in Statistics to say: \"Under suitable conditions, the following result holds.\" Now, it is very silly to scoff at this with a purist remark that it is meaningless. What the statisticians mean is clear: \"The result is known to be true in all the cases which we shall meet; but even to state the technical conditions under which it holds would complicate things to an unacceptable extent.\" By contrast, in Probability, it is possible to state quite simply EXACTLY the conditions under which such results as the Strong Law of Large Numbers ( 'Law of Averages') hold and to show what goes wrong when they fail. By taking for granted a few things from Measure Theory, we can get really clear understanding. As mentioned in the Preface, the 'Two-Envelopes Paradox' concerns a simple problem where clear, indeed measure-theoretic, understanding is essential. So, here's my solution. Everywhere, I shall concentrate on intuition. But I shall in passages marked with an ® symbol provide some measure-theoretic results, which we shall take for granted, so that the whole edifice does stand on secure foundations. You could leave the Measure Theory to a second reading if you so wish, but please give it a try the first time through. There is one section of Chapter 2 which contains a reasonable chunk (4 pages, counting motivation, magic, discussion, etc) of Measure Theory (in passages marked with the CD symbol). There is much less Measure Theory in Chapter 3, and hardly any at all in most subsequent chapters. To continue ... Great fun can be had playing Probability as Pure Maths. One of the surprising developments of the second half of the 20th century was the way in which Probability links deeply with areas of Maths such as Complex Analysis, Potential Theory, Partial Differential Equations, Differential Geometry. It is amazing that in such a well-developed subject as Complex Analysis, the first solutions of certain important problems were obtained via the use of Probability. Using an axiomatic system is great, because the fundamental entities are not defined: they just are. An abstract group is a set of elements carrying a `multiplication' satisfying certain rules: we do not define what an individual `element' is. In Probability, no attempt is made to define probability in the real world. Any attempt at such a definition is doomed; and the magic of Maths is that we do not need to — indeed, we must not — make such an attempt. The following discussion will be expanded later. 1.4. Probability as Applied Maths \u0000 25 B. Discussion. The idea of probability as 'long-term relative frequency' is an extremely helpful intuitive one, as we have seen; but it just cannot be made into a rigorous definition. Just suppose that I try to define the probability p that this coin in my pocket will fall Heads as the long-term proportion of Heads (the limiting value, assumed to exist, of the Number of Heads divided by the Number of Tosses) if I were to throw the coin infinitely often. Apart from certain impracticalities in this 'definition', it is fundamentally flawed from the point of view of logic. If we assume that the long-term proportion will be p for every sequence of tosses which could be named in advance, then this statement would have to be true for the sequence consisting only of those tosses which (during the experiment) produce Heads (and for the corresponding sequence for Tails). Trying to be 'precise' by making a definition out of the long-term frequency' idea lands us in real trouble. Measure Theory gets us out of the difficulty in a very subtle way discussed in Chapter 4. As wise and subtle Francis Bacon wrote: \"Histories make men wise; the mathematics, subtile\". Remarks. I add some remarks which can serve as pointers to why we need Measure Theory. In a way, the difficulty concerning the long-term relative frequency' idea discussed above is related to a much simpler situation. Suppose that I choose a number between 0 and 1 uniformly at random. (Thus the chance that the chosen number will fall in a subinterval (a, b) of [0, 1] will be b — a.) Then, before the experiment, the probability that I will choose any particular number is 0: there are infinitely many numbers all 'equally likely' in some sense; yet the probability that I pick some number is 1. Exactly how does this tally? Contrast the following situation. I cannot think of 'choosing a positive integer uniformly at random' in such a way that each positive integer has the same probability p of being chosen as any other. Clearly, p must be 0; and now, there is no way of making things tally. Likewise, I cannot think of choosing a rational number in [0, 1] 'uniformly at random'. 1.4 Probability as Applied Maths Of course, Probability is more than a mathematical game. Experience shows that mathematical models based on our axiomatic system can be very useful models for everyday things in the real world. Building a model for a particular real-world phenomenon requires careful consideration of what independence properties (or conditional-probability properties) we should insist that our model have. We shall look at random walks, martingales, Poisson processes, the normal distribution and the Central Limit Theorem. Many of the random processes which we study are in 26 \u0000 1: Introduction fact Markov chains; but we do not study the general Markov chain because James Norris's recent book [176] covers that theory so well. It is tempting to argue that our axioms of Probability are so 'self-evident' that the real world must conform to them. However, at its most basic level, our Universe is a universe of elementary particles: electrons and so on. We know that the behaviour of electrons, etc, is governed entirely by probabilistic laws. Yet the Quantum—Probability laws of calculation are very different from those used in the everyday Probability which we study in this book. (To illustrate this, I shall later describe the mind-bending situation relating to Bell's Inequality and the experiments of Alain Aspect and others.) It is somewhat perplexing that the Quantum Probability calculus is profoundly different from the Classical Probability calculus, but there it is. We cannot dictate how the real world is to behave. Our version of Probability is as good as we need for the many important applications within Statistics, and for the large number of direct applications within Science and Engineering which do not require Statistics because we know the various parameters either from symmetry or from (say) Physics or from huge data sets. 1.5 First remarks on Statistics Sadly, it is the case that, as mentioned in the Preface, the world's probabilists and the world's statisticians form two separate communities with relatively little interaction between them. The two subjects do have different ethos; and probabilistsr natural inclination towards Pure Maths and/or Physics has tended to lead them further and further from Statistics. But this is crazy: Statistics is based on Probability; and much Probability can be applied to the real world only with the help of Statistics. The extremely important applications of Statistics jolly well ought to interest probabilists and students of Probability. I therefore include in this book some first steps in Statistics (to accompany the first steps the book takes in Probability). True story. I learnt the value of Statistics early. In a lecture to students, I had mentioned the result in Subsection 130 that if you have 23 people in a room then there is a probability of more than z that two of them have the same birthday. One student came to see me that evening to argue that 23 wasn't anywhere near enough. We went through the proof several times; but he kept insisting that there must be a flaw in the basic theory. After some considerable time, I said, \"Look, there must be 23 people still awake in College. Let's ask them their birthdays. To start with, when's yours?\" \"April 9th\", he said. I replied (truthfully!!) \"So's mine\". 1.5. First remarks on Statistics \u0000 27 ►►► A. Probability and Statistics. Here's an attempt to explain how Probability and Statistics differ and also how they are fundamentally connected. • In Probability, we do in effect consider an experiment before it is performed. Numbers to be observed or calculated from observations are at that stage Random Variables to be Observed or Calculated from Observations — what I shall call Pre-Statistics, rather nebulous mathematical things. We deduce the probability of various outcomes of the experiment in terms of certain basic parameters. • In Statistics, we have to infer things about the values of the parameters from the observed outcomes of an experiment already performed. The Pre- Statistics have now been 'crystallized' into actual statistics, the observed numbers or numbers calculated from them. The precise mathematical formulation of the 'crystallization' will be studied later. Pre-Statistics will be denoted by Capital (Upper-Case) Roman Letters, actual statistics by lower-case roman letters. • We can decide whether or not operations on actual statistics are sensible only by considering probabilities associated with the Pre-Statistics from which they crystallize. This is the fundamental connection between Probability and Statistics. B. General Remarks. The main reason for including the 'Introduction to Probability' in Sections 1.1 and 1.2 was to get far enough into that subject to appreciate that it is interesting before starting on its (initially less interesting) theory. There is less reason for a similar 'Introduction to Statistics' because, once we have the requisite Probability, Statistics is immediately interesting. It is also immediately controversial! There are no absolutely right answers; and one finds oneself saying; \"On the one hand, ...; but on the other, ...; but one can counter that with ...; etc, etc.\" We don't want too much of that sort of thing in an Introduction. The only statement about Statistics with which everyone would agree is that it is important. Still, some remarks are in order. For the simple situations which we consider, I shall not express conclusions about data in terms of Hypothesis Testing — and that for Frequentist reasons as much as for Bayesian ones. I believe that, whenever possible, one should express conclusions about data in terms of appropriate Confidence Intervals (CIs) and Confidence Regions (CRs). 28 \u0000 1: Introduction My criticisms of formal Hypothesis Testing in simple situations should not be taken to mean that all Hypothesis Testing is worthless. I believe that it retains value in certain situations: for example, in tests of independence. The idea has an important part to play in exploratory work, where it allows one to test quickly whether certain factors are likely to prove important or not before one proceeds (if there is now any point in so doing) to a more thorough analysis leading if possible to Confidence Intervals or whatever. I do not want to give the idea either that one should ignore books which follow very strictly the Hypothesis Testing route. Indeed, one can acquire a very great deal of statistical wisdom from (for example) Freedman, Pisani and Purves [83]. ►► C. Least squares, regression, ANOVA. I do want to say a word or two here about the wonderful ideas connected with the Analysis of Variance (ANOVA) method which form a recurring theme. Of course, we shall discuss normal distributions, variance, etc, in full later; but I'm sure that you will already know something about these things. Such details do not really matter in this subsection: it is enough to know that there are things called normal distributions, variances, t distributions. Least-squares ideas tie in perfectly with certain 'likelihood' geometry associated with the normal distribution. I try hard to make this geometry as simple as possible: it can be made quite complicated! The idea of the Classical Theory is to use a mathematical model Observation as Pre-Statistic = systematic part + True Error, True Errors for different Observations being independent normally distributed Random Variables (RVs) with zero mean (because the systematic part has taken up the mean) and unknown common variance a 2. The simplest case is when we are going to make n measurements Y1, Y2/ • • • , Yn (at the moment Pre-Statistics) of some fixed quantity: Yk = [I+ Ek, \u0000 (Cl) it being the true value of the quantity and the Ek Observational Errors. (Since the Ek can never be observed or calculated from observations, they are Random Variables, but not Pre-Statistics.) After the experiment is performed, we have n actual statistics (actual numbers) yl, Y21 • • • , yn, and we mirror (Cl) via Yk = m ek, \u0000 (C2) where m is a number, the 'estimated systematic part' and ek is an unexplained `residual'. The likelihood geometry tells us that the correct thing is to do a least- squares analysis, choosing m so as to minimize the residual sum of squares rss \u0000 e k2 = E(yk — m)2• 1.5. First remarks on Statistics \u0000 29 We find that m = y, the average of the yk, and use m as the best estimate of µ. We then use rss/ (n-1) to estimate a 2, and use this estimate to obtain a (t distribution) CI for pi centred on m. To justify this, we have to study the Pre-Statistics M and RSS which crystallize to the actual statistics m and rss. In Linear Regression, we measure (say) the electrical conductivity Yk of a certain material at temperature xk, where the `covariates' xi , x2, . , xi., are non- random numbers predetermined by the experimenter in some small temperature range which makes the following linearity assumption reasonable. Before the experiment, we postulate that Nature's model is Yk = it+ P(Xk - X) Ek. Hence,µ is the mean value of Y when x = Y, and /3, the slope of the regression line, is a first indication of how conductivity changes with temperature. As usual, we assume that the 'errors' ek, a combination of experimental errors and 'random effects' ( a term which will later have a different meaning) are independent Random Variables each with the normal distribution of mean 0 and unknown variance a 2. (There are very good reasons for 'shifting the origin of the x-values to Y' in the way we have done. All computer programs should follow suit! But not all do!) After the experiment, we mirror Nature's model with yk = m + b(xk — + ek. We choose m and b so as to minimize rss= Eqc, and then use in and b as best estimates forµ and Q. By considering the Pre-Statistics M, B, RSS corresponding to m, b, rss, we obtain CIs forµ and 0 centred on m and b by using rss/(n — 2) to estimate o-2 and then using a t-distribution. If we have 97.5% CIs , it+) for it and (i3_, /3+) for 0, then we can be at least 95% confident that both pt E (p_, p+) and 0 E \u0000 0±) are true. We now consider two-factor ANOVA without interaction. Though agricultural field trials are done in a more sophisticated way than I now describe, the following remarks can still give the idea of ANOVA. We might (for example) apply J fertilizers 1, 2, . , J in a sensible way to equal areas in each of K fields 1, 2, . , K. We assume a model for yields: Yjk = it+ aj Ok+ Ejk) whereµ is an overall mean, ai is the amount by which fertilizer j increases the yield on average, f3k is associated with the natural fertility of the field k. Because is an overall mean, the ai and Ok will have mean 0. After the experiment, ... (Guess what!) You will know by now that I have no interest in the test of the Null 30 \u0000 1: Introduction Hypothesis that all fertilizers are equally good, except as a first exploratory step. Rather, I shall want to quantify differences between fertilizers via Confidence Intervals. Let us focus on one defect in the experiment just described. (You will be able to think of other defects.) One fertilizer may be better than another on one type of soil, worse on another. So, we upgrade to a model (Two-factor ANOVA with interaction and replication) Yjke = \u0000 aj + Ok \"-Yjk E Pet) where now j refers to fertilizer type, k to soil type, jke to the f-th region of soil type k treated with fertilizer j, and -yak is an interaction term concerning how well fertilizer j works on soil type k. We need more than one value of •e in order to estimate the natural underlying variation described by o-2. Of course, we arrange the treated plots sensibly. And, certainly, we need to give careful consideration to whether the assumptions of the model tally with common sense and with the data. And so on; and so on. There is essentially no limit: we can have many factors, all kinds of interactions. I never did like the standard 'Y = X,0 e' method of doing ANOVA. I hope that you will find the way presented here easier because it better respects the structure and allows one to see the geometry in the basic cases which we consider. Our ANOVA cases possess the special property of having 'orthogonal factors'. They include the examples given above, and are the essential first step to understanding ANOVA, the only cases usually considered in detail in books at this level. But they are special, and many cases of experimental design do not fit the 'orthogonal factor' pattern which we study. Our development of the theory does allow non-orthogonal factors, and we look rather carefully at non-orthogonality in connection with Regression. Moreover, we do study the 'Y = X/3 + E' model, partly to show that we can cope with it and understand its geometry. Watch how considerations of 'sums of squares' develop throughout the book, beginning with the very definition of variance, and guiding the Probability at Lemma 69Ja and elsewhere. The likelihood geometry of the normal distribution, the most elegant part of Statistics, has been used to great effect, and remains an essential part of Statistics culture. What was a good guide in the past does not lose that property because of advances in computing. But, as we shall see, modern computational techniques mean that we need no longer make the assumption of 'normally distributed errors' to get answers. The new flexibility is exhilarating, but needs to be used wisely. 1.6. Use of Computers \u0000 31 D. A final remark for the moment. Statistics is, fundamentally, a practical subject justified by its applications to the real world. The subject proper starts after the mathematical theory (which itself is only begun in this book) is complete. A statistician has to be steeped in the field of application, ideally being involved in deciding how data are to be collected. Possible mathematical models should then be discussed with workers in the field of application. When an appropriate model (or family of models) is agreed, and when the extent of prior knowledge about the values of various parameters is ascertained, analysis of data can begin. Any outliers in the data (results which seem 'out of keeping' with the others) which are shown up by this analysis will need close investigation to see whether they are genuine or the result of experimental error or recording error. We spend much time examining the analysis of data in simple cases, assuming agreed models and no outliers. We shall have some discussion on identifying outliers, and shall discuss (for example) testing of stability of methods via simulation. More importantly, we shall look at how modern computational methods do allow us to utilize more realistic models. One has to begin somewhere; and if one doesn't know what is in this book and more, then one can do nothing. 1.6 Use of Computers A. Statistical Packages. It is self-evident that all numerical computations in Statistics should now be done with suitable computer packages. The Minitab package is widely used for teaching because it is very easy to use and because it is affordable. I shall include some Minitab examples and some examples from the remarkable WinBUGS package. Switching packages is not difficult. Comments on the very important S-PLUS and R packages may be found in the Preface. B. Simulation. The ability of computers to generate pseudo-random numbers which are for many purposes reasonable simulations of truly random sequences, has many uses. In Probability, we approximate the probability of an event by the proportion of times that that event occurs in a long series of simulations of the experiment. Here's an example which you are certainly bright enough to understand in principle (using the comments within /* . . . */) even if you have never seen `C' before. 32 \u0000 1: Introduction ►► Ba. Simulation example: A simulation of the average waiting time for HH. `C' is not my favourite language, but it is so widely used that it seems sensible to use it here. This simulation uses a Random Number Generator RNG . o which we shall study later. Only the procedure Next Tos s () and the 'function' main( ) matter. The 'C' compiler always begins at the function main 0 which here returns the value 0 after the program is completed. The instruction \\n within a printf command tells the printer to go to a new line. The scarf is C's highly bizarre way of reading in data. The the first scarf tells the machine to input the value of the long integer Nexpts to be stored in address &Nexpts. /*HH.c : Average Wait for HH for fair coin \u0000 DW *To compile: \u0000 cc HH.c RNG.o -o HH -lm *(RNG.o compiled Random-Number Generator) *(-lm to call up mathematics library) *To run: \u0000 ./HH */ #include \"RNG.h\" /* long int Ntosses; /* long int Nexpts; /* int next; \u0000 /* int showall; \u0000 /* double average; Header file -- explained later */ Total number of tosses */ Total number of experiments */ 1 if next toss gives 'H', 0 if 'T' */ 1 to show all results, 0 just for average */ GetParameters(){ printf(\"\\nlnput total number of experiments: \u0000 \"); scanf(\"%ld\", &Nexpts); /* Read in Nexpts */ printf(\"Input 1 to show all results, 0 just for average: scanf(\"%d\",&showall); \u0000 /* Read in showall */ setseeds(); \u0000 /* from RNG */ } void NextToss(){ Ntosses++; \u0000 /* C parlance for 'Increase Ntosses by 1' */ next = WithProb(0.5); /* (from RNG) next = 1 with probability 0.5, 0 otherwise */ if(showall==1)-(if (next=1) printf(\"H\");else printf(\"T\");} } int main(){ int last; long int k; 1.6. Use of Computers \u0000 33 GetParameters(); Ntosses = 0; printf(\"\\n\"); for(k = 0; k < Nexpts; k++){ /* do Nexpts times */ NextToss(); do{last = next; NextToss();}while (last + next < 2); if (showall == 1) printf(\"\\n\"); } /* end for loop */ average = Ntosses/(double)Nexpts; printf(\"\\nAverage wait for HH was 7,8.5f\\n\\n\", average); return 0; } with results (lines 3-8 relating to setseeds 0 in a way explained later): Input total number of experiments: \u0000 8 Input 1 to show all results, 0 just for average: \u0000 1 Which generator? Input 1 for Wichmann-Hill, 2 for Langlands 1 Input 3 positive-integer seeds a<30269, b<30307, c<30323 6829 3716 2781 HTTTHH HH TTHH THTTTHTHTHTTHTTHTTHTHH THH TTTTHTTHTTHH HH TTHH Average wait for HH was 6.87500 Input total number of experiments: \u0000 1000000 Input 1 to show all results, 0 just for average: \u0000 0 Which generator? Input 1 for Wichmann-Hill, 2 for Langlands 1 Input 3 positive-integer seeds a<30269, b<30307, c<30323 2791 7847 5395 Average wait for HH was 5.99246 Input total number of experiments: \u0000 1000000 Input 1 to show all results, 0 just for average: \u0000 0 Which generator? 34 \u0000 1: Introduction Input 1 for Wichmann-Hill, 2 for Langlands 2 Input 3 positive-integer seeds a,b,c < 65536 3471 7745 58861 Average wait for HH was 5.99899 There are many complicated situations where analytic solutions seem impossible and where simulation seems to be the only available method for studying qualitative behaviour. There are other situations, much more complicated still, where one cannot envisage simulation as ever being of any use, and where analysis does work. C. Simulation in Statistics. The use of Gibbs sampling and other `MCMC' techniques completely transforms Statistics, as we shall see. There are many other uses of simulation in Statistics: for the bootstrap of Simon and Efron, see, for example, Efron and Tibshirani [71], Davison and Hinkley [55]. 2 EVENTS AND PROBABILITIES We start again 'from scratch', developing Probability in the only order that logic allows. This chapter is important, but not too exciting (except in discussion of what is perhaps the most brilliant piece of magical trickery in Maths): important in that it covers the Addition Rule; not too exciting in the main because it does not go beyond the Addition Rule to the Multiplication Rules involving conditioning and independence. Without those Multiplication Rules, we can assign actual probabilities to events only in an extremely limited set of cases. As mentioned at Discussion 25B, we are concerned with constructing a Mathematical Model for a Real-World Experiment: we make no attempt to define probability in the real world because such an attempt is doomed. In the mathematical model, probability P is just a function which assigns numbers to events in a manner consistent with the Addition Rule (and with conditional- probability considerations) — and nothing more. And events are just (technically, `measurable') subsets of a certain set, always denoted by a This set is called the sample space and it represents the set of all possible outcomes w of our experiment. In Statistics, the experiment has been performed, and we have some information — in many cases, all information — about the actual outcome Wact , the particular point of St actually 'realized'. The relation between 'sample' in Statistics and our 'sample point' and 'sample space' will become clear as we proceed. 36 \u0000 2: Events and Probabilities 2.1 Possible outcome w, actual outcome wad; and Events ►► A. Sample space \u0000 of of possible outcomes w. As was stated earlier, Probability considers an experiment before it is performed, and Statistics considers an experiment after it has been performed. Probability considers an abstract set 12, the sample space, which represents the set of all possible outcomes of our experiment. A possible outcome, or sample point, w is mathematically a point of the 'abstract' set I. For example, for the experiment of tossing a coin three times, 12 = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}, \u0000 (Al) a finite set. For the experiment of choosing a point at random between 0 and 1, we naturally take S2 = [0,1], an uncountable set. See Appendix A4, p496, for a discussion of countable and uncountable sets. For the experiment, discussed at Subsection 19B, of tossing a coin until the first time we obtain MI, we can justify taking It -= {HH, THH, HTHH, TTHH, ...}, a countable set. However, there is the difficulty, which must be addressed, that there are uncountably many 'crazy' outcomes in which the pattern HH never occurs. The set of all such crazy outcomes must be proved to have probability 0 before we can reduce to the desired a ►► B. Crystallization of actual outcome; realization. Our picture is that Tyche, Goddess of Chance, chooses a point watt of 12 'at random and in accordance with the probability law P' in a sense explained below. This is Tyche's 'experiment', and it determines the actual outcome act in the real world. The whole sample space S2 is a nebulous, abstract thing. Tyche's choice, as it were, 'crystallizes into existence' one sample point CA,' which becomes real. It is important that we regard Tyche's single choice of coact as determining the entire outcome, the entire realization, of the real-world experiment. If the real-world experiment consists of many stages (for example, many coin tosses), then Tyche is in a sense revealing her choice to us in instalments. But she made just the one choice wad . ►► C. Event as (measurable) subset of C2. For the 'Toss coin three times' experiment, the real-world event '2 Heads in all' occurs if and only if act belongs to the subset {HHT, HTH, THH} of 12 at (Al) consisting of those possible outcomes 2.1. Possible outcome w, actual outcome wact; and Events \u0000 37 (.4) which would produce 2 Heads in all. In the mathematical theory, the event '2 Heads in all' is regarded as the subset {HHT, HTH, THH} of the sample space Q. And so for any event for any experiment. (Technical note. Except in simple cases, essentially those in which Q is finite or countable, we have to restrict the type of subset of Q which constitutes an 'event': an event is a `measurable' subset of Q.) We therefore have the following table in mind: Probability Model \u0000 Real-world interpretation Sample space 1-2 \u0000 Set of all outcomes Point w of SZ \u0000 Possible outcome of experiment (No counterpart) \u0000 Actual outcome wact Event F, 'measurable' The real-world event corresponding to F subset of fl \u0000 occurs if and only wact c F IP(F), a number \u0000 Probability that F will occur for an experiment yet to be performed. We see that Tyche must perform her experiment in such a way that she will choose watt to be in the set F 'with probability P(F)'. This is what is `meant'(!) by `Tyche chooses in accordance with the law P'. Because we are now regarding events F, G, etc, as subsets, we can extend this table: Event in Maths \u0000 Real-world interpretation SI, the entire sample space The certain event 'something happens' The empty subset 0 of C2 \u0000 The impossible event 'nothing happens' The intersection F 11 G \u0000 `Both F and G occur' Fi n F2 n ... n Fn \u0000 `All of the events F1, F2, • • • , Fn occur simultaneously' The union F U G \u0000 `At least one of F and G occurs' FiU F2 U ... U Fn \u0000 `At least one of F1, F2, . .. , Fn occurs' Complement Fc of F \u0000 `1' does not occur' F \\ G \u0000 `I' occurs, but G does not occur' F C G \u0000 If F occurs, then G must occur ►► D. Set theory and Probability. Combining events is exactly the same as combining sets in elementary set theory. I am sure that you know about Venn diagrams. Figure D(i) illustrates parts of the table just considered. The amazing fact (explained 38 \u0000 2: Events and Probabilities F \\G F G F U G FC Figure D(i): A simple Venn diagram later) that the Fundamental Experiment Choose a number between 0 and 1 uniformly at random is Universal in that every other experiment is contained within it makes the 1-dimensional pictures particularly appropriate, especially for constructing counterexamples. You should practise giving probabilistic interpretations of set-theoretic results. For example, one of de Morgan's rules states that (Fi n F2 n n F„)c = Fl uF2 U U \u0000 (Dl) In real-world terms, with L for the left-hand side and R for the right-hand side of (Dl), L: it is not true that all of the events F1, F2 . . . Fn do occur; R: it is true that at least one of the events F1, F2, . , I', does not occur. Thus (D1) is obvious. Or again, consider the distributive law: A n (B U C) = (A n B) U (A n C). Exercise. Use 'event' language (so that the left-hand side signifies ' A definitely occurs and at least one of B and C occurs') to make the distributive law 'obvious'. [Note. Whether an electron regards the distributive law as even being true (even when C = BC) is a moot point. Some people believe that the appropriate logic for quantum theory is 'non-distributive'. Once again, I emphasize that we cannot force Nature to obey our ideas of 'obvious'.] 2.2. Probabilities \u0000 39 2.2 Probabilities Mathematically, probability is just a function P which assigns to each event F a number IP(F) in [0, 1] such that P(1) = 1 and the so-called Addition Rule holds. One important part of the Addition Rule states the following. ∎ A. Addition Rule, Part 1 — an Axiom. For events F and G, F n G = 0 implies P(F U G) = P(F) +P(G). If F n G = 0, then F and G are called disjoint or exclusive. I emphasize: in the mathematical theory, Rule (A) is an axiom that the probability laws P which we use must satisfy: and we do not prove this rule. We have seen its motivation at Subsection 6F. ► B. Lemma. (a) For any event F, P (Fc) = 1 — IP(F). (b) For any two events F and G, we have the Inclusion-Exclusion Principle IP(F U G) \u0000 P(F) +P(G) IP(F n G). Proof Part (a) appeared in Subsection 6F. Part (b) may be seen as follows. Figure 38D(i) may help. The event F may be written the disjoint union of F \\G and F n G. So, IP(F) = IP(F \\ G) +P(F n G); and, since F U G is the disjoint union of F \\G and G, P(F u G) = IP(F \\G) +P(G) = P(F) — P(F n G) + P(G), the desired result. \u0000 ❑ C. Lemma. We have the extended Addition Rule: if the events F1, F2, . , Fn are disjoint (that is, F, n \u0000 = 0 whenever i j), P (Fi F2 U \u0000 U Fn) = TED(F1) P(F2) ± • • • ± P(Fn). In shorthand: rt \u0000 n P \u0000 Fk) = >_d iED(Fk) • k--1 \u0000 k=1 40 \u0000 2: Events and Probabilities Proof If A, B and C are disjoint events, the A U B is disjoint from C, so that two applications of the Addition Rule for disjoint sets give P(A U B u C) = P((AU B) U C) = P(A U B) + P(C) = P(A) +P(B) + P(C). It is not worth dignifying the 'n-event' case with a proof by induction. \u0000 ❑ D. General Inclusion-Exclusion Principle. What about the Inclusion- Exclusion Principle for n events? (Let me be honest and say that one reason for discussing this is so that we can at least do one problem in this chapter!) Well, for 3 events A, B, C, it would say P(A U B U C) = — E2 E3) where E i \u0000 P(A) P(B) P(C), E2 := P(A n B) +P(A n C) + P(B n C), E3 := P(A n B n C). You can see how this extends Lemma 39B, and that the general case will read as follows. ► Da. Lemma: General Inclusion-Exclusion Principle. Let F1, F2, . , Fn be any events. Then P(FiU F2 U ...0 Fn) = F1 — E2+E3 —... +(- 1)n+l En, where Er is the sum of terms P(Fii n Fi2 n n Fir ) over all subsets fil, i2, . . , \u0000 of size r from {1, 2, ... , n}. We shall prove this in the next chapter. ► E. Example: The Hat-Matching Problem. In this well-known problem, n absent-minded hat-wearing professors attend a meeting. At the end of the meeting, each picks a hat at random. We want to show that the probability at least one of them gets the right hat is 1 \u0000 1 \u0000 1 1 — — 2! + — 3! — \u0000 (-1)n+1 -171! Thus the probability that none of them gets the right hat, namely 1 minus the above expression, is exactly the sum of the first n 1 terms in the expansion of e-1, and so is extremely close to 1/e even for moderate IL Solution. Let Fk be the probability that the kth professor to leave (Prof k) gets the right hat. Now, it is all just as if someone shuffled the whole 'pack' of hats and gave them out randomly to the various professors. So, for every k, the chance that the k-th professor to leave gets the right hat is 1/n. For i j, the chance that Prof i and Prof j both get the right hat is (1/n) x (1/(n — 1)). One way to see this is by using the idea from the (El) \u0000 2.2. Probabilities \u0000 41 National-Lottery' proof 10L of Lemma 9Ka. But since we have not done conditional probabilities, we can argue (more rigorously?!) as follows. Since there are n! permutations of the n numbers {1, 2, ... , n}, there are n! ways of `giving out the hats'. We need to know how many ways there are of giving out the n hats such that Prof i and Prof j both get the right hats (others being allowed to get the right hats too). In other words, we need to know how many permutations of {1, 2, ... , n} keep i and j fixed. But this is just the total number of permutations of the remaining n — 2 numbers. Hence for i j, (n — 2)!1 P(Fi n F;) = \u0000 = n! \u0000 n(n — 1) and, more generally, for any subset VI, i2, \u0000 , i,} of {1, 2, ... , n}, (n — r)! \u0000 n Fiz n n Fi,) = \u0000 n! Hence, since by Lemma 9Ka there are (7) subsets of size r from {1, 2, ... , n}, n! \u0000 (n — r)! \u0000 1 Er = \u0000 X \u0000 = r!(n — r)! \u0000 n! \u0000 ! and the result follows. \u0000 ❑ F. True story — Marx brothers, please note! I was once the last to leave a restaurant in Swansea, one Friday lunchtime. Just one anorak remained on the hooks outside. It looked exactly like mine, and I took it without thinking. When I put the anorak on that evening, I found it contained house and car keys which were not mine. A lot of detective work identified the true owner (A, say) of that anorak and keys. He lived far away, but his son who lived at Swansea had a spare set of keys to his car. I phoned A, and we agreed to meet at the restaurant on Monday morning to swap coats. I arrived before him, and since I had to go off to give a lecture, left his anorak with the restaurant manager who put it in the restaurant safe. I returned after the lecture. The restaurant manager said that A had been and gone, and that he had left my anorak in the safe. The manager gave me my anorak (it was mine!), and I thought that that was the end of the story. However, I later had a furious phone call from A from his home, asking what I was playing at, since the anorak he had collected from the safe (without bothering to check it — for how could it be wrong?) was not his! I was puzzled. But it turned out that the submanager of the restaurant was the only other person with a key to the safe — and guess what kind of anorak he had, and where A's anorak was when A collected for the second time the wrong one! G. Exercise. (a) What is the probability that exactly one professor gets the right hat? Express your answer in terms of a suitable PO,m, where this latter probability, which we 42 \u0000 2: Events and Probabilities know, is the probability that if there are m professors in all, then none of them gets the right hat. (b) What is the probability that exactly r professors get the right hat? Show that, as n —> oo, this probability converges to e-1/r!. 2.3 Probability and Measure I explained my philosophy about Measure Theory in Section 1.3. I think that you should • have some idea what the Strong Law of Large Numbers, which underpins the whole subject, states; • understand how one gets around the difficulty raised in Discussion 25B; • know what pieces of logic justify things that we must do. An example of the last point is that we need to be sure that the probability that a population will become extinct at some finite (random) time is the limit as n —> oo of the probability that the population will be extinct by time n. In any case, I now get a chance to explain to you one part of the utterly amazing Banach—Tarski Paradox! Towards the end of the section I discuss the (most) Fundamental Model, that of choosing a number at random between 0 and 1. Amazingly, as stated earlier, every other model, no matter how complicated the process we are modelling, is contained within this one (as we shall become convinced later). We can use the Fundamental Model to understand clearly statements of 'Strong Law' type. Please note that in one brief (and even interesting) page (with surprises!) at Appendix A4, p496, I include everything you need in this book about countable and uncountable sets. ► A. ® The Full Addition-Rule Axiom. If F1, F2 , . . . is an infinite sequence of disjoint events, then Fk) = EP(F k ). k=1 \u0000 k=1 It is impossible to prove this by 'letting n —> oo in Lemma 39C': that is the whole point. With property A one can build a marvellous theory. But there is a price to pay. Except in very simple cases, we cannot arrange that Property A will hold if we insist that all subsets of S2 are events. 2.3. Probability and Measure \u0000 43 IN. B. Banach— Tarski Paradox (!!!) These Polish magician-mathematicians brought off one of the most brilliant tricks in Mathematics. They showed — assuming the Axiom of Choice (see Appendix A5, p498) — that one can find a subset A of the unit sphere S = S2 (the surface of the familiar unit ball in R3) with the most remarkable property. First of all, we can fit together three sets which are rotations of A so that they exactly cover S without overlap. Nothing surprising so far. It seems clear that the area of A is a/3, where a is the area of the sphere. However, for any n = 3, 4, 5, ... , co, we can fit together n sets which are rotations of A so that they exactly cover S without overlap. Hence the area of A must be a/3 and a/4 and ... and 0. Conclusion: it is impossible to assign an area to A: A is 'non-measurable'. The 'event' that a point chosen at random on S belongs to A cannot be a true event, for its probability would have to be simultaneously 1/3, 1/4, , 0. See Wagon [233]. ► C. ® Full Axiomatization. In the general theory then, not all subsets of Q need be events. We axiomatize things again. The class of all subsets of Q which are events must be what is called a o--algebra: this means that • 1 E • Fel' implies that Fc E • F1, F2, ... E .F implies that U Fn E Then P is a map P : \u0000 -+ [0, 1] such that P(Q) = 1 and Property 42A holds whenever the Fk are disjoint elements of T. We now have the full axiomatization of the Addition Rule. We say that P is a probability measure on (Q, and that (Q, , IP) is a probability triple. What makes things work is that we can always set up an (12, P) triple for the experiment which we wish to model in which is large enough to contain every event of which we could ever wish to find the probability. In the remainder of this section, (C2, ,E1 is a probability triple used to model some experiment. ► D. ® Fact: Monotone-Convergence Properties. (a) Suppose that we have events F1 C F2 C F3 C . and that F = U Fn. Then P(Fn ) fi P(F): the sequence {P(Fn )} is non-decreasing with limit IP(F). (b) Suppose that we have events G1 D G2 D G3 D ... and that G = n Gn. Then P(Gn ) 4 P(G): the sequence {P(G,i )} is non-increasing with limit P(G). (This uses the fact that IP(Q) is finite.) 44 \u0000 2: Events and Probabilities This is important in that, for example, Fri might be 'population is extinct at time n' and then F is 'population is eventually extinct'. ► E. C) Almost surely. A statement S about outcomes is said to be almost surely true or to be true with probability 1 if the truth set T of S, the set of outcomes w for which S is true, is an element of \u0000 and P(T) = 1. If a statement is certain, true for every w, then, of course, its truth set is Q and, since P(Q) = 1, it is almost surely true. (It had better be!) The important point is that many of the things in which we are most interested are almost surely true without being 'absolutely certain'. Do note that the probability of an almost sure event is exactly 1, not 99.9% or anything similar. ► F. ® Null set, null event. An event N is called a null event or null set if N E .F and P(N) = 0. We see that a statement about outcomes is almost surely true if the outcomes for which it is false form a null event. G. C) Fact. If N1, N2, . . is a sequence of null sets, then U Nk is a null set. If H1, H2, . . . is a sequence of events each of probability 1, then n H, has probability 1. ► H. ® The Fundamental Model. Consider the experiment 'Choose a (real) number uniformly between 0 and 1. The outcome must be a real number between 0 and 1, so we take Q to be [0, 1]. For each x in [0, 1], we wish the statement 'chosen number is less than or equal to x' to have a probability and for that probability to be x. It is a theorem of Borel and Lebesgue that if is the smallest a-algebra of subsets of Q = [0, 1] containing every interval [0, x] where 0 < x < 1, then there is a unique probability measure F on (Q, .F-) such that P([0, x]) = x whenever 0 < x < 1. We therefore have our complete model for choosing a point at random between 0 and 1. Let's call the resulting triple (fl, \u0000 F) the Fundamental Triple. ► I. 0 Null sets for the Fundamental Model. As already mentioned, this most fundamental model contains all other models in a sense to be explained later. It is therefore good that we can understand easily what 'almost sure' or 'with probability 1' means for this model. As explained at F, we need only understand what is a null set for the Fundamental Triple. (Your fears that 'for every e > 0' might feature are, I'm afraid, justified!) A set N in 1. is a null set (it has measure 0) if and only if, for every e > 0 we can find a sequence of disjoint open subintervals In = (an, bn) of [0, 1] (such an open interval is also allowed to have the form [0, b) or (a, 1] to deal with the endpoints) such that N C U in and \u0000 4/7,) < E, where t(In,) is the length of Imo. In short, for any E > 0, you can find an open subset Ge of [0, 1] containing N and of length at most E. J. @ Remark. Step 1 towards resolving the difficulty at Discussion 25B. Work with the Fundamental Triple. For X E Q = [0, 1], let N N \u0000 {x}. Then each Ns is a null event. (For > 0, G E = (x — 3e, x+ \u0000 n [0, 1] is an open subset of [0, 1] of length less than e and containing Ni .) However, Q = MXEil NZ, \u0000 Q and is certainly ■-, not a null event. The point here is that (as you probably know from Cantor's Diagonal Principle — see Appendix A4, p496) the set Q = [0, 1] is not countable: we cannot write 2.3. Probability and Measure \u0000 45 SZ -= {xi , x2, x3, . .} for some sequence (xk) of points of Q; S2 is too large a set to allow us to do this. Otherwise, we would have a contradiction with Lemma G. The set of all subsequences of the sequence of positive integers is also uncountable; and the way out of the difficulty at Discussion 25B begins to emerge. ► K. ® Fact: First Borel—Cantelli Lemma. Let J1, J2, . . . be a sequence of events. If E Tv, < cc, then it is almost surely true that only finitely many of the events Jk occur (Assumed fact) This First Borel—Cantelli Lemma is a very useful result. In the next chapter, we shall see very clearly why it is true. ► L. ® Borel sets and functions. The Borel a-algebra B(S) on a topological space S is the smallest a-algebra of subsets of S which contains all open subsets of S (equivalently, the smallest a-algebra of subsets of S which contains all closed subsets of S). A Borel subset of S is an element of this Borel a-algebra. If S -= IR, then the Borel a-algebra is the smallest a-algebra containing every set of the form (—co, x], where x E R. This is just what is needed to study distribution functions in Probability and Statistics. If S = , then the Borel a-algebra is the smallest a-algebra containing every subset of the form {(xi, x2, . , xn,) : xk < a} where a E R and k E {1, 2, .. . , n}. For the Fundamental Model, .2 = B( [0, 1]). Recall that a continuous function f : S \u0000 R is one such that the inverse image f'(B) := {s E S : f (s) E B} of any open subset B of R is open in S. Analogously, we define a function f : S R to be a Borel function if the inverse image 1-1(B) of any Borel subset B of R is Borel in S: equivalently, if, for every x E IR, S : f (s) < xl is Borel in S. Every subset of Rn and every function on ir you are ever likely to meet will be Borel. Continuous functions are Borel. Limits, limsups, what-have-you of sequences of Borel functions are Borel. You need to be clever to construct explicitly a function which is not Borel; but it can be done. Technical Remark. After I wrote 'but it can be done' about this last point on a previous occasion, I received many emails demanding 'How?' . So, to avoid a repeat, see Appendix A6, p499. ► M. ® The 7r-system Lemma. The last thing I would do is to add unnecessary bits to this discussion of Measure Theory. I must give the idea of the 7r-system Lemma because it is crucial for proving that certain key things are uniquely defined, for establishing essential properties of independent Random Variables, etc. We shall not actually use the 7r-system Lemma to do these things, but I shall tell you where it would clinch an argument. In Subsection 44H, I stated that there exists a unique measure P on what we now know to be B[0, 1] such that P([0, x]) = x for every x in [0, 1]. The hard thing is to 46 \u0000 2: Events and Probabilities prove the existence of P. But it matters greatly that IP is unique; and this is where the 7r-system Lemma comes in. The subsets of [0, 1] of the form [0, x] form a 7r-system I, which simply means that the intersection of any two elements of I is again in I. The 7r- system Lemma states that two probability measures which agree on a 7-system must also agree on the smallest a-algebra which contains that ir-system, which guarantees the desired uniqueness for the Fundamental Triple. 3 RANDOM VARIABLES, MEANS AND VARIANCES In this chapter, as in the last, we do some fundamental things, but without the benefit of conditioning and independence (the subjects of the next chapter) to enliven things with really good examples and exercises. I do not give you (more than about two very easy) exercises in calculating means and variances by summing series or working out integrals, and this for two reasons: firstly, it teaches you nothing about Probability and Statistics; and secondly, we shall calculate lots of means and variances later by efficient indirect methods. I do on rare occasions cheat by giving you exercises which use independence properties familiar from Chapter 1, just to keep some interest in the proceedings. That the next chapter is MUCH more interesting is a promise. 3.1 Random Variables Intuitively, a Random Variable (RV) is 'a number determined by Chance'; but this is hardly adequate for a mathematical theory. The formal definition is as follows. ►► A. Mathematical formulation of Random Variable (RV). A Random Variable is defined to be a function (@ strictly, an F- measurable function) from 1/ to R. Aa. Question: Why is it appropriate to axiomatize the notion of Random Variable as being a function on our sample space ft? Answer. Suppose that our RV Y is 'total number of Heads' if I toss my coin 3 times. We can make the picture w : HHH HHT HTH HTT THH THT TTH TTT Y(w) : \u0000 3 \u0000 2 \u0000 2 \u0000 1 \u0000 2 \u0000 1 \u0000 1 \u0000 0 48 \u0000 3: Random Variables and this already displays Y as a function on S-2. Any 'intuitive RV X' will assign a value X (w) to every possible outcome w. Here's another example. Suppose that our experiment consists of throwing a die twice, that X is the score on the first throw, Y that on the second, and Z is the sum of the scores. A typical possible outcome w has the form (i, j), where i is the first score and y the second. Then X (w) = i, Y (w) = j, and Z(w) = i j, and we have the picture: (1,1) (1,2) • • \u0000 • (1,6) 1 1 • • • 1 S-2 \u0000 : (2,1) (2, 2) (2, 6) X : 2 2 2 (6, 1) (6, 2) • • \u0000 • (6, 6) 6 6 • • • 6 1 2 • • \u0000 - 6 2 3 - - \u0000 - 7 Y: 1 2 6 Z: 3 4 8 1 2 - \u0000 - \u0000 • 6 7 8 • • • 12 For an RV X, we wish to be able to talk about its Distribution Function (DF) Fx : R —> [0, 1] (of which more, of course, later) defined by Fx (x) := IED(X < x) \u0000 (x E R). We therefore require that, for every x E R, the subset Lx := {w : X (w) < x} of Q (the mathematical formulation of the 'event that X < x') be a true mathematical event, that is, an element of the class Y of events; and then, of course, we interpret P(X < x) as P(L x ). Ab. Question: Why the restriction to 'measurable' functions in the proper theory? Saying that for every x in R, Lx E .F is exactly saying in Measure Theory that X is an F-measurable map from C2 to R. Compare subsection 45L. Fact: Except for that crazy Banach–Tarski context, every function on St we meet in this book is F-measurable. The point, already made at 45L, is that sums, products, pointwise limits (if they exist), etc, of measurable functions are measurable. You cannot break out of the world of measurable things without being rather clever. So, we shall ignore measurability questions, except for intuitive discussions on how more subtle types of measurability which are in the background allow us to do better things. ►► B. Crystallization, Pre-Statistics and actual statistics. Again let Y be the number of Heads in 3 tosses of my coin. In Probability, we consider the experiment before it is performed, and Y is a function on the nebulous, abstract set Q. As discussed at 36B, Tyche's choice 'crystallizes into existence' the actual outcome watt. Suppose that act = {HET}. Then the observed, or realized, value yobs of Y is yobs \u0000 y( .4jact ) = 2. We call (the terminology being my own) 3.1. Random Variables \u0000 49 • Y, the RV, a Pre-Statistic, • yobs, the observed value of Y, an actual statistic, and regard the crystallization of coact as changing Y to yobs = y pact ). A Pre-Statistic is a special kind of Random Variable: a Random Variable Y is a Pre-Statistic if the value Y(coact ) will be known to the observer (it may have to be calculated from things directly observed) after the experiment is performed; and then Y pact \\ ) becomes the observed value VI's of Y. Thus, A Pre-Statistic is an Observable Random Variable. The model may involve RVs the actual values of which cannot be determined after the experiment: these RVs are not Pre-Statistics. We refer to coact rather than C4J°1' s precisely because full information about coact may never be known. ►► C. Indicator function IF of an event F. For an event F, we define IF(W) if co E F, 0 if co F. Intuitively, IF = 1 if F occurs, 0 if it doesn't. (Technical Note. For any x, {w : IF (W) < x} can only be one of three things: 0, Fc, Q, depending on where x lies in relation to 0 and 1. Hence IF is certainly measurable.) We use indicator functions to do our counting. The number Y of Heads I get in n tosses of a coin is Y = X1+ X2 \u0000 Xn, \u0000 (C1) where Xk is the indicator function of the event 'Heads on kth toss': for the sum counts 1 for every Head, 0 for every Tail. Ca. Exercise. Let F and G be events. Prove that = 1 — IF) \u0000 -1-FnG = 'FIG) as function identities (`true for every w'). Deduce from de Morgan's rule 38(D1) that IFuG = IF + IG — IFIG, and explain why this is otherwise obvious. 50 \u0000 3: Random Variables 3.2 DFs, pmfs and pdfs As a student, I found the study of DFs: \u0000 Distribution Functions (Idea: Fx(x) = P(X < x)) pmfs: probability mass functions (Idea: px (x) = lP(X = x), X 'discrete') pdfs: \u0000 probability density functions (Idea: f x (x)dx = P(X E dx), X 'continuous') a real `turn-off' — all horrible integrals and that (and I was someone who liked integrals!) Take heart: crafty methods will allow us almost completely to avoid summations and integrations, nice or nasty. ►► A. The Distribution Function (DF) Fx of X. Let X be an RV. We define the Distribution Function (DF) Fx of X to be the function Fx : —> [0, 1] with Fx (x) :=4 P(X < x) \u0000 (x E R). As already explained, this is the probability of the event {X < x} interpreted as : X(w) < xl. Note that for a < b, P(a < X 5 b) \u0000 Fx (b) Fx (a). Reason: For a < b, events {X < al and {a < X < b} are disjoint with union {X < b} so that P(X < b) = IP(X < a) + lP(a < X < b). \u0000 ❑ One advantage of the DF is that it is defined for any RV X . Most of the RVs one meets are either 'discrete' RVs or 'continuous' RVs (these are described below) or mixtures of those types. However, not all RVs are so simple: for example, RVs with values in fractal sets, studied more and more these days, are much more complicated; but DFs and Measure Theory can deal with them. Important note. Statisticians usually refer to the Distribution Function (DF) as the Cumulative Distribution Function (CDF). In Minitab, for example, one calculates the value of a DF via a cdf command. Several examples are given later. ► B. The `F-inverse' principle for theory and simulation. If F is the distribution function of an RV X, then F is certainly non-decreasing, and it follows from the monotonicity property 43D of measures that F(y) = 0, \u0000 F(y) = 1, F is right-continuous: for x e \u0000 F(y) = F (x). Suppose now that F : 1W \u0000 [0, 1] has properties (B1). Both for the theory and for simulation, it is very useful to be able to construct from an RV U with the U[0, 1] distribution an RV X with distribution function F. (B 1) 3.2. DFs, pmfs and pdfs \u0000 51 Ba. Exercise. Suppose in addition that F is continuous and strictly increasing on IR. Explain why, for 0 < u < 1, there exists a unique number G(u) such that F(G(u)) = u. In usual notation, G = F-1. Prove that if U has the U[0, 11 distribution, so that P(U u) = u for 0 < u < 1, then X := G(U) has distribution function F. Equivalently, if X has a continuous strictly increasing distribution function F, then F(X) has the U[0, 1] distribution. For general F satisfying properties 50(B1), we define G(u) := min{y : F(y) > u}, the right-continuity of F guaranteeing that this is a true minimum, not just an infimum. Then, again, X := G(U) has distribution function F. (Assume this.) Only in special situations (some of them very important) would we use this idea in simulation, because computation of G(U) can be very time-consuming. Some more efficient methods of simulating from some standard distributions will be explained later. ► C. The probability mass function (pmf) px of a discrete RV X. By a discrete Random Variable, we mean one which takes values in the set Z of all integers. Let X be a discrete RV. We define the probability mass function (pmf) px of X to be the function px : Z — > [0, 1] with px(x) := TP(X = x), \u0000 (x E Z). \u0000 (Cl) Of course, X may take values just in Z+, in which case px (x) = 0 for x negative; etc. (We should allow a discrete RV to take values in an arbitrary finite or countable set, but can generally manage without that.) Important examples of pmfs (illustrated for certain parameters in Figure C(i)) are the Bernoulli(p) pmf the binomial(n, p) pmf the Poisson(A) pmf px (x) = px (x) px (x) = { b(n,p;x) { e —AAx/x! IP if x = 1, q := 1 — p \u0000 if x = 0, 0 \u0000 otherwise; if x = 0,1, ... 0 \u0000 otherwise; if x = 0,1,2, 0 \u0000 otherwise. , n, ..., (C2) (C3) (C4) 52 \u0000 3: Random Variables I 0 1 2 3 4 5 6 7 8 9 10 \u0000 0 1 2 3 4 5 6 7 8 9 10 Binomial b(10, 0.3) pmf \u0000 Poisson(3) pmf Figure C(i): Binomial and Poisson pmfs If X has the Bernoulli(p) pmf, then we say that X has the Bernoulli(p) distribution and write X — Bernoulli(p), with similar remarks for the binomial(n,p) and Poisson(A) distributions. Of course, the Distribution Function Fx of a discrete RV X with pmf px satisfies Fx (x) = \u0000 px(Y) := E px(y), y<x \u0000 Z9y<x and, for a, b E Z with a < b, we have P(a < X < = Fx (b) — Fx (a) = E px(y)• y=a+i (C5) We have already met the binomial pmf at Lemma 10Ma, and we shall be seeing a lot of the Poisson pmf. We have seen in Exercise 41G that, even for moderately large n, the number of professors who get the right hat has almost exactly the Poisson(1) distribution. D. Exercises. Useful practice! Da. Exercise. This will be referred to later, so do it! A fair coin will be tossed twice, the number N of Heads will be noted, and then the coin will be tossed N more times. Let X be the total number of Heads obtained. Decide on S2, and make a table with headings w \u0000 P(w) \u0000 X(w) Find px (x) for x = 0, 1, 2, 3, 4. 3.2. DFs, pmfs and pdfs \u0000 53 Db. Exercise. A coin with probability p of Heads is tossed until the first Head is obtained. Let X be the total number of tosses. Find px (x) for x = 1, 2, 3, .... We say that X has the geometric(p) distribution. (The distribution of the total number X — 1 of Tails is also called geometric.) Dc. Exercise. A coin with probability p of Heads is tossed until the first time that a total of n Heads is obtained. Let X be the total number of tosses. Find px (x) for x = n, n + 1, n -I- 2, .... We say that X has the negative- binomial(n,p) distribution. (The distribution of the total number X — n of Tails is also called negative-binomial.) ►► E. The probability density function (pdf) fx of 'continuous' variable X. We shall call an RV X (with values in R) 'continuous' if there exists a piecewise-continuous function fx, called a probability density function (pdf) of X, such that for a, b E R with a < b, b P(a < X < b) \u0000 Fx(b) \u0000 '.K(a) \u0000 fx (x) dx, \u0000 (El) Compare equation 52(C5). See Figure E(i). (Note. We refer to a pdf rather than the pdf because the pdf is only defined modulo sets of measure zero. We'll return to this later.) a IP(a < X < b) \u0000 P(X E dx) \u0000 (y = x + dx) Figure E(i): Probabilities as areas Of course, it is then true that P(X = x) = 0 for all x E 111 and that P(a < X < b) = P(a < X < b) = P(a < X < b) = P(a < X < b). Note that fx is determined by Fx via fx(x) \u0000 Fix (x) except perhaps at if y many points. \u0000 (E2) 54 \u0000 3: Random Variables See the examples below for clarification on exceptional points. We must have J: f x (x) dx = P(—oo < X < oo) = 1. \u0000 (E3) We often write (El) in the helpful alternative form (analogous to 51(C1): P( .t,\"; \u0000 c ..) \u0000 ,) dx, \u0000 (E4) the left-hand side being thought of as `P(x < X < x + dx)' and (E4) making rigorous sense of 53(E1) when integrated over x e (a, b]. Ea. Exercise. A point A is chosen at random in the unit disc { (x, y) : x2 + y2 < 1}. Let R be the distance from the origin to A. Find fR(r) and explain the good sense of the answer. Eb. Note on terminology. Random Variables are functions on Q. We often need to topologize Q, and then truly continuous functions on Q play an important part; and they are not related to the use of 'continuous' which we have been making. I shall always use 'continuous' within \" when meaning 'as in Definition 53E'. (When I speak of a continuous DF, I mean a DF which is continuous, not the DF of a 'continuous' variable.) Ec. Key examples of pdfs. Important examples of pdfs to be studied later (and illustrated for certain parameters in Figure E(ii)) are: the uniform U[a, b] pdf f x (x) = the exponential E(rate A) or E(A) pdf f x (x) = the normal N(P, 0-2) pdf f x (x) { (b — a)-1 if a < x < b, 0 otherwise; (E5) { Ae—Ax if x > 0, 0 if x < 0; (E6) 1 (x _ µ)2 u -N127r exP \u0000 2o-2 ) (E7) If X has the uniform U[a, b] pmf, then we say that X has the U[a, b] distribution and write X U[a, b], with similar remarks for the E(rate A) and N(p, a2) distributions. Notes. That the N(p, a2) pdf does integrate to 1 is verified at Subsection 146B below. How the U[a, b] pdf is defined at a and b is irrelevant; likewise for the way the E(rate A) pdf is defined at 0. 3.2. DFs, profs and pdfs \u0000 55 • -1 \u0000 0 \u0000 1 \u0000 2 \u0000 3 \u0000 -2 \u0000 -1 \u0000 0 \u0000 1 \u0000 2 U[-1, 1] pdf \u0000 E(rate 2) pdf \u0000 N(0, 0.52) pdf Figure E(ii): Various pdfs It will often be convenient to use indicator-function notation in relation to pdfs, so that we write the U[a, b] and E(rate A) pdfs as (b — a)-1I[a,b](x), .fie—Ax/[0 ,,)(x) respectively. ►► F. A first transformation rule for pdfs. Suppose that X is a 'continuous' RV with pdf fx and that V) is a strictly increasing differentiable function. Write X = 7b(Z). Then Fz(z) := P(Z < z) = P(1b(Z) < zb(z)) = P(X < 0(z)) = Fx (0(z)) • Hence, by 53(E2), the pdf of Z is given by fz(z) = fx(iP(z))0 1(z). We can use heuristics: x = \u0000 dx = '(z)dz, and fz(z)dz = f x (x)dx = f x (1(z))01(z)dz. In many ways, it is better to think in these terms. See the lacobian' treatment of the multi-dimensional case in Chapter 7. Fa. Exercise. Use this formula to show that if U ti U[0,1], then X = \u0000 U)/A has the E(rate A) distribution. (This is very useful for simulation.) How does this result relate to the F-inverse principle at 50B? ► Fb. Exercise: the standard Cauchy distribution. A line is drawn through the point (1, 0) in a randomly chosen direction — you formulate this! Let (0, Y) be the point at which it cuts the Y-axis. Show that Y has the standard Cauchy density fy(Y) = \u0000 (1 y 2) (y E R). 71- \u0000 + Prove that 1/Y has the same distribution as Y, and explain this geometrically. (Note. In this exercise, you can ignore the possibility that the randomly chosen line is parallel to one of the axes, because this has probability 0.) 56 \u0000 3: Random Variables Fc. Exercise. Let X be a 'continuous' RV and let Y = X 2. Prove that, for y > 0, Fy (y) = Fx \u0000 — Fx (—V5), whence fy (y) G. Exercise: Buffon's needle. This is not really the right place for this example, but it is fun to do now. A large square region of the plane is marked with parallel lines 2 units apart. A needle, 2 unit long is thrown randomly onto the square area. Prove that the probability that the needle crosses one of the lines is 2/7r (if one ignores 'edge effects'). Y r taii.\\\\ Figure G(i): Hint for the `Buffon's needle' problem Hint. Use Figure G(i), in which Y is the distance from the centre of the needle to the nearest line, and O the angle between the needle and each of the lines. 3.3 Means in the case when S2 is finite A. Assumption: Assume in this section that C2 is finite. Assume also that (as will occur in all the cases which concern us) all subsets of SZ are events and all functions on St are RVs. For any event H, we clearly have P(H) = \u0000 P(w), \u0000 (Al) wEH where P(w) := IP({w}) is the probability of the individual outcome w. ► B. Definition of E (X) for finite C2. For an RV X, we define Px := E(X) \u0000 X(w)p(w). \u0000 (B1) Thus, E (X) is the P-weighted mean of the function X on 12. 3.3. Means in the case when 1-2 is finite \u0000 57 Ba. Discussion. The `long-term average' idea. Suppose that El = {wi , W2, • • • , cox }. The long-term relative frequency idea would tell us that if the experiment associated with E2 were performed a very large number N of times, then it is likely that we would have outcome wk on about NIP(wk ) performances. If, therefore, we look at the 'sum of the observed X-values over the performances', then since we will have X-value X(wk) about NIP(wk ) times, the sum of the X-values is likely to be about Expo x NP(w k) = N icx = NE (X). Hence the average of the X-values should be close to /ix. It is very important to stick with the precise 'abstract' definition at 56(B1), and not to try to build in any long-term average' idea. We shall eventually prove the Strong Law of Large Numbers which formulates the long-term average' idea precisely for the mathematical model. C. Heuristic solution to Puzzle 19A. If I keep throwing a fair die with scores 1, 2, ... , 6 on its faces, then the average gap between successive accumulated totals will be the average of 1, 2, ... , 6, that is, the average of 1 and 6, namely 7/2. Since the gaps are on average 7/2, the proportion of 'accumulated scores' amongst all positive integers will be 2/7. Hence, for any reasonably large number N the probability that at some time my score will equal N would be approximately 2/7. In particular, this is true for N = 10000. Always be careful about the following though. Suppose instead the scores marked on the die's faces are 2, 4, 6, 8, 10, 12. What is the average gap between successive accumulated totals now? What is the approximate chance that at some time my score will be 20000? Be careful! ❑ I emphasize again that for us, 56(B1) is the definition of the mean or expectation of X. At the moment, no long-term average' ideas feature in the mathematical theory: they will return in correct form in the proved 'Strong Law' theorem. ► D. Lemma. Suppose that it is finite, that X and Y are RVs, and that c E Et Then E(cX) = cE(X), \u0000 E (X ± Y) = E(X) + E (Y ), so that expectation is a linear functional. Moreover, for any function h :R -4 IR, E (h(X)) = >_., h(x)P(X = x) \u0000 (D1) x summed over the possible values of X. Especially, for an event F, IE(IF) = IP(F). Note. We need the case h(x) = x 2 to find the variance of X, etc, etc. 58 \u0000 3: Random Variables Proof By the usual rules for functions, (cX)(w) := c X(co) and (X + Y)(w) := X(w) + Y (w), and the first part follows. Summing over w in St can be achieved by first summing over every value x which X can take, and for each such x summing over w-values for which X(w) = x. Thus, E (h(X)) \u0000 h(X (6. ) )P(w) \u0000 E E h(x(w))p(w) „,En \u0000 x 1,‘„x(.)=4 = E E h(x)P(w) = E h(x) E P(w) fu,:x(w)=x1 \u0000 x \u0000 {,,,x(w)=x} = E h(x)P(X = x). We have used result 56(A1) for H the event `X = x'. The indicator function IF takes the value 1 with probability P(F), else the value 0. So E(/F) = 1 x P(F) 0 x P(Fe) = P(F), and that completes the proof. \u0000 ❑ Da. Exercise. Calculate the mean of the binomial (n, p) distribution by using 49(C1). Db. Exercise. Prove that in the 'hat-matching' problem 40E, if R, is the number of professors who get the correct hat, then E (Rn ) = 1. Hint: Again use the idea at 49(C1). Dc. Exercise. Use intuition to write down without any calculation the value of E (X) for Exercise 52Da. Then verify that the answer is correct by using that exercise and equation 57(D1). ► E. Application. Proof of the Inclusion-Exclusion Principle. Let F2, \u0000 , En be events, let cok = IF„, and let U = Fk. By de Morgan's rule 38(D1), tIc = \u0000 n whence, by Exercise 49Ca, 1 — Iu = (1 — cp1)(1 — co2) . . (1 — (pn). Expanding out the product on the right-hand side (you do this in full for n = 2 and n = 3 to see what is going on), we obtain 1—IU = 1- i j>i 3.4. Means in general \u0000 59 whence IU = \u0000 --EEcozco,+—•-•+(-1)n -f1co1co2•••com, i j>i = \u0000 EE IFinF; — • • • + (-1)71+11FinF2n...nF,- i j>i Taking expectations, and using the results of Lemma 57D, we obtain P(U) = EP(Fi ) — EP(Fi n F3 ) + EE y;P(Fi n n Fk) i j>i \u0000 i j>: k>j — - • + (-1) n±11P(Fi n F2 n \u0000 n Fn ); and this is the desired result. 3.4 Means in general Now we drop the assumption that SZ is finite. I shall explain/recall the familiar practical rules in terms of pmfs and pdfs at Theorem 62Ha below. But first, I am going to tell you the simple coherent all-embracing version of the theory that Measure Theory provides. Don't worry: just read on — there's no heavy Measure Theory here — I'm putting hardly any 'Warning' symbols! ► A. SF±, the space of simple non- negative RVs. We call an RV Y simple and non-negative, and write Y E SF+, if we can write Y in the form Y= k=1 where each ak E [0, co) and each Fk is an event. For this Y, we define (how else?!) E (Y) : \u0000 akIP(Fk ). k=1 This defines E unambiguously on SF+. (This last statement needs proof, but we skip that. It's messy!) ► B. Mean of an arbitrary non- negative RV. For two RVs X and Y, we write Y < X to mean Y(w) < X (w) for every w in a For a general non-negative RV X :SZ —> [0, oo], we define — with the dreaded `supremum' revised below — E (X) \u0000 X(w)P(dw) \u0000 sup{E(Y) : Y E SF+, Y < \u0000 5. 60 \u0000 3: Random Variables We say that X is integrable if E (X) < oo. The integral, which is defined to be the supremum, is a Lebesgue integral which exactly describes the idea of the P-weighted mean of X over 52. The same remarks about the need to stick with an abstract definition (which will eventually be 'explained' by the Strong Law) apply as in the earlier case of finite 52. Allowing infinity. We allow E (X) = oo for non-negative X, and even allow X(w) to be co for some values of w, because it is very convenient to do so. Of course, if X is non-negative and P(X = oo) > 0, then, certainly, E (X) = oo. The point of these remarks is that, as we shall see, there are a lot of important non-negative RVs for which P(X = oo) = 0 but E (X) = oo. Clarification of the 'sup' bit. Let X be a general non-negative RV. Then, firstly, we have E (X) > E (Y) whenever Y E SF +, and Y < X — sensible, yes?! Secondly, if E (X) < oo, then, for any e > 0, we can find Y E SF+ with Y < X and E (Y) > E (X) — e; that is, we can approximate X from below as closely 'as we like in the sense of expectation' by a simple RV. (If E (X) = oo, then, for any K > 0, we can find Y E SF+ with Y < X and with E (Y) > K.) C. Facts. (a) If X is a non-negative RV, then E (X) = 0 if and only if IP(X = 0) = 1. (b) If X is a non-negative RV and E (X) < oo, then IP(X < co) = 1. These are hardly a surprise! But they are useful. We mentioned result (b) earlier. ►► D. Fact: Monotone-Convergence Theorem. If (X,,) is a sequence of non- negative RVs, and Xri(w) t X (w) for each w, then E(X7,) t E(X) < oo. Lebesgue's Monotone-Convergence Theorem is the result on which the whole of integration theory rests. Da. C) Application. Proof of the First Borel—Cantelli Lemma 45K. Let Xk be the indicator of A. Let Yr, := X1 + • • • + Xn. Then, for every m, E (Y.) = IP(J1) + • • • + P(Jn ). If E P(Jk) < oo, then by the Monotone-Convergence Theorem, E (Y00) = E P(Jk ) < 00, where Yo lim Yn. Hence Yoo is finite with probability 1, and the result follows. ►► E. Means for general RVs. If X is a general RV, so X : 52 —> Ill, then we decompose X into its positive and negative parts: X _ X = X + — X —, X +(w) := X(w) if X(w) > 0, t o \u0000 X if X(w) < 0, 3.4. Means in general \u0000 61 (Of course, X = X+ — X — has the usual meaning for functions: we have X(w) = X+ (w) — X — (w) for every w.) Then X+ and X — are non-negative RVs. We say that X is integrable if both E (X 4 ) and E (X —) are finite, and then define E (X)= E(X+) Let's make a standard useful definition. ► ► F. The vector space .C1. An RV X is said to be in C1 if it is integrable, that is, if E (IX') < oo. Now, IX I = X = X+ + X —, so IX I is integrable if and only if both X+ and X — are integrable. Moreover, since X+ and X — are non-negative, \u0000 IE(X)I = IE(x+)-E (x- )1 \u0000 11E(x +)1 + 1E(x11 = E(x+)+E(x - ) = the familiar result that the modulus of an integral is less than or equal to the integral of the modulus. Note that since IX + Y I < 'XI + IYI, if X, Y E C', then X + Y E .C1. We see that C' a vector space over III ► G. A useful result and a useful criterion. Let Y be an RV with values in Z+. Then 00 E(Y) = EP(Y >n) < 00. n=1 For an arbitrary RV X, we have X E Li if and only if E Pax' 77') < 00. n>1 \u0000 Proof If F(n) denotes the event {w : Y(w) > \u0000 then 00 Y = EIF(n), and IE (Y) = EP(F(n)) \u0000 P(Y >n). n>1 \u0000 n>1 \u0000 n=1 Any worries about rigour you might have evaporate when you replace Y by min(Y, r) E N) and then let r oo using the Monotone-Convergence Theorem. The criterion is obvious since if Y is the integer part of IX', then Y < 1X1 < Y 1. 0 62 \u0000 3: Random Variables H. Rules for expectations. Now let's look at the practical rules. \"Here he goes\", you say, \"giving us 'practical rules', but with those stupid technical conditions he seems to like. At least he's put them in small type\". My reply. I wouldn't do it if there wasn't some point. You see, it is simply not always the case that long-term averages converge to a 'mean'. For Cauchy-distributed variables, the 'sample mean of several independent observations' always has the same distribution as a single observation, as we shall see later. Cauchy RVs are not integrable. For some distributions, the long-term average tends even to 'spread out'. Now the whole story is known exactly. Why therefore put on blinkers by fudging? ►► Ha. Theorem: Rules for Expectations. Suppose that F is an event, X and Y are integrable RVs (in symbols, X, Y E L1), and that c E Ilk. (a) Then cX and X + Y are integrable, and E(IF) \u0000 JP(F), E(cX) = cE(X), TE(X +Y) \u0000 E(X) +E( ). (b) If X takes integer values, and h is a function defined on the set of integers, then E (h()) \u0000 h-(x)P(X = x), xcz [Q it being understood that h(X) is integrable if and only if the series converges absolutely.] (c) If X is 'continuous' with pdf fx, then for any piecewise-continuous function h : R R, I.E(h(X)) = f h(x)P(X E dx) \u0000 h(x) f x (x) dx, xER \u0000 xER [@ it being understood that h(X) is integrable if and only if the familiar final integral `converges absolutely', that is, if and only if L ER h(x)I f x (x) dx < co.] We are going to take these rules for granted (except for Discussion I). We have seen Rules (a) and (b) in some detail for the case when Q is finite. Rule (c) is the 'obvious' analogue of Rule (b) for the case when X is 'continuous', so you should believe Rule (c). The real point is that Rule (c) belongs to Measure Theory, where it has a simple and natural proof. See [W] for the details if you really want them now. I. ® Discussion. \"You are always almost pedantically rigorous\", you say, \"so give us a clue why E (X) = f xfx (x) dx.\" OK. To keep things simple, let's suppose that X 3.4. Means in general \u0000 63 is bounded and non-negative: 0 < X < K for some integer K. For each positive integer n, define an approximation X(n) to X via X (n) (cv ) : = so that X(n) is a simple RV k \u0000 k — if — k < X(w) < \u0000 +1 n \u0000 n nK --, \u0000 , X (n) = 2 k — - 1 F (n,k), where F(n,k) = co : k — < X(co) < k ± 1 1 \u0000 n \u0000 n \u0000 n k=0 We have E (X (n)) \u0000 E \"iP(F(n,k)) = where 5(m) is the 'staircase' function with E k f (k+1)/n K S(n) (X) fx (x)dx, 0 f x (x) dx n Ain s(n)(x) := —k if 1, k < x < k \u0000 +1 , \u0000 0 < x < K. 7 Draw a picture of 8(m) , and note that Is(n)(x) x I < 1/n for every x. Note too that X(n) = s(m)(X), so that the above expression for EX(n) agrees with Rule (c). Since `the modulus of an integral is less than or equal to the integral of the modulus', we have E (X (n)) — f xf x (x) dx < f S (n) (X) — x f x (x)dx f Is (n)(x)— x} fx(x)dx < f fx(x)dx = —. n But if n(r) denotes 2T, then your picture for s(n) shows that s(n(r))(x) t x for every x, whence X (n(r)) X and, by the Monotone-Convergence Theorem 60D, E X (n(r)) E X; and the result follows. Rule (a) can be proved by vaguely analogous (but, in Measure Theory, much better) approximation techniques. ► J. Examples. I emphasize that the following examples do not involve any measure-theoretic ideas. If X has the uniform U[a, b] distribution on [a, b] (see 54(E5)), then 1 f b \u0000 x dx b — a a b E (X) = f xfx(x)dx = 11Z \u0000 fa x 1 b — a dx = = \u0000 1 \u0000 b2 — a2 \u0000 a + b b —a \u0000 2 2 of course! 64 \u0000 3: Random Variables If X has the exponential E(rate A) distribution (see 54(E6)), then, we could work out E(X) = f x fx(x)dx = f x Ae-As dx itz \u0000 A I have not worked out the integral by integration by parts (but you can if you want): I promised you that we shall see crafty methods which avoid the need to do such things. And it would be silly to work out E (Y) if Y has the binomial(n, p) distribution via E(Y) = E kP(Y = k) = E k(:)p k (1 - pr-k np; k=0 \u0000 k=0 when we do not need to — who wants to do such a sum anyway? (OK, then, just for you. The 'k = 0' term makes no contribution. For k > 1, the kth term in the sum is np times the probability that Z = k — 1 where Z has the binomial (n — 1, p) distribution — and it's done!) The right way to find E (Y) was explained at Exercise 58Da. I repeat that I am not going to set exercises which just test whether you can work out sums or integrals. Ja. Exercise. Two points A and C are chosen independently at random in the unit disc (x, y) : x 2 + y2 < 11. Let D be the distance between them. Prove that E (D2) = 1. Hint. Use Pythagoras's Theorem and exploit symmetry. Later on, you are invited to check that the answer looks right by simulation. ►► K. Jensen's Inequality. Let c be a smooth convex function on a (finite or infinite) interval I in that c\" exists and is continuous on I, and c\"(x) > 0 \u0000 for all x E . Let X be an RV in rl with values in I, such that c(X) E Gl. Then E c(X) > c(p), \u0000 ,u := E (X). If c is strictly convex on I in that c\" > 0 on I except perhaps for a finite number of points at which c\" = 0, and if P(X = p) # 1, then E c(X) > c(p). Examples: c(x) = x2 is strictly convex on R; for any real non-zero A, c(x) = e Ax is strictly convex on R. Proof Of course, ti E I. Since c\" > 0 on I, the function c' is non-decreasing on I, so that, for x > it and x E I, c(x) - c(u) = f c'(y)dy f c'(p)dy = (x - p)c' (11)• A Check that c(x) > c(p) (x — µ)c'(µ) for all x E I, whence, c(X) > c(A) + (X — µ)c'(µ). Taking expectations yields the result E c(X) > c(p). The 'strictly convex' version is now easily obtained. \u0000 ❑ 3.5. Variances and Covariances \u0000 65 ►► L. Bounded- Convergence Theorem. Suppose that Xn, (rt E N) and X are RVs such that P(Xn —> X as n oo) = 1. Suppose further that for some absolute constant K, we have 1Xn(w)i < K for all n and w. Then, E(IXn — XI) 0, and hence E(X7,) E (X). Heuristic proof For a non-negative variable Y and an event F, let us write E (Y; F) E (YI F), a notation which will be very useful later. The intuitive idea for the proof of the theorem is that, for fixed e > 0, we can write E(IXn — XI) = EaXn — XI;IXn — XI < \u0000 +IE(1Xn — XI :1Xn — > 1E) < 1E ± 2KP(IXn, —XI > 14E), and that (this is not obvious) we can choose no such that for it > no we have — XI > \u0000 < El(4K). \u0000 ❑ More general convergence theorems are known: in particular, Lebesgue's celebrated Dominated-Convergence Theorem and the 'necessary and sufficient' Uniform-Integrability Theorem. See [W], but remember that the Monotone- Convergence Theorem 60D is the fundamental result. It is a great pity that Measure Theory courses often give the impression that the convergence theorems are the main things in the subject. The truth is that they contain hardly any of the topic's great subtlety. 3.5 Variances and Covariances Figure 67E(i) will explain what variance is. The geometry of the space £2 now to be introduced is absolutely fundamental to ideas of regression, and Analysis of Variance. ►► A. The space L2. An RV X is said to belong to ,C2 if E (X 2) < oo; and we then define the .C2 norm of X as ilX112 =T VE (X ). We shall see that the L2 norm of X agrees with the concept of standard deviation of X if E (X) = 0. (Note. Yes, analysts, I know that my 'norm' is not quite a norm, but that's not a point worth quibbling about.) ► B. Lemma. Suppose that X, Y E L2 and that c E R. Then cx E L2, x y E L2, XY EGl , X E ri. 66 \u0000 3: Random Variables Proof It is obvious that cX E G2 because E {(cX)2} = c2E (X 2). Next, we have 0 < (X ± y-)2 \u0000 2(x2 ± y2) pc 17)2 < 2(x2 ± y2), and this makes it clear that X + Y E ,C2. Since 0 < (IX' _ iyi)2 \u0000 X2 +Y2 2ixi iyi we see that E(XY) < 2{E \\ ( X 2) +E (Y 2)} < oo, so that XY E Gl. Of course, the constant function 1 is in .C2, so that X = X x 1 is in Gl \u0000 ❑ ► Ba. Exercise: Important inequalities for the Strong Law. Let X and Y be RVs. Prove that (X + Y)4 < 8 (X 4 + Y4) , \u0000 (B1) and that, for 1 < r < s, 1 + IX15. \u0000 (B2) Note that, in particular, as also follows from the last part of Lemma 65B, c r2. \\ Hints. Of course, it is enough to prove that (x \u0000 < 8(x4 + y4) for real numbers x, y (for which read the proof of the above lemma), and that for non-negative x, xr < 1 + xs, for which you say \"Either x < 1 . ..\". C. The Cauchy-Schwarz Inequality. If X, Y E £ 2, then 11E(XY) \u0000 i1x112HY112, equivalent {E(XY)}2 :5 E (X 2) IE ((Y2) (C1) with equality if and only ifP(aX + by = 0) = 1 for some real numbers a and b, not both zero. Proof We may suppose that E (X2) A 0 (for if E (X2) = 0, then we have P(X = 0) = 1 and E(XY) = 0). Let 13 = E (XY)/E (X2). Then (you check!) 0 < E {(Y — OX) 2 } = E(Y 2) — {E(XY)}2 /E(X 2), with equality if and only if IP(Y = OX) = 1. \u0000 ❑ ►► D. Variance, ex or Var(X), of X. If X E £ 2, then X E r i, so ktx exists, and we can define {('- ttx)2} \u0000 (D1) 3.5. Variances and Covariances \u0000 67 Since (remember that fix is just a number) E (X itx)2} = E (X 2 — 2ttxX tt3c) = E (X 2) — 2,axE (X) [12x = E (X 2) — 2112x + Pt2x, we also have Var(X) \u0000 E (X 2) ,a2v- \u0000 E (X 2) — E(X)2. \u0000 (D2) This result, often called the Parallel-Axis result because of the analogous result on moment of inertia, is the first of a whole series of results on sums of squares which leads up to the Analysis-of-Variance technique. Watch the idea develop. And do note that we have Var(cX) = c2Var(X), \u0000 E (X 2) = E (X)2 + Var(X), \u0000 (D3) results which we often use. Da. Example. Suppose that X has the exponential E(A) distribution, so that the pdf of X is Ae-Ax/[0,e.3)(x). We already know from Examples 63J that X has mean p = 1/A, so that, by Rule (c) of Theorem 62Ha, we have (as you can check by integration by parts if you wish) 0.0 Var(X) = E(X 2) — p,2 = f x2Ae-Ax dx — p2 = 1 o \u0000 A2 • You are reminded that we shall work out such integrals by efficient indirect methods. ► E. Discussion. Variance as a measure of spread. Figure E(i): Variance and moment of inertia 68 \u0000 3: Random Variables Variance provides a 'quadratic' measure of how widely the distribution of an RV is spread about its mean. The 'moment of inertia' idea makes this precise. Suppose that X is 'continuous'. Imagine that we make a very thin sheet of metal the shape of the area under the graph of fx, of unit mass per unit area. The variance of X measures how hard it is to spin this metal sheet about a vertical axis through the mean — see Figure E(i). One precise form of this statement is that if the sheet is spinning, at 1 complete revolution per second, then its total kinetic energy is a certain constant (272) times Var(X). Experience shows that variance provides a much more useful measure of spread than, for example, E (IX — ix1)• F. Dull but important exercise. Label the people in the city of Bath (say in alphabetical order) 1, 2, ... , n. Let hk be the height of person k. For the experiment of choosing a person in Bath at random, the sample space is Il = {1, 2, ... , n} and 1P(w) = 1/n for every w. Let X be the height of a person chosen at random from those in Bath. If w = k, then X(w) -= hk. Prove that n Ehk, k=i and that 1 cr2 := Var(X) = — n E(hk /1)2 = ► ► G. Standard deviation, ax or SD(X), of X. For X c £ 2, we define the standard deviation (Tx or SD(X) of X to be the square root of the variance of X: \u0000 ax := SD(X) \u0000 NrVar(X), so SD(cX) = icl x SD(X). (Applied mathematicians call the SD the radius of gyration of our metal sheet.) ► ► H. Covariance Cov(X, Y) of X and Y. Suppose that X, Y E £ 2. Then XY E Li and (why?!) (X — px)(Y — py) E Gl. We may (and do!) therefore define (you check the equivalence of the alternative forms!) \u0000 Cov(X,Y) := E \u0000 — ttx)(Y PO} = E (XY) E (X)E CO. ►► I. Correlation Coefficient p(X, Y) of X and Y. Under the same assumptions on X and Y (and the assumption that ax and ay are non-zero), we define p(X \u0000 Cov(X, Y) ,Y) That —1 < p(X, Y) < 1 is proved in Lemma Ja below. We say that X and Y are positively correlated if p(X, Y) > 0, re — p2 . • k = 1 \u0000 k = 1 Crx (Ty 3.5. Variances and Covariances \u0000 69 equivalently if Cov(X, Y) > 0, X and Y are negatively correlated if p(X, Y) < 0, equivalently if Cov(X, Y) < 0, X and Y are uncorrelated if p(X, Y) = 0, equivalently if Cov(X, Y) = 0. What does all this mean? A good answer lies in the next topic which clarifies the role of p(X, Y) as a 'measure of the degree of linear dependence between X and Y'. ■ J. 'Least-squares' ideas; linear regression in Probability. The important idea here is that we wish to give the 'best predictor' Z, a random variable, of another random variable Y, Z having to take a certain form (as is illustrated in this subsection and in others). The least-squares-best predictor Z is that which minimizes the mean-square- error MSE := E{ [Y — 4 2}. Two important cases are covered by the following lemma. Ja. Lemma. (a) Let Y be an RV in L2. Suppose that we consider deterministic- constant predictors c of Y, so that MSE := {[Y — c]2}. Then the best choice of c is c = my and then MSE = 4. (b) Let X and Y be RVs in .C2, each with non-zero variance. Let p := p(X, Y). Suppose that we wish to use a 'regression line' y = ax to predict Y from X in the sense that we use aX \u0000 as our predictor of Y, so that MSE := E [Y — (aX OW} . Then the regression line which leads to the minimum value of MSE may be rewritten: Regression line: \u0000 bcy) = p-- \u0000 itx) (Tx and then MSE E{[Y — (aX 4 2) \u0000 p2). We therefore have 5- p = P(X,Y) < 70 \u0000 3: Random Variables and if IpI = 1 then we have 'almost perfect linearity' in that o-y (Y 11Y) = p \u0000 (X — ,ux) with probability 1. a x Note that the slope of the regression line always has the same sign as p, in other words the same sign as Cov(X, Y). ► Jb. Exercise. Prove Lemma 69Ja. Jc. Exercise. Show that if X has the uniform U[-1, 1] distribution on [-1, 1], and Y = X 2, then p(X,Y) = 0. Thus X and Y are uncorrelated even though Y is a deterministic function of X. This emphasizes that p can only 'assess' linear dependence. Note. Linear regression in Statistics, which has many analogies with what we have just done, is studied in Chapter 8. ►► K. Lemma. Variance of a sum. Suppose that we have random variables Xi, X2, . , X n E £2. Then COV (Xi X3). n terms iii n(n — l) terms Proof Let ilk := E (Xk ). Then, (Xi + X2 + • • • + X.)2 = (Xi + X2 + • + \u0000 + X2 + • • + Xn) Xi X 3 (12 terms). and, similarly, ( + 2 + • • • + /in )2 = /./2/21 (n 2 terms). If we take expectations of the first of these equations and then subtract the second, we obtain precisely the result we want. (To get the second form, we use the fact that Cov (Xi, Xi) = Var(Xj).) Ka. Exercise. Prove that if Rn is the number of professors who get the right hat in Example 40E, then Var(Rn) = 1 whenever n > 2. Hint. Let Xk be the indicator function of Fk in the Solution to Example 40E. Find E (Xk ), Var(Xk) and Cov(Xi, X 3) (i # j). Why is it not surprising that the covariance is positive? 3.5. Variances and Covariances \u0000 71 ► Kb. Exercise. The problem here is to show that if I take an ordered sample of two different people from the n people who live in the city of Bath and record their heights H1, H2, then the correlation coefficient p(Hi , H2) is —1/(n-1). Why is it not surprising that the covariance is negative? Hint. Suppose that I get the computer to arrange all the people in Bath in random order, all n! permutations being equally likely. Let Xk be the height of the kth person chosen by the computer. Then what is Var(Xi + X2 ± \u0000 Xn)? (If I repeated the experiment of finding X1 + X2 + • • \u0000 Xn, how much different is the answer I would get?) Now use Lemma 70K to express the common covariance C := Cov(Xi, Xi) (i j) in terms of the common variance a2 = Var(Xi). The above is a 'trick' solution. A much more illuminating solution of this problem will be given later. But here's a dull solution. Dull, but instructive, solution. Consider the experiment of choosing 2 different people at random from the population of Bath. We regard a possible w as an ordered pair (i, j), where i, j E {1, 2, ... , n} and j i. There are n(n — 1) such ordered pairs, and the statement that we choose 'at random' here conveys that every w has probability {n(n — 1)}-1. For w = (i, j), we have HI(w) = hi and 112(w) = h3. We have E (H1H2) = \u0000 1 EE hi hi n(n — 1) Hence, n — 1 n \u0000 lit 2 1 1 ( n — 1 1 n 1 E hi v , = n — 1 1 \u0000 n — 1 \u0000 2 \u0000 22 1 \\ \u0000 (11 ± 0.) \u0000 122 a 2 n — 1. C := Cov (Hi , H2) = E (H1H 2) — E (H1) E (H2) = and the desired result follows. \u0000 ❑ Kc. Exercise. \u0000 Adapt the trick method of doing the last example to show that if X1, X2, . \u0000 X n, are any RVs in L2, such that Var(Xi) = 1, \u0000 p(X,,, X3) = p (i j), then p > —1/(n — 1). First though, explain why it is obvious that if n = 3, then p cannot be —1. Kd. Exercise. Prove that if X, Y, Z are RVs in L2 such that \u0000 Var(X) = Var(Y) = Var(Z) = 1, \u0000 p(X, Y) = p(X, Z) = p, a 2 n — 1 72 \u0000 3: Random Variables then p(Y, Z) > 2p2 — 1. First though, explain why it is obvious that if p = +1, then p(Y, Z) = 1. Hint for the main part. Consider Var(Y + Z — aX). Note. The described result will become obvious for other reasons given in Chapter 8. ■ ■ L. Exercise. Median and the r 1 analogue of Part(a) of Lemma 69Ja. Suppose that X is a 'continuous' variable with strictly positive pdf f on R. Then (why?) there will exist a unique real number m, called the median of X such that P(X < m) = ]P(X > m) = 2. Suppose that X E Gl , that is, E (IX') < oo. Prove that the value c = m minimizes the mean absolute error E (IX — cl). Hint. Draw a graph of the function x ,— lx — ml — Ix — cl in the cases m < c, m > c, and confirm that, for x m, Ix — ml — Ix — ci < (m — c) {I (—.,.)( x) — -4.,00)(x)} . 4 CONDITIONING AND INDEPENDENCE Hallelujah! At last, things can liven up. 4.1 Conditional probabilities ► A. Definition of conditional probability IP(B A). Suppose that A and B are events, (measurable) subsets of the sample space fi for our experiment E. Suppose further that P(A) # 0. Then we define the conditional probability P(B I A) of B given A (`that B occurs given that A occurs') via P(A n B) (Al) We saw motivation for this at 6G. We also considered briefly there, in relation to an experiment already performed, Bayesian and Frequentist views of the interpretation of conditional probability in the real world. Definition (AI) exhibits P(B I A) as the P-weighted fraction of A which is (also) covered by B. Aa. Remark. If S2 is discrete and (as is the case for all examples which then concern us) each one-point set {w} is an event, then conditioning on A effectively means replacing SI by A and P(w) by 111(w I A) := P({w} I A) for w E A. Ab. Example. For the 'Car and Goats' problems at 15P, let's write w = (Contestant's original choice, Host's choice). For an outcome w E 12, write X (w) = 1 or 0 according to whether the contestant would win or lose if she sticks to her original choice, Y(w) = 1 or 0 according to whether the contestant would win or lose if she switches. 74 \u0000 4: Conditioning and Independence w P(co) X (co) Y(w) (C,G1) 6 1 0 (C,G2) -4 1 0 (GI ,G2) A 0 1 (G2,G1) 1 0 1 Table A(i): Original problem For the original problem at 15Pa, we have Table A(i) with no conditional probabilities. We see that E (X) = A, E (Y) = For the modified problem at 15Pc, we have Table A(ii), where A stands for the event that the host happens to reveal a goat. We see that (with obvious 'conditional expectation' notation) E (X I A) = E (Y I A) = Conditional expectations are a major topic in Chapter 9. w P(w) P(w I A) X(w) Y(w) (C,G1) 6 4 1 0 A (C,G2) i 1 1 0 (G1,G2) 6 * 0 1 (G2,G1) 6 * 0 1 12 \\ A (G1,C) * 0 0 0 (G2,C) t 0 0 0 Table A(ii): When host doesn't know Ac. Exercise. Prove that if B and C are disjoint events, then (if P(A) 0) P(B U C I A) = P(B I A) P(C I A). Ad. Exercise. I saw in Lindley's thought-provoking book [153] the following exercise. `Let A, B and C denote events of positive probability. Say that A favours B if P(B I A) > P(B). Is it generally the case that if A favours B and B favours C, then A favours C?' Because of the 'Universality of the Fundamental Model' already alluded to, if you wish to produce a counterexample to any such statement, you can always take for the basic experiment E that of choosing a number between 0 and 1 according to the uniform distribution. But you don't have to do the question this way. ► B. General Multiplication Rules. As several examples in Chapter 1 showed, we often use the General Multiplication Rules P(A n B) \u0000 P(A)P(B I A), P(A n B n C) = P(A) (B I A)P(C I A n B), etc, (B1) 4.1. Conditional probabilities \u0000 75 to assign probabilities to events. Have another look at the 'National Lottery' Proof 10L of the binomial-distribution formula, and at the Birthdays Problem 130 from this point of view. C. Bayes' Theorem. \u0000 This theorem, due to the Reverend Thomas Bayes (1763), is now an immediate consequence of the definition 73A of conditional probability. However, it systematizes arguments such as that used in the 'Test for disease' problem with which we began Chapter 1. More significantly, it is the basis for the Bayesian approach to Statistics. Ca. Theorem (Bayes). Suppose that H1,112, . , H,, are exclusive (disjoint) and exhaustive events: Hi n H3 = (i j), \u0000 U Hk = Thus, precisely one of the Hk will occur on any performance of our experiment E. Assume that P(Hk) > 0 for every k. Let K be some event with P(K) > 0. Then we have the decomposition result P(K) = IP(Hin K) Now, P(Hi 1K) = P(K) We write P(Hi I K) cc P(H,) P(K \u0000 (C2) the constant of proportionality (in fact, 1/IP(K)) being determined by the fact (see Exercise 74Ac) that P(Hi I \u0000 = 1 \u0000 = 1P(S21K)). Proof The way I have worded it, the theorem virtually proves itself. As an exercise, you should work through it, beginning with K = Knit = Kn (H u H2 u u H ii) whence, from the distributive law of set theory, \u0000 K = (K n Hi) u (K n H2) u u (K n \u0000 . This expresses K as a disjoint union. Now use the Addition Rule and the General Multiplication Rule. \u0000 ❑ P(Iii)P(K I Hi) 76 \u0000 4: Conditioning and Independence Of course, the rule (C1) generalizes the rule P(B) = IP(A)P(B I A) + P(Ac)P(B Ac), \u0000 (C3) which we used several times in Chapter 1, where the equation appeared as 8(11). ► D. Exercise: P6lya's Urn. This model is more 'a thing of beauty and a joy for ever' than ever Keats's Grecian urn was! It is very rich mathematically. We shall return to it on several occasions. At time 0, an urn contains 1 Red and 1 Black ball. Just before each time 1, 2, 3, ..., Polya chooses a ball at random from the urn, and then replaces it together with a new ball of the same colour. Calculate (for n = 0, 1, 2, ... and for 1 < r < n 1) := IP(at time n, the urn contains r Red balls). (Hint. If the urn contains r Red balls at time n > 0, then it must have contained either r or r — 1 Red balls at time n — 1. As Polya himself would tell you (Polya [185, 186]), you should work out the answer using formula 75(C1) for n = 1, 2, 3, guess the answer for the general case, and prove it by induction.) It is a theorem that the following statement (discussed in Chapter 9) is true with probability 1: as n oo, the proportion of Red balls in the urn converges to a limit 0 (a Random Variable). What is P(0 < x) for 0 < x < 1? Hint. Apply common sense to your earlier result. E. Exercise*: A car-parking/dimerization problem. This problem has been studied by many authors, of whom perhaps the first was E S Page. On the side of a street, sites 1 \u0000 2 \u0000 3 are marked, a car's length apart. A car must park so as to occupy a closed interval [i, \u0000 1] for some i with 1 < i < n-1 (and then no car may park to occupy [i —1, i] or [i+1, i+2]). The first driver to arrive chooses one of the n — 1 available parking positions at random. The next to arrive then chooses his parking position at random from those then available; and so on, until only isolated sites (no good for parking) are left. Show that if n = 5, then, for the final configuration, we have (with obvious notation): P(1 2 3 4 5) = P(1 2 3 4 5) = 3/8, P(1 2 3 4 5) = 1/4. [For clarity: Site 5 ends up isolated in the first configuration, Site 1 in the second, Site 3 in the third.] For general n, let prz be the probability that the right-most site n ends up isolated. Show that (n 1)Pn = pi + p2 + • \" Pn - 2, \u0000 Pn Pn-1 = —(Pn-1 — Pn-2)/(n — 1), 4.1. Conditional probabilities \u0000 77 and deduce that n-1 (-1)i Pn = 3=0 \u0000 ! so that pn e -1 for large n. Argue convincingly that pi,n :=- IP[site i ends up isolated] = pip.--i+1, which, when n is large is very close to e-2 for nearly all i. Hence, it is very plausible (and true!) that if pn is the mean number of sites which end up isolated when we start with n sites, then /In /n \u0000 e-2 as n \u0000 Do. Is it true that lP(interval [i — 1, i] becomes occupied) = (1 — pi)P.-Ft — i? Notes. Chemists are interested in this type of dimerization problem. Problems of '2 x 1'- dimer coverings of regions on a '1 x 1' lattice are absolutely fascinating and relevant to Statistical Mechanics. See references at the end of Chapter 9. ►► F. Reformulation of result 75(C2). We now reformulate result 75(C2) in the language of Bayesian Statistics, though what we do here is uncontroversial Probability. Examples will make clear what this is all about. Think of H1,112, . , _Wm as `hypotheses' precisely one of which is true. Think of K as representing the known information we have about the outcome of the experiment: we are told that event K occurred. The absolute probability P(Hi) is called the prior degree of belief in Hi. The conditional probability IED (Hi K) of Hi given that K occurred is called the posterior degree of belief in Hi. The conditional probability P (K I Hi) of getting the observed result K when Hi is true is called the likelihood of K given Hi. Then 75(C2) reads Posterior degree of belief oc (Prior degree of \u0000 x Likelihood. \u0000 (F1) Here's a key 'Weighing the Odds' formula! Fa. Example. Return again to the 'Test for Disease' Example 3B. Consider the two `hypotheses': H1: the chosen person does not have the disease; H2: the chosen person does have the disease; We have the following actual information: K: the test on the chosen person proved positive. Table F(i) shows a Bayesian table for analyzing the problem: We work with 'numbers proportional to' (`odds ratios') in all but the last column, where we normalize so as to make the column sum 1. We have of course used formula (F1). 78 \u0000 4: Conditioning and Independence Hypothesis H Prior P(H) a Likelihood P(K I H) oc Posterior P(H I K) cx Simplified ratio oc Posterior ]P(H I K) = H1 99 10 990 11 11/12 112 1 90 90 1 1/12 Table F(i): Bayesian table for test-for-disease model ► G. A step towards Bayesian Statistics. The Bayesian approach to Statistics regards the whole of Statistics as a set of applications of (F1), but allows a much wider interpretation of 'degree of belief' than is allowed in Probability or in the traditional view of Statistics. I now ask you to consider a situation which keeps things strictly within Probability. Please do not regard the description in light-hearted' language as being a kind of `send-up' of the Bayesian approach. I think that Bayesian Statistics has very great value. In the following discussion, we treat at an intuitive level conditional pdfs, etc, studied rigorously later. Suppose that God chose a number 0 between 0 and 1 at random according to the uniform U[0, 1] distribution, and made a coin with probability 0 of Heads. Suppose that He then presented the coin to His servant, the Rev'd Thomas Bayes, and explained to Bayes how He had chosen 0, but did not reveal the chosen value of 8. Suppose that Bayes then tossed the coin n times, and obtained the result K : THHT ... HT, with r Heads in the n tosses. \u0000 (G1) Think of dividing up [0, 1] into lots of little intervals 'do = (9, 0 + dOP , and of Hde as 0 E dO. The absolute pdf of 8, denoted in standard Bayesian fashion by 7(•) rather than fe (.), is given by 7-(0) = 1 on [0, 1], \u0000 uniform prior, so that P (Hde) := P(0 < 0 < 0 + dO) = 7r(19)(10. The likelihood lhd(9; K) of getting the actual observation K if Hde is true (that is, to all intents and purposes, if 0 = 9), is, by the usual independence argument lhd(9; K) = (1 — 0)00(1 — 0) _0(1 — 0) = 9T(1 — 0)\" . By rule 77(F1), IP (Hde I K) a P(Hdo) x lhd(9; K), leading to the formula for the conditional pdf of 0 given K: 7r(9 I K) a ir(0) x lhd(9; K) = Or (1 — 0)' (0 < 0 < 1). \u0000 (G2) 4.1. Conditional probabilities \u0000 79 We want fol. 7(01 K)c1.8 = 1. Now, without doing any integration, we shall prove at 107L below that 1 Or(1 — 0)sclO = \u0000 r!s! \u0000 (r, s E z+ ) . \u0000 (G3) (r±s±1)! Hence, Bayes' degree of belief as to the value of 0 based on the result 78(G1) and the uniform prior is summarized by the conditional pdf 7401K) = (n + 1)( n ) Or(1 — 0)n —r/[0,1] (0). \u0000 (G4) r H. A surprising isomorphism. It is no coincidence that I used the symbol 0 for the limiting proportion of Red balls in Polya urn because Polya's urn is mathematically identical (isomorphic) to the Bayesian analysis of coin tossing using a uniform prior for the probability 9 of Heads. We shall see more of this later, but discuss the key point now. The absolute probability that Bayes would get outcome K = THHT . . . HT, with r Heads and n — r Tails, is fo Li Br (1 — 19)'cle = r!(n — r)! (n + 1)! ' (H1) using (G3). Intuition. Use decomposition rule 75(C1), thinking of the left-hand side as IP)(K) = EIED(Hde)P(KIR-de) = f (d0)0r(1 - 0)n-r . The absolute probability that Polya would choose the sequence BRRB . . . RB of colours corresponding to K, with R for H and B for T, is (see Clarification below) 1 \u0000 1 \u0000 2 \u0000 2 \u0000 r \u0000 n — r \u0000 r!(n — r)! —x—x—x—x...x x \u0000 2 3 4 5 \u0000 n n + 1 \u0000 (n + 1)! ' so the absolute probability that Bayes would get any result is the same as that that Polya would get the corresponding result. Clarification. For the described Polya result corresponding to K, just before time n — 1 (equivalently, at time 71 - 2) there would be r Red balls (the original plus r —1 new ones) out of n balls, and just before time n (equivalently, at time n — 1) there would be n — r Black balls out of n, + 1 balls. Note. Under the isomorphism as completed via the 7r-system Lemma (see 45M), the existence of the limit 9 for Polya's urn is essentially just the Strong Law for coin-tossing. I. Solution to Eddington's Problem 19C. For the 'Two Liars' version, for which K is `C affirms that D lied', we make the Bayes table I(i). The 'T/L' under D or C signifies whether or not that person spoke the Truth or Lied. The 'T'L' in the first row of the table proper under 'Summary' signifies that 'D spoke the Truth and C lied'. The probability of this is 3 x 3 a 1 x 2= 2. 80 \u0000 4: Conditioning and Independence Hypothesis D T/L C affirms that D lied C T/L Summary Odds a D spoke truth T Yes L 'IL 2 D lied L No T LT 2 Table I(i): Bayes table for 'Two Liars' problem Hyp D T/L (C) C T/L (B) B T/L (A) A T/L Summary Odds oc D spoke T Yes L Yes L Yes T TLLT 4 truth T Yes L No T Yes L TLTL 4 T No T Yes T Yes T 1'1'1'1 1 T No T No L Yes L TTLL 4 D lied L Yes T Yes L Yes T LTLT 4 L Yes T No T Yes L LTTL 4 L No L Yes T Yes T LL'IT 4 L No L no L Yes L LLLL 16 Table \u0000 Bayes table for 'Four Liars' problem The summed odds that D spoke the truth is 2 and the total of the odds is 4. Hence the conditional probability that D spoke the truth is 2/4 = 1/2. For the 'Four Liars' problem, we make the Bayes table I(ii) in which we use the shorthand (C): C affirms that D lied, (B): B denies that C affirms that D lied, (A): A affirms that B denies that C affirms that D lied. The table should be self-explanatory. The summed odds that D spoke the truth is 13, and the total of the odds is 41. Hence the conditional probability that D spoke the truth is 13/41. ► J. Hazard functions in discrete time. \u0000 Engineers study failure times of components, and actuaries life-times of people, in terms of hazard functions which are conditional failure rates. Let's first examine the situation in discrete time. Let T be an RV taking values in N := {1, 2, 3, ...}. Think of T as the time just before which a component fails. Define the Reliability Function R for T via := R(n) := P(T > n) = \u0000 F (n), F being the DF of T. We use whichever of Rn and R(n) is typographically neater at the time. The hazard function h for T is defined via hn := P(T = n1T > \u0000 1) = P(failure just before n I OK at time n — 1). 4.1. Conditional probabilities \u0000 81 (This is really defined only provided that Rn_1 > 0.) Since IT > n — 1} n {T = n} = IT = n} , we have However, so that P(T = n) \u0000 P(T = n) = Rn—ihn• hn — P(T > n — 1)' lP(T = n) = Rn-1 — Rn (you check!), Rn = R n_1(1 — hn) = (1 — h1)(1 — h2) . • • (1 — hn), pr. = (1 — h1)( 1 — h2) - - - ( 1 — hn—Ohn, where p is the pmf of T: pn := P(T = n). Geometric distribution. If the hazard function is constant, hn = c for n E N, then T has the geometric distribution with pmf P(T = n) = (1 — c)n—lc (n = 1,2,3, ...). This is the distribution of the time of the first Head if we toss a coin with probability c of Heads repeatedly. ► K. Hazard functions in continuous time. Now let T be a 'continuous' variable with values in [0, oo). Define its Reliability Function R as in the discrete case: R(t) := P(T > t) = 1 — F(t). The intuitive idea is that the hazard function of T is defined via h(t)dt = P(t — dt < T < tIT>t—dt). \u0000 (K1) This is the exact analogue of the discrete case, and it is exactly the right way to think in more advanced theory where much more general 'hazard functions' are considered. We interpret (K1) in the obvious way: IP(t — dt < T < t) \u0000 f(t)dt h(t)dt = \u0000 — IP(T > t — dt) \u0000 R(t) ' where f is the pdf of T, so that f (t) = F'(t) = — R' (t). Putting things together, we have R' (t) \u0000 d t \u0000 h(t) = \u0000 — dt In R(t), R(0) = 1, R(t) so that, for t > 0, \u0000 t \u0000 t R(t) = exp { — f h(s) ds} , \u0000 f (t ) = h(t) exp { — fo h(s) ds} . o 82 \u0000 4: Conditioning and Independence These are the analogues of the 'discrete' results in the previous subsection. If h is constant, h(t) = A for t > 0, then T has the exponential E(A) distribution with pdf .fie—At/[000)(t). It is usually assumed for failure of components that the hazard function increases with time. Important cases are the Weibull(a, b) distribution in which, for t > 0, R(t) = expf—atbl, \u0000 h(t) = abtb-1, with b > 1, and the shifted Weibull for which it is assumed that T is definitely greater than some to and that T — to is Weibull(a, b). ►► L. Lack-of-memory property of the exponential distribution. Let T have the exponential E(A) distribution. We always think of this distribution via its reliability function: R(t) := IP(T > t) _ e—At. The fact that T has constant hazard function is reflected in the crucial 'lack of memory' property of the exponential distribution which I now explain. Suppose that s > 0 and t > 0. Then Hence, IED(T >t±sIT> s) := = P(T > s; T > s ± t) IP(T > s) IP(T > s + t) \u0000 e —A(t+s) IP(T > s) \u0000 e—As = e — At. IP(T — s > t I T > s) = e —at = IED(T > t). In other words, if we are waiting for time T to occur, and we have already waited up to current time s without T's having occurred, then (conditionally on this information) the further time T—s which we have to wait has the same distribution as the time we have to wait from the beginning! The variable has no memory of the fact that a time s has already elapsed. The 'lack of memory' property is somewhat counter-intuitive — especially in forms which we shall meet later. The exponential distribution is as important to probabilists as the normal distribution. ►► M. The recursive nature of conditioning. This important topic adds greatly to the power of the conditioning idea. This will be illustrated in the subsection 840 below. We consider Bayes' Theorem in which the known event K has the form K = Ki n K2 n ... n Kn. 4.1. Conditional probabilities \u0000 83 Imagine that an experiment is performed in n stages, and that Kr represents information about the rth stage. For an event H regarded as a 'hypothesis', we wish to calculate P(H K) by calculating recursively: Po (H) := P(H), \u0000 (H) := P (H I ) P2 (in := \u0000 I \u0000 n K2) , etc. We know that Po (H)Po (Ki I H) ( H) = \u0000 cc Po (H)Po (Ki I H) • Po (KO In the above equation, we think of P0 (H) as the prior probability of H at the first stage, P(K1 I H) as the likelihood of K1 given H at the first stage, and P(H K1) as the posterior probability of H given the information K1 about the outcome of the first stage. We now want Pi (.) to play the role of the prior probability as we go into the second stage. So, it makes sense to define (Ki n A n P (B \u0000 n A) . CB I A) := 1111 p(11107) \u0000 P (Ki n A) We have P(1-1 I Ki n ic2) n K2 n H) P(Ki n K2) P(K, n H n K2) \u0000 P(K1)P(HIK1)P(K2IH n K1) p(Ki n K2) \u0000 P(Ki) P(K2 I xi) so that we do indeed have P2 (H) = (H)P1 (K2 P1 (K2) This is exactly the kind of recursion we were looking for, but do note that the 'likelihood' P1 (K2 H) at the second stage is P(K21HnK1). Extending this idea, we arrive at the following lemma. Our recursion takes the form: N. Lemma. (H) P2 (H) P3 (H) P(H)P (K1 I H) P (K1) P1 (HM (K2 I H) Pi (K2) 1P2 (H)P2 (K3 I H) P2 (K3) a P(H)P (Ki I H), a P i (H)? (K2 I H n K1) OC P2 (H)P (K3 I H n K1 n K2) . 84 \u0000 4: Conditioning and Independence Example. Suppose that in the 'Test for disease' problem, a person is chosen at random and tested twice for the disease, and that we have K1: the first test proves positive, K2: the second test proves positive. Each test, independently of the other and of whether or not the person has the disease, has a 90% probability of being correct. The recursive calculation is described in the Bayes table N(i). Hypothesis H: P(H) F (Ki H) El (H) El (K2 I H) P2 (H) P2 (H) Person is OC OC CC CC CX = OK 99 10 11 10 110 0.55 Diseased 1 90 1 90 90 0.45 Table N(i): Recursive calculation Of course, we could have used K = Kl n K2, and used K: both tests positive in the single calculation in Table N(ii). Hypothesis H: Person is I1(H) cc P(K I H) CC P(H K) o P(H I K) OK 99 0.12 99 0.55 Diseased 1 0.92 81 0.45 Table N(ii): 'Single' calculation The whole point of recursion is that the two methods agree. 0. A Bayesian change-point-detection filter. A machine produces items at a rate of 1 per minute. Suppose that the time T (in minutes) at which a certain crucial component in the machine fails is a discrete RV with constant hazard function c; that before time T each item produced by the machine has a probability pi of being defective; but that, from time T on, each item has probability p2 of being defective. We wish to find recursively Phappened[n] := P(T < n Tn) where Tn is the information available to us at time n, that is, the sequence E1, E2, • • • , En = ePs[1], ePs[2], •••, ePs[n] where Ek = 1 if the kth item is defective, 0 otherwise. Because the type of calculation now being discussed can only be done sensibly on the computer, I mix mathematical and programming notation freely. 4.1. Conditional probabilities \u0000 85 Actual T-value (vertical line) and observed Random Walk pi = 0.10, p2 = 0.18 pi -= 0.10, P2 = 0.30 c = 0.008 Probability T has happened given current information Maximum-height-scaled histogram for T given entire Random Walk ) L Figure 0(i): Change-point detection filter Write oldp[1] := pi , oldp[0] := 1 — oldp[1], newp[1] := p2, newp[0] := 1 — newp[1], For eps = 0 or 1, write mixedp[eps] := c*newp[eps] + (1 — c)*oldp[eps]. Thus Phappened[n] := P(T < n I Tn.) = numer denom ' 86 \u0000 4: Conditioning and Independence We visualize the way in which the available information evolves by plotting the graph of a Random Walk in which Walk[n] := (number of defectives by time n) - (n x oldp[1]). Before time T, the walk has no tendency to drift: each increment is on average 0. After time T each increment is on average p2 - How do we calculate Phappened [n] recursively? Suppose that we write P = Phappened[n - 1] and eps for the result regarding the nth item. We have the Bayesian table 0(fi). H: IP (-111.Fn-i) P(eps I H, Fn-1) P(// I Tn) = = CX T< n- 1 P newp Ceps] P*newp Ceps] T = n (1-P)*c newp Ceps] (1-P)*c*newp Ceps] T > n (1-P)* (1-c) _ \u0000 oldp Ceps] (1-P)*(1-c)*oldp[eps] _i Table 0(ii): Recursive filtering where numer := P * newp[eps] + (1 - P) * c * newp[eps], denom := numer (1 - P) * (1 - c) * oldp[eps] The above logic is built into the procedure Update 0 in the program which produced Figure 850(i). void Update(){ int k, eps; double W, P, numer, denom, temp; n++; if (n < T) eps = WithProb (oldp [1]) ; else eps = WithProb (newp [1] ) ; W = Walk[n] = Walk [n- 1] + eps - oldp [1] ; P = Phappened [n-1] ; numer = P*newp [eps] + (1.0 - P)*c*newp Ceps] ; denom = numer + (1.0 - P) * (1 . 0 - c) *oldp [cps] ; Phappened [n] = numer/denom; 4.1. Conditional probabilities \u0000 87 for(k=0; k<n; k++) OddsTeq[k] = OddsTeq[k]*newp [eps] ; OddsTeq[n] = OddsTeq[n] * mixedp[eps] ; for (k=n+1; k <= L; k++) OddsTeq[k] = OddsTeq[k]*oldp[eps] ; if (n == L){ maxOddsAfter = 0.0; for (k=0; k<=L; k++){ temp = OddsAfter[k] = OddsTeq[k]; if (temp > maxOddsAfter) maxOddsAfter = temp; } } } OddsTeq [k] represents odds that T = k given the current information Tn. Check out the logic for that part of the program. Figure 850(i) shows two runs, using the same seeds for comparison purposes, the left-hand side being with pi. = 0.10, p2 = 0.18, and the right-hand side with p1 = 0.10, P2 = 0.30. The value of c was 0.008. The value of T was 'cooked' to be 150, right in the middle of each run. The `Langlands' generator was used with seeds 3561, 2765, 3763. The histogram for T at the bottom plots the OddsAfter function scaled to have constant maximum height when, ideally, it should have constant area under the graph. The operation of estimation in real time is called filtering. Note that the filter is always very eager (too eager!?) to believe that T has already happened once the Random Walk shows any noticeable increase, and that for a long time, it is willing to change its mind very considerably. The fluctuations can well be more marked than in the example shown. This kind of program can be generalized very considerably and made much more sophisticated (for example, learning more about unknown parameters on each run on real-world data), and it has a huge variety of applications. I have studied some medical ones where one wished to detect as soon as possible growth of bacteria in a laboratory culture using a sample from a patient, but in which there was substantial 'noise' in the observations. In practice, the tendency of filters to fluctuate rather wildly needs tuning out. In our example, a crude way to do this would be to plot the minimum of Phappened [m] over a window n — a <m <n which moves with n. Of course, this would delay 'proper' detection. One has to strike a balance. P. Application of conditional probability to Genetics. Genetics provides an excellent area in which to practise your skill with conditional probability — as well as being of fundamental importance in the real world. Extensions of the arguments we use here are important in genetic counselling. We cheat in that we use independence (and even conditional independence) intuitively in this discussion. 88 \u0000 4: Conditioning and Independence We shall have to content ourselves with a very over-simplified picture of things, though one which conveys some of the main ideas. As stated in the Preface, helpful comments from Richard Dawkins and from Richard Tilney-Bassett made me revise substantially an early draft which was misleading in several respects. Any faults in this draft are of course mine. First, we take a very naive view, skipping most of the verbose terminology typical of Biology (and Chemistry). A little of the reality is mentioned in Subsection 94S. Every normal cell in one human being has the same genetic makeup. Gametes (sperms or eggs) are the exception. Each normal cell contains 23 pairs of 'chromosomes'. One pair of chromosomes is special, and is, amongst other things, the pair which determines the sex of the individual. In this subsection, we concentrate on the other 22 `autosomal' pairs of chromosomes. The story for the final pair is in subsection 91Q. Each of the pairs of chromosomes can be thought of as a string of beads, each bead on one chromosome being 'matched' with one on its companion. A matched pair of beads or sites such as in the diagram constitutes a locus. We focus attention on one particular locus on a particular chromosome pair. On each of the two sites forming this locus sits one of the two types (alleles), a or A, of the gene associated with that locus. The pair of genes at the locus forms one of the 3 genotypes aa, aA (indistinguishable from Aa), AA. Think of a gamete (sperm or egg) as being formed by randomly picking one chromosome from each pair of chromosomes in a normal cell, so that a gamete has 23 single chromosomes. This is, you understand, a mathematical picture of a process which, from any point of view, borders on the miraculous. When a sperm and egg fuse to create the first cell of a child, the 23 single chromosomes in the sperm will pair up correctly with the 23 single chromosomes in the egg to form a normal cell with 23 pairs of chromosomes. Each chromosome pair in Baby therefore consists of one chromosome obtained from Mum and one obtained from Dad. Important Note. A chromosome 'obtained from Dad' need not be identical to one of the chromosomes in Dad's normal cells. See discussion of crossing-over in Subsection 94S below. For the one pair of genes at one locus, the focus of our current study, this does not affect probabilities. ❑ For the particular locus in which we are interested, we have, with {•, •} signifying unordered pair, the probabilities in Table P(i) for Baby's genotype given the genotypes of Mum and Dad. The second column in the table will be explained below. 4.1. Conditional probabilities \u0000 89 Hypothesis H: {Mum,Dad} Hardy— Weinberg P(H) ]P(aa I H) Baby P(aA I H) P(AA I H) { aa,aa} { aa,aA} {aa,AA} {aA,aA} { aA,AA} {AA,AA} p4 4p3q 2p2Q2 4p2Q2 4pq3 q4 1 -I- i 1 1 t- I Table P(i): JP(Baby {Mum,Dad}) We now assume random mating and equal fertility for genotypes. Thus, people choose mates entirely at random, in particular there being no preference for someone of the same genotype for our locus for instance. Genotypes for our locus have no effect on fertility. (This is of course very far from true for certain genes.) Suppose that we have a large population in which at what we consider to be generation 0, the fractions of aa, aA and AA genotypes are uf , 2v1, w f amongst females, um, 2v,„ Wm amongst males. Then the proportion of a genes amongst the female population is pf := uf + VI, and of A genes amongst the female population is q f = 1 — p f = v f +w f. We have the obviously analogous results for males. For a couple chosen at random from the 0th generation, the probabilities that their first baby will be of genotypes aa, aA and AA are respectively u := p f pm, 2v := pfqm + qfpm, w := q fqm irrespective of whether Baby is male or female. (The baby will be of genotype aa if and only if it inherits an a gene from its Mum (which has probability pf ) and an a type gene from its Dad (which has probability pm), and random mating makes these events independent; hence the p f pm. So, u, 2v, w will be the approximate genotype frequencies in generation 1 for both males and females. Hence, for each sex in the 2nd generation, we shall have approximate genotype frequencies p2, 2pq, q2 where p = u + v, q = v + w. And you can see that for the 3rd generation, the frequencies will be approximately the same as for the second; and so on. This is the famous Hardy—Weinberg law. But be very careful about the 'and so on'. Genotype frequencies will fluctuate randomly from their predicted values, and there is no restoring force. The Hardy—Weinberg law is a reasonable guide to the state of affairs for a very large population over a few generations. We now assume that genotype frequencies for our locus are, and have been for a generation or two, p2, 2pq, q2 for aa, aA, AA, 90 \u0000 4: Conditioning and Independence for both sexes. Check out the {Mum,Dad} probabilities in Table 89 P(i). For example, the {aa,aA} possibility is assigned probability 4p3q because two ordered pairs each with probability p2 x 2pq contribute to this probability. Note that the sum of the probabilities in the second column of Table 89 P(i) is (p q) 4, namely 1, as it should be. Always try to cross-check your calculations. Phenotypes, recessive and dominant genes. In some important cases, the a-type gene is recessive and aa-people are 'exceptional' in some good or bad (or neither!) way, while both aA and AA people are 'standard': one says that aA and AA people have the same phenotype because there is no observable difference. The recessive a gene in an aA person is dominated by the dominant A gene. [[For certain kinds of flowers, for example, an aa combination results in a white flower, whereas both aA and AA result in purple. (It is often stated that Mendel's work which started Genetics involved this situation with sweet peas, but one doesn't see much mention of this case in Mendel's papers! See Stern and Sherwood [217].) For other kinds of flowers (snapdragons, for example) , the aA combination can produce flowers of 'intermediate' colour: there is incomplete dominance. There are situations in which one has codominance, where an aA individual has characteristics of both the aa and aA individuals (for example, two types of antigen on blood cells). Whatever I could say about a certain human disease being caused by a recessive gene would be an oversimplification. Pa. Exercise. Consider the 'recessive gene' situation with genotype frequencies p2, 2pq, q2 as just described. John and Mary (who are not related) are standard. They decide to have two children — perhaps against the 'random mating' rule! Find — using the hints below — (a) the probability that their 1st child is standard (answer: 1+2P ) (l+p)2 ' (b) the conditional probability that their 2nd child is standard given that their 1st child is exceptional (answer obvious), (c) the conditional probability that their 2nd child is exceptional given that their 1st child is standard( \u0000 )answer- 3P2 • 4(1+2p) • Hints. Use the notation K: John and Mary are standard, S1: their 1st child is standard, E2: their 2nd child is exceptional. For (c), We wish to find P(K n S1 n E2)/P(K n S1). Let Hi (1 < i < 6) be one of the 6 'hypotheses' about {John, Mary}. See Table 89 P(i). Then P(K n S1 n E2) = EP(HOP(K n S1 n E2 I Hi). For a fixed couple, the division of chromosome pairs for the 2nd child is independent of that for the 1st child. More precisely, S1 and E2 are conditionally independent given Hi: P(si n E2 Hi) = (Si I Hi) IP (E2 I HO 4.1. Conditional probabilities \u0000 91 We can obviously extend this to include K because each P (K I Hi ) is either 0 or 1. Make a table with headings H P(H) P(H n K) P (H n K n Si ) (H n K n n E2) • The column sums of the last four columns are 1, P(K), P (K \u0000 P (K n S1 11 E2), respectively. Pb. Exercise. Again consider the recessive-gene situation we have been studying. Suppose we know that Peter and Jane (who are not related) are standard and their four parents are standard. (a) Show that, with the implicit conditioning here made explicit, 2p P(Peter is aA I he and his parents are standard) = 1 + 2p . (b) Show that the chance that the first child of Peter and Jane is exceptional is p2/(1+2p)2. Hint. Use Part (a) in a 'recursive' argument. Pc. Remark. Blood type in humans is controlled by the genotype at a particular locus, but in that case the gene has 3 types A, B and 0, not our 2 types a and A. However, AA and AO give rise to the same phenotype (blood group 'A') and BB and BO give rise to the same phenotype (blood group 13'). There are 4 phenotypes: 'A', 13', 'AB' and '0', the last corresponding to genotype 00. But there are much more serious complications than these facts indicate, in a full study of Genetics. Q. X-linked (or sex-linked) genes. In females, that 23rd pair of chromosomes is a proper pair with loci just like the 22 other pairs. Each chromosome in the 23rd pair in females is called an X-chromosome. For a gene with two types b and B associated with a locus on her 23rd chromosome pair, a female can be of genotype bb, bB or BB, just as before. In males, however, the 23rd chromosome pair consists of an X-chromosome and a quite different, much smaller, Y-chromosome. There is now no concept of a locus as a matched pair of sites for genes. The man will have either a b gene or a B gene on the X-chromosome in the 23rd pair, so his genotype is either b or B. We suppose for a gene associated with an X-chromosome that the gene frequencies are p2, 2pq, q2 for bb, bB, BB in females, and p, q for b, Bin males. If b is recessive, then a man will be exceptional if he has genotype b, a woman if she has genotype bb. [[One can show (assuming random mating, etc) that whatever the current genotype frequencies for males and females in a large population, something very close to a (7)2 7 2pq7 q2; p, q) genotype frequency situation will evolve in very few generations.]] 92 \u0000 4: Conditioning and Independence A child will be female if it inherits its father's X-chromosome, male if it inherits its father's Y-chromosome. In relation to the X-linked gene we are currently considering, a man cannot pass it to his son, but can to a grandson via a daughter. (This is the situation for the haemophilia gene, for example. The royal families of Europe provide a well- known demonstration. Queen Victoria was a carrier of the haemophilia gene, and indeed all of those royals infected were her descendants.) Qa. Exercise. (a) Show that the (p2, 2pq, q2; p, q) genotype frequencies for our X-linked gene are approximately carried forward to the next generation. Suppose now that we have the (p2, 2pq, q2; p, q) situation, and that our X-linked gene is recessive. A couple have two children, a boy and a girl. Use the result in Part (a) in showing that (b) the conditional probability that the boy is standard given that the girl is standard is (1 — IP2)/(1 + p), (c) the conditional probability that the girl is standard given that the boy is standard is 1 — 1p2. R. Pedigree analysis for rare diseases associated with one locus. we concentrate on diseases known to be associated with the situation at one locus. Let's begin with some comments which apply whether or not the disease is rare. In deciding whether or not the gene which causes the disease is recessive or dominant (even rare diseases can be caused by a dominant gene!), we can use the following principle: if a dominant gene is responsible for the disease and a child has the disease, then at least one parent must have the disease. In deciding whether or not the gene which causes the disease is X-linked, remember the principle: if a recessive X-linked gene causes the disease, then the daughter of a healthy father must be healthy. In pedigree diagrams, a square represents a male, a circle a female. An individual with a black number on a white background represents a healthy individual, one with a white number on a black background a diseased one. Consider diagram R(i)(a). The principles mentioned above show that the disease is recessive and autosomal (not X-linked). Note that individuals 1 and 2 must both be aA. (We equate aA and Aa.) Given this, it is true that, independently of the fact that individual 5 is aa, individual 6 has absolute probabilities 1, of being aa, aA, AA. However, this has to be conditioned by the fact that individual 6 is healthy. Check that, given the information in the diagram, the probability that the first child of 6 and 7 would be diseased is 1/9. Rare diseases. Now look at Figure R(i)(b) which relates to a rare disease caused by the situation at one locus. The picture is copied from Exercise 25 of Chapter 4 in the superb book by Griffiths et at praised earlier. Again it is clear that the bad gene is recessive autosomal. We wish to know the probability that the first child of individuals 21 and 22 would be diseased. Now, if that child were to be diseased, then both 21 and 22 must be aA. Since a 4.1. Conditional probabilities \u0000 93 (a) (b) (c) Figure R(i): Some pedigree diagrams alleles are rare in the population, we can assume that they have not been brought into the family by any of 8, 15, 16, so, we assume that each of these is AA. Ra. Exercise. Show that (under the above assumptions) the probability that the first child of individuals 21 and 22 would be diseased is 1/72. Rb. Exercise. Suppose that Figure R(i)(c) shows a pedigree for a rare disease. Do you think that the bad gene is recessive or dominant? Explain your answer. The examples in Griffiths et al are splendid brain-teasers; and of course, things get 94 \u0000 4: Conditioning and Independence much more interesting when you look at several genes rather than one. Are the two loci on the same chromosome? How likely is it that two genes on the same chromosome are `separated' by the crossing-over described in the next subsection? (Of course, this depends on how far apart they are on the chromosome.) And so on; and so on. Fascinating — even in the Human Genome age! S. Chromosomes, crossovers, etc: a bit of the real science. Genetics is immensely complicated. 'Essentially' in the following brief discussion will hide a multitude of complications. Life is based on DNA-type molecules. A DNA-type molecule has the celebrated double-helix structure discovered by Watson and Crick, and is therefore capable of the fundamental property of replication. Each helix is a sequence of 'bases': each base being described by one of the four symbols A,C,G,T (A for 'adenine' etc). In the double helix, A on one helix is always linked to T on the other; and C to G. We therefore have 'base pairs'. The double helix can therefore unwind into two single helices, each carrying the information to build its 'partner half' to form a new double helix. A chromosome is essentially a DNA-type molecule, which can therefore be replicated into two identical sister chromosomes. Essentially, each chromosome is a sequence of genes, so that a gene is (essentially) a portion of a DNA-type molecule. These genes are our earlier 'beads'. We know that 22 of these pairs of chromosomes (in a human) are autosomal pairs of chromosomes, each chromosome in such a pair being partnered by a homologue. Each gene on an autosomal chromosome is an allele of the gene at the other half of the locus on its homologous partner. The human genome of 23 pairs of chromosomes contains between 25, 000 and 40, 000 genes within a total 'length' of about 3 x 109 base pairs. Genes vary considerably in length. There are two types of cell division: one, mitosis, designed to replicate exactly the genetic material of a whole cell, producing 2 identical daughter cells from a normal `parent' cell; the other, meiosis, producing 4 gametes (sperms or eggs) from a normal cell. Mitosis is the more straightforward (but when it goes wrong, it can cause disease ...). Essentially, each chromosome is replicated and the daughter chromosomes migrate, each pairing up correctly with a homologue, to form at cell division the desired two daughter cells. Meiosis also starts off by duplicating each chromosome in a normal cell. Recall that the object is to produce 4 gametes, so that after each original chromosome is replicated, we have the correct amount of genetic material. However, in later stages of meiosis, it is possible for two non-identical but homologous autosomal chromosomes to cross over so that, for example, from the two gene sequences of the original chromosomes • • • Ili • • • 1.17-111,11,±1 • • • U n • • • V,V r +1 • • • V n • • • 4.1. Conditional probabilities \u0000 95 we obtain the new (recombinant) chromosomes • • • ul • • • 11,11.1,,V 7.+1 • • • V T, • • - • • • Vi • • • V 7._1V r Ilr +1 • • -11n • • ' • Crossovers can be more complex than this simple one involving two chromosomes. If one ignored the possibility of crossover, Mum could produce 223 different eggs, Dad 223 different sperms. When one takes crossovers into account, these numbers become immeasurably larger. (However, one still sees the 223 in many accounts.) A subsection of a chromosome is said to be a centimorgan (after the American geneticist T H Morgan) if there is a 1% probability that it will contain a crossover point. A centimorgan contains roughly 1 million base pairs, so there are about 3000 centimorgans in the human genome. The probability of crossovers during the production of a gamete is therefore very high indeed. T. Thoughts. Richard Dawkins has explained with unsurpassed clarity Darwin's insight that random events sieved in a non-random way by Natural Selection can explain most 'miracles of creation' such as the human eye. (I am still not sure quite how it explains the much greater miracle of Bach, Mozart and Beethoven!) Do read Dawkins' books The Selfish Gene, The Blind Watchmaker, etc, etc. So successful is Evolution that a very important application of Probability mimics it in random algorithms, genetic algorithms, neural nets, etc. (There are some words for Web searches!) A (classical) computer operating 'randomly' can solve deterministic problems forever outside the range of deterministic techniques, but a quantum computer could in some situations achieve much more still. In River out of Eden [57], Dawkins describes the crossover phenomenon with his customary vigour: 'A child's chromosomes are an unbelievably scrambled mishmash of its grandparents' chromosomes and so on back to distant ancestors.' His book is a veritable psalm (for atheists?!), full of poetic 'verses' which haunt the mind: 'The universe we observe has precisely the properties we would expect if there is, at bottom, no design, no purpose, no evil and no good, nothing but blind, pitiless indifference. ... DNA neither cares nor knows. DNA just is. And we dance to its music.' As Dawkins has pointed out, there is a sense in which we are now beginning to break free from the tyranny of Natural Selection: we can 'adjust the musical score', valuing and protecting the weak, seeking to eliminate certain diseases, etc, all without entering in a type of brave new world about which I would have as many qualms as anyone. Apology. I could not have done justice to Genetics in this small space even if I'd had the expertise. I have not explained what constitutes a gene within a chromosome (or chromosome pair), and I have not explained about alleles, about homologous chromosomes, etc. When I finish this book, learning more about Genetics will be a high priority, though not as high a one as trying at long last to learn to play Bach, Mozart and Beethoven acceptably well. Those guys did have some very remarkable genomes. Millions of extraordinary 'ordinary' people have very remarkable genomes too. 96 \u0000 4: Conditioning and Independence 4.2 Independence ► A. Independence for 2 events. Two events A and B are called independent if (and, since this is a definition, only if) ]P(A n B) = P(A)P(B). \u0000 (Al) \u0000 We have already seen the motivation, recalled here. If P(A) \u0000 0 then A and B are independent if, and only if, P(B A) = P(B), that is, if and only if knowledge that A has occurred does not affect our degree of belief that B has occurred. (You give an explanation which will satisfy a Frequentist.) Of course, the definition is symmetric in A and B. Equation (Al) is the first of many 'Independence means Multiply' Rules. Note that it may be written E (/A/B) = E (IA) E (/B) and that (still assuming that A and B are independent) we have, for example, (Ac n Be) = E {(1 — /A) (1 — /B)} — E (1 — /A — \u0000 IAIB) = 1 — P(A) — P(B) P(A)P(B) = {1 — P(A)}{1 — 11P(B)} = P(Ac)P(Bc). The general moral is that if A and B are independent, then so are AC and BC; likewise for AC and B; and for A and BC. ►► B. Independence for a sequence of events. Suppose that A1, A2, . is a finite or infinite sequence of events. Then A1, A2, ... are called independent if whenever ii , i2, . . . , i,. are distinct elements of N such that each Ai, is defined, then P (Ai, n Ai2 . . . n Air) = \u0000 (Azi )1[1) (Az,)...P (Air ) . \u0000 (B1) By using indicator functions as in the case of 2 events, we can prove that one can put complement symbols on any sets on the left-hand side of (B1) provided, of course, that one does the same on the right-hand side. The '7r-system Lemma' from Subsection 45M may be used to do such things much more neatly. ►► C. Discussion: Using independence to assign probabilities. This too we have seen before. If we toss a coin with probability p of Heads, q = 1 — p of Tails n times, then outcome w = HTT H is assigned a probability P(w) = pqq . . .p 4.2. Independence \u0000 97 because we believe that the results on different tosses are independent: results on one subset of tosses do not influence those on a disjoint subset of tosses. We take for C2 the Cartesian product {H, Tr of all sequences w of length n of H's and T's. We assign the appropriate probability based on independence to each w, thereby defining a 'product measure' on the Cartesian product. Define Xk (w) = 101 if wk = H, if wk = T. Then E (Xk) = 1 x P(Xk = 1) + 0 x (Xk = 0) = p. (We take this as intuitively obvious; strictly speaking, one ought to justify for the sake of consistency that the sum of all P(w) values with wk = 1 does equal p, but we skip this.) If Y is the total number of Heads, then Y = + X2 ' Xn E (Y) = E (X1) + E (X2) + • E (Xn) = P -FP+ -Fp = np. (You are reminded however that the Addition Rule does not require independence.) This coin-tossing case typifies the way in which we use independence to assign probabilities — at least in simple situations. Independence is always closely associated with 'product measure on Cartesian products'. We do not pursue this here. Ca. Exercise. Suppose that we throw a fair coin twice. Let A = 'H on 1st toss', B = 'H on 2nd toss', C = `111-1 or TT'. Prove that A and B are independent, B and C are independent, C and A are independent, but that A, B and C are not independent. D. Exercise: The 'Five Nations' Problem*. This is a good exercise for you to puzzle out, easy once you spot the way to do it. Each of five people A, B, C, D, E, plays each of the others (making 10 games in all) in a fair-coin-tossing game: for each game, independently of the others, each of the two players involved has probability 2 of winning. Find the probability that each wins two games. [[Someone asked me this after it happened one year that each team in the 'Five Nations' Rugby Championship won two games. This was in the period between the great days when Wales were incomparable and the modern era when England and France dominate the Northern Hemisphere. We have now welcomed Italy into the 'Six Nations' Championship.]] 98 \u0000 4: Conditioning and Independence ► E. The Second Borel— Cantelli Lemma. This result helps us understand many things, including what can 'go wrong' for the Strong Law for variables not in G1. The result says the following. Lemma. Suppose that J1, J2, . . . is an infinite sequence of independent events such that EF(J.), Then the probability that infinitely many Jr, occur is 1. Ea. Exercise. Prove that under the assumptions of the Lemma, IP(none of the events J -1, - ./ 2, • • • , J r, occurs) < 1 +Pi +P2 + •-• Pn Hint. Let pk := P(4). Calculate the probability in (El) exactly. Then use the facts that for 0 < p < 1 and x, y > 0, 1—p < \u0000 ±p , (1+x)(1+y) > 1-Ex+y. Proof of the Lemma. Inequality (El) makes it clear that the probability that none of the events Jk occurs is 0. But we can just as easily show for every 71 that llp(lin ) = 1 where fin is the event that at least one of the events J - n+1, 4+21• occurs. Measure Theory (Lemma 44G) now clinches the fact that the intersection n \u0000 of all the events Hn has probability 1; and this is just what we want. \u0000 ❑ Eb. Exercise. Give another proof based on the inequality at 140b. Question. Why would the Second Borel—Cantelli Lemma be obviously false if the independence assumption were dropped? ► ► F. Independence of Random Variables. Random Variables X1, X2, . . . in a finite or infinite sequence are called independent if whenever il , i2, \u0000 , it are distinct numbers such that each Xik exists, and xi,, xi2 , \u0000 , xi,. E R, we have P (Xi, < xi, ; Xi2 < xi2; \u0000 Xi, < xir ) = P (Xi, < xi, ) P (Xi2 < xi2 ) \u0000 P (Xi, < xi,,) If X1, X2, . . . are discrete, this is equivalent (assumed fact) to the statement that whenever il , i2, , it are distinct numbers such that each Xi, exists, and xi1,x ~2 ,...,xir E Z, we have P (Xi, = x1,1; Xi2 = x22; \u0000 = Xir ) = \u0000 = xi,) P ( Xi2 = xi2 ) P (Xir = xir ) . 1 (El) 4.2. Independence \u0000 99 One can prove (by the 7r-system Lemma mentioned earlier) that if X1, X2 7 ... are independent, if 1 < m < n and U and V are nice (`Borel measurable') functions of Rm and Rn m respectively, then { (xi, x2, • • • , X m) E U; (Xm+17 Xrn+27 • • • Xn) E V} = IID { (xi, X27 • • • Xm.) E U} I {(xm+i, xn,,+2, • - • Xn) E V} as common sense suggests. The following Fact is a consequence, and we make much use of it. ► G. Fact. \u0000 If X1, X2 7 . . . are independent RVs and fl, f2, . .. are nice (technically, Borel measurable) functions on Ill, then fi (Xi) , 12 (X2) , - are independent RVs. Moreover, if f is a nice function on R n , then f (X1 7 X2, . Xn) Xn+17 Xn+2, ... are independent. This result is assumed. ► H. Sampling with replacement. Suppose that we take a sample of size n with replacement from the N people, numbered 1, 2, ..., N in their alphabetical order, who live in Bath. 'With replacement' signifies that after choosing our first person at random, we make the choice of the next from the entire population; and so on. We may therefore choose the same person many times. It is 'obvious' that the heights X 1, X2, . . Xi, of the n people we choose (listed in the order in which we choose them) are independent RVs; for, since we are sampling with replacement, no choices can influence any others. To model the situation, we take for Q the Cartesian product {1, 2, ... , N} n of all Nn ordered n-tuples CLI — (4-0 11 0-12) • • • Wm), \u0000 each wk in {1, 2, ... , N}. To each w E Q we assign a probability JP(w) = 1/N'2. Now let Zk (w) \u0000 Wk, so that Zk represents the kth person chosen. Then, if w = (z1, z2, • • • , zn), P(Z1 = z1; Z2 = z2; ... ; Zn = Z n ) = P(W) = N = P(Z i = zi)P(Z2 — z2) • • •P(Zn — Zn) • n Since, for fixed k, there are N n-1 values of w with Zk (w) = zk, we do have the 'obvious' fact that TP(Zk = zk ) = k. We see that, as anticipated, Z1, Z2, . \u0000 Z n are independent. If h(i) is the height of person i, then Xk = h(Zk), and by Lemma G, X1, X2, \u0000 , Xn are independent. The independence of X1, X2, . , X n is, as we shall see, a highly desirable theoretical property, so sampling with replacement is almost always assumed in theoretical studies. In practice, we sample without replacement and rely on the fact that n << N to say that the difference is negligible. See Exercise 71Kb for a partial study of sampling without replacement. 100 \u0000 4: Conditioning and Independence ► ► I. The `Boys and Girls' problem. This problem makes a start on rather counter- intuitive properties of Poisson distributions and processes which will reappear in our later studies of both Probability and Statistics. It is our first look at surprising independence properties. Ia. Exercise. The number of births during a day at a hospital is assumed to be Poisson with parameter A. Each birth is a boy with probability p, a girl with probability q = 1— p, independently of other births (and of the total number of births). Let B be the numbers of boys, G the number of girls, born during the day. Prove that =- PU3 e — AP (4) 6 e- Aq (AO b, G = g) = \u0000 b! 9. and deduce that B has the Poisson distribution of parameter Ap, G has the Poisson distribution of parameter Aq, B and G are independent. [If there are on average 20 births per day and on one day 18 boys are born, you would still expect on average 10 (not 2) girls to be born on that day — a bit weird!' J. Exercise: A 'number-theoretic' problem. This is a good problem from which to learn Probability, so do it even if you are the unique person who doesn't like Number Theory a heck of a lot more. Recall that a prime element of N is one of 2,3,5,7, . .., my point being that 1 is NOT a prime. Let s > 1, and choose a random number X in N according to the Euler(s) (or Dirichlet(s)) distribution: S P(X = n) = c(s) (n E where is the famous Riemann zeta function defined by C(S) := n=1 the series converging for s > 1. [Remark. The most important unsolved problem in Maths is that of proving the Riemann Hypothesis about the behaviour of ((s) where s ranges over the set C of complex numbers. The function 'blows up' (has a 'simple pole') at s = 1 but extends to a very nice (`analytic') function on the remainder of C. Amazingly, the function on C has a strikingly simple probabilistic definition — not given here. The Riemann Hypothesis states that all infinitely many non-trivial zeros of lie on the line R(z) = (The 'trivial' zeros are at the negative even integers —2, —4, —6, .. .).] Let Em be the event ' X is divisible by (a) Prove that P(Em ) = m' for m E N. (b) Prove that the events (EP : p prime) are independent. Hint. If pi and p2 are distinct ►►► K. Theorem: 'Independence means Multiply for expectations'. Let X1, X2, , Xrt be independent RVs, each in CI. Then Xi X2 Xn E and E (XiX2...X„) E (Xi)E (X2) .. -E (Xn) .41 \u0000 ,rr 4.2. Independence \u0000 101 primes, then a number is divisible by both p1 and /92 if and only if it is divisible by plp2; and similarly for more than two distinct primes. (c) By considering n(Epc), prove Euler's formula 1 = \u0000 - ) p prime S C( ) • Don't worry about rigour here: Lemma 43D provides that. (d)* Let Y, independent of X, be chosen with the Euler(s) distribution, and let H be the highest common factor (greatest common divisor) of X and Y. Let B p (p prime) be the event that 'both X and Y are divisible by p'. How does the event n(Bpc) relate to H? Prove that H has the Euler(2s) distribution. Proof (This result is really an application of Fubini's Theorem in Measure Theory.) Suppose that X and Y are RVs each taking only finitely many values. Then x- =Exipc=x l , E (X) = \u0000 x P(X = x), Y = y Ify=0, E(Y) = EYP( T = Y)• Then XY = EE xy f x=z1 y=y} = EExy/ fx=x;y—o , and E (XY) = EE xylP(X = x; Y = y) =EE xyIP'(X = x)P(Y = y) (by independence) = (E xP(X = x)) (Ey1D(Y = y)) = E(XY). The Monotone-Convergence Theorem 60D is now used to extend the result to yield E (XY) = E (X )E (Y) < for arbitrary non-negative RVs. The linearity of expectation now yields the desired result for two arbitrary variables in L'. Induction on the number of variables completes the proof. \u0000 ❑ 102 \u0000 4: Conditioning and Independence ►► L. Lemma: Variance of a sum of independent RVs. Suppose that X1, X2, . , X T, are independent RVs, each in L2. Then Cov (Xi, \u0000 = 0 (i j), Var (X1 + X2 ± • • + Xri) = Var (X1) Var (X2) ± • • ± Var (Xn) Proof. This is now trivial. We have, for i j, using Theorem 101K, Cov (Xi, X3) = E (XiX 3) — E (Xi) E (X 3) _)E (Xi) E (X3) — E (Xi) E (X 3) = O. The War' result now follows from the general addition rule for variances, Lemma 70K. M. Proving almost- sure results. We can prove for random sequences results of remarkable precision on how they must behave 'with probability 1', results which come close to defying our notion of randomness. I give here a simple example. Suppose that X1, X2, ... are independent RVs each with the exponential E(1) distribution. Thus, P(Xk > x) =- We know that the series 1 \u0000 1 \u0000 1 Tic \u0000 n(lnn)c' \u0000 n(ln n) (lnln n)e all converge if c > 1 and all diverge if c < 1. Let h :N \u0000 (0, 00), and define \u0000 Jn := \u0000 h(n)}. By using the two Borel—Cantelli Lemmas 45K and 98E, we see that if h(n) = cln(n) or h(n) = Inn + c In In n or h(n) = Inn + In In n + c In In In n, then, in each case, IP(infinitely many in occur) = { 0 if c > 1 1 if c < 1. If you know about lim sups, you can see that we can prove for example that each of the following statements holds with probability 1: Xn \u0000 Xn — Inn \u0000 lira \u0000 Xn — In n — In In n = 1; \u0000 rn sup \u0000 lim sup Inn = 1; lim sup \u0000 = 1; lnlnn \u0000 ln ln ln n etc, etc. Moreover the statement that all of these statements are true has probability 1. You can see why it's all a bit worrying — and there are still more amazingly precise 'with probability 1' statements. Of course, results such as those just described cannot be illustrated by simulation. Statisticians might well add that they cannot be seen in the real world either! 4.3. Laws of large numbers \u0000 103 4.3 Laws of large numbers ►► A. IID (Independent, Identically Distributed) RVs. By a (finite or infinite) sequence of IID Random Variables X1, X2, ..., we mean a sequence of independent RVs each with the same distribution function F: P(Xk x) F(x) for all k and all x. Then, if one Xk is in Gi, so are they all, and they all have the same mean (say); and if one Xk is in .C2, so are they all, and they all have the same variance U2 (say). B. Fact. Suppose that Xi, X2, . . . are IID Random Variables with the same continuous distribution function. Then, with probability 1, the Xk's take distinct values. This result, useful in many situations (see the later study of Order Statistics, for example) is included at this stage to allow you to do two important exercises (which are not unrelated!). C. Exercise: Renyi's 'Record Problem'. Suppose that X 1, X2, . . . are IID Random Variables with the same continuous distribution function. Say that a record occurs at time 1, and write E1 := Q. For n = 2, 3, ..., let E n be the event that a record occurs at time n in that Xn > Xn, for every m < n. Convince your teacher that the events El , E2 . . are independent, En having probability 1/n. You may assume the intuitively obvious fact that if one arranges X1, X2, ... X n in increasing order as X mi,n), X N(2,n), - \u0000 X N(n,n) then N(1, n), N(2, n), \u0000 , N(n, n) will be a random permutation of 1, 2, ... , n, each of the n! permutations being equally likely. Think about how this fact may be proved. D. Exercise: The 'Car Convoy' Problem. Cars travel down a one-way single-track road. The nth driver would like to drive at speed Vn, where V1, V2 , . . . are independent random variables all with the same continuous distribution function. (With probability 1, all the Vk will be different.) Cars will get bunched into 'convoys'. If V2 > V1 > V3, then the first convoy will consist of cars 1 and 2, and will be of length 2. Let L be the length of the first convoy. Find JP(L = n), and deduce that E (L) = oo. How would you explain to the layman that this is not crazy? Later on, you will be asked to prove that the probability that the nth convoy consists of precisely 1 car is O n. Any ideas now? 104 \u0000 4: Conditioning and Independence E. Lemma: Mean and variance of Sample Mean. Let Xi, X2, • • • , Xn be HD Random Variables in .C2, and hence with common mean u and common variance o.2. Define (S signifying 'sum', and A 'average') Sn \u0000 + X2 + • • X n7 X, + X2 + • + X„, \u0000 Sn Then 02 E (Sn) = np, Var (S,,) = no , E (An) = j, Var (An) = — n Proof By the linearity property 62Ha(a), we have (without assuming independence) E (Sn) = E (Xi + X2 ± • • ' Xn) \u0000 E (Xi) + E (X2) + • • • + E (Xn) = tt+kt+ .--Pkt = E (An) = E ( 1-isn) = ;.1-1 E (Sn) = µ. ByLemma 102L, now using independence, Var(Yn) = Var (Xi) Var (X2) + • • • + Var (Xn) and, by the first property at 67(D3), Var(Yn) = 1 77 Var(S„„) = n . = no-2, 0 Statisticians call An the Sample Mean of X1, X2, . . , Xn, and usually denote it by X. (You will see later why I am using capital letters!) This does not show the explicit dependence on n – not a problem is Statistics in which sample size is generally fixed, but certainly a problem in Probability. Ea. Exercise. Let Yn be the total number of Heads in n tosses of a coin with probability p of Heads. Show that Var(Yn) = npq. Please do this without a binomial coefficient in sight! Show that the maximum value of pq is 1. ►► F. Exercise: Unbiased Estimator of Sample Variance. Suppose that 4.3. Laws of large numbers \u0000 105 X1, X2, ... are IID Random Variables each with mean p and finite variance 0-2. Let ± X2 ± • • • ± Xn X := \u0000 (F1) n RSS := E (X, _ X) 2 = E x-z _ nx2, (F2) k=1 \u0000 \\k=1 the last equality being a Parallel-Axis result. We use `RSS' for Residual Sum of Squares' — for reasons which will appear later. Using E (X 2) = E (X) 2 Var(X), prove that nRSS i E (V) = a2, where V Dividing by n —1 rather than by n is sometimes called `Bessel's correction'. Contrast the division by N in the 'population variance' in Exercise 68F. We use V as a Variance Estimator for a2 partly because it is unbiased: as you see, it is right on average. However, there are much better reasons to do with 'degrees of freedom', as we'll see later. It is not all clear that being right on average is that good a thing when you are dealing with just one sample. This is one of many situations in which criticisms once levelled by Frequentists against Bayesians apply with greater force to things on which they themselves used to concentrate. G. Discussion. Suppose that X1, X2, ... is an IID sequence in ,C2, each Xk having mean p and variance a2. Then, for large rt, An has mean p and small variance u 2 /n, so that the distribution of An is 'concentrated around p'. We see the possibility of proving various rigorous assertions of 'convergence of An to p' in our mathematical model. First we make precise the notion that if an RV Z in .C2 has small variance, then there is small probability that Z differs from its mean by a significant amount. The name of the discoverer of the celebrated result which follows has more English transliterations than Tchaikowsky: he features as Cebysev, Tchebychoff, . ► ► H. Tchebychev's Inequality. Let Z be an RV in L2 with mean p. Then, for c > 0, c2p(iz \u0000 < VaT(Z). 106 \u0000 4: Conditioning and Independence Proof Let F be the event F := {w : 1Z(w) — //1 > c}. Then, for c > 0, \u0000 (Z — /1)2 > C21-F, \u0000 (H1) as I'll now explain. Inequality (H1) says that for every w, (Z (w) — p) 2 > c2IF(w). \u0000 (H2) Why is this true? Well, if w e F, then left-hand side > c2 by definition of F, right-hand side = c2 by definition of IF. If w 0 F, then left-hand side > 0 because it is a square, right-hand side = 0 by definition of IF. Hence (H2) is indeed true for every w. Taking expectations at (H1), we obtain Var(Z) = E (Z — 1421 > c2P(F) \u0000 Ap,(1Z as required. \u0000 ❑ Ha. Exercise. (See Exercise 104Ea.) Let Y be the number of heads obtained if we toss a fair coin 100 times. Use Tchebychev's inequality to obtain the bound: P(41 < Y < 59) = P(1Y — 501 < 10) > 0.75. Notes. The Central Limit Theorem (done later) gives the approximation P(41 < X < 59) = P(40.5 < X < 59.5) \u0000 .1)(1.9) — 43.(-1.9) = 0.9426. For this problem, the exact value (to 5 places of decimals) is 0.94311. (Even though Tchebychev's inequality is quite crude, it is good enough to allow us to prove the Weak Law of Large Numbers and other important results.) Prove that if Sr, is the number of Heads in n tosses of a coin with probability p of Heads, then \u0000 (1n-1Sn — p1 > b) < \u0000 1 472(52 ► I. Exercise: Markov's Inequality. This inequality is more fundamental than Tchebychev's. It is possible to get tight bounds by using it in clever ways. Prove that if W is a non-negative RV in .C1 and c > 0, then (for non-negative W and c > fl) f clF'( [Hint. What do you choose for the event F?] Why does Markov's inequality imply Tchebychev's? \u0000 4.3. Laws of large numbers \u0000 107 J. Theorem: Weak Law of Large Numbers (WLLN). Suppose that X1, X2, . . . is an infinite sequence of IID Random Variables, each in L2, each with mean 1.1 and variance o-2. Let Xi ± X2 ± • • ' Xn \u0000 An := \u0000 Then, for every fixed E > 0, \u0000 P(lAr, — \u0000 n Proof By Lemma 104E and Tchebychev's inequality, 0_2 \u0000 0.2 E2IP(1An-ILI >E) < \u0000 , P(1A„ - > E) 5 nE2 \u0000 The desired result is now obvious. \u0000 ❑ K. Discussion. The conclusion of the WLLN is expressed by saying that 'An —> in probability': Sample Mean converges in probability to true mean. The Strong Law of Large Numbers (SLLN) is in many ways a much more satisfying result for \u0000 mathematicians: it says that the set of w for which An (w) \u0000 in the true sense of convergence you meet in Analysis has probability 1; and it does not need the finiteness of a2. One has to be careful about whether the WLLN or the SLLN or the later Central Limit Theorem (CLT) is really what corresponds to the long-term relative frequency' idea in the real world, at least as far as Statistics is concerned. In the real world, we do not take infinite samples; and good estimates of how far, for example, Sample Mean is likely to be from true mean, or how far the Empirical Distribution is likely to be from the true distribution (see later), is what matters to a statistician. (The business of getting such estimates, however, belongs to Probability.) I want to take you through to a clearer statement of the SLLN in a way which, I hope, you can understand intuitively even if you have not read Section 2.3 on Measure Theory. The proof of the general case of the Strong Law is too difficult for this book, but we shall see why the result is true in a class of situations which covers nearly all those met in practice. First, however, we see Tchebychev's inequality in action in other important contexts, some important in Statistics, and one relevant to Mathematics. ► L. Order Statistics. \u0000 Let r and s be positive integers. Suppose that Ui, U2, .. , Ur±s+1 are IID Random Variables each with the U[0, 1] distribution. By Lemma 103B, with probability 1, no two of the U's are equal. We may therefore rearrange the U's in ascending order as the Order Statistics V1 <V2 < • • • < Vr+s-Fl• 108 \u0000 4: Conditioning and Independence We want to find the pdf of Vr±i . We have, intuitively, r+s+1 E Equ, E dv; Ak), k=1 where Ak is the event that precisely r of the r + s values \u0000 Uk_i, Uk+i, \u0000 , Ur+ s+1 are less than v. Because Uk does not feature in the event Ak, the event {Uk E dv} is in- dependent of the event Ak. Moreover, Ak is the chance that if we throw a coin with probability v of Heads r + s times, then we shall obtain precisely r Heads. Thus, 11D (Vr+1 E dv) = (r + s + 1) (r + s) vr(1 — v)sdv = r!s! (r + s + 1)! vr (1 — v)'clv, and since this must integrate to 1, we have proved formula 79(G3). In the terminology of Chapter 6, V,. has the Beta(r + 1, s + 1) distribution. La. Exercise. Prove that r + 1 E (Vr+t) r + s + 2 Var (14+0 = (r + 1)(s + 1) (r+s+2) 2(r+s+3) . 1, TT _ - U2 , • • • , U2r+1 M. Convergence in probability of Sample Median. Let be IID Variables, each U[0, 1]. Since llp(Uk < = P(Uk \u0000 = we say (see Subsection 72L) that 2 is the true median of each Uk. Let V1 , V2, ... V27.+1 be the U's arranged in increasing order, the Order Statistics. Then the 'middle' value M := Vr±i is called the Sample Median of the sample U1, U2, .. • , U2,±1. By Exercise La with s = r, we have 1 E (M) =2Var(M) = 4(2r + 3) • Tchebychev's inequality would now make precise the idea that if r is large, then the Sample Median M is likely to be close to the true median 1. The details are now obvious, if you wish to supply them. Now suppose that X1, X2, ... X2 7.+1 are IID Random Variables with strictly positive pdf f on R. Let m now denote the true median of a typical X, so that F(m) = Z , where F is the common distribution function of the X's. Now U1, U2, • • , U2r+1, where Uk = F (Xk), are IID Random Variables, each with the U[0, 1] distribution. See Subsection 50B and Lemma 99G. Let Y1, Y2, Y2r+1 be the Xk's arranged in increasing order, the Order Statistics for the X's. Then the Order Statistics V1, V2, • • • , V2T+1 for the U's are given by Vk = F(Yk). Hence, F (Yr+i) is likely to be close to 2, whence, by continuity of F 1, the Sample Median Yr±i of Xi, X2, , X2r+1 is likely to be close to m. If you like Analysis, formulating this precisely will cause you no difficulty (the proof of the Weierstrass approximation in Subsection 0 below will help); if you don't like Analysis, you will be happy to see things intuitively. See Subsection 165L for a much stronger result. (14-1-1 E dv) = 4.3. Laws of large numbers \u0000 109 N. Using the Sample Median in Statistics. A situation in which the results of the previous subsection become useful is the following. One of the infamous Cauchy distributions has pdf f (x) = 7T (1 ± (X — 0)2) . Suppose that we are interested in estimating B from a sample. As mentioned earlier, the Cauchy distribution has no mean: a variable with this distribution is not in G l . The Sample Mean of 2r + 1 HD observations from this Cauchy distribution does not concentrate: it has the same Cauchy distribution as a single observation. (As already mentioned, we shall see how to prove such things later.) Thus the Sample Mean is here useless for estimating O. However, 9 is the true median of each X, and by the previous subsection, for large r, the Sample Median M is likely to be close to O. So, we can use M as an Estimator for 9. See Note 196Ec for how to do this with precision. 0. Probabilistic proof of the Weierstrass approximation theorem. I mentioned earlier that Probability may be used to prove results in other branches of Maths. Here's an example. The Weierstrass approximation theorem, a fundamental result in Analysis, states the following: Let f be a continuous function on [0, 1]. Let E > 0 be given. Then there exists a polynomial B (called after Bernstein) such that If(x) - B(x)i < E for all x in [0, 1]. Proof. Let p E [0,1] and let Sn be the number of Heads if a coin with probability p of Heads is tossed n times. Then Bn(p) := E { f (n-I-Sn)} = E f (n-l s) (S n = s) E f (n-1 s) ( n)psqn 3=0 so that Bn is a polynomial of degree n in p. We have Bn(P) f(P)1 = 1E lf (71-1Sn) f(P)} 1 - We need to exploit with precision the fact that for large n, n —iSn is likely to be close to p; especially, we need our bound on the left-hand side to hold simultaneously for all p. You can see the possibility of making the proof work now. If you like Analysis, read on; if not, jump to the next subsection. We can draw two standard conclusions from the fact that f is a continuous function on the compact (in the present context, closed and bounded) interval [0, 1]. First, f is bounded: for some K > 0, If(x)1 < K for all x in [0, 1]. 1 110 \u0000 4: Conditioning and Independence Second, f is uniformly continuous on [0, 1], whence, for our given e, we can find, and fix, (5 > 0 such that x, y E [0,1] and Ix — yj < 6 imply that 11(x) — f(y)I < 2 E.. Write Wn := If (n-l Sn) — f(P)1, \u0000 zn := In-1S. — PI. Then Zn < 8 implies that Wn < ze. Let Fn := {w : Zn (co) < 8}. Then we have just seen that (for every w) /FWn < 0 (think!). Since 0 < Wn < 2K for every w, we have / F1 Wn < 2K/F, for every w. Using at the first step the result that 'the modulus of an expectation does not exceed the expectation of the modulus' (see 61F), we have, for every p in [0, 1], 1137(P) — f (P)1 < E (Wn) = E (iF„ Wn) -1- E (IF;Wn) < 16 ± 2KE (/F,0 = 4' e ± 2KP (Fric) = 1 e ± 2KP ( Zn > 6) < 26 + 2K 4n62 ' the last inequality from Exercise 106Ha. Hence, for n > no, where no is the least integer with 2K/(47/62) < 1r, we have I Bn(p) — f (p)1 < E for all p in [0, 1]. \u0000 I=1 ►► P. A model for tossing a fair coin infinitely often. Advice. Read this through the first time skipping the small-font indented sections; then read it all. The model we use is the Fundamental Model for choosing a point U in [0, 1] at random according to the uniform U[0, 1] distribution. So we take Si = [0, 1], U(w) := w. We define the unique probability measure on nice subsets of 12 which satisfies IP>([0, x]) = x for every x. [We want each V < x' where 0 < x < 1 to be an event: in other words we want each subset of Q of the form [0, x] to be an event. We take the smallest appropriate class (`o--algebra') of events which contains each such subset. That there is a unique measure on the class of events such that P([0, x]) = x for every x, is known. Section 2.3 contains more on all this.] Imagine expanding each w in SZ in binary: w = 0.(4.4w2w3 • • • (binary) and writing H for each 0 and T for each 1. [Each dyadic rational of the form r2' in [0, 1) has two binary expansions: for example, 1 =0.10000...=r0.01111... \u0000 (binary). However, the set D of such dyadic rationale satisfies P(D) = 0: the chance that Tyche would choose a point in B is exactly 0. The awkward points in B are not relevant. We can pretend that they do not exist — or even force them not to exist by working with 9 \\ D.] Ci4W2W3 0 000 1 001 2 010 3 011 4 100 5 101 6 110 7 111 8 8 \u0000 8 \u0000 8 \u0000 8 \u0000 8 \u0000 8 \u0000 8 \u0000 8 \u0000 g HHH \u0000 HHT \u0000 HTH \u0000 HTT \u0000 THH \u0000 THT \u0000 TTH \u0000 TTT Figure P(i): Coin tossing and the Fundamental Model Figure P(i) shows what happens if we divide It = [0, 1] into 8 equal intervals. Between 0 and 8, the binary expansion of w begins 0.000, leading to the sequence HHH. Thus each of the 8 outcomes of the first 3 tosses is assigned a probability 8 exactly as required by independence. The Fundamental Model contains every `finite fair-coin-tossing' model; and, really because of the uniqueness of P discussed earlier (really because of the 7r-system Lemma at 45M), it forms the essentially unique model for tossing a fair coin infinitely often. For define w = 0.w1co2w3 ... (binary), Xk (w) := wk, Xi (co ) + • - - + X n, (w) An (w) := \u0000 . n Thus An (w) is the proportion of Tails in the first n tosses for the realization co. ►► Q. Theorem: Borel's Strong Law for Coin Tossing. Let G be the set of ' good' outcomes w for which An(w) -4 in the usual sense of convergence in Analysis (explained in Clarification below). Thus, G is the set of outcomes for which the 'long-term relative frequency' (LTRF) idea works. Then G is an event, and P(G) = 1. For proof, see Subsection 113T. ►► Clarification. We put w in G if and only i ffor every c > 0 there exists an integer no (w ), which is allowed to depend on w, such that for Ti > no (w), we have IAn(w) — fl < E. Another way of expressing Borel's result is that the set N of bad outcomes w for which An (w) does not converge to 2 (the sequence may not converge at all, or converge to the wrong value) satisfies P(N) = 0. We can understand the geometrical significance 112 \u0000 4: Conditioning and Independence of P(N) = 0 in a way recalled here from Chapter 2. A (Borel) subset N of 9 = [0, 1] satisfies P(N) = 0 if and only if, for every E > 0 we can find a sequence of disjoint open subintervals In = (an, bn) of [0, 1] (such an open interval is also allowed to have the form [0, b) or (a, 1] to deal with the endpoints) such that N c U In and Etc/To< e, where t(In) is the length of I. In short, for every e > 0, you can find an open subset G e of [0, 1] containing N and of length at most s. I hope that that helps explain the statement of Borel's Strong Law. The statement of the general Kolmogorov Strong Law is now no more difficult. ►► R. Universality of the Fundamental Model. As mentioned earlier, it is an amazing fact that every experiment, no matter how complicated (perhaps involving the evolution in time of several interacting populations, for example) can be reduced to the single experiment of choosing a number between 0 and 1 according to the uniform distribution for which we have our Fundamental Model. You will start to believe this very shortly. Again, let w = 0.w1w2w3 ... (binary), in the Fundamental Model. Now define Ui (La) := 0.w1c4-)3(4)6 • • • \u0000 (binary), U2(W) := 0.W2W5W9 ... (binary), U3 (CO) := 0 .Cil 4C.J8C4313 . . . \u0000 (binary), and so on, the suffices being arranged in a series of '45°' lines. Now use your intuition. Because each Uk corresponds (R1) to a fair-coin- tossing sequence, each Uk has the U[0, 1] distribution. Because the coin-tossing sequences for the different Uk's involve different subsets of tosses, the Uk 's are independent. So, U1, U2, . .. is an IID sequence, each with the U[0, 1] distribution. Thus our Fundamental Model contains infinitely many independent copies of itself!! Now, by the F-inverse Principle at 51Ba, we can produce from the U sequence a sequence of independent RVs with arbitrary distributions. If we want an IID sequence, then we simply take Xk = G (Uk), with G as explained at Exercise 51Ba and the discussion which followed it. We still have the same geometrical interpretation of a null subset of St, so we can understand: \u0000 4.3. Laws of large numbers \u0000 113 ►► ► S. Fact (Kolmogorov): Strong Law of Large Numbers (SLLN). Suppose that X1, X2, are IlD Random Variables. Define Xl+-•-+X n An := \u0000 (a) If each Xk is in 41, and p. denotes the common mean, then P(An \u0000 II) the set of w such that An(w) p, being an event (b) if each Xk is not in 41, then the set of w for which lim A„ (w)exists (the limit being allowed to depend on w) is an event of probability 0. Here then is the ultimate expression in the mathematical theory of the long- term-average idea. Please re-read the Clarification which follows Theorem 111Q and which obviously applies here too. What I have been saying is that the geometric picture of null sets for the Fundamental Triple allows us to understand the SLLN provided that we use the Fundamental Model with Xk = G(Uk) with the Uk as at 112(R1). That the theorem follows for any probability triple (C2, .T, P) may be deduced from its truth in the 'Fundamental Representation' via an Isomorphism Theorem which hinges on the 7r-system at Lemma 45M. This is all about interpretation: proving the SLLN for an arbitrary triple is no more difficult (and no less difficult) than for the Fundamental Representation. Kolmogorov's theorem is difficult to prove in full generality, but it is possible to give a simple proof of Part (a) assuming an additional condition which holds in most practical situations. It is also possible to prove Part (b) quite easily. I now do these two things. ► T. Theorem: A special case of the Strong Law. Suppose that X1, X2, . are HD Random Variables and that the common value K ofE (Xi!) is finite. Then (An —> = 1, where ,a is the common mean of the Xk's and An is as usual. Proof. We can replace Xk by Xk - pt and thereby reduce the problem to the case when, as we now assume, = 0. 114 \u0000 4: Conditioning and Independence For justification of this step and later steps, inequality 66(B2) shows that ]E ( IX I), E (X2) and E (IX13) are finite, and inequality 66(B1) shows that E [(Xk — p)4] < 8(K + A 4), and we now redefine K to be the expression on the right-hand side. Let Sr, := X1 + • • • + Xn, as usual. Then Sn4 = (Xi + + X n)(Xi + • + Xn)(Xi + • • • + Xn)(Xi + • • • + Xn). (TI) Consider calculating E (S„4) by using this expansion. Let i, j, k, £ be distinct numbers. Then, since E (Xi) = 0, the 'Independence means Multiply' rule shows that E (XiX;) = 0, E (XiX 3XO = 0, H (X,X3XkX.e) = 0. So, n E (5\";,) = EE (Vc) + E EE (VXD . k=1 \u0000 i j>i The 6 is (42): think of the ways in which X?X3 can appear in the right-hand side of (T1). But, by the proof of Lemma 65B, E (V X3?) < 2 {E (V) + E PCP < K. Hence, H \u0000 < 1 3n(n — 1)K] < 3K and E E A4„ < 00. Thus, with probability 1, E A47, < oo, and so An —> 0. Rigour for these last steps mirrors exactly that used in the proof 60Da of the First Borel—Cantelli Lemma. The very keen reader will have noticed that I skipped proving that the set of w for which An (w) it is an event. See [W] if you are that interested. ❑ U. Proof of Part (b) of Theorem 113S. If w is such that An (CO) —} L(w), then Xn(w) \u0000 An(w) n — 1 \u0000 An_i (w) > L(w) — L(w) = 0. Now, if X is an RV with the same distribution as each Xk and if H (IX I) = oo, then, by result 61G, EP (1)(711 \u0000 = EP(Ixl Th) = 00, and, by independence and the Second Borel—Cantelli Lemma 98E, the set of w for which Xn(w)I > n for infinitely many 7/ has probability 1. The desired result follows. \u0000 ❑ 4.3. Laws of large numbers \u0000 115 pp V. Probability and long- term relative frequency. Let's stick to the case of the mathematical model for coin tossing, for a coin with probability p of Heads. So suppose that X1, X2, ... are IID Random Variables with (as in our discussion of Borel's Theorem) IP(X = 0) = p, IP(X = 1) = q. Then, the number of Tails in the first n tosses is sn := xi + x2 + • • • + xn. Manifestly, E (X:1) is finite: it is q. We therefore know that, with probability 1, the proportion An := &In of Tails in the first n tosses will converge to q. For any fixed subsequence n(1) < n(2) < ..., 1 — (X n(1) + Xn(2) + • ' • + X n(r)) -4 q r with probability 1. But the set of w such that r- (x n (i )(w) + X n(2)(w) ± • • • + X n(r)(w)) —> q simultaneously for all subsequences n(•) is clearly empty. Convince yourself of this. But there is no contradiction. The set of all increasing subsequences of N is uncountable (see Appendix A4, p496); but Measure Theory restricts the second part of result 44G to a countable collection of H's. So the logical flaw in the naive use of the long-term relative frequency' idea mentioned at Discussion 25B is completely resolved. Philosophers and others, please note! W. Convergence in probability (to a constant). \u0000 The conclusion of the Weak Law does not prevent there being rare occasions on which An differs significantly from ii. Let me try to give you some understanding of this. We saw in subsection 102M that if X1, X2, ... are IID Random Variables each with the exponential E(1) distribution, then it is not true that XTh/(In n) —> 0 with probability 1. However, it is obvious that for e > 0, we have (for n > 1), IP ( X n \u0000 > 6) = RI \u0000 (X,,> Elnn) = e-e1nn = n-e , Inn so that P ( Ix.; > E) --> 0 as n -+ oo, and Xn /(ln n) converges to 0 in probability. Suppose that Xi, X2, . . . are IID Random Variables with P (Xk = ±1) = 1. Tchebychev's inequality shows that for every e > 0, P(17n > 6) 0, where Yn := N/2n In In n Sn 116 \u0000 4: Conditioning and Independence so that Yn —> 0 in probability. However, the celebrated Law of the Iterated Logarithm (LIL) states that, with probability 1, lim sup Yn = 1. There are astoundingly precise extensions of the LIL, Strassen's Law being the most remarkable. See, for example, Deuschel and Stroock [61]. ► X. Empirical Distribution Function. \u0000 Let Y = \u0000 Y2, Y3, . . .) be a sequence of IID Random Variables with common Distribution Function F. The Sample (Y1, Y2, \u0000 Yn) determines the so-called Empirical Distribution Function (EDF) Fn (• ; Y), where Fn(x; Y) is the Random Variable Fn(; Y) n \u0000 k < n; Yk < \u0000 (X1) the proportion of k < 7/ such that Yk < x. By the SLLN, we have for a fixed x the result that with probability 1, Fn(x;Y) \u0000 F(x). ►► Y. The 'Fundamental Theorem of Statistics' (Glivenko— Cantelli). Make the assumptions and use the notation of the previous subsection. Then, with probability 1, Fn( • ; Y) converges to F(.) uniformly over R. Clarification. We say that w E G if and only if it is true that for every e > 0, there exists an integer no (w), which is allowed to depend on w, such that for n > no (w), IFn(x;Y (w)) — F(x)1 < E for every x in R, where, F„(x;Y (w)) := n- 4{k : k < n; Yk(w) GA.)) < 4. Then G is an event and P(G) = 1. This fact can be seen as providing part of the justification for (amongst many other things) the bootstrap technique of Simon and Efron — see Efron and Tibshirani [71], Davison and Hinkley [55]. ► Ya. Very instructive (but tricky) Exercise*. Prove the Glivenko—Cantelli Theorem for the case when F is continuous and strictly increasing. 4.4 Random Walks: a first look Later, we shall see other methods of doing several of the problems in this section. 4.4. Random Walks: a first look \u0000 117 ►► A. Random Walk; IP,, ; Simple Random Walk SRW(p). For a general Random Walk (Wn : n > 0) on the set Z of all integers, Wn represents the position of a particle at time n, and Wn = a + Xi + X2 + • • • + Xn, so Wo = a, where a E Z is the starting position, and the 'jumps' X1, X2, . . . are HD Z-valued RVs. Probabilities associated with the random walk will depend on a and on the distribution function of the Xk variables. It will be very convenient to be able to indicate the dependence on a. We write Pa for the probability law for the random walk when it starts at a and E a for the corresponding expectation. Simple Random Walk SRW(p), where 0 < p < 1, is the case when each Xk takes one of the values +1, —1, and P (Xk = +1) = p, P (Xk = —1) = q := 1 — p. Then Pa (Xk = 1) = p for all a and k. In this case, W, is the fortune at (just after) time n of a gambler who at each of the times 1, 2, 3, ... plays a game in which, independently of previous results, she wins $1 with probability p or loses $1 with probability q. ► B. Gambler's Ruin: probabilities. We use SRW(p) started at a > 0 as our model for the gambler's fortune. She plays until her fortune reaches either b (where b > a) or 0. Let G be the event that her fortune does reach b. We wish to find x(a) := Pa (G) = Pa (W visits b before 0). We can do this by describing the function x as the solution of the difference relation x(a) = px (a + 1) + qx (a — 1) (0 < a < b); \u0000 x(0) = 0; x(b) = 1. (B1) The boundary conditions are obvious, and the 'recurrence relation' is just decomposing x(a) according to the value of X1: in detail, for 0 < a < b, Pa (G) = Pa (Xi = 1) Pa (G I Xi = 1) ± Pa (Xi = —1) Pa (G I Xi =- —1) . However, for 0 < a < b, we have Pa (G I X1 = +1) = ]a+1 (G), because the X1 = 1 has taken the Random Walk up to a + 1 and since X2, X3, ... are independent of X1, the whole system 'starts afresh' at time 1. For a little more on the rigour here, wait for Discussion 387Ca. Solving equation (B1) is just algebra. For the record, here's a refresher course on difference equations. 118 \u0000 4: Conditioning and Independence C. Solving difference equations, 1. \u0000 You will remember that to solve the differential equation ax\"(t) + bx' (t) + cx(t) = 0, \u0000 (a 0) \u0000 (C1) you introduce the auxiliary equation arn2 + brn + = 0, and if this has different roots a and /3, then the general solution of (C1) is x(t) AO' + Beat for some constants A and B, but that if a = \u0000 then the solution is x(t) = (A + Bt)e' t. The story for difference equations is very similar. To solve the difference equation \u0000 ax(k + 2) ± bx(k 1) + cx(k) = 0. \u0000 (C2) we solve the auxiliary equation 2 am ± om+c = and if this has different roots a and 0, then the general solution of (C1) is x(k) -= Aa k BO k for some constants A and B, but that if a = then the solution is x(k) = (A + Bk)ak . For our SRW example, we have, with k = a — 1, px(k + 2) — x(k + 1) + qx(k) = 0, pmt — m + q = (pm — q)(m — 1) = 0, a = \u0000 = 1. (Note: 1 will be a solution of virtually every equation we shall see!) Hence, absorbing an a into A and a into B, x(a) = Aaa + B, 1 = Aab + B, 0 = A + B, if p q, and x(a) = (A + Bk), 1 = A + Bb, 0 = A, if p = q = One can now find A and B from the boundary conditions. ► D. Hitting and return probabilities. Consider SRW(p). Let H denote the event that W„ = 0 for some strictly positive time n: H := {W7, = 0 for some n > 1}. (This restriction to strictly positive times is standard in the advanced theory, partly because it leads to perfect tie-up with electrostatics!! All concepts in electrostatics (equilibrium potential, equilibrium charge, capacity, etc) have very illuminating probabilistic interpretations. Indeed, several of the subsections in this present section relate to electrostatics, but there isn't room to reveal why.) 4.4. Random Walks: a first look \u0000 119 Let x(a) := Pa (H). If a 0, then x(a) is the obvious probability that the walk started at a will hit 0; but x(0) is the probability that the walk started at 0 will hit 0 at some positive time, that is, that it will return to 0. By the arguments of Subsection 117B, we have x(1) = px(2) + (q x 1) = px(2) + q. It would not be correct to have q x x(0) here: if our walk starts at 1 and jumps down at time 1, then it definitely hits 0 at a positive time, whereas the 'return probability' x(0) need not be 1. We now use intuition for which the rigour will be provided in the next section. We have x(2) = x (1)2, \u0000 (D1) because to go from 2 down to 0, the walk has to get from 2 down to 1, and then from 1 down to 0. It is obvious that the probability of getting from 2 down to 1 is the same as that of getting from 1 down to 0. It is intuitively clear that if the walk gets down from 2 to 1, then after first hitting 1, it starts afresh, so, using independence, x(2) = 1E2 (hit 0) = P2 (hit 1)1E2 (hit 0 I hit 1) = 1P2 (hit 1)Pi (hit 0) = x(1)2. \u0000 (D2) We now have x(1) = px (1)2 + q, [px(1) — q] [x(1) — 1] = 0, whence x(1) = q/p or x(1) = 1. If q> p, then since x(1) cannot be greater than 1, we must have x(1) = 1. What happens if q < p? Let's again use our intuition. Since E (Xk ) = p — q, then, by the SLLN, the walk will tend to +co, so there will be a positive probability that if started at 1 it will drift to +oo without hitting 0. Hence we must have x(1) = q/p, not x(1) = 1. Da. Bringing-in time. But can we prove this without appealing to the SLLN? Yes, we can. We bring the time into the story (thus switching from electrostatics to the theory of heat flow!) So, let lin := {Wm = 0 for some m satisfying 1 < m < n} . Let x(a, n) := Pa (Ha ). Our intuition, confirmed by property 43D(a), is that x(a) is the 'monotonically increasing limit' x(a) = t lim x(a,n). \u0000 (D3) n--> o0 120 \u0000 4: Conditioning and Independence For n > 1, we have x(1, n) = px(2, n — 1) + q — Yes?! Also, we have x(2, n) < x(1,71 — 1)x(1, n — 1). Think about this. If the particle gets from 2 to 0 in n steps, it must get from 2 to 1 in at most n — 1 steps and from 1 to 0 in at most n — 1 steps. Hence, x(1, n) < px(1, n — 1)2 + q, and since x(1,1) = q < q/p, we have x(1, 2) < p(q/p)2 + q= q/p. By induction x(1, n) < qlp and now, by 119(D3), x(1) < You can now check that in all cases, 1 — x(0) = IED0 (no return to 0) = 1p — ql, a nice fact if we think of ip — ql as the bias in a coin and of the random walk as accumulated 'Number of Heads minus Number of Tails'. ► E. Expected number of visits to initial state. Consider any Random Walk on Z. Let r be the probability of a return to 0 if, as we now assume, the process starts at 0. Let N be the total number of visits to 0 including that at time 0. Using the intuitive idea that the Random Walk starts afresh at the time of first return to 0 (if this is finite), we have, for the expectation of N when the starting point is 0, \u0000 E o(N) = 1+ rEo(N) + (1 — r) x 0, \u0000 (El) the '1' representing the visit at time 0. We saw such things at subsection 21E. Hence, Eo(N) = 1 1 \u0000 (— oo if r = 1). \u0000 (E2) However, if en := /{wn=0}, then \u0000 N = 6 + 6 + 6 + \u0000 (E3) the sum counting 1 for every visit to 0. Thus E o(N) = \u0000 o \u0000 = >2, TO (Wm = 0) , \u0000 (E4) n=0 \u0000 n-=-0 and this raises the possibility of finding the return probability r by summing the series on the right-hand side of (E4) and then using (E2). An important example is given in the next subsection. /NJ 1 7rn' 4.4. Random Walks: a first look \u0000 121 The intuition about 'starting afresh' which we have used here is justified in Section 4.5, and the type of decomposition used at (El) is studied in detail in Chapter 9. ■ F. Recurrence of Symmetric RW on Z2. Consider the integer lattice Z2 in the plane, the typical point of which is (u, v), where u, v E Z. The lattice is illustrated on the left-hand side of Figure F(i). Each point of Z2 has 4 neighbours; and in Symmetric Random Walk on Z2, a particle jumps from its current position to a randomly chosen one of its 4 neighbours with probability each, independently of its past history. We are interested in the return probability to 0 = (0, 0). • 1r The lattice Z2 \u0000 Lattice for 'product RW' Figure F(i): To help derive recurrence of RW on Z2 Suppose that U = (Un : n = 0, 1, 2, . . .) and V are independent SRW( 1 ) processes on Z, with Uo = Vo = 0. the 'product' process W with Wn =- (Un, Va ). A little thought shows that W is a Symmetric Random Walk on the 45° lattice on the right of Figure F(i). However, P (W-' 2n = (0, 0)) = P(U2n = 0)P (V2n = 0) = b(2n, 2;n)2 by result 11(N2). Hence, > P (W2n = (0, 0)) = oo, and, by the argument of the previous subsection, the return probability to (0, 0)for Symmetric Random Walk on Z2 is 1: we say that the Random Walk is recurrent. For Symmetric Random Walk W on Z3 (or on Zd for d > 3), the return probability is strictly less than 1: we say that the Random Walk is transient. We cannot use the same type of argument because each site in Z3 has 6 neighbours whereas a site for the 'product' of 3 SRW processes on Z would have 8 neighbours. The intuitive point is that, for d > 3, Zd is a 'big' space; and the Symmetric Random Walk on it will (with probability 1) drift to infinity. However, (with probability 1) it will not drift to infinity in any particular direction: the set of values Wn/IIWn Il will be dense in the unit sphere. It is not easy to see intuitively what is a 'big' space: for example, one can find a Random Walk on the set Q of rationals which will with probability 1 visit every rational II \u0000 II 122 \u0000 4: Conditioning and Independence infinitely often. There is no similar recurrent Random Walk on the 'free group fractal' of the next exercise. G. Random Walk on the 'fractal' free group on 2 generators. Random It \u0000 \u0000 ti C) Figure G(i): Free group as fractal walks (or rather, their continuous analogues, Brownian motions) on non-Abelian groups such as rotation groups, are very important. But here's a simple example which is reinterpretable via a picture. You should enjoy the Exercise! The free group G on 2 generators a and b consists of the empty word (identity or unit element) e and all 'words' obtained by stringing together symbols a, a-1, b, b-1. Thus, a-1bb—labba —lb-1ba-1 is a word which we can cancel down to the reduced word b2 a-2 of length 4 (in that as far as length is concerned, it is regarded as bba — 1 a-1). Henceforth, we think of G as consisting of reduced words. The group is 'free' in that two reduced words are regarded as equal if and only if they are identical. There are no relations such as ab = bat between the generators. We can build a random walk on G by starting with the element e and then multiplying it on the right by a randomly chosen one of a, a-1 b , b 1, each having probability of being chosen, independently of what has happened previously. At each jump, the reduced word either increases by one in length, or is reduced by one in length. 4.4. Random Walks: a first look \u0000 123 We can picture F as the fractal sketched in Figure G(i). Of course, the fractal should go on for ever, not stop at reduced words of length up to 5. We have (for one possible convention) 0 = e, A = a, B = b, H = 10-1a, etc. The 'freeness' of the group means that there are no loops in the picture. (Physicists might call the picture a Bethe lattice.) From any point, the particle is equally likely to jump to any of its 4 'neighbours'. The shrinking in size as we move away from 0 is a misleading feature of the picture. Still, the picture is helpful, as you should learn from the following problems. It must be understood that 'hit A before B' means 'the particle does hit A and does so before it hits B (if it does)'. Ga. Exercise*. Prove that 'Po (hit A) = A , \u0000 Po (hit A before B) = o , Po (hit both A and B) = 1 , Po (hit H before either A or B) = p±n. • With Po probability 1, n'(length of reduced word) will tend to a deterministic limit c. What is the value of c? H. Fractal random variables. Consider the full geometric picture of which part is sketched in Figure 122G(i). We are now thinking of the points of the free group as the nodes of the graph in Figure 122G(i) embedded in the plane, with the distances as shown. The Random Walk (W, say) on the vertices of the fractal will, with probability 1, converge to a point Woo which belongs to the set E (say) of 'endpoints' of the fractal. This set E is a true fractal, a set of fractional dimension. The RV Woo is not discrete because IP(W = x) = 0 for every point x in the plane; but neither is it 'continuous': it has no pdf in the plane. However, such RVs present no problem to Measure Theory. The set E is a much prettier relative of the classical Cantor subset C of [0, 1]. We can think of Cantor measure on the Cantor set as the law of the RV Woo := lirn Wn where 1 \u0000 Xk := 2 \u0000 3k k=1 where X1, X2, ... are IID RVs with P (Xk = ±1) = 1. Everything is coin tossing! Again, Woo is neither discrete nor 'continuous'. ►► I. Sharpening our intuition. Consider a Random Walk W on Z: Wn := a + + • • • + Xn, where X1, X2, . . . are IID with IED(Xri = — 1) = \u0000 P(Xn =1) = \u0000 P(Xn = 2) = Since the Random Walk W can only go down by 1 unit at any step, we have, for k > 1, Pk (hit 0) = cek 124 \u0000 4: Conditioning and Independence for some a. Compare subsection 118D. We have Pi (hit 0) = + .11P2 (hit 0) + P3 (hit 0), so that a = + \u0000 + \u0000 (a — 1)(3a — 1)(a + 2) = 0. Since E (X 9.,) = > 0, the Random Walk will drift to +oo, so we must have a < 1. Hence, a , For k > —1, noting the definition carefully, let Yk := PO (Wn = k for some n > 0) . Then y_i = a = s, yo = 1 and, for k > 1 (but not, note, for k = 0), Yk = Yk +1 ± 7A-1 ± ;yk -2. The auxiliary equation (see subsection 118C) 2 3 \u0000 2 \u0000 2 .7Tri \u0000 M \u0000 .7M ± = 0 has roots 1, —2, 3 (the inverses of those for the a equation), so we have yk = A + B(—,2)k c (3)k \u0000 k > —1. Since 0 < yk < 1 for every k, C must be 0. Next, 1 = yo = A + B, \u0000 = y_ i l = A — 2B, A 9, B = t. The return probability r to 0 satisfies r = \u0000 + 27 a + \u0000 = 31. Hence, if the process starts at 0, as we now assume it does, then the total number of visits to 0 is 1/(1 — r) = 3/2. If k is very large, then the probability yk that k is hit is very close to A = 9; and if k is hit, then the process spends on average a total time 3/2 at k. Hence, for large K, \u0000 IE 0 [total time spent in states 0,1,2, ... , \u0000 K x 9 x 2 = K. K. \u0000 This tallies exactly with the fact that since E \u0000 = 6/7, the process drifts to infinity at rate 6/7, so will hit K at about time 7K/6. The total time spent in states less than 0 and the period between the first hit on K and the last visit to 0,1,2, ... , K are negligible compared with the total time in 0,1,2, ... , K. 4.4. Random Walks: a first look \u0000 125 x in \u0000 n Figure J(i): A simple path from (m, x) to (n, y) J. Counting simple paths. \u0000 We are now going to look at some counting arguments for Random Walks. Many of the things we do in the remainder of this section via counting can be done efficiently by other arguments. However, not all can: for some problems, we need the counting arguments. Figure J(i) shows a 'simple path' joining (in, x) to (n, y), where in, n, x, y are integers with 71 > m. Each piece of such a simple path on the 'integer grid' has slope 1 or —1. We think of the horizontal axis as the time-axis. ► Ja. Lenuna. Suppose that m, n, x, y are integers with n > m. Then the number of simple paths from (in, x) to (n, y) is (n — m) , whereu = n—m+y—x u \u0000 2 If u is not an integer in the range {0, 1, 2, ... , n — m}, then the binomial coefficient is 0: of course, if there is a simple path from (m, x) to (n, y), then n — m must have the same parity as y — x. Proof To get from (m, x) to (n, y) requires u 'ups' and d 'downs', where u d = n — m, u — d = y — x, whence u = n — m+y — x 2 The IL 'ups' can occupy any subset of size u of the n — in available time-intervals. \u0000 ❑ K. The Reflection Principl e. Suppose that x > 0, y > 0 and n > m. Then the number of simple paths from (m, x) to (n, y) which touch or cross the time- axis (`x = 0') equals the total number of simple paths from (m, x) to (n, —y), equivalently, the total number of paths from (m, —x) to (n, y). 126 \u0000 4: Conditioning and Independence Figure K(i): Reflection Principle illustrated Proof For any path from (m, x) to (n, y) which touches or crosses the time-axis, we can reflect the path in the time-axis from the first time it hits the time-axis on. In the figure, the original path is a thin black line, the transformed path the wide grey line, the two paths agreeing until the time-axis is hit. You can see that this 'reflection' sets up a one-one correspondence between the two sets of paths which we want to prove have the same cardinality. ❑ ► L. The Ballot Theorem. Consider an election between two candidates A and B in which A gets a votes and B gets b votes, where a > b. Suppose that the ballot slips on which people register their votes are arranged randomly before the count takes place. Then the probability that A is strictly in the lead from the first ballot slip counted right through the count is (a — b)I (a b). Please note that this is an important result, not in any way confined to its `election' context. Clearly one feels that such a simple answer must have a simple explanation. I shall provide one in Chapter 9. In the meantime, ... La. Exercise. Prove the Ballot Theorem. [Hints. Think of a simple path as going up [down] for every ballot slip in favour of A [B]. What is the total number of paths from (0, 0) to (a + b, a — b)? How many paths go from (1,1) to (a + b, a — b)? How many such paths do so without touching or crossing the axis?] Also find the probability that B is never in the lead throughout the count. (Paths may now touch the time axis.) Lb. Exercise: Application to return times. Consider SRW(p) started at 0. Let T be the first time that the particle returns to 0 with (standard convention) T = co if the particle never returns there. Prove that 2n 2 \u0000 2nn— 1 pnqn. Po (To -= 2n) = \u0000 Hints. If T = 2n, then W 2n-1 is either 1 or —1. Use the Ballot Theorem to calculate the conditional probability that Wk 0 for 1 < k < 2n — 1 given that W 2n-1 = 1. 4.5. A simple 'strong Markov principle' \u0000 127 Lc. Exercise: Maximum of SRW. Let W be SRW(p), and define M, maxk<n Wk. Prove that, for m > w and m > 0, Po (Mn > m; Wrt = w) = pvqn-v where u := m + (n — w) and v= 1 (w + n). Deduce that if p = q = 2 and w < m, then Po (Mn = m; Wn = w) = Po (Wn = 2m w) — (Wn = 2m + 2 — w) and then that P(mn = m) = Po (wn m) + Po ( wn = m + 1). 4.5 A simple 'strong Markov principle' ORIENTATION. You might wish to skip this section on a first reading. We now take a closer look at equation 119(D2), where we claimed that for SRW(p), P2 (hit 0) = 1[12 (hit 1 )Pi (hit 0). We claimed therefore that the process starts afresh at the first time T that it hits 1. But T is a random time, and our claim, however intuitively appealing, demands proof. (A further complication is that T might be infinite: in other words, the particle may never hit 1 when started from 2.) To make sense of starting afresh needs a little thought, but it involves fundamental ideas which will reappear later — as fundamental ideas do. This is why this topic is having a whole section to itself. Perhaps the first use of stopping times was in Statistics — in Wald's sequential sampling (described at 416H). We need the ideas in the next section for very practical simulation issues. For what kind of random time will the process start afresh? Suppose that we consider coin tossing, the coin being tossed at time 1,2,3 .... Let T be U — 1, where U is the time of the first Head after time 1. The process certainly does not start afresh at this time T, because the next toss will definitely produce a Head. Here T is 'cheating' because it looks forwards in time. We need to formulate the idea of a stopping time T, that is, a random time which `depends only on history up to the present'. We then need to describe the class FT of events which describe history up to a stopping time T, and then we want to prove that things start afresh at such a stopping time. The intuitive ideas of stopping times and of FT are easy, and are what I want you to understand. I have put them in bold type. OK, let's take it in stages. 128 \u0000 4: Conditioning and Independence ►► A. Stopping time T; history Yn up to time n. The reason for the name `stopping time' will become clear in Chapter 9. The situation with which we are concerned is the following. Suppose that we have a stochastic process X, that is, a sequence X1, X2, X3, ... of RVs. Suppose that T is an RV taking values in {1, 2, 3, ... ; co}. Think of the information available at time n as being knowledge of the values X1, X2, • • Xn. Intuitively, T is a stopping time if whether or not T < n can be calculated from the values X1, X2, . , Xn. In our discrete-time setting, it is easier to say that T is a stopping time if whether or not T = n can be calculated from the values X1, X2, . Xn. Thus, T := min {n : Xn > , T := min In : Sn > a} , where Sn := X1 + X2 + • • + Xn, T := min {n : Xn_i > a} are stopping times. Generally, a stopping time is the first time that something observable by that time happens. Here is an example which is not a stopping time: T := min In : Xn+i > a} . We don't know Xn+1 at time n. We write .Fn for all the collection of all events which depend only on the values X1, X2, , X n. Each such event is of the form {w : (Xi (C4.)), X2 (b.)), \u0000 Xfl,(W)) E U} where U is a nice subset of Itn+1; and here, 'nice' means precisely 'Borel' — see 45L. The class Fn is then a u-algebra. We think of Tr, as the mathematical formulation of the history of X up to time n. Formally then, T is defined to be a stopping time relative to the evolution described by {T7, : n E N} if for every n, the event {T < n} is in Fn, equivalently if, for every n, the event {T = n} is in (q) If T := min{n : Xn > a}, then {w : T (w) = n} = ful : (Xi (w), , X n(w)) E U} where U := {(xo, \u0000 , xn) E Ir +1 : xk < a (1 < k < n); xn > \u0000 . The set U is Borel. Indeed, u = \u0000 n {xk <a} n {xn > a}, 1<k<n and the sets {xk < a} are closed (hence Borel) and {xi, > a} is open (hence Borel). As a finite intersection of Borel sets, U is Borel. 4.5. A simple 'strong Markov principle' \u0000 129 ► B. History .FT up to time T. We know that Fes, represents the history up to fixed time n. But if we want to formulate and prove the idea that things start afresh at time T, we have first to formulate the history TT up to time T. So when will an event H belong to FT? The answer is simple (once you get used to it): we say that H E .TT if and only if for each n, H n {T < n} E T; equivalently, if and only if for each n, H n {T = n} E Tn. So, H E .FT if, for every n, for those outcomes for which T = n, you will know by time n whether or not H has occurred. If T := min fn : Xn > a}, and b < a, then H := {X n < b for some n < T} is in TT. ► C. A simple 'Strong Markov Principle'. Let the process X = (X1, X 2, . . .) be an HD sequence, each Xk having distribution function F. Let T be a stopping time for X, and let H E FT. If T(w) is finite, then we define XT+,- (w) := XT(„ )±„.(w). (a) Suppose first that T (w) is finite with probability 1. Then the variables IH,XT+1,XT+2,.•. are independent; and each XT+k has DF F. To summarize: XT+1, XT+2,. • • is an HD sequence with common DF F and is independent of any event in TT. For any k, for x 1, x2, . . . , xk \u0000 and for H E TT, we have P(II; XT+1 < Xi; XT+2 < X2; • • • ; XT±k < xk) = P(H)F(x )F (x2) . • • F(xk). \u0000 (C I ) (b) For general T for which P(T = oo) might be positive, equation (C1) holds when H is replaced by H n {T < oo}, and then there is no problem about the meaning of XT+1, etc, because we restrict attention to the event that T is finite. 130 \u0000 4: Conditioning and Independence Proof. The short proof is easier than the statement to which we have built up. We have I(D I \u0000 n IT < oo}; XT+1 < Xi; - • • ; XT-kk < Xk) = E P(H n {T — n}; XT+1 < Xi; • • ; XT±k Xk) \u0000 (C2) 1<n<co - E P(H n IT = n}; Xn±i < xi; • • • ; XTh-Fic < xk) 1<n<co However, for fixed n, H n {T = n} E .Fn by definition of .FT : it therefore depends only on the RVs Xi, X2, .. , X n, and so is independent of X n+1, Xn+2, Xn+k• Thus the expression at (C2) is E P(R- n \u0000 -=- n})P(X92+1 < xi)...P(Xn-Fk xk) 1<n<co = F(Xi)F(X2)... F(Xk) \u0000 P(H n IT = n1) 1<n<co = F(xi )F(x 2) . . . F(xk)11° (H n {T < oo}) . The proof is complete. D. The Random Walk example. We had Wn 2 + + X2 + • • + Xn, where Xi , X2, ... are IID, with IP (Xk = 1) = p and IP (Xk = —1) = q. We had T = min{n : Wn = 1}, and this is a stopping time for the X-process. Conditionally on < ool, which is trivially in .FT, the variables XT+1, XT+2 7 . are IID each with the same distribution as X1, and the process truly starts afresh at time T, leading to P2 (hit 0 I hit 1) = Pi (hit 0) as required. The intuition about 'starting afresh' used in subsections 21E and 120E are also justified by our Strong Markov Principle. 4.6 Simulation of IID sequences ► A. 'Random-number generators'. The fundamental problem is to get the computer to produce a sequence ui , u2, ... of numbers between 0 and 1 which one can regard as a simulation of a realization of values chosen independently each from the uniform U[0,1] distribution. What the computer actually does is generate a sequence of 4.6. Simulation of IID sequences \u0000 131 integers in a range {0, 1, 2, ... , M — 1} and then take uk = Xk /M. The initial value xi is a 'seed' which is either input by the user or calculated from the readings on the computer clock. The computer is programmed with a function which I'll call N (for `next'), and it calculates the x-sequence recursively via xk = N (x k _i ). We shall discuss three types of generator. In the discussion, it will be convenient to use Euler's co-function defined on positive integers as follows: (n) := # (C(n)) , where C(n) := {rn : 1 < m < n; hcf(m, n) = 1} where 'ha' denotes highest common factor (or greatest common divisor). Thus, C(n) is the set of positive integers less than n which are `coprime' to 71, and co(n) is the number of elements in C(n). For a prime p, we clearly have co(p) = p — 1. B. Multiplicative generators. The idea here is to take Xk = axk_1 mod M. One of the cases in which this makes good sense is when M is a prime p and a is a `primitive root' for p. (One of Fermat's theorems says that for 1 < a < p — 1, we have aP-1 = 1 mod p. We say that a is a primitive root if a d 1 mod p for 1 < d < p— 1.) Then whatever number xi is taken in {1, 2, ... , p — 1}, the set {x1, x2, . . , } is the set {1, 2, ... , p — 1} arranged in some order: we have 'full period'. For example, if p = 3 and a = 2, then x will cycle through the values (1 2), and if p = 5 and a = 3, then x will cycle through (1 3 4 2). By this, I mean in the latter case, for example, that if xi = 3, then 12 = 4, 13 = 2, 14 = 1— and then we had better stop. Of course in practice, the computer will use a huge prime p. It is known from Number Theory that a prime p has yo(p — 1) primitive roots. C. 'Mixed' multiplicative generators (MMGs). \u0000 No random-number generator is perfect. It has been suggested that defects can be 'averaged out' if one mixes a number of different random-number generators as follows. Let the rth generator (1 < r < s) generate a sequence \u0000 via the multiplicative generator xk,r. = ar xk_ 1,r mod pr, where pi , p2, \u0000 , A, are distinct primes, and for each r, ar is a primitive root for pr. For each r, we can think of u1 r , u2,,, ..., where uk,r = Xk,r , /pr , as a perhaps crude random `uniform' sequence. One then mixes these sequences by defining Uk := (Uk,j, Uk,2 + • \" Uk,$) mod 1, the 'mod 1' signifying that we take the fractional part: 2.67 mod 1 = 0.67, for example. Ca. Exercise. Prove that if U has the uniform U[0,1] distribution and V is an RV independent of U with any distribution at all, then (U + V) mod 1 has the U[0,1] distribution. 132 \u0000 4: Conditioning and Independence So, there is good sense in the MMG idea, and it seems to work well in practice. Even so, mathematicians will feel some unease because there is a sense in which mixing does not really achieve what it was intended to do. Note that xk Uk = M where M := p1p2 . • • Ps) and xk = ( p2P3 • • • Ps) xko. + • • • + (p1.P2 • • • Ps-1) X k,s mod M. It follows already that x e C(M), and it is well known that co(m) = (Pi — 1) (1)2 — 1) ... (ps — 1) . According to the Chinese Remainder Theorem, there is a unique number a in {1, 2, ... , M — 1} such that a = a, mod pr for 1 < r < 8. We find that xk = axk_1 mod M, so the new 'mixed generator' is still a multiplicative generator, but one not of full period, by which I mean not even of period cp(M). The period, Period, of the generator is the lowest common multiple of pi — 1, 132 — 1, . . . , p., — 1. Consider mixing the two trivial examples we looked at earlier: Xk,1 = 2 Xk-1,1 mod 3, xk,2 =- 3xk-1,2 mod 5, xk = 5Xk1 + 3Xk,2, M = 15. We spot that a = 8 in the Chinese Remainder Theorem, so xk = 8xk_ 1 mod 15, and x will cycle either through (1 8 4 2) or (14 7 11 13) depending on which cycle contains the seed. This is, of course, an extreme example. In general, C(m) will be partitioned into (p(M)/Period sets each of Period elements and the x values will cycle through one of these sets. D. Congruential generators. The idea here is to use a recurrence xk = axk_ i + b mod M. Usually, M is taken to be a power of 2 for ease of computer calculation. Lemma. If M = 2r, where r > 2 and b is odd, \u0000 0 < b < 2r, a = 1 mod 4, \u0000 0 < a < 2r, 4.6. Simulation of IID sequences \u0000 133 and we set M = 2r, xk := axk_i +b mod M, then, for any x E {0,1,2, \u0000 , M — 1}, the numbers xi , x2, ... ,x m are the numbers 0,1,2, ... , M — 1 in some order: we have full period. For proof, see Ripley [196]. E. Discussion. The period is just one of many factors to be considered in assessing the appropriateness of a random-number generator. For example, if we take a = 1 and b = 1 in the Lemma, we obtain full period but a sequence which we can hardly think of as random! Generators must be subjected to lots of tests: see Ripley [196]. Not only must (uk : k E N) be uniform, but also { (74, Uk+1, • • Uk+d) : k E must be uniform in the d dimensional cube [0, l]d. It is important to check this for values of d up to (say) 6; some early generators failed this test. Testing out the generator on known results is of course useful. We tested two generators on the 'Waiting for HH' example. Warning. Ripley's book alerts us to the possibility of very disturbing 'lattice structures' in multiplicative and congruential generators and 'spiral structures' in the most common way of generating normally distributed RVs. ► F. Generators used in this book. In each simulation done in 'C' in this book, I use one of two random-number generators, both of which are programmed into RNG . o as described below. One generator, due to Wichmann and Hill, is the mixed multiplicative generator with s = 3; al = 171, pl = 30269; a2 = 172, p2 = 30307; a3 = 170, p3 = 30323. My thanks to the Stats group at Bath for telling me about this one. Any worries one has (I had!) because of the 'mathematical' reasons discussed above are completely dispelled by the fact that very sensible people are happy with the generator (and have tested it in several ways) and by the sheer 'scale' of the thing. As a single multiplicative generator, it reads xk = axk_1 mod M, where a = 16555425264690, M = 27817185604309, period = 6953607871644. The set C(M) is partitioned into 4 disjoint cycles in the sense explained earlier. The other generator I use is a congruential generator which I saw in a paper by the great number-theorist, Langlands, and co-workers, testing by simulation absolutely astonishing predictions by the theoretical physicist Cardy on Percolation, one of the most 134 \u0000 4: Conditioning and Independence challenging branches of Probability. See Langlands, Pouliot and Saint-Aubin [145], and the book [102] by Grimmett. The generator reads xk = axk_ i + b mod m, where a = 142412240584757, b =- 11, m = 248 = 281474976710656. By our Lemma, it has full period m. The percolation problems are very subtle, and I was impressed by the very close agreement between the simulation results and Cardy's predictions. The random-number generator is getting right some extremely complex probabilities. I simulated 'Waiting for HHH' for a coin with probability 0.25 of Heads by Wichmann- Hill and Langlands, performing the experiment 1000000 times. Results: Wi chmann-Hill (2561, 7539, 23307): mean 83.93570; Langla_nds(9267, 47521, 37551): mean 84.03192. By Exercise 22Ga, the true answer is 84. Moreover, the (true) standard deviation of the average of a million waiting times is 0.081756 (to 6 places). Each of the two generators is here very good on something which could catch generators out. But it is always a good idea to have two or more generators at one's disposal, especially in the light of the Warning. ► G. C code for a random-number generator. Please note that there is an explanation of 'static' variables in 'C' at Appendix A7, p499. Part of the C code for the random-number generator I use is contained in a header file which begins /* RNG.h \u0000 DW */ #if defined RNG_h #else #define RNG_h #define E 2.718281828459 /* #define PI 3.141592653590 necessary on some machines */ int WhichGen; /*decides which generator*/ void setseeds(); void ShowOldSeeds(); double Unif(); /*Sample from U[0,1]*/ int WithProb(double p); /*Gives 1 with probability p, 0 otherwise*/ The corresponding beginning of the program RNG. c - some of which was written by Bill Browne and some by me - reads: /* RNG.c - William Browne and David Williams Contains some random number generation routines 4.6. Simulation of IID sequences \u0000 135 cc -c RNG.c -o RNG.o \u0000 to compile */ #include <stdio.h> #include <math.h> #include \"RNG.h\" /* Following used by 'Langlands' generator */ static const C2 = 33157; static const C1 = 61187; static const CO = 53; static const M = 65536.0; /* x0,...,x3 current 'seeds'; oldx0, etc, original seeds */ static unsigned long int x0, x1, x2, x3, oldx0, oldx1, oldx2, oldx3; static double Msq; void setseeds(){ printf(\"Which generator?\"); printf(\"\\nlnput 1 for Wichmann-Hill, 2 for Langlands\\n\"); scanf(\"Ad\", &WhichGen); printf(\"\\n\\nlnput 3 positive-integer seeds\\n\"); if (WhichGen==1){ printf(\"a<30269, b<30307, c<30323\\n\\n\"); scanf(\"74%nd\", &oldxl, &oldx2, &oldx3); x1 = oldxl; x2 = oldx2; x3 = oldx3; } else{ printf(\"a,b,c < 65536\\n\"); scanf(\"And'hd\", 8toldx0, gioldx1, &oldx2); x0 = oldx0; \u0000 xl = oldxl; x2 = oldx2; Msq = (double) M * M; } } void ShowOldSeeds(){ printf(\"\\n\\nGenerator used was \"); if(WhichGen==1) printf(\"Wichmann-Hill(%7d, 7.7d,7.7d)\", oldxl, oldx2, oldx3); else printf(\"Langlands(%7dA7d,7.7d)\", oldx0, oldxl, oldx2); } double WHunif(){ xl = (171 * x1)%30269; x2 = (172 * x2)%30307; x3 = (170 * x3)%30323; return fmod(x1/30269.0 + x2/30307.0 + x3/30323.0, 1.0); 136 \u0000 4: Conditioning and Independence double Lunif(){ unsigned long y0, yl, y2, tO, t1, t2, ul, u2, w2; double temp; tO = CO*x0 + 11; y0 = tO % M; u1 = tO/M; t1 = C1*x0 + CO*x1 + u1; y1 = t1 % M; u2 = t1/M; w2 = (C1 * x1)%M; t2 = C2*x0 + w2 + CO*x2 + u2; y2 = t2 % M; x0= y0; x1 = y1; x2 = y2; temp = (double) x2 * M + xl; return((temp+0.5)/Msq); } double Unif(){ if (WhichGen == 1) return WHunif(); else return Lunif(); } int WithProb(double p){ if (Unif() <= p) return 1; else return 0; } From now on, I quote parts of RNG . c only, the corresponding parts of RNG .h being obvious. ► H. Simulating from exponential distributions. Compare Exercise 55Fa. Let A > 0. If U has the uniform U[0, 1] distribution, then V := —(ln U)IA has the exponential E(rate A) distribution because, for x > 0, P(V > x) = P(ln U < — Ax) =P(U < e —As) = e-Ax . This is applying the F-inverse principle at 50B but to the uniform 1 — U instead of U. We therefore have, for generating E(rate A) variables: double Expl(double lambda)-( return (-log(Unif())/lambda); } The two-sided exponential distribution E2S(rate A). The exponential distribution E(rateA) has pdf Ae —Ax on [0, oo). The two-sided exponential distribution E2S(rate A) has pdf z Ae----Alx1 on JR. The idea is explained in double E2S(double lambda){ double oneside = Expl(lambda); if (WithProb(0.5)==1) return oneside; else return (-oneside); } 4.6. Simulation of IID sequences \u0000 137 ► I. Rejection Sampling. As remarked earlier, the F-inverse principle is not that efficient for sampling from most distributions. Rejection sampling is a useful general technique. Lemma 139J provides the theoretical basis. Here's the idea. We wish to obtain a sample corresponding to pdf f . Suppose that we have a means of simulating from a distribution with pdf g where f (x) < K g(x) for all x. \u0000 (I1) Every time the computer provides us with a value z corresponding to pdf g, we accept z with probability v(z) := f (z)I(K g(z)); otherwise, we reject z. Then the accepted values x1, x2, ... (say) form a sequence corresponding to IID values from the distribution with pdf f . On average, it takes K z-values to get each x-value, so we want K to be small. Example. Suppose that f is the pdf of the standard normal distribution (or Gauss distribution) N(0, 1), so that f (x) = \u0000 7r exp (—Ix2) , and take g to be the pdf of the E2S(A) distribution: g(x) = 2A exp(— A I xl ). Then f (x) = sup 2 \u0000 2 sup \u0000 exp ( \u0000 -F X 2 AX) = \u0000 exp (02) /A, xER g(x) \u0000 x>o \u0000 AV-271- the supremum being achieved when x = A. To obtain the smallest K, we need to minimize the last expression, equivalently, to minimize its logarithm, and we find that (1A2 In A) = A — A —1 = — 0 so A = 1. Thus, pick z according to pdf Ze Ix', and accept z with probability v = 2 exp (-1z2 + z) /-‘7r. This gives us a realization of IID N(0, 1) variables X1, X21 • -- For each X, we have to try on average /2e/ir = 1.315 values of z. Rearranging so as to avoid the use of I • I, we can use double aGaussOf /* simulates N(0,1) variable */ double z,v,w; do{z = Expl(1.0); w = z - 1.0; v = exp(-w*w/2.0); }while (WithProb(v)==0); if (WithProb(0.5) == 1) return z; else return (-z); } Do check that the above rearrangement works. Results obtained in conjunction with Figure I(i): 138 \u0000 4: Conditioning and Independence Generator used was Wichmann-Hill(3718,6734,23115) Number of z's per x = 1.3154 Empirical P(RV<=1.960)=0.9750 Empirical P(RV<=2.576)=0.9951 __------1 Figure I(i): Histogram from aGauss ( ) Figure 1(i) shows a 128-bin histogram obtained from a sample of size 1 million from aGauss 0 with the normal curve superimposed. The little spikes at the ends correspond to tail probabilities. For an N(0, 1) variable N, ]P(N < 1.96) = 0.975. The proportion of simulated X-values with X < 1.96 was, as stated, 0.9750. From this and the corresponding result for 0.995, we see that aGauss () is getting the small tails correct. The number of z-values used in the simulation shows spectacular agreement with the theoretical expectation. The computer 'rejects z with probability v' by choosing a U[0, 1] representative u and rejecting z if u < v. Lemma J provides full theoretical justification for the method. Ia. Speeding up Rejection Sampling. Again suppose that we have f (•) < K g(•). One way to speed up Rejection Sampling is to pick 'lower' and 'upper' functions a•) and b(•) with a(z) < v(z) := f (z)< b(z) Kg(z) — where a(• and b(•) are 'close' and where they are simple functions which may be evaluated very rapidly. We can then adopt the strategy: 4.6. Simulation of HD sequences \u0000 139 Repeat{ Choose z according to pdf g, Choose u according to U[0,1]; If u > b(z) then reject z; Else if u < a(z) accept z; Else if u < v(z) accept z; } until z is accepted. This type of idea is utilized in Chapter 7. Ib. Adaptive Rejection Sampling. This is a much cleverer idea (due to Gilks and Wild, and in more general form to Gilks) which works for log-concave functions f, that is for functions f for which ln(f) is concave, that is, — ln(f) is convex. The idea is to keep refining the pdf g (in our original version of Rejection Sampling) to bring it closer and closer to f, utilizing numbers already calculated to update the piecewise-linear function ln(g). I wish I had space to explain further. See Gilks [95] and the references therein. Ic. Exercise. Check the result of Exercise 64Ja by simulation. (A solution is given in Chapter 11.) J. Lemma. (The statement of the lemma looks much more complicated than it is!) Suppose that f and g are pdfs, and that 137(11) holds for some K. Let Z1, Z2, . . . be HD RVs with pdf g and U1, U2, . . . be an IID U[0,1] sequence independent of the (Zk ) sequence. Let Vy, := f (Zri) I K Define T o := 0 and for k = 1, 2, 3, ..., define Tk := min {n : n > Tk_i; Un < Vn} , X k ZTk • Then X1, X2, . . . is a IID sequence of RVs with common pdf f. Addendum. Define the event An := {Un < Vn} that Z„ is accepted. Then the events A1, A2, . .. are independent, each with probability p := 1/K. The gaps Tj. — To, T2 T1, T3 T27 • • • between acceptance times are independent each with the geometric distribution of mean K: IED (Tk — Tk_i = M) = qm-1 p (m = 1, 2, 3 ...), where q := 1 — p. The IID sequence X1, X2, ... is independent of the HD sequence T1 — To , T2 — T1, T3 T2, .... Partial proof. We have, with 'X E dx' thought of as 'x < X < x dx' as usual, IID(T1 = 1; X1 E dx) = lP(Z1 E dx; U1 < f (x)I Kg(x)) K9()) (x \u0000 1 = g(x)dx K g(x ) ) = pf(x)dx where p := K . Hence, the probability that Z1 is accepted is ED(Ti = 1) = f P(Ti = 1; X1 E dx) = p f f(x)dx = p. 140 \u0000 4: Conditioning and Independence Next, we have, using 'starting afresh at the fixed time 1' which does not need the Strong Markov Principle, P (Ti = 2; Xi E dx) = P (Zi not accepted) IP (Ti = 1;X1 E dx) = (1 — p)p f (x)dx =- qp f (x)dx (q := 1 — p), and P (Ti = 2) = qp. And so on. We therefore have P (Xi E dx) = E lip (Ti = n; Xi E dx) = E qn-l p f (x)dx = f (x)dx . 1<m<co We see that T1 and X1 are independent. All the other independence properties follow from the Strong Markov Principle since if we regard the information Fn available at time n as being knowledge of the values of Zi., U1 7 Z2,U2, • • • ) Zn)Un) then each Tk is a stopping time relative to the evolution determined by the Tn. \u0000 Ill Ja. Exercise. This is a place to have another think, if necessary, about the last part of the car-convoy problem at Exercise 103D. 5 GENERATING FUNCTIONS; AND THE CENTRAL LIMIT THEOREM ORIENTATION. Sums of IID RVs are of central importance in both Probability and Statistics. Pmfs and pdfs are not that easy to use: for example it is already rather tricky to find the pdf of the sum of n IID RVs each with the uniform U10,1] distribution. To handle sums of IID RVs effectively, we need new methods; and generating functions provide these. The main result in this chapter is the miraculous Central Limit Theorem (CLT) which says that the sum of a large number of IID RVs each with ANY distribution of finite variance has approximately a normal distribution. The normal distribution is therefore very special in this sense: we discover why. (We shall see later that the normal distribution is very special in several other ways, too.) There are at least three nice methods of proving the CLT, the first by generating functions, the second by using the fact that one can find the sequence of sums of IID Variables within a 'Brownian-motion' process, and the third (not quite so general) by using the maximum-entropy property of the normal distribution. We take a heuristic look at the first of these. Section 1.8 of Volume 1 of Rogers & Williams [199] describes the second method: it requires much more background material. For hints on the maximum-entropy method, see 198H below. Historical note: The first case of the CLT was proved by de Moivre in 1706! Some results in this chapter are too difficult to prove in full at this level; but precise statements (and, sometimes, sketched proofs) of the results are given. The chapter contains many important exercises, but they are all rather routine, I'm afraid — not much challenge here. But do them, nevertheless. 142 \u0000 5• Generating functions and the CLT 5.1 General comments on the use of Generating Functions (GFs) In this book, we mainly use three types of Generating Functions: ► the probability generating function (pgf) gx(a) := E (ax ) (0 < a G 1) for 7L+-valued X; ► the Moment Generating Function (MGF) Mx(a) := E (eax ) for suitable X and suitable a; ► the Characteristic Function (CF) Sox(a) := IE (eic'x ) valid for all X and all real a. As usual, Fx will denote the Distribution Function of X. For any generating function G x(a) of X, we need the following. ► 1. A Uniqueness Theorem: each of the functions Fx and Gx (on the appropriate domains) determines the other. Of course, Fx always determines Gx. The result in the other direction follows from the Convergence Theorem described below. ► 2. An effective method of calculating from Gx the moments pr(X) := E (X') (r = 1, 2, 3, . . .) of X which exist. ► 3. An 'Independence means Multiply' Theorem: if X and Y are independent, then Gx+y(a) = Gx(a)Gy(a)• \u0000 (1) Obviously, by combining results 1 and 3, we can in principle find the distribution of the sum of two independent RVs. ► 4. A Convergence Theorem: we have Gx„ (a) -+ Gx (a) for all relevant a if and only if Fx,,„ (x) —> Fx (x) at every point x at which Fx (x) is continuous. Such continuity points of F are dense in 11, so this is good enough. (If for every w, Xri(w) = 1/n and X(w) = 0, then Fx,, (0) = 0 for every n, but Fx (0) = 1. This explains why 'convergence at continuity points of Fx' is the right condition.) Remark. We shall prove the relevant properties only for the 'pgf' case. It requires too much Analysis to deal with the other cases. 5.2. Probability generating functions (pgfs) \u0000 143 We shall use two other types of generating function: the Laplace Transform (LT) L x (a) := E (e —ax ) (a > 0) for RVs with values in [0, oo) and the Cumulant Generating Function (CGF) C x (a) := ln M x (a). Of course, we have 'Independence means Add' for CGFs. 5.2 Probability generating functions (pgfs) ►► A. Probability generating function (pgf) gx (.) of X. Let X be a Z+ RV taking values in Z := {0, 1, 2, ...}. We define the probability generating function (pgf) gx(•) of X on [0,1] via 00 gx( ) \u0000 E (ax) \u0000 kliqX 1c), 0 < a < 1. k,-Cr Note that gx(1)= 1, .9x (0) = P(X------ 0). In this section, it is understood that we deal only with Z RVs. ► Aa. Example: pgf of Poisson(A). If A > 0 and X has the Poisson(A) distribution, then Ak P(X = k) = e–A — k! (k E Z± ), so that gx (a) = (aA)k = e –A eco, k! k=0 and gx (a ) =_ eA(oe--1). a result of considerable importance. ► B. The Uniqueness Theorem for pgfs. If we know the values gx(a), E [0,1], then we can find P(X = k) for k E Z. This follows from the Convergence Theorem E below. 144 \u0000 5: Generating functions and the CLT ► C. Finding moments. We have - with discussion of rigour below - gx(a) = E (ax ) = EakP(X = k), 9x(a) = E (Xax-1) = Eka k-1P(X = k), 9x (1) = E (X) = EkItI(X = k) G oo, gik(a) = E (X(X — 1)cxx-2), gl(1) = E(X(X— 1)) < oo, so we can find E(X), E (X 2), and hence, if these are finite, Var(X), from gx. Rigour is provided by standard results on differentiation of power series together with monotonicity properties arising from the fact that the coefficients in the power series determining a pgf are non-negative. Of course, the pgf power series has radius of convergence at least 1. In a moment, we do some serious Analysis on the Convergence Theorem; and that's enough for this section. Ca. Exercise: Poisson mean and variance. \u0000 Use gx to show that if X has the Poisson(A) distribution, then E (X) = A and Var(X) = A. ► D. 'Independence means Multiply' Lemma. If X and Y are independent (Z valued) RVs, then g X +Y (a) = 9x (a) 9Y (ca). Proof Using Lemma 99G and Theorem 101K, we have gx±y(a) = E (ax+Y ) = E (a x aY ) = (E a x ) (E aY ) = gx(a)gy(a). 1=1 Da. Exercise. (a) Use Lemma D to show that if Y is the number of Heads in n tosses of a coin with probability p of Heads, then gy (a) = (q + par . This ties in the Binomial Theorem with the binomial distribution. ► (b) Summing independent Poisson variables. Use Lemma D and Example 143Aa to show that if X and Y are independent, X — Poisson(A), Y ,-, Poisson(p), then X + Y -, Poisson(A + p). E. The Convergence Theorem for pgfs. Suppose that X and Xn (where ii = 1, 2, 3, ...) are Z valued RVs, and that gxn (a) —> gx(a) for 0 < a < 1. Then IP (Xn = k) —> 111(X = k) \u0000 (k E Z+ ) . We need the following lemma. 5.2. Probability generating functions (pgfs) \u0000 145 Lemma. Suppose that ak,n and ak (k E Z±, n E N) are non-negative constants with lk ak,n < 1 for every n and E ak < 1. Suppose that for 0 < a < 1, we have co an (a) := > ak,na k -> a(a) := \u0000 aka k. \u0000 k=0 \u0000 k=0 Then ao,n \u0000 ao. Proof of Lemma. Note that, for 0 < a < 1, an (a) = ao,n + An (a), a(a) = ao A(a), where CO \u0000 00 0 < An(a) := Ea knak = a Eak,na k-1 < a E ak,n < a, k=1 \u0000 k=1 and similarly 0 < A(a) < a. Let e be given with 0 < E < 3. Choose N so that for n > N, I an (1) \u0000 a (3) I < Then, for n > N, \u0000 lao,n - ao = fan \u0000 - An (AE)} - fa (3e) - A W.)} \u0000 (3E) a (is) + \u0000 (iE) ± \u0000 (3E) < \u0000 + \u0000 Ae = E. Proof of Theorem. Immediately from the Lemma, P (Xn = 0) -> P(X = 0), \u0000 gx, (0) -> gx (0). Next, for 0 < a < 1, gxn (a) - P (Xn = 0) _> gx (a) - P (X = 0) a as n -+ oo. Thus, 00 \u0000 00 EP(X n = k) al' - > E P (X = k) ak-1 for 0 < a < 1. k-=1 \u0000 k=i By the Lemma, P (Xn = 1) -> lP(X = 1). You can see that the whole result now follows by induction. \u0000 ❑ If X and Y are RVs with the same pgf, then we can take X n = Y for all n, whence the DF of Xn converges to the DF of X: in other words, Fy = FX. This proves the Uniqueness Theorem 143B. 146 \u0000 5: Generating functions and the CLT Ea. Exercise: 'Poisson approximation to Binomial'. Suppose that A > 0 and that Xn has the binomial B(n,pn) distribution, where E (Xn) = npn —> A as n — > oo. Prove from the Convergence Theorem and result 144Da(a) that ( X n = k) \u0000 pa(k) := e-A A ! \u0000 (k = 0,1, 2, ...). k' You may assume initially that if nxr, \u0000 y then (1 + xn)n \u0000 e'. How do you prove this by taking logarithms and using PHopital's Rule? Compare b(20, 0.15, k) with p3(k) for k = 0, 1, 2, 3. Figure 52C(i) compares the Binomial(10, 0.3) and Poisson(3) pmfs. We shall see several uses of pgfs in Chapter 9. 5.3 Moment Generating Functions (MGFs) ►► A. Moment Generating Function (MGF) Mx of X. Let X be an RV with values in R. We say that X is MGF good if E (eao ) < oo for all a in some non-empty open interval (-6, 6) (6 > 0) containing the origin. For definiteness, we define 8(X) to be the supremum of all such 6, so that 6(X) may now be oo. We then define a) E (ex) for — S(X) < a < 6(X). If X is 'continuous' with pdf fx, then, of course, Mx(a) = f e\"fx(x) dx, —6(X) < a < 6(X). ► Aa. Exercise. (a) Suppose that X has the exponential E(rate A) distribution. Check that 8(X) = A, Mx (a) — A a \u0000 < a < A). A — (b) Suppose that X has the Poisson(A) distribution. Check that 6(X) = co, Mx (a) = exp {A (ea — 1)} . B. The 'normal integral'. The standard normal N(0, 1) distribution has pdf cp(x) := \u0000 (B1) 5.3. Moment Generating Functions (MGFs) \u0000 147 To verify that / := f 4p(x) \u0000 = 1, \u0000 (B2) fft. we use a change to polar coordinates (`dxdy = rdrdO') (see the lacobian Theorem' 246C) and a substitution u = 17-2 within the calculation: / 2 = f f 2 co(x)co(y) dxdy = \u0000 f f exp \u0000 (x2 + y2)} dxdy DE \u0000 27r 2ir \u0000 Do 1 f p° \u0000 2 exp( \u0000 )r dr f dB = f e' du = 1. 27r Jo \u0000 0 \u0000 0 Suppose that a Random Variable G is distributed N(0, 1). Let X = µ + aG, where E R and a > 0. Then Fx(x) := P(X x) = P (G < x \u0000 P) = FG ( X \u0000 so that, since Fb(y) = co(y), al co (x \u0000 —a p) = \u0000 1 \u0000 (x — 12)2 fx( x) = F.c.(x) = \u0000 exp \u0000 \u0000 ( \u0000 20-2 ) ' as given at equation 54(E7). 11. C. MGFs for normal variables. Suppose that G has the standard normal N(0, 1) distribution. Then, since (p(x) tends to 0 as x ---+ co faster than any exponential, we have 6(G) = oo. For a E R, we have 1 \u0000 1 2 \u0000 1 2 f 1 \u0000 l a 2 X MG(a ) = \u0000 f27r \u0000 dx = eV \u0000 = e2 e_1(x-')2 dx because the second integral is that of the pdf of the N(a, 1) pdf and so must be 1. Important advice: the 'Change pdf' trick. You will find that it is often possible to work out an integral in this way — by expressing it as a multiple of the integral of a 'new' pdf. ❑ So \u0000 if C N(0, 1), then itIG (a) e « \u0000 (c E \u0000 (C1) Of course, there is something very special about this MGF. If X N(it, 0-2), then we can write X = µ + o-G (use the result at 55F), so we have eax = ealie\"G, and if X N N 0-2), then Mx (a) \u0000 E R). \u0000 (C2) 148 \u0000 5: Generating functions and the CLT ► Ca. The log-normal distribution. This distribution is often used in Statistics to model the distribution of a non-negative Random Variable Y. One assumes that ln(Y) ^ N(p, \u0000 (C3) It is easy to calculate the mean and variance of Y from (C2); but it is much preferable to think of ln(Y) as the 'Observation', an important case of transforming variables to get appropriate models. ► ► D. The Gamma function F(.) on (0, oo). We define this important function via x r(K) :---- \u0000 xK e dx You can easily prove by integration by parts that, (K > 0). for K > 0, F(K + 1) = KF(K), so that F(n) = (it — 1)! for n E N. (D1) One method of numerical calculation of In l'(K) for K > 0. We can use the asymptotic expansion (see Note below) which generalizes Stirling's formula: In F(K) = (K - In K - K + 2 ln(27r) 1 \u0000 1 \u0000 1 \u0000 1 , 12K 360K3 + 1260K5 1680K7 + 0(K-9 ) \u0000 (D2) in a C program: double loggam(double K){ int i; double y, corr; if (K>= 10){ return ((K - 0.5)*log(K) - K + 0.5 * log(2*PI) + 1.0/(12.0 * K) - 1.0/(360.0 * pow(K,3)) + 1/(1260.0*pow(K,5)) - 1/(1680.0*pow(K,7))); } else{ corr = 0.0; y = K; do{corr = corr - log(y); y=y+1;}while (y<=10); return (corr + loggam(y)); } } Notes on asymptotic expansions and on the above program segment. Formula (D2) is an asymptotic expansion. The 'series', in which the coefficients are calculated from the so-called Bernoulli numbers, does not converge for any K. As we know from Appendix 5.3. Moment Generating Functions (MGFs) \u0000 149 Al, p495, the term 0(K -9 ) remains bounded in modulus by some constant multiple of K -9 as K co. Note that in the program we use the relation F(K) = F(K + 1)/K to end up with a value of K at least 10, for which the asymptotic expansion gives small percentage error. You can try it out for r(i) = 1 and for r(0 = \\Fr. Da. Exercise. Deduce from equation 147(B2) that r(1) = \\Fr. • • E. The Gamma(K, rate A) distribution on (0, oo). \u0000 The pdf of this distribution (of which, for example, the celebrated x 2 distributions — of which much more later — are a special case) is defined to be AKx ff-l e-Ax fX(X) \u0000 r(K) \u0000 )( Note that the exponential E(rate A) distribution is the same as the Gamma(1, rate A) distribution. \u0000 You will see pictures of (`standardized') Gamma(K, 1) for K = 1,2,4,10 in Figure 157B(i). Sketch the density for K = 2. Warning. Especially in the Statistics literature and in statistical packages, the parameters used are K and p = 1/A. We then refer to the Gamma(K, mean /./) distribution, of which the mean is K pt(!). If X Gamma(K, mean /.0, then X/ iL Gamma(K, 1). Packages and Tables. On Minitab, one would find P(X < 67.50), if X — Gamma(25, mean 2.0), via cdf 67.50; gamma 25 2.0. giving 0.975. This tallies with the table, p517, for the chi-squared distributions studied in Subsection 152J because, if 2K is an integer, then the Gamma(K, mean 2) distribution is exactly the AK distribution, \u0000 (El) so that if H has the Ao distribution, then P(X < 67.5) = 1 — P(H > 67.5) = 1 — 0.025 = 0.975. Ea. Programming DFs of Gamma distributions. Let K > 0, and define h(x) : n=0 F(K + n + 1) a rapidly convergent series, this time. Then using the 'recurrence relation' 148(D1), we find that (Exercise!) xhi (x) = \u0000 (x — K)h(x), (K) 00 \u0000 X n 1 150 \u0000 5: Generating functions and the CLT whence (xl e' h)' = 1 r(K) xK-le', and the DF of the Gamma(K, 1) distribution is given by X y1C- le- y F(x) := \u0000 F(K) \u0000 dy = x Ke-x h(x). It is therefore easy to calculate F(x) on the computer. The FGrate function gives the DF for the Gamma(K, rate A) distribution and the FGmean function gives the DF for the Gamma(K, mean distribution. The htol is a parameter less than 'half tolerance' (usually set for the purposes of this book to 10-5 to guarantee an answer correct in the 4th decimal place.) double Fgamma(double K, double x){ int n; double sum, f, g, ratio, term; g = loggam(K + 1); term = sum = exp(-g)*pow(x,K)*exp(-x); n=1; do{ratio = x/(K+n); term = term * ratio; sum = sum + term; n++; }while ((fabs(term) > htol) II (ratio > 0.5)); return sum; } double FGrate(double x){return Fgamma(K, lm * x);} double FGmean(double x){return Fgamma(K, x/mu);} ► Eb. Exercise. By making the substitution Ax = y, show that the pdf of Gamma(K, rate A) does integrate to 1. By using the 'Change pdf' trick, or otherwise, show that if X - Gamma(K, rate A) then, for -A < a < A, Mx(a) = ( A \u0000 A K ce) ► F. Simulating Gamma variables. It is important, particularly for Bayesian Statistics, to be able to simulate RVs with Gamma distributions efficiently; and it is not at all obvious how to do this. Very many methods are available — see Ripley's book for those used in 1986. We look at two taken from that book (but solving the exercises there by providing sketched proofs). Fa. Gamma(K, 1) simulation for K < 1 (Ahrens and Dieter). Here is the 'two-type rejection sampling' method: Repeat Choose U with the U[0,1] distribution; 5.3. Moment Generating Functions (MGFs) \u0000 151 if U > e/(K e), let X = — ln{(K e)(1 — U)/(Ke)} so that X > 1 and accept X with probability XK -1 ; if U < el (K + e), let X = {(K + e)U/e}l / K so that X < 1, and accept X with probability e —x; until X is accepted. Sketched proof that this works. For x > 1, we have du = Ke e'dx K + e and when we allow for acceptance probability, the probability that U will lead to an accepted value of X in dx (x > 1) is Ke \u0000 e —x x K-1dx. K + e For x < 1, Ke du = K + e X K-l dx, and after allowing for acceptance probability, we see that the probability that U will lead to an accepted value of X in dx (x < 1) is also Ke \u0000 e —xx K-1 K + e \u0000 dx. Hence X is truly Gamma(K, 1) and Ke acceptance probability = K e ['(K). As K J. 0, this tends to 1; as K 1, it tends to e/(e 1); and when K = z it equals eVi/(1 + 2e); so the method is good. `C' code for the above Ahrens—Dieter method and for simulating Gamma variables for which K > 1 is given in Subsection 2561. ► G. Finding moments: the name `MGF'. For `MGF good' X and for l al < S(X), we have — at least formally, but by easily justified rigour — a2x2 \u0000 2 MX( 0 ) = E eax = E (1 + \u0000 + 21 + \u0000 = 1 + pla + pu2a +..., where pr is the rth moment of X: it, := 1.4(X) := E (X r)(r = 1, 2,3, ...). We may therefore read off the moments pr as coefficients in this power series. 152 \u0000 5: Generating functions and the CLT ► Ga. Exercise: Moments of normal and Gamma distributions. (a) Prove that if G has the N(0, 1) distribution, then E (G) = 0 and Var(G) = 1. Deduce that if X \u0000 then E (X) -= u , Var(X) = o-2. (b) Use its MGF to prove that an RV X with the E(rate A) distribution has kth moment E (X k) = k!/A k. Hence prove 148(D1). Note that 1 if X E(rate A), then E (X) = 1 Var(X) = v . (c) Prove more generally that if X Gamma(K, rate A), then E (X) = K, Var(X) = ► H. Fact: The Uniqueness Theorem for MGFs. Suppose that X and Y are `IVIGF good' RVs, and that for some 6 > 0, Mx(ct) = My(ci) for —8 < a < 6. Then Fx = Fy. As already stated, this is too difficult to prove at this level. ► I. The 'Independence means Multiply' Lemma for MGFs. Suppose that X and Y are independent VIIGF good' RVs. Then for la! < min (6(X), 6(Y)), we have Mx+Y(a)= Mx (a)MY-(a)• Ia. Exercise. (a) Prove Lemma I. Hint. See the proof of 144D. • (b) Sums of independent 'normals' are 'normal'. Prove that if X and Y are independent, X N \u0000 a?), Y N (/2, cr3), then X + Y N \u0000 + 112, a? + Cry). ► (c) Sums of independent Gamma variables. Prove that if X and Y are independent, and X — Gamma(Ki , rate A), Y Gamma(K2, rate A), then X + Y Gamma(Ki + K2, rate A). Note that it follows that the sum of 71, HD RVs each with the E(rate A) distribution has the Gamma(n, rate A) distribution, a result of considerable importance. ► ► J. Chi- squared distributions. These distributions play a fundamental role in Statistics. For v = 1, 2, 3, ..., the x2 distribution with v degrees of freedom is defined to be the distribution of GT + + + G 2,,, where G1, G2, \u0000 , C,, are IID each N(0, 1). You have been told at 149(E1) that the x,2, distribution is the Gamma(zv, rate 1) distribution. \u0000 (J1) 5.3. Moment Generating Functions (MGFs) \u0000 153 Proof That the xi distribution is the Gamma(1, rate 1) distribution is immediate from \u0000 Exercise 56Fc. The case of general v is now obvious from Exercise Ia(c). \u0000 ❑ ► Ja. The non-central x2 (p2) distribution. This is another important distribution for Statistics. Again let Gi, G2, \u0000 G, be IID, each N(0, 1). Let al, a2, • • • , av be real numbers. Then, as we shall see in Chapter 8, the distribution of W := (al + G1)2 + (a2 ± G2)2 ± • • • + (a,, G,,)2 depends only on v and p2 = 4 + 4 + • • • ai2, (p > 0); it is called the non- central x (p2) distribution, and p2 is called the non-centrality parameter. We write W N non-central x2,, (p2). Jb. Exercise. Prove that in the case when v = 1, Fw (w) = 4> ( \u0000 - p) - 4> ( -VW) - p) \u0000 (w > 0), fw (w) = (27rw)- le- 'tuA. P2 cosh (p N/Tv) \u0000 (w > 0). ► K. Fact: The Convergence Theorem for MGFs. Suppose that X and X,, (n = 1, 2, 3 ...) are RVs such that for some b > 0, we have 6(X,i) > S for each n and S(X) > S. Suppose also that Mxn (a) -> Mx(a) for I a l < S. Then Fx„(x) -> Fx (x) at every point x at which Fx(x) is continuous. This is another result too difficult to prove here. ► L. Cumulant Generating Function Cx ; cumulants kr (X). If X is MGF good, then we define the Cumulant Generating Function Cx of X via Cx (a) := ln Mx (a) for lal < 6(X). Using the Taylor series ln(1 y) = Z Y 1+ 1 t dt = foY (1 — t t2 — t3 + • • .) dt = Y - - 2 Y` + -3 Y' - - 4 Y-* + • • • we see intuitively that, provided lal is small, Cx (a) = ln ( 1 ± pia ± 2 —1 ! p2a2 ± • • • _ ( pia ± 11.12a2 ± ...) \u0000 1 2 0 2 ± 1 3 0 3 ... k ) \u0000 ) 1 \u0000 1 = icia + — 2!1c2a2 + — 3! k3a3 + ' ' • 154 \u0000 5: Generating functions and the CLT for some constants = Kr The constant Kr (X) is called the rth cumulant of X . ► M. Lemma. Let X be MGF good, and let c be a constant. Write Kr for Kr (X). Then the following results are true. (a) Ki (X + c) = Ki + c, Kr (X + c) = K r (r > 2). (b) Kr (CX) = Cr Kr. (c) = /21 = := E (X). (d) K2 = E {(X — 11)2 } = 4 = Var (X). (e) K3 = E {(X — /2)31. fc4 = E {(x p) 4} — 3E f(X — /2) 212. It is clear that if all the moments ilk := E (X k ) of X exist, then we can define the cumulants Kr (X) as polynomials in the /2k even if X is not MGF good. We have (with statisticians understanding 'skewness' and 'kurtosis') mean, 1d2 = variance, 3/2 =: 'skewness', IC2 4 =: 'kurtosis'. 1C2 See Subsection 163J below for 'correcting the CLT by allowing for skewness, kurtosis, etc'. Ma. Exercise. Prove Lemma M. Hint. Use Part (a) and the fact that X = (X — it) + 1.1 in doing Parts (c)—(f). For r > 2, Kr (X) = tc,(X — /2), and 'p i for X — p, is 0'. ► N. Lemma. The following results hold and (Fact) the described properties characterize Normal and Poisson distributions amongst distributions with finite moments. (a) If X — N(0, 1), then K1 = 0, K2 = 1, Kr = 0 (r > 3). (b) If X Poisson(. ), then Kr = A (r > 1). Na. Exercise. Prove results (a) and (b) of Lemma N. Accept the 'characterization' part for now. There are notes on it at (Pb) below. ► 0. Lemma. If X and Y are independent MGF good RVs, then with 6 := min (6(X), 6(Y)), Cx+ y(a) = Cx(a)± CY(ce) \u0000 < 6), Kr(X +Y) = K r (X) + Icr(Y) (r > 1). The second equality is true for a fixed r provided only that E VI') and E (Iyir) exist. 5.3. Moment Generating Functions (MGFs) \u0000 155 Oa. Exercise. Prove the above lemma, assuming throughout that X and Y are MGF good. P. A cumulant guarantee of convergence to the normal distribution. This is the way that statisticians see things - and for good practical reasons. Mathematicians prefer Theorem 168E below. ► Pa. Fact. If Vn, is a sequence of RVs each possessing moments of all orders, and if K2 (Vn ) —>' 1, kr (Vn) -- 0 (r 2), \u0000 (P1) then, with N ,--, N(0,1), we have, for every x in 118, IP (Vn, < x) -> IP(N < x) = 4)(x) := f x co(y) dy. A heuristic idea is that under suitable extra conditions, (P1) implies that, for small la I, MvJa) = exp(Cva (a)) — ) exp (za2) = MN(a), and now we use Theorem 153K. However, this heuristic idea is dangerous ... . ► Pb. Warning. It is possible to find two RVs X and Y with E (IX I' ) < oo and E ( Irr) < oo for r E N, such that E (X' ) = E (Y r) for every r E N but X and Y have different distributions. Note that X and Y have the same cumulants. If Xr, := X for n E N, then Kr (Xn) --> kr (Y) for every r E N, but the distribution of X7, does not tend to the distribution of Y. See Appendix A8, p500, for how to construct the pair (X, Y). Regard proving the positive results stated as Facts within (154N) and at Pa (in each of which normal distributions and their cumulants play a key part) as easy exercises after you have read [W]. [[Technical Notes for such exercises. At 154N, prove that the characteristic function of X must be that of the normal distribution. In regard to Fact Pa, it follows already from the fact that ni (Vii) —>- 0 and 1c2(V„) —> 1 that the distributions of the lin form a 'tight' family. Of course, Result 154N is needed to complete the proof of Fact Pa.]] ► Q. The Laplace transform Lx (a). If X is a non-negative Variable, then Lx(a) := E (e-ax ) exists for all a > 0. The distribution of X is determined by the function Lx(•) on [0, oo). We have - lix (0) = E (X) < oo, etc. For independent non-negative X and Y, we have Lx+y- = LxLy. Finally, for non-negative X7, and X, we have L xn (a) -+ Lx (a) as n -> oo for all a > 0 if and only if Fx,i (x) -> Fx (x) at every point x of continuity of Fx. 156 \u0000 5: Generating functions and the CLT 5.4 The Central Limit Theorem (CLT) ► A. Standardized form Y* of a Random Variable Y. Let Y be an RV in ,C2, so that E (Y) and Var(Y) exist. Let SD(Y) denote the standard deviation /Var(Y) as usual. We suppose that SD(Y) > 0, so that Y involves some randomness. We define the standardized form Y* of Y via Y Y) Y* = SD(Y) The RV Y* is standardized in the sense that E (Y*) = 0, SD (Y*) = 1. We have Y N(p, a-2) for someµ and a2 > 0 if and only if Y* N(0, 1). ►►► B. Fact: The Central Limit Theorem (CLT) (de Moivre 1706, Laplace 1812, Lindeberg 1922, ...). Let Xi, X2, . be HD RVs with ANY common distribution of finite mean p and variance 0-2 > 0. Let Sn := Xr + X2 ' \u0000 Xn, as usual. We know from Lemma 104E that E (Sn) = np, Var (Sn) . 2> sp (so (a) To formulate precisely the idea that \"Sn is approximately N(n,u,, n \", we let .57; be the standardized form of S. Then, as n oo, for every x in P(Sn < x) \u0000 4)(x) . \u0000 co(y) dY = \u0000 -V2ir \u0000 d (b) If An denotes the average An := &/n, then E (An) p, Var (An) = a2/n, SD(An) \u0000 a. Then An = Si*, and we formulate precisely the idea that An is approximately N(p, a2/n) via P(A9,* \u0000 x) = (S:; x) --+ 43(x). (c) if each Xk has the N(p, a2) distribution, then we have, exactly, Sn N(np,ncr2) and An e•-, \u0000 a2/n). n = 4 n = 2 5.4. The Central Limit Theorem (CLT) \u0000 157 n = 1 Figure B(i): pdf of standardized sum of n E(1) variables Sketched proof under 'finite moments' assumption. By replacing each Xk by its standardized version, we can – and do – reduce the problem to that in which p = 0, o -2 = 1, \u0000 = \u0000 Let us assume that if X is an RV with the same distribution as each E (IXIk) < oo for every positive integer k. Then Ki (X) = E (X) = 0, n 2(X) = a-2x = 1. By Part (b) of Lemma 154M and Lemma 1540, we have 1 \u0000 r Kr (S„.1) = (— 1F ) K T. (5n ) = \u0000 1 AFt r ) nk r (X), 1 so that as n \u0000 oo, ni (S9c)=0, K2 (S7*,.) = 1, K r (S;;) \u0000 0 (r. \u0000 3). By Fact 155Pa, the result follows. Xk, then 0 158 \u0000 5: Generating functions and the CLT How the general case is proved — and I mean proved — is sketched at Section 168F below. Figure B(i) shows the pdfs of Si', S;, S4, Sf 0, superposed on the shaded area under the N(0, 1) pdf for the case when the Xk's are IID each exponential E(1). It shows very clearly the inexorable march towards the normal shape. Note. The vertical scale is taken bigger than the horizontal in most of our pictures of normal distributions — for aesthetic reasons. —1.96 \u0000 1.96 Figure B(ii): Some N(0, 1) probabilities In practice, we commonly use the facts (illustrated in Figure B(ii), where the graph is that of co, and discussed in the next subsection) that if G N(0, 1), then, to sufficient accuracy, P(G < 1.9 = 0.975, \u0000 P(G! < 1.96) \u0000 0.95, P(G < 2.58) = 0.995, \u0000 < 2.58) = 0.99. Hence, we can say that, if n is large, then PaSn \u0000 (Sn) < 1.96 SD (Sn)) = INISn — nµ < 1.96a NN = P(lAn til < 1.96a/ /) R-2. 0.95. C. Working with normal distributions. The N(0, 1) Distribution Function 43: ()(x) = P(G < x) = f c.o 1 \u0000 2 dy is tabulated and available on all Statistical Packages. A simple method of calculating 4. in 'C' is given later in this subsection. One problem with the tabulations is that many different ways of tabulating Distribution Functions are used. The tables at the end of (B1) (B2) 5.4. The Central Limit Theorem (CLT) \u0000 159 this book follow the most commonly used convention of tabulating 4 in the obvious way, but basing other tabulations on tail probabilities (focusing on 1 — F(x) instead of F(x)). Do be careful whether a table in another book gives (for whatever distribution) IP(Y < x) or lP(Y > x) or P(0 < Y < x) or whatever. It is easy to switch from one to the other. Because the famous bell-shaped graph of the pdf co is symmetric about 0, we have, for example (as illustrated in Figure B(ii) for the case when x = 1.96 for which 4i(x) = 0.975), 4)( — x) = 1 — 4:0(x) (x E E8). If G N(0, 1), then P(a < G < b) = (1,(b) — (P(a). Check from the table, p515, that if G N(0, 1), then P(-1.234 < G < 0.567) = 0.7147 — 0.1086 = 0.606. Minitab uses cdf for (Cumulative) Distribution Function, and, with \\\\ denoting 'new line' (not of course typed in Minitab ), minitab \\\\ cdf - 1.234; \\\\ normal 0 1. gives 4)(-1.234) = 0.1086. If X — N (A, 0-2), then X = p, + o-G and so P(a < X < b) = P (a—A X—,a b-1.1 \u0000 < \u0000 < \u0000 o- \u0000 o \u0000 o- ) = P ( a — a A G b \u0000 — it ) < < a — (b 0, A) 4. (a --u 1.1) 4. Check that if X — N(1.2, 1.69), then P(0.1 < X < 1.7) = 0.451. Warning. Most computer packages use the parameters it and a (the standard deviation, not the variance) for the N(ii, a2) distribution. So, for example, on Minitab , \\\\ cdf 1.7 K2; \\\\ normal 1.2 1.3. \\\\ cdf 0.1 K1; \\\\ normal 1.2 1.3. \\\\ print K2-K1 would be needed. Calculation of 4:1, in C. There is a huge literature on clever ways to compute 4.. Nowadays, one doesn't have to be clever. One method is as follows. 160 \u0000 5: Generating functions and the CLT Let f (x) = exp (1x2) fox exp (- 1y2) dy. Then f(0) := 0, \u0000 f(x) = 1+ x f (x), \u0000 f (x) := f {1 + yf(y)} dy. Picard's Theorem on Ordinary Differential Equations solves this via \u0000 fo (x) = 0 (Vx), \u0000 fn+i(x) = f \u0000 + yfm(y)} dy, and here guarantees that fri(x) — > f (x) as 71 -} CC . Hence, we see that \u0000 x x3 \u0000 x5 \u0000 x7 \u0000 f (x) = — + — + \u0000 + \u0000 + . \u0000 1 \u0000 1.3 \u0000 1.3.5 \u0000 1.3.5.7 Of course, \u0000 1 \u0000 1 _ \u0000 4)(x) = 2 + \u0000 r e _2 f (X). This may be implemented as follows: double Phi(double x){ int j; double sum, term, ratio; term = x*exp(- x*x/2.0)/ sqrt(2.0 * PI); sum = term; j = 2; do{ ratio = x*x/(2*j - 1); term = term * ratio; sum = sum + term; j++; }while ((fabs(term) > htol) II (ratio > 0.5)); return(0.5 + sum); } Ca. Simulating from a general normal distribution. For simulating from general Normal distributions, we use (with precision prec the inverse of variance): double SDnormal(double mean, double sd) { double random; random = mean + sd * aGauss(); return random; } double PRECnormal(double mean, double prec) { double sd = 1/sqrt(prec); return SDnormal(mean,sd); 5.4. The Central Limit Theorem (CLT) \u0000 161 ► D. 'Integer' or 'continuity' correction. Suppose that each Xk in the CLT takes only integer values. Then we have for integers a and b, P(a < < b) = P(a — < Sn < b+ by obvious logic. A picture would suggest (correctly) that it is better to use the right-hand side in applying the CLT: • ( b + — \u0000 4) ( a — — np) P (a < Sn < b) a \\ fit- ) \u0000 o- \u0000 ) This is obviously better when a = b(!), and you can now explain result 11(N2) that b(2n, n) \u0000 (71-n)-1 Check that the CLT gives the estimate P(41 < Y < 59) ti 0.9426, mentioned at 106Ha. As stated there, the exact answer to 5 places is 0.94311. ► E. Important note on convergence in the CLT. In the CLT, we have \u0000 P(S, < x) \u0000 (D(x) = P(G < x), where G is an N(0, 1) variable. Thus, the statement that P (,57, E B) —> P(G E B) \u0000 (El) is true if B has the form (—oo, x]. However, (El) need not be true for all nice sets B. Suppose for example that X1, X2, . are IID, each with lP(Xk = +1) = 2. Then every S:,'; must take values in the countable set C of numbers of the form r/ N/7s, where r is an integer and s is a positive integer. We have 1 = P (Si*, E C) 7L IP(G E C) = 0. Here is a positive result, which depends on the famous Scheffe's Lemma in Measure Theory. ► Ea. Fact. If Z, has pdf f n and Z has pdf f, and fn(x) f (x) for every x, then 111(Z, E B) —> P(Z e B) for every nice (measurable) set B. Sometimes we can prove cases of the CLT directly (and with stronger conclusion) by showing that the hypotheses of Fact Ea hold where Z7, = .5,,* and Z = G. This will be illustrated by examples. 162 \u0000 5: Generating functions and the CLT F. Application to exponential RVs. We discuss the case illustrated at Figure 157B(i). Suppose that each Xk has the E(rate A) distribution for some A > 0. We know that the E(rate A) distribution has mean 1/A and standard deviation 1/A — see Exercise 152Ga. We know — see Part (c) of Exercise 152Ia — that Sr, is distributed Gamma(n, rate A) (with mean n/A and variance n/A2, of course), so that for y E R, we have, as n \u0000 oo, \u0000 if y E R and zn := — n + y \u0000 then \u0000 f z A \u0000 A Anxn—ie —Ax \u0000 P(Sn 5 zn ) = J R \u0000 dx = P(S7,, < y) —÷ 4,(y). \u0000 (Fl) \u0000 o \u0000 (n — 1)! Especially, for large n (and n does need to be large when dealing with exponential variables), P (1.4n — p,I < 1.645 \u0000 /1 I N 90%, \u0000 (F2) where µ = 1/A is the mean of one of the Xk's. We can prove the CLT for this situation directly. By Subsection 55F, we have \\Fn \u0000 fs:,(x) = \u0000 ‘fs, (—n +x \u0000 , and, by using Stirling's formula 11(N1), and the facts that n \\ n-1 — 1) (see 496(ApA 2.4)) we find that (Exercise!) \u0000 fs.7,(x) \u0000 (p(x) for all x. By Fact 161Ea, this is a stronger conclusion than that of the CLT. G. Exercise. Show that if each Xk in the CLT has the Poisson(A) distribution, then, for large n, P (O n — AI < 1.960/n) r-;.; 95%. Show that if Yn \u0000 Poisson(n), so that Y has the same distribution as the sum of n IID Poisson(1) variables, then for y E R, as n —> c>o, n k \u0000 P Yn n + y \u0000 = \u0000 e — k! —> 4)(y)• 0<k<n±y.17/. H. Exercise: Numerical Examples. Use the Integer Correction when appropriate. (a) Let S be the number of sixes and T the total score if I throw a fair die 60 times. What are E (S) and Var(S)? Show that E (T) = 210 and Var(T) = 175. Show that the CLT leads to the estimates: \u0000 (i) P(10 < S < 13) \u0000 0.456, (ii) P(200 < T < 230) \u0000 0.726. e, ln(1 + y) = y — zy 2 + 0 (y3) for small y 5.4. The Central Limit Theorem (CLT) \u0000 163 Note. The exact answer to (i) (to 4 places) is 0.4384. (b) An RV X has the Poisson distribution with parameter 25. Use the CLT to show that P(22 < X < 28) 0.5160. Note. The exact answer (to 5 places) is 0.51610. (c) Show that if Y is the sum of 10 HD RVs each with the exponential E(1) distribution, a case pictured in Figure 157B(i), then 162(F2) would lead us to estimate the values y such that IP(Y > y) = 0.025 and 0.01 respectively as 16.2 and 17.4, whereas the correct values from the table, p517, are 17.1 and 18.8. See 149(E1). Better methods of approximating Gamma distributions by normal ones are given in the Subsections K and J below. I. Accuracy of the CLT. We have already seen numerical instances of how accurate the CLT is. Here now is an amazing theoretical result on the accuracy, a uniform bound over x. ► Fact: The Berry— Esseen Theorem. In the CLT, we have sup IP(S < x) — (1,(x)1 < 10 E (lx — '13) (I1) xER \u0000 0\"3-V7i, \u0000 • Suppose that we consider approximating the Binomial(2n, 1) distribution using the CLT. We know from Exercise 106Ha that the jump b(2n, 2 i n) in P (Sri* < x) at 0 is asymptotic to 1/ ern. Since 41, is continuous, the supremum in (I1) must be at least equal to this jump. The right-hand side in this case is 10/V2n. The order of magnitude in the Berry- Esseen Theorem cannot be improved. For proof of the Berry—Esseen Theorem, see Stroock [221]. ► J. Improvements of the CLT: Edgeworth expansions; saddle- point method. If in the CLT each Xk has mean 0 and variance 1 with higher cumulants ..., then, as our heuristic proof of the CLT showed, a 2 k3a 3 k4a 4 MS,*, (a) = exP { cs;ja)} = exp \u0000 2! ± \u0000 4!n 3 1 2 \u0000 { n3a3 +43 (1)} \u0000 el a2 {1+ e2\" exp \u0000 3613a \u0000 + 0 ( — 1 ) . 3!-VT/, JJJ But, by differentiating fil, eaxco(x)dx = ea 2 with respect to a three times and combining the results obtained, we find that e\" (x 3 — 3x) yo(x)dx = ct3e1' 2, suggesting that a better approximation than the CLT would be (under suitable conditions) P \u0000 < x) \u0000 4)(x) + 1c3 3!f f (y3 — 3y) (P(Y)dY = 4)(x) 31:F13 (x2 — 1)c0(X). 164 \u0000 5: Generating functions and the CLT (You check that (d/dx){(1 — x2)co(x)} = (x3 — 3X) cp (x).) One can of course expand in similar fashion to include the higher cumulants. We saw at (c) of Exercise 162H that the CLT does not approximate Gamma distributions particularly well. Using the example there, we have if Y Gamma(10, 1), then lP(Y < 17.08) = 0.975. With x = (17.08 — 10)/ 10, we have 4)(x) = 0.988, \u0000 tc3 = 2, \u0000 4.(x) \u0000 \" \u0000 (X2 — 1)c0(X) = 0.974, 3!-VT/ showing that correcting for skewness here leads to significant improvement. There are more powerful ways of improving on the CLT. Of particular importance is the saddle-point method applied to the inversion formula for obtaining Distribution Functions from Characteristic Functions. This was much developed in the statistical context in papers by Daniels. See, for example, Daniels [53], Reid [192], Barndorff- Nielsen and Cox [8, 9]. K. Another method of approximating Gamma distributions. This topic is optional, but is referred to in most sets of tables. For large K, it is the case that if V — Gamma(K, 1), then N/fi is approximately N( \\/K — 4 (Z) 2). \u0000 (K1) Ka. Note. Because of 149(E1), this is equivalent to the statement usually mentioned in x 2 tables, that, for large v, if H 7d then .\\/f/ is approximately N(i/2v — 1, 1). For µ = 100, we have (/199 + 1.96) 2 /2 = 129.07, (✓199 — 1.96) 2 /2 = 73.77, agreeing quite closely with the values 129.56 and 74.22 in the last row of the table on p517. Kb. Exercise. Show that if Y is the sum of 10 IID RVs each with the exponential E(1) distribution, a case pictured in Figure 157B(i), then (K1) would lead us to estimate the values y such that P(Y > y) = 0.025 and 0.01 respectively as 16.8 and 18.4 — significantly better than the CLT approximations at part (c) of Exercise 162H. Proof (and rigorous formulation) of (K1). Let a E R. We now use 55(F) to obtain the distribution of Z if X has the Gamma(K, 1) distribution and \\a = Z + K — a. We have f x (x) = r(K) ' ( ) (z + \u0000 — a) 2 5.4. The Central Limit Theorem (CLT) \u0000 165 Thus, \u0000 f z (z) = f x (11)(z))11/ (z) \u0000 2 (z + -VK — a) 2K-1 \u0000 ± K -a)2 r(K) whence In fz (z) = ln 2 + (2K - 1) In (A/K - a + z) - (z + \u0000 - a) 2 — In r(K) = ln 2 + (K - 4) ln(K - a) + (2K - 1) ln (1 + Atkz a ) - (z + ✓K - a) 2 — In r(K). Fix z. After writing (see Appendix Al, p495 and Appendix A3, p496 for '0' and 'o' notation) z \u0000 1 z 2 In Cl + A/K - a) = vK -a 2K - a 4-0(K-3/2), \u0000 z - \u0000 z \u0000 a -VK -a \u0000 AN 1-1-117+0(K-2)), \u0000 In F(K) = (K - In K - K \u0000 ln(27r) + o(1), the first being 496(ApA 2.4), the second the result of applying 496(ApA 2.2) to the expression (1 - alK)-1, and the last Stirling's formula 11(N1), we find that, whatever the value of a, as K oo, f z (z) \u0000 2 the pdf of the normal distribution with mean 0 and SD 1. Now note that if W N WK - a, 2 2), then E (W2) = E (W)2 + Var(W) = K - a + so it makes sense to choose a = L. 'Central Limit Theorem for medians'. The following result leads to a simple effective method of obtaining Confidence Intervals in some situations. La. Theorem. Suppose that Y1, Y2, \u0000 , Yn are IID, each 'continuous' with strictly positive pdf f on R (or on some subinterval of ]R). Let mo be the median of each Yk, so that F (mo) = 1, where F is the Distribution Function of each Yk. Then the Sample Median M = Mn of (Y17 Y2, \u0000 Yn) has approximately the \u0000 N (mo, \u0000 1 2 ) 4n f (mo)- distribution in the precise sense that, as n -+ oo, P(20 f (mo)(Mn - m o) < x) (1.(x) (x E R). e-2z2 166 \u0000 5: Generating functions and the CLT Recall that the Sample Median of (Y1, Y2, \u0000 , Yn) is defined to be Y(2 (,,,+11) if n is odd, and 2 {Yq n) + Y(a,.,±1) if n is even, where Y(i) < Y(2) < • • • < 170-0 are the Yk 's arranged in increasing order. (With probability 1, no two of the Yk 's are equal.) Proof when each Yk has the U[0, 1] distribution. We discuss only the case when n is odd: n = 2R + 1. From Subsection 107L, we know that P(M E dm) = (2R + 1) CRR) m R (1 — m)Rdm. \u0000 (L1) If we put and use m = 1 — 2 + 1 w 2 ./2R' dm = \u0000 dw 2V2R O) W 2 ) R (see 11(N2)), \u0000 (1 — \u0000 —> e-1 2w2 R ) 22R \u0000 ti' FR \u0000 2R \u0000 ' we find that, as n —> oo, the right-hand side of (L1) converges to yo(w)dw, where co is the pdf of N(0, 1). Hence, for large n, we have approximately 1 W M = 2 + 2-V2R where W N(0, 1). This proves the result — even rigorously because of Fact 161Ea. \u0000 ❑ Proof of the general case for odd n. Each F(Yk) is U[0, 1], by Exercise 51Ba. But F(M) is the Sample Median of the F(Yk), and so, with obvious notation, 1 W \u0000 = 2 \u0000 2 \u0000 /7 F(M) \u0000 F (mo) + (M — mo) f (rno) -‘/ and, since F (mo) = 1, we have (M — mo) W/ {2f (mo)}. The result follows, and it is not difficult to make this step fully rigorous. 5.5 Characteristic Functions (CFs) ► ► A. The Characteristic Function cox of X. (Apologies for the fact that co's denotes CFs and the pdf of the normal — both standard notations.) The CF cox of X is the map cox : —> C defined by cox (a) \u0000 E \u0000 E COS X) + HE (sin aX), \u0000 (Al) 5.5. Characteristic Functions (CFs) \u0000 167 the integral existing for ALL a in R. One obvious advantage of the CF is that it is defined for all a. Another advantage is that I Cox (a) I < 1 for all a, and that cox has other nice boundedness properties. A huge additional advantage is that it is possible to calculate cox exactly in numerous important cases via Cauchy's miraculous Calculus of Residues which you study in courses on Complex Analysis. Moreover, there is an explicit inversion formula giving the Distribution Function in terms of the Characteristic Function — a strong version of the required Uniqueness Theorem. In particular, if fR cpx (a) da < oo, then X has pdf fx given by fx(x) = 27r J e \u0000 (a) da. \u0000 (A2) It is all part of Fourier-transform theory. We can replace a by is in the MGF formulae we found for binomial, Poisson, normal, Gamma distributions (provided that we use the correct branch of the logarithm in the last one). Especially, we have WG(ct) e \u0000 (a E ) \u0000 (A3) for G N(0, 1). B. CF for the standard Cauchy distribution. \u0000 The standard Cauchy distribution has pdf {71-(1 \u0000 x2)}—' on R. A variable X with the standard Cauchy distribution has no moment µk for any k = 1, 2, 3, .... But, of course, it has a CF, which is found to be (Px(a) -= by using Residue Calculus. Note how this checks with (A2). ► C. Fact: Uniqueness Theorem for CFs. If cp x (a) = cOy (a) for ALL a E IR, then Fx (x) = Fy (x) for all x E Notes. It is not enough here to have cox (a) = co y(a) for all a in some open interval containing 0: it has to be on the whole of R. I should also mention that (pix (0) can exist even for X not in L. ► D. The 'Independence means Multiply' Lemma for CFs. Suppose that X and Y are independent RVs. Then, for all a E cox+y(a) = Cox (a)CoY(a)- \u0000 (Exercise!) 168 \u0000 5: Generating functions and the CLT Da. Exercise: sums of Cauchy RVs. Show that if X1, X2, . , Xn, are IID RVs each with the standard Cauchy distribution, then the sample mean X also has the standard Cauchy distribution. This fact that the mean of Cauchy variables is just as spread as a single variable was discussed at 109N, where it was seen that the median does concentrate. For more on the Cauchy median, see 196Ec. ►► E. Fact: Levy's Convergence Theorem for CFs. Let Xri and X be RVs. We have coxn (a) —> cox(a) for all a E R if and only if Fxn(x) Fx(x) at every point x at which F is continuous. This is the perfect mathematical result which can be used to prove convergence to (say) Cauchy distributions as much as to normal ones. (For a wide class of variables without finite variance, there are other forms of the CLT involving different normalizations from and non-normal limits.) ► F. Note on 'CF' proof of the CLT. For the details of rigour, see, for example, [W]. The heuristics are obvious. Take µ = 0 and a = 1 in the CLT. Then cox(a) = 1 \u0000 o(a 2) as a —> 0. Hence, 2 \u0000 2 \u0000 n a p s:1(a) = p x \u0000 = V ( T/ 1 2n — \u0000 + (-n )) e Z a2 = cPc (a) and the result follows. The miracle is not lost: it has been shifted from the result to the proof. 6 CONFIDENCE INTERVALS FOR ONE-PARAMETER MODELS 6.1 Introduction • • A. Overview. This chapter presents a thorough treatment of both Frequentist and Bayesian theories (both of which you should learn) for the simplest, namely one-parameter, situation. The principles studied here extend to the multi- parameter case, and we shall see several of the extensions in later chapters. We have in mind three notations: Y = \u0000 Y21 — ,Yn) for Observations as PreStatistics; y'bs = (yYbs, Obs, \u0000 , gb.) for the corresponding actual statistics after the experiment is performed, so that es = Yk(w-t ): y (368 is the realization of Y; Y = \u0000 , y2, • • • Yn.) for possible values taken by Y. For the sample mean of yobs we shall write yobs (rather than yobs or Vbs) for typographical reasons. As far as possible, I stick to the above conventions when it helps clarity; but there are times when sticking to them too ruthlessly becomes so cumbersome as to be unhelpful. So, sometimes, whether Tyche (or Zeus — see later!) is choosing y?cbs or yk is essentially irrelevant: you will understand when you read on. ►► The probabilistic model. What exactly are w and ft in this context, and how are the Random Variables Y1, Y2, . . . , Yn functions on Q? Well, a typical sample point w is a typical observation or sample y = (Y1, Y2, . , yn) and then Yk(w) -= Yk SO w = y = \u0000 Y2) • • • , yn) \u0000 Yk ( w ) yk , Y (w) y , y (wact) = yobs. 170 \u0000 6: Confidence Intervals for one-parameter models Of course, the sample space SI is the set of all possible samples w. The probability measure on (measurable) subsets of SI will depend on a parameter 0, and we shall write P(B I 0) for the probability of the event B when the parameter is 0. Exactly how to define this probability for the context in which we shall be working, will be seen at 181(A2) below. The notation IP(B I 0) has no 'conditional probability' significance in Frequentist theory: it is just a convenient notation. In Bayesian theory, IP(B I 0) does have a 'conditional probability' significance. I everywhere try to bring the notation and terminology (and results!) of the two approaches into line, thereby undoubtedly offending both schools. The notation P(B; 0) would better reflect the Frequentist view. ►► Notational Nightmare. The standard notations of Probability and of Statistics are often in conflict. Thus, for example, It always denotes the sample space in Probability, but often denotes the parameter space in Statistics. More importantly for us, probabilists always write Sn for sum, writing S Sn YI + Y2 + \u0000 Yn, \u0000 (We use) which takes the forms sobs = Y1)1)5 + Y2b8 \u0000 gbs, (We use) s = Y1±Y2+•••+yn, \u0000 (We use) for observed and possible values. However, statisticians use s2 = E (yk - v)2 , \u0000 (We do NOT use) n s2 — 1 \u0000 n i \u0000 1 i (yk - y) 2 , \u0000 (We do NOT use) for variance estimates, with sn and sn_i, their square roots, as standard-deviation estimates. We use (in later chapters) (We use) even though ern_1(Y) is not the Maximum-Likelihood Estimator. This is an occasion where in strict accordance with our 'upper-case versus lower-case' conventions, we should use En_i (Y), but the E sign suggests a sum rather than an Estimated Standard Deviation. If my use of sobs makes statisticians weep, so be it. \u0000 ❑ The one-parameter model which we assume throughout the Frequentist parts of this chapter is that 6.1. Introduction \u0000 171 Y1, Y2, - • • Yn are Iii} each with pdf or pre f (y 10) depending on an unknown parameter 0 which lies in a subinterval of R. Thus for example f (y 0) could be the pdf of N(0,1) (with 0 E R) or of E(mean 0) (with 0 E (0, oo)) or the pmf of Poisson(0) (with 0 E (0, oo)) or of Bernoulli(0) (with 0 E [0, 1]). We are interested in obtaining Confidence Intervals (CIs) or Confidence Regions for 0. In the last section of the chapter, I discuss how to test hypotheses on 0 if you must. The Likelihood Function llid(0;y) \u0000 1110; yi, Y2, \u0000 Yu) T f (Y110)f (Y210) \u0000 f (ynio)• \u0000 (Al) is the second most important thing in (both Frequentist and Bayesian) Statistics, the first being common sense, of course. Here 0 denotes the unknown parameter and y = (yi, y2, ... , yn,) a possible vector of observations. As a function of y for fixed 0, lhd(0; y) gives the likelihood (joint' pmf or pdf) of obtaining the observations y if the true parameter value is 0. There will be a study of joint pmfs and pdfs in the general context in the next chapter. The independence assumptions made in the current chapter simplify matters greatly. In applications, the validity of such assumptions needs to be examined carefully. In Statistics, we concentrate on the function 110(0; \u0000 a function of 0 for the actual observations yths. This is reflected in the fact that 0 appears before yobs in the notation. A value is 'large' for the observed data yobs = y(Wact ) is one of 0 for which lhd(0; yobs) _ which tallies well with the data. We begin the chapter with commonsense ways of obtaining Frequentist Confidence Intervals for 0 for familiar distributions. Then we start to develop a general theory. Fisher's brilliant concept of sufficient statistic justifies our 'collapsing' of the full information y1bs, yips, yobs to information just about a lower- dimensional statistic such as gobs in certain important cases. Ancillary statistics represent a step in the opposite direction, representing information which we must take fully into account. (This last sentence will be clarified later!) We look briefly at Point Estimation, where one wants to give a single `best estimate' of an unknown parameter, but do so largely because key ideas such as Fisher information help clarify the fundamentally important theorem on Asymptotic normality of Maximum-Likelihood Estimators (Frequentist) or of posterior distributions (Bayesian). 172 \u0000 6: Confidence Intervals for one-parameter models We then study the Bayesian theory of CIs. Apart from other major advantages (and some disadvantages), this greatly clarifies the theories of sufficient statistics and of ancillary statistics, and copes effortlessly with the important matter of predicting a new observation, a task over which Frequentist theory can stumble. However, Bayesian practice is a more messy matter than the elegant Bayesian theory. In the last section of the chapter, we look briefly at Hypothesis Testing from both Frequentist and Bayesian viewpoints. I believe that (even within the terms of Frequentist theory) the fundamental Likelihood-Ratio Test needs some modification. I shall disagree strongly with claims that Bayesian and Frequentist theories are sometimes in serious conflict for large sample sizes. In Subsection 236J we take a first look at the fundamental problem of choosing a model, with particular attention to Akaike's Information Criterion (AIC). Note on terminology. In fact, Bayesians speak of Credible Intervals (credo: I believe). They never use the term Confidence Interval, perhaps because Frequentists do! However, Confidence Intervals provide one of the more sensible pieces of Frequentist theory, and it seems to me entirely appropriate to transfer the term 'Confidence Interval' to describe the strictly analogous concept on Bayesian theory. A Frequentist CI is then in some important cases a Bayesian CI associated with a particular prior (the Trequentist prior'); and Frequentist theory then makes much more sense. (One of the many pieces of heresy in this book is that I shall — for certain special situations only — refer to a Frequentist prior (FM = 1 for a location parameter 0, 71-(-y) = -y' on (0, oo) for a scale parameter - y.) We shall see however that, in spite of heroic efforts, attempts at a general theory of prior densities which represent 'no prior information' are far from being entirely satisfactory. ► B. Regular pdfs. There are lots of occasions when we need to differentiate an integral with respect to a parameter. For example, we might wish to show that 0e —Ny = 1 implies that f (1 — 0y)e—evdy = 0 by differentiating the first integral with respect to 0. Note the effortless way that this shows that the expectation of an exponential variable with rate 0 is 1/0. The key requirement in differentiating through an integral is that the range of integration does not depend upon 0. I am not going to fuss over the rigour of things in this book. A rigorous result on when differentiation through an integral is justified is given in, for example, [WI, Chapter A16. The idea there is to transform things into theorems on integration in Measure Theory. (X) \u0000 CO 6.2. Some commonsense Frequentist CIs \u0000 173 By a regular pdf, we mean one with which one can safely differentiate through various integrals. We shall see that things work in many examples; and that is good enough for now. 6.2 Some commonsense Frequentist CIs ► ORIENTATION:Commonsense' in the title of this section is meant to convey that the main purpose of the section is to present important situations in which you may easily calculate sensible (sometimes rough-and-ready) Frequentist Confidence Intervals (CIs) without yet having any theoretical justification that these CIs are in any sense best. For the time being, we stick to the 'Frequency School' or `Frequentist' concept of a Confidence Interval. It is appropriate to use Frequentist Confidence Intervals when • either one has no reliable prior information about the values of the unknown parameter in which one is interested, • or one is using a sample so large that one's conclusions are insensitive to prior information (something clarified when we study the Bayesian approach!). ►► A. The Frequentist Definition of a Confidence Interval. To keep things simple, suppose that the probabilistic structure of observations 111.1 , Y2, . , (Pre-Statistics) to be made in an experiment yet to be performed, is completely determined by one unknown parameter 0 c R for which we seek a C% CI. We write IP(E I 0) for the probability of an event E if the true value of the parameter is 0. In the Frequentist approach, the notation has no 'conditional probability' significance. \u0000 Examples. (a) The RVs \u0000 , Y, might have the form \u0000 Yk = \u0000 Ek, \u0000 Ei,E2, \u0000 ,E r, IID, each N(0, I). For example, 0 might be the true value of a quantity to be measured, and Ek the error in the kth measurement (all in suitable units). It is being assumed that long experience has convinced us that this model is reasonable. Aa. Definition. Suppose that we can find functions T_ and T± mapping Rn to 111 such that, for every 0, P(T_(Y) < 0 G T+(Y) = P(T_ (Yi, Y2, • • • , Yn) < 0 < T-I- (1717Y -2) • • • 7 Yn) 0) 0) = c%. (Al) Thus, whatever the true value of 0, the probability that the random interval [T_ (Y), T± (Y)] contains 0 is C%. After the performance of the experiment, the Pre-Statistics Y1, Y2, • • • , Yn have been crystallized into actual statistics es , es, \u0000 ynobs where yi0e bs = yk (wact), the point coact being Tyche's choice of w from Q. See 36B and 48B. We say that [T—(Yobs) T+(Yobs)1 := [r_ (es, nobs . . . , ynobs) , T+ (Yl obs, Y2 obs, - - - , y'; bs)] is a C% CI for 0. 174 \u0000 6: Confidence Intervals for one-parameter models (b) We might be interested in estimating the fraction 0 of the adult population who will vote for some particular political party. We take a large sample of size n, and let Yk = 1 if the kth chosen person says that he/she will vote for that party, Yk = 0 if not. Frequentists, as their name implies, interpret this in LTRF (Long-Term Relative Frequency) terms. We have sorted out the Strong Law, so will no longer be fussy about precise wording of the LTRF idea. Ab. Interpretation. \u0000 A Frequentist will say that if he were to repeat the experiment lots of times, each performance being independent of the others, and announce after each performance that, for that performance, 0 E [T_ (observed y), T±(observed y)] , then he would be right about C% of the time, and would use THAT as his interpretation of the statement that on the one occasion that the experiment is , performed with result yA \u0000 (yobs) T± (yobs)] s, the interval [T_ \u0000 is a C% CI for 0. Once the experiment has been performed, [T_ (yobs), T+ (yObS )]1 is a 6.2. Some commonsense Frequentist CIs \u0000 175 deterministic interval with actual numbers as endpoints. From the Frequentist standpoint, either 0 E [7-__ (yobs), TH- (y°bs)] in which case, for any concept Prob' of 'probability', Prob (0 E [T_ (y obs) , T+ (y obs )] ) \u0000 or 0 cz ],r_ (yobs), T+ (yobs)] [T_ (yObS) , T+ (yobs)]) \\) _ in which case Prob (0 E \u0000 0. A Frequentist cannot say that Prob (0 E [ ,r_ (y obs), T+ (y obs)] ) = C%. \u0000 (A2) There are many echoes here of the 'Homer Simpson' situation at 4C. ■ B. CIs for an IID N(0, 1) sample. Now assume that Yk = 9 Ek \u0000 6 17 6 2) en HD, each N(0, 1). We know that Y — 0 has the N(0, 1/n) distribution. In that its distribution does not depend on the unknown parameter 9, it is called a Pivotal Random Variable. Hence if we choose a with (1.(a) — (I.( —a) = C%, then P \u0000 < Y — 0 < anA whence (logic!) P (Y — \u0000 < 0 < + an- 0) = C% and, after the experiment is performed with observed sample mean yobs, we say that [gobs — an gobs + \u0000 (B1) is a C% CI for 0. There are lots of other C% CIs for 9, but this is the best (shortest) two-sided one. Ba. Near-heresy, 1. Numerically — but in the Frequentist approach definitely NOT philosophically — we obtain a C% CI [T_ , TA for 0 by pretending that 0 is a Random Variable 8 with the N(yobs, n ) distribution, and so requiring that P(0 e [T_, T+]) = C%. Thus if 7-(0 yobs) (the 'posterior density function') denotes the pdf of the N(yobs, distribution, then (you check!) an interval I is a C% CI for 9 if and only if f 7(0 yobs)do = c %. This is sound Frequentist stuff (well, more-or-less), with Bayesian notation. The 'best' (shortest) C% CI will clearly take the form : 740 I yobs) > c}, and it will be that at (B1). 176 \u0000 6: Confidence Intervals for one-parameter models ►► Bb. Determining sample size in advance. If, as is being assumed, we really know from past experience that errors are IID each N(0, 1), and we want (say) a 95% CI for 0 of length at most 0.2, then we require that Zan- z < 0.2, where a = 1.96, so that we know in advance that we shall need at least 385 observations. Bc. One-sided CIs. It is sometimes appropriate to decide in advance of an experiment to obtain a CI for 0 of the form {0 : 9 < B} for a constant B. Now, for Pre-Statistics, P (Y > 0 — bn-1 I 0) = P (0 < Y + bn-1 I 0) = C%, where 1 — (1.(—b) = C%, or, using the symmetry of (,o (b) = C%. Thus ( —C)°, Yobs + bn —l] is a C% CI. One really must decide before the experiment whether to obtain one-sided or two- sided CIs. Analyzing data in ways which depends on the data can be dangerous, but there are many occasions where it is unavoidable. From now on, I generally stick to two-sided CIs, leaving you to develop one-sided ones. C. CIs for samples from an exponential distribution. We shall see later that certain distributions must be exponential to a high degree of accuracy. Let Y1, Y2, • Yn be IID RVs, each exponential with MEAN 9 (and hence rate- parameter A = 1/0). Then, each Yk /0 has the exponential distribution of mean 1, whence E = nY 10 has the Gamma(n, 1) distribution. We see that nY/0 is a Pivotal RV. We can therefore find b, c such that P (b < nY < c 8) = C% = P nY < 0 < ±j- b 0) giving an exact C% CI [nyobs1 c,nyobs1 1)] after the experiment. Ca. More near-heresy. Let Sobs = i yjcbs. Numerically — but in the Frequentist approach definitely NOT philosophically — we can pretend that the rate parameter A = 1/0 is a Random Variable A with the Gamma(n, rate sobs) distribution in that an interval I will be a C% CI for A if and only if P(A E I) = C%; it's as if Asobs is Gamma(n, 1). You check this. We could find the shortest C% CI on the computer, but there is not much point in doing so. Cb. CLT approximation. If n is large, then the CLT used via 162(F2) yields an approximate 90% CI [ Yobs Yobs 1 +al,NFC 1- alVTI, for 9, where a = 1.645. 6.2. Some commonsense Frequentist CIs \u0000 177 Suppose, for example, that n = 25 and gobs = 2.3, and we want a 90% CI for the mean 0. From 149(E1), we may take b to be half of the entry for xL in the table on p517 with 95% tail probability, namely b = 2 x 37.46. Similarly c = 2 x 67.50, and our exact 90% CI is [1.70, 3.31]. The CLT method gives the approximate 90% CI [1.73, 3.43] — quite good. ► D. Difficulties with discrete variables. \u0000 If the Random Variables 171, Y2 - • - Yr, are discrete, then we are not going to be able to arrange that 174(A 1) holds exactly for every 0, since, if for example, each Yk can take only finitely many values, then for each 0, the probability Per_ (Y) < 0 < 74(Y) I 0) can take only finitely many values. The sensible thing to do is to require that 174(A1) hold approximately for each 0 of real interest, giving an approximate C% CI for 0. Alternatively, we can insist that P(T_(Y) < 0 < T+ (Y) e) > C% for every 0, leading to a 'conservative' C% CI for 0: we are 'at least C% confident' that our CI contains the true value. Da. Exercise. Suppose that n is large and Y1, \u0000 , Yr, are IID, each Poisson(A). Use Exercise 162G to show how to find an approximate 95% CI for A. Important Note. There are great difficulties in obtaining exact CIs when we have `continuous' pdfs outside the exponential family defined in Subsection 184E. Hence, we are often forced to rely on approximate CIs in the 'continuous' context. We shall see later how Maximum-Likelihood Estimation can allow us to achieve reasonable approximations on the computer. ► E. CIs for proportions. Consider the situation mentioned earlier, in which we might be interested in estimating the fraction 0 of the adult population who will vote for some particular political party. We take a large sample of size n, and let Yk = 1 if the chosen person says that he/she will vote for that party, Yk = 0 if not. Because our sample size, though large, will be much smaller than the total population size, we can ignore the difference between sampling without replacement and sampling with replacement, and assume that Y1, , (Yk = 1) = 0 = 1 — IED (Yk = 0) \u0000 for each k. Then E (Yk) -= 0, Var (Yk) = 0(1 — 0), E (Y) = 0, SD (Y) = and the CLT gives, for large n, the approximate result are IID each Bernoulli (0) so that P (1Y— <1.96 0(1 — \u0000 0)) \u0000 95%. 178 \u0000 6: Confidence Intervals for one-parameter models If we wish our Estimator Y of 0 to lie within 0.01 of the true value of 0 with 95% probability, we must ensure that 1.960(1 — 9)/n < 0.01. Now, N/0(1 — 0) is always at most 0.5, and if 0 lies anywhere close to 0.5 (say, even between 0.3 and 0.7), then \\/9(1 — 0) is acceptably close to 0.5. Since 1.96 x 0.5 is 1 (as nearly as makes no difference), then, in most cases of interest to politicians, P (1Y — 01 < 1/Nrn) \u0000 95%, so that to get the desired accuracy, we need to take a sample of size at least 10000. If Y differs significantly from 0.5 (or even if it doesn't!), we can estimate N/0(1 — 0) by \\/Y(1 —11 to get an approximate 95% CI for 0 after the experiment is performed of the form [Yobs — 2 \"Yobs (1 — gobs)/n1 gobs + 2 \"Yobs (1 — Yobs)/rd • \u0000 (El) Ea. Notes. In practice, no-one takes such a big sample. Samples of size 1000 are common, leading of course to a CI about N/lb = 3 times as wide. The inevitability of bias in sampling procedures makes taking larger samples just not worth the effort. One often finds in practice an unwillingness of administrators either to believe, or (if they do) to act upon, the 'square root' phenomenon which means that very large sample sizes are necessary for reasonably good accuracy. Eb. Instructive Exercise. (This is a two-parameter example, but who cares?!) Suppose that one takes a large sample of size n from a population. Of these Si say that they will vote for Party 1, 52 for Party 2. Let pi and p2 be the fractions of the whole population who will vote for Party 1 and Party 2 respectively. Show that (assuming the independence associated with sampling with replacement) we have E (82 — = n — /32) and Var (82 — \u0000 n [pi + /32 — (P2 — /302] . How do you find an approximate 95% CI for p2 — pi after the experiment is performed? F. An Important Example. (Another two-parameter one!) Suppose that one wants to compare the effectiveness of two Treatments A and B in curing some new disease, and that a randomly chosen 1000 diseased people are given Treatment A and 1000 Treatment B with the results in Table F(i). Let pA [respectively, pB] be the probability that a person treated with A [B] will recover. Fa. Important Exercise. Practitioners in the field use the difference in log- odds- ratios: In PB \u0000 In PA 1 — Ps \u0000 1 — pA to quantify difference in effectiveness, but we can here safely use pB — pA which is more comprehensible to the layman and me. In regard to PreStatistics, for the kth person 6.2. Some commonsense Frequentist CIs \u0000 179 Treatment Recover Die Total A 870 130 1000 B 890 110 1000 Total \u0000 1760 240 2000 Table F(i): Treatment results treated with A [respectively, B], let Yk [respectively, \u0000 be 1 if the person recovers, 0 if not. Assume that Y1, • • • Yn , Z1, • • • , Z7, are independent, where n = 1000. Show that E — Y) = pB — pA and Var (Z — Y) = {pA (1 — pA) p B (1 — pB)} /n. Obtain the rough-and-ready '2 estimated SDs of Z - Y to either side' 95% CI [-0.009, 0.049] for pB — pA. The way in which this relates to the x2 test (which is not appropriate here) will be explained later. ► G. Confounding. Confounding is one of statisticians' worst nightmares. Here are two instances. (We shall later meet confounding again in connection with regression.) Ga. Smoking and lung cancer: 'confusion worse confounded'. \u0000 The percentage of smokers who develop lung cancer is without question greater than the percentage of non-smokers who develop it. That in itself shows only that smoking and lung cancer are associated: it does not prove that smoking causes lung cancer. Sir Ronald Fisher, greatest of statisticians and one of the greatest geneticists, thought that there is a confounding factor, something in the genetic makeup of some people which makes them both (a) more susceptible to lung cancer and (b) more inclined to become smokers: thus, smoking and lung cancer would be associated because in many of the cases where they occur simultaneously, they have a common genetic cause. Later studies showed conclusively that Fisher was wrong in thinking that such confounding could be a complete explanation: we now know that smoking does cause lung cancer. But the case shows that one always has to be on the lookout for confounding factors. Gb. Simpson's Paradox; the perverse behaviour of ratios. (The 'Simpson' here refers to real statistician E H Simpson, not to Homer, Marge and family. The paradox had long been known in connection with bowling averages in cricket.) I present the paradox in an extreme form. Table G(i) summarizes the situation which I now describe. Suppose that 6M (6 million) men and 6M women were infected with a certain disease, making a total of 12M people. First we are told that 6M of these people were given a certain treatment, of whom 2M recovered and 4M died; and that of the 6M who were not treated, 4M recovered and 2M died. If this is all the information we have, then, clearly, the treatment should not be given to people. 180 \u0000 6: Confidence Intervals for one-parameter models People Men Women Recover Die Recover Die Recover Die Treated 2M 4M 1M 0 1M 4M Untreated 4M 2M 4M 1M 0 1M Table G(i): 'Simpson Paradox' table WT \u0000 PT Recover Figure G(ii): Simpson's Paradox: a picture However, we are then told that amongst men, only 1M were treated, all of whom recovered, and that of the 5M men not treated, 4M recovered. We deduce that amongst women, 5M were treated of whom 1M recovered, and 1M were not treated and all of those died. Given all this information, it is certainly best to treat a man, who is thereby guaranteed a cure; and it is certainly best to treat a woman, for otherwise she will certainly die. The People table confounds the two disparate populations of men and women, and leads to a false conclusion. It seems hardly fair that Statistics, already having to face difficult philosophical problems, should have to deal in addition with the fact that ratios can behave in a perverse way totally counter to one's first intuition. A picture (Figure G(ii)) might help by providing a way of 'seeing' our example. 6.3. Likelihood; sufficiency; exponential family \u0000 181 In the figure, 'MT' signifies 'Men Treated', etc. The shaded areas reflect the 2 x 2 tables. The MU vector is steeper than the MT, and the WU vector is steeper than the WT, but the sum PU is less steep than the PT. The lengths of the lines are as important as their directions. Of course, one is now inclined to worry, given any 2 x 2 table as one's only information, that it might split into two natural tables which would lead one to a different conclusion. Nothing in the table itself can indicate whether or not it is 'confounded'. 6.3 Likelihood; sufficiency; exponential family First, we define the likelihood function. ►► A. Likelihood Function lhd(0; y). (This definition will be illustrated by many examples in Subsection C.) Again, suppose that Yi , Y2, . Yn are HD RVs each with pdf (or pmf, if each Yk is discrete) f (y 0), 0 being an unknown parameter. For a possible outcome Y27 • • Yn, we define Ihd(0; y) = lhd(0; Y1) Y21 • ° I lin) \u0000 f (Y110) f (Y210) • • f (Y n10). \u0000 (Al ) If each Yk is discrete, then, by independence, P(Yi = Yi.; Y2 = Y2 \u0000 ;17n = Yri 10) = (Yi = yi I 0) 1 P (Y2 = Y21°) • • • P (Yn = YTh I e) = f (m.10) f (y210) • • • f (yn16) = lhd(e; Y), so that lhd(0; y) is the probability of outcome y: thus lhd(0; y) acts as a 'joint pmf' for (Yi, Y2, • • • 7 Yn ) If each Yk is 'continuous' with pdf f I 0), then lhd(0; y) acts as a 'joint pdf' for (Y1, Y2, • • • , Yom,) in the sense that (as is proved heuristically below) for any nice (Borel) subset B of Rn, P(Y E B I 0) = f lhd(0; y)dy \u0000 (A2) := f f \u0000 f /B(y)lhd(0; Yi, Y2, • • • ,Yn)dYidY2 • • • dYn• If we think of Q as Rn the set of all possible samples, then B is the event Y E B, so the probability measure lP(B 1 0) is defined to be either of the integrals at (A2). 182 \u0000 6: Confidence Intervals for one-parameter models Heuristic proof of (A2). Equation (A2) is true if B is a 'hypercube' of the form {y Yk E [ak, bk] (1 < k < rt)} because then the integral turns into a product (f bk f (Yk 0) dyk) = \u0000 PI Yk E tak;bki k=1 \u0000 '2 k \u0000 k=1 tallying with the 'Independence means Multiply' rule 0), P(Yi c [cti,bi];- -;Yn E [an, bn] 0) = HIP(Yk [ak, bid k=1 0). By standard measure theory, result 181(A2) then holds for all Borel sets B and hence for any set B we need ever consider. It is obvious that 181(A2) holds for sets which are unions of disjoint hypercubes, and intuitively obvious that any nice set B may be approximated by such unions. ❑ ► ► B. Log- likelihood £(0; y). It will be very convenient to make the definitions: P(0; y) \u0000 In Ihd(0; y), \u0000 f (y 1°) := 111 f (°;Y). Then, of course, f(0; y) C. Important Examples. The formulae here will be much used later. Ca. Normal N(0, 1). If each Yk is N(0,1), then for y E R, f (y10) = (27) 1e-1(Y-6)2 , \u0000 f* (y 1 0) = \u0000 ln(27r) — (y — 0)2, so that n t(0; y) =- —2 -t,1n(27) — 2 E(yk — 0)2 k=1 n = \u0000 ln(27) — z E(yk — y-)2 — 171,(y — 0)2, k=1 by the Parallel-Axis Theorem'. Cb. Exponential E(mean 0). \u0000 If each Yk is exponential MEAN 0, then, for y E [0, oo), so that (Y I = exp (--o) , \u0000 f* (y 10) — ln — n £(0; y) = —nine — 0 Vy k = —n1n0 — k=1 6.3. Likelihood; sufficiency; exponential family \u0000 183 Cc. Poisson(0). If each Yk is Poisson(0), then, for y E Z±, I (y 10) = e— a BY \u0000 f* (y 10) = — 0 y In B —1n(y!) , Y• i so that 40; y) = — n0 + ny In B — \u0000 ln(yk!). k=1 Cd. Bernoulli(0). \u0000 If each Yk is Bernoulli(0), then (you check the convenient expressions of the formulae!) for y e {0, 1}, f 1 0) = 0Y(1 — 0)1—Y, \u0000 f* (y 1 0) = y In 0 + (1 — ln(1 — 0). so that 40; y) = \u0000 ln 0 + (1 — ln(1 — 0)}. ►► D. Sufficient statistics, 1. It has to be said that Bayesian theory gives a much more satisfying definition of a sufficient statistic. Here we give the first of two Frequentist definitions; but we call it a 'criterion' because the second, given later in the book, is more satisfactory as a definition. Da. First criterion for sufficiency. Suppose that Y1, Y2, ... , Yn are IID, each with pdf (or pmf) f (. 10). Define lhd(0; \u0000 lhd(0; yi, 02, • • • , yn) = f ( y 10 ) f (y2 1 0) \u0000 f ( yni , as usual. A statistic t = T (Y) = \u0000 Y27 • • • , yn) (where T is some nice function on R) is called a sufficient statistic for 0 based on the actual sample Y17 Y21 • .. yn if there is factorization \u0000 lhd(0; y) = g(0;t)h(y) = g (0; 7-(y))h(y) \u0000 (D1) for some functions g and h with the obvious domains. Do note that h(y) does not involve 0. Note that if t is a sufficient statistic, then so is any one-one function of t. The Frequentist interpretation of sufficient statistics. Numerous illustrations of what follows are to be found in this book. Subsection 185F already contains several. 184 \u0000 6: Confidence Intervals for one-parameter models Db. Fact. Let t be a sufficient statistic, and let T be the PreStatistic which crystallizes to t: T = \u0000 Y2, \u0000 yn) . Let fT(• 10) be the pdf of T when the true value of the parameter is 0. Then Zeus (father of the Gods) and Tyche can produce IID RVs Y 1, Y2, • • • , Yom, each with pdf f (• 10) as follows. Stage 1. Zeus picks T according to the pdf fT(• I 0). He reports the chosen value of T, but NOT the value of 0, to Tyche. Stage 2. Having been told the value of T, but not knowing the value of 0, Tyche performs an experiment which produces the required Y1, Y2, • • • , Y. Since Tyche did not even know the value of 0 when she performed Stage 2, the only thing which can matter to making inferences about 0 is therefore the value of T chosen by Zeus. In practice, then, given the actual observations yobs, we must base our inference about 0 solely on tobs = ',(Wact). I hasten to add that, normally, Tyche knows the value of an unknown parameter. But, in regard to sufficient or ancillary statistics, Zeus sometimes uses Tyche to do a large part of the experiment without her knowing 0; and in these cases, the value of 0 is known only to him. I must stress that while we have been concentrating here on a 1-dimensional sufficient statistic, we generally have to allow our sufficient statistic to be multidimensional. For example, the vector y of all observations is always a sufficient statistic. (Note. The study of Minimal Sufficient Statistics is deferred until Subsection 262D.) ►► E. The exponential family of pdfs and pmfs. A pdf (or pmf) f (y 1 0) is said to belong to the exponential family if it has the form f (y 1 0) = exp{a(0)b(y) c(0) d(y)}. \u0000 (E1) If Yl , Y2, ... Yn are IID each with pdf (or pmf) f (y10), then, clearly, b(yj) is a sufficient statistic for 0 based on yi , y2, . . . , yi,. Amongst regular pdfs, the converse is true: only pdfs (or pmfs) in the exponential family lead to 1-dimensional sufficient statistics. We skip the proof. Note that the pdfs (or pmfs) of the N(0, 1), E(mean 0), Poisson(0), and Bernoulli (0) distributions belong to the exponential family. Indeed it is worth making Table E(i), in which we do not include d(y), because it will not be relevant in future discussions (except implicitly). In the Gamma(Ko, mean 0) entry, K 0 is assumed known. Recall that in N(0, 0), 0 signifies variance (not SD). 6.3. Likelihood; sufficiency; exponential family \u0000 185 a(0) b(y) c(0) Normal N(0, 1) 0 Y — 02 Exponential E(mean 0) — 1/0 y — 1110 Poisson(0) In 0 y —e Bernoulli(0) 1 \u0000 {0/ (1 — 0)} y 41(1 — 0) Gamma(Ko, mean 0) — K010 y/Ko — K0 In 0 Normal N(0, 0) —1/(20) y2 — 2 In 0 Table E(i): Some members of the exponential family F. Illustrations of Fact Db. The first two examples will be discussed later. It would be too cumbersome to discuss them with our current technology. Fa. Normal N(0, 1). Zeus and Tyche decide to produce IID vectors Yi , Y2, • • • , Yn each with the N(0, 1) distribution. In Stage 1, Zeus chooses T = Y with the N(0, 1/n) distribution, and reports the chosen value of T to Tyche. In Stage 2, Tyche then chooses the vector Y in Rn so that (171 — Y2 — Y , • • • Yn 17) has the standard normal distribution in the hyperplane through (T,T,... ,T) perpendicular to (1, 1, \u0000 , 1). Then Y has the desired property. Fb. Exponential E(mean 0). Here, Zeus and Tyche decide to produce IID vectors , Y2, • • • , Yn each with the E(mean 0) distribution. Zeus chooses S (which will become E Yk) according to the Gamma(n, mean 0) distribution, and reports the value of S to Tyche. Tyche then picks n — 1 points uniformly at random in [0, S], and she labels these points in ascending order as + \u0000 , + Y2 + • • + Finally, she sets Yn = S — (171 ± Y2 ' ' • ± Yn-1). Fc. Poisson(0). Now, Zeus and Tyche decide to produce HD vectors Y , - - Y 2 , - • • , Y71 each with the Poisson(0) distribution. Zeus chooses S according to the Poisson(n0) distribution, and reports the value of S to Tyche. She then makes S independent choices each uniformly from the discrete set {1, 2, ... n}, and lets Yk be the number of times that k is chosen. Fd. Bernoulli(0). Finally, Zeus and Tyche decide to produce IID vectors Y1, Y2, \u0000 , Yn each with the Bernoulli(0) distribution. Zeus chooses S according to the Binomial(n, 0) distribution, and reports the value of S to Tyche. She now chooses a random subset K of size S from {1, 2, ... , n}, and sets Yk = 1 if k E K, 0 otherwise. Fe. Discussion. \u0000 Consider the Poisson case. Let yx, y2, \u0000 , yn E Z+ and set s := yi yz \u0000 • • + yn. If we make s independent choices of numbers each uniformly 186 \u0000 6: Confidence Intervals for one-parameter models within {1, 2, ... , n}, and if 77, is the number of times that k is chosen, then by extending the `multinomial' arguments we used in Chapter 1, we see that s! \u0000 1 Prob (7/1 -= Yi; • ; \u0000 = Yn ) X Yi.! Y2 • • Yn ! \u0000 ns so that s! \u0000 Y1!Y2! • . • yn! \u0000 ns 1 e-rn e—e(0)Yk \u0000 = \u0000 = yi; • • • ;Yn Yn) k=i \u0000 Yk- the last probability relating to the case when 171, Y2, \u0000 , Yn are IID each Poisson(0). Ff. Exercise. Discuss the Bernoulli case. Fg. Exercise. If Zeus chooses E Yk2 in the N(0, 0) case, how do you think Tyche then chooses the vector Y? Just give the intuitive answer without worrying about precise formulation. G. A 'non-regular' example: the U[0, 0] distribution. I have no interest in producing silly counterexamples. This example matters in practice. Suppose that Y1, Y2, \u0000 , Yn are IID each with the uniform U[0, 0] distribution on [0, 0]. Then, 1 \u0000 1 f (y 1 0) = ir[o,o(Y) \u0000 Ihd(0; yl, Y2, • • , yn) = 6T.J[9,1,00](61), where m := max (yi , y2, \u0000 , yn). Thus, m is a sufficient statistic for 0 based on Yt; Y2; • • • ; Yn • Ga. Exercise. Describe intuitively the Zeus—Tyche construction of (Yi., Y2, \u0000 , Yn), with Zeus first choosing M := max (Y1,117 , • • • \u0000 ) • ► H. Ancillary statistics; location parameters. The theory of Ancillary Statistics is an important topic, but a tricky one; and I defer discussion until Subsection 263E. However, we now examine the implications of the theory for the important case of location parameters, implications studied at 264Eb. ► Ha. Location parameter. Suppose that f (y 0) = h(y — 0), where h is a pdf on R. (For example, f (y 0) might be the pdf of the N(0,1) distribution, in INT (Y1 = Yl; • • • ; Yn = yn ) e —n° *O s \u0000 s! \u0000 X \u0000 X — 6.4. Brief notes on Point Estimation \u0000 187 which case h = co.) The final Frequentist story (see 264Eb) says that CIs may be calculated from 740 I y obs), where 7 ( 9 I yobs) OC 11149; yobs), \u0000 (H1) the 'constant of proportionality' (depending on y obs) being chosen so that f 749 I y obs)do = (Calculation of this normalizing constant — even on the computer — can prove a real headache, but MCMC methods make its calculation unnecessary.) Thus, for this 1-parameter situation, Frequentist theory agrees exactly with `Bayesian theory with a uniform prior density', as we shall see. Hb. Example. Suppose that Y1, Y2, ... Yr, are IID each with the uniform distribution on [0 — 2 , B + 1]. Then 740 I yobs) is the pdf of the uniform distribution on [ „,obs \u0000 ,„olas Y(n) \u0000 7 W(1) \u0000 d I an interval which we know by simple logic MUST contain 0. If we ignore the theory of ancillary statistics, that is, if we concentrate on absolute probabilities consistent with Definition 174Aa, we could miss out on this logic. Note. This example has special features to do with minimal sufficiency, as we shall see later. But I shall argue there that if Statistics is to have any coherence, then we must obtain the correct answer from the general location-parameter result. 6.4 Brief notes on Point Estimation As explained earlier, this topic is done not so much for its own sake as for the light it throws on Confidence Intervals obtained via Maximum-Likelihood Estimation. ► A. Estimators and estimates. \u0000 Consider the usual situation where Y1, Y2, • • • , Yn are IID, each with pdf (or pmf) f (y 0), and where, therefore, lhd(0; y) = lhd(0; yi, y2, \u0000 , yn) = f(Y110)f(Y210) • • • f(Yril°)- An Estimator T of 0 is just the same as a PreStatistic, and here therefore a function T = r(Y) = T (Yi, -17-2, ... , Yom,) of the vector Y of Observations considered before the experiment is performed. After the experiment, T leads to the estimate Lobs = T( Wact) \u0000 T (y obs) = \u0000 ( yibs, u2obs, \u0000 ynobs) where ck,y bs yk (W act ) is the value to which Yk crystallizes. An Estimator is therefore a rule for getting estimates. 188 \u0000 6: Confidence Intervals for one-parameter models ■ B. Unbiased Estimators. An Estimator T is called Unbiased for 0 if, for every 0, it is correct 'on average' in that we have E (T 10) = 0, that is, in full, f r(y) lhd(9; y)dy = f f \u0000 fr (Yi Y2, ... , yn,)1hd(9; y1, Y2, ... , yn)dyi dy2 \u0000 dyn, = 9. The integral becomes a sum in the discrete case. We have extended the rule E h(X) = f h(x) f x (x)dx in the obvious way. With our definitions, it is meaningless to speak of an unbiased estimate: the estimate is just a number, and could be considered unbiased only if it is exactly equal to the unknown parameter 9. If each Yk has mean 9, as is the case for the N(9, 1), E(mean 9), Poisson(9) and Bernoulli(0) cases we have been studying, then, of course, Y is an Unbiased Estimator of 9. We studied an Unbiased Estimator for variance in Exercise 104F. Ba. Exercise. Show that if each Yk is E(rate A), and n > 2, then ( 1 f 1 Ansn-le-As ds = A S) ,10 s (n — 1)! n— 1 where S = E Yj, whence (n — 1)/S is an Unbiased Estimator of A. Bb. Exercise. Prove that if Yi , Y2, \u0000 , Y7, are IID each with the U[0, 0] distribution, then (n 1)M/n is unbiased for 9 where M := max (Y_ , Y2 , Bc. Exercise. Lehmann showed that sometimes there is only one Unbiased Estimator and it is absurd. I am not claiming that the following example really matters; but it does matter that you can do this exercise! Suppose that Y has the truncated Poisson distribution with pmf (e° — 1)-10Y I y! for y = 1, 2, 3, .... Show that the only Unbiased Estimator T of (1 — e-° ) based on Y is obtained by taking T = 0 if Y is odd, T = 2 if Y is even. Since 1 — e-0 E [0, 1), this Unbiased Estimator is really absurd. An Unbiased Estimator with small variance is likely to be close to the true value of 0. There is however an explicitly known lower bound to the value of Var(T) if E (T) = 9. In order to establish this, we need the concept of Fisher information, and for that we need the notation and identities in the next subsection. Terminology: efficiency. If one Unbiased Estimator has smaller variance than another, it is said to be more efficient. 6.4. Brief notes on Point Estimation \u0000 189 ► C. Useful notation and identities. Recall that t(0; y) = In lhd(0; y) \u0000 E f* (Yk I 0). Write Note that a a, (C1) aolhd.(0;y) affe(°;Y \u0000 111(1(0; y) (C2) Terminology: score. I should tell you that the expression aot(0; y) is called the score statistic at parameter value 0. For regular pdfs or pmfs (see 172B), we have (with sums for integrals in the discrete case) I. lhd(0; y)dy = P (Y E R ) = 1, and E (80(0; Y) 0) = f {0040; y)} lhd(0; y)dy f aouad(e;y) lhd(0; y)dy = f a, lhd(0; y)dy lhd(0; y) \u0000 = ae J lhd(e; y)dy = 001 = 0. \u0000 (C3) We therefore see that the Score Pre-Statistic at 0 has zero E (• I 0) mean. Next, by differentiating f {e(0; y)} lhd(0; y)dy = 0, with respect to 0, we obtain (since 801hd(0; y) = {ago ; y)} lhd(0; y)) 0 = f -440; y)} lhd(0; y)dy f {MO; y)}2} lhd(0; y)dy \u0000 = E (440; Y) 0) +E ({ag (0; Y)}2 0) . \u0000 (C4) ►► D. Fisher information. We now define \u0000 It(0) := E ({ag (o; y)}2 o) = Var (a .e \u0000 (0; Y) I , \u0000 (D1) 190 \u0000 6: Confidence Intervals for one-parameter models the equality following from (C3). Thus h(0) is the P(• 1 0) variance of the Score Pre-Statistic at parameter value 0. Equation (C4) enables us to rewrite (D1) as h(0) = E (4t (0; Y) 1 0) \u0000 —nE (8?) r (Y 0)) = ni f. (0). (D2) The expression /t(0) is a measure of the amount of information a sample of size n contains about 0. This measure is proportional to sample size. Convince yourself that the definition at 189(D1) seems sensible. For a good explanation of Fisher information, wait until Exercise 198Fc. If, for example, each Yk has the N(o, o-g) distribution, where the variance o is known, then (you check!) h(0) =- nlue). If al, is very small, then we have a lot of information in that we can predict 0 with high accuracy. (Bayesians refer to the inverse of the variance of a normal variable as the precision — on which topic more later.) ►► E. The Cramer—Rao Minimum-Variance Bound. We work with regular pdfs/pmfs. Suppose that T = r(Y) is an Unbiased Estimator of 0 based on 172 , • • • , Yn , where, as usual, Yi,Y-2, \u0000 Yn are IID each with pdf (or pmf) f (y 10). Then Var(T 1 0) > le o) , \u0000 (El) with equality if and only if \u0000 T — 0 = T(Y) — o = 100(0; Y)} x function(0). \u0000 (E2) The right-hand side of (El) is called the Minimum-Variance Bound for an Unbiased Estimator. When T is an Unbiased Estimator of 0 for which (El) holds with equality, T is called an MVB Unbiased Estimator of 0. Note that then T is a Sufficient Statistic for 0 based on Y. Proof. Suppose that T = r(Y) is an Unbiased Estimator of 0, so that f T(y)lhd(0; y)dy = 0. Differentiate with respect to 0 to obtain f r(y)aelhd(0; y)dy = 1 = f r(y) W(o; y)} lhd(9; y)dy. Hence, using 189(C3), 1 = E {MAO; Y) 0} = E {(T — 0)8040;Y) 0} , 6.4. Brief notes on Point Estimation \u0000 191 whence, by the Cauchy—Schwarz Inequality 66C, 1 = 12 < E (T — 0)2 with equality if and only if 0} E {80(0; Y)2 0} = Var(T10)/t(0), T — 0 = {aet(0;Y)} x function(0). ► F. A special exponential family. If f (y I 0) has the special exponential form with b(y) = y: f 10) — exP {a(0)Y + c(0) + AY)} f*(Y 10) = a(0)Y + c(0) + d(Y), then, taking n = 1 in 189(C3), 0 = E (5(Y1 0) 0) 19 r \u0000 = (0)E (Y 10) + (0), so that (we assume that d(0) 0) e E (Y10) = E(Y 10) = a' (8) , (0)' and Y is an Unbiased Estimator of 0 if and only if Oa' (0) + (0) = 0. \u0000 (F3) Assume now that Y is unbiased, equivalently that (F3) holds. Then \u0000 00.40; y) = n {a' (9)y + g(e)} = na/(0) — 0} , \u0000 (F4) so that condition 190(E2) holds and Y is an MVB Unbiased Estimator of 0. In particular, 1 \u0000 1 Var (Y) = \u0000 (F5) nI f. (0) \u0000 na'(0)' because f. (0) = —E lan(0)Y c\"(0)} = —a\"(0)0 — c'(0) = -- d {Oa' (0) + ci(0)} + d(0) = (0). d0 Of course, the pdfs/pmfs of the N(6, 1), E(mean 0), Poisson(0) and Bernoulli(0) distributions are in the special exponential family for which (F3) holds. Exercise. Consider the N(0, 0) and Gamma(Ko, mean 0) cases. ► Fa. Exercise and Warning. If Y E(mean 0), then Y is an MVB Unbiased Estimator of 0. But for what value a is the mean-square-error E {(aY — 0)2} minimized? so that (F1) (F2) 192 \u0000 6: Confidence Intervals for one-parameter models 6.5 Maximum-Likelihood Estimators (MLEs) and associated CIs This is a particularly important topic, crucial to Frequentist theory and, in its large-sample results, to Bayesian theory too. The setup is the familiar one in which V - - -V 2, \u0000 , Yom, are IID each with pdf/pmf f (y 10). ►► A. Maximum- Likelihood Estimators (MLEs); mles. We have discussed the idea that a value of 0 for which lhd (0; yobs \\ ) is large is one that tallies well with the data. the maximum-likelihood estimate (mle) of 0, the value (assumed unique) bobs at which 0 1--> lhd(0 ; yobs) is a maximum, has some claims to be the 'best' estimate of 0. We have written the `obs' in gobs because 00b, may be calculated from the data. We clearly have the likelihood equation 8911K40; y\"b8) \u0000 0, equivalently, 80,q0; yobs) = 0, when 0 ----- Oohs. (Al) If there is a sufficient statistic t, then, as we see from the Factorization Criterion 183(D1), Bobs must maximize 0 1--> g(0;t0b5) whence 0,,b, is a function of ebs. By our usual conventions, the Maximum-Likelihood Estimator (MLE) e of 0 is the PreStatistic which crystallizes to Bobs, so e maximizes 0 H lhd(O;Y). It is worth mentioning now that we sometimes 'correct' MLEs so as to make them Unbiased. We now meet an important case where no such correction is necessary. If f (y 0) has the special exponential form at 191(F2): f (y 10) = exp fa(0)y + c(0) + d(y)} , where a'(0)0 + c'(9) = 0, we already know that Y is an MVB Unbiased Estimator of 0 with variance therefore equal to {If M} —'. Since, from 191(F4), atit(9; Y) = na' (0) {Y — 0} , it is clear that Y is also the MLE of 0. For large n, the CLT shows that, since Y is a sample mean of IID variables, Y is approximately N (0,1.4(0)1-1) . Fact B, one of the most important results in Statistics, establishes an analogous result for all sufficiently regular pdfs/pmfs. 6.5. Maximum-Likelihood Estimators (MLEs) and associated CIs \u0000 193 ∎►► B. Fact (Fisher, Wilks, Wald): Asymptotic Normality of MLEs. Suppose that f (y ( 0) satisfies certain regularity conditions satisfied in all the examples which we shall meet. Then, for large n, the MLE B of 0 based on , Y2, .. Yn is both approximately MVB Unbiased and approximately normal. Thus, for large n, e is approximately not \u0000 mai N \u0000 — N (0, II (0) ni f. (0) We use this to obtain CIs for 0 by pretending that 0 has posterior density from the 1 N (°ObS 7 \u0000 ^ It (° ObS) distribution. We have seen enough instances of this kind of thing. We shall examine some heuristic reasons why the Fisher—Wilks—Wald result is true, and make computer studies of an interesting case. First, we have to convince ourselves that, for large n, o is likely to be close to the true value O. ► C. Fact: Consistency of the MLE. Again make appropriate regularity assumptions. Assume that Y1, Y2, ... are IID each with pdf/pmf f (y I 0). Let en be the MLE of 0 based onY..1,Y2, • • • , Y. Then, with P(• I 0) probability 1, Bn —> 0 as n —> oo. Heuristic idea for proof If B is the true value of the parameter and yo is some other value of the parameter, then with 1P(. I 0) probability 1, lhd(co; \u0000 Y2, • • Yn \u0000 (;) a s n \u0000 co. \u0000 (C1) Ihd(0; Y17 - - Y 2) • • Yn) To prove this (really prove it!), note that E (PY Co) .f(17 1 0) e) = f ff((Yy li61 c°)) f (Y I 0) dY = f f (Y 49)dy = 1, whence, by Jensen's Inequality 64K, E ( in (Y f (Y I I 0 co) ) 0) \u0000 o. f The strict inequality in this application of Jensen's inequality is because (— In) is strictly convex on (0, ox)) and because we may and do ignore the situation where we have 194 \u0000 6: Confidence Intervals for one-parameter models f (Y I co) = f (Y 19) with liD• I 0) probability 1 (in which case the Pe I cp) and Pe I 0) probabilities would be equal). The Strong Law now tells us that, with Pe I 0) probability 1, In lhd(cp; Yi, Y2) • • Y.) = \u0000 In En \u0000 f (Irk I (P) --> —oo lhd(0; \u0000 Y2, . Yn) \u0000 k=1 f (Yk I9) as n —> oo, and hence 193(C1) is true. It is intuitively clear that 193(C1) is a big step towards proving Fact 193C, and clear that, for full rigour, it needs some uniformity assertions which are significantly more technical. ► D. A heuristic Frequentist view of Fact 193B. (For the heuristic Bayesian view, see Subsection 204F.) Make the assumptions of the theorem. Then O satisfies 002(9;1 )1e=6 = whence by Taylor expansion (since O and 0 are likely to be close) 0e2(0; Y) \u0000 — 0) 42(0; Y) \u0000 0 'at the point 0' so that (a kind of Newton—Raphson statement) Nt(e ; Y) {-80e(9; Y)} Now, the denominator --0,340; Y) = > {-4fork 69} is, by the Strong Law (or Weak Law), likely to be close (`proportionately') to its expectation nI f. (0). Next, the ao f* (Yk 10) are independent each with zero mean and variance I f.(0). By the CLT, the numerator n 0049; y) = Eaer(Irk 0) k=1 in (D1) is therefore approximately N(0, nI f. (0)). The result follows. E. Example: The Cauchy distribution. We already know that Fact 193B works well for N(9, 1), N(0, 0), E(mean 0), Poisson(0), Bernoulli(0). Let's try it out on the Cauchy density f (Y 0) = 7r {1+ (y — e)2y Though this density is notorious for having no mean (hence no variance), that does not influence its behaviour in regard to Fact 193B. Fact 193B suggests that, for large n, (DO 1 N (0, —2 ) . \u0000 (El) 6.5. Maximum-Likelihood Estimators (MLEs) and associated CIs \u0000 195 Ea. Exercise. By differentiating with respect to a for some of the steps, check that f \u0000 1 dy = 1 , a 1 r \u0000 2a, i 1 a 2 ' 3 7 (a2 + y2) r \u0000 1 — J 71-72 + y42a)2 'Y d dy 2 J \u0000 7r. (a2 ± y2) 2a3 ' J 7r (a2 + y2)3 \u0000 y 20 ' i \u0000 d a2 \u0000 3 J 7 (a2 + y2)3 y \u0000 8a3 ' 2 y2 (1 2 3) a3 1 8 ) 1 J \u0000 71- (a2 ±y 2)3 clY 8a3 I f . (0) = 1 for all O. (Alternatively, prove the last result directly using the Calculus of Residues.) \u0000 ❑ We know that, since 9 is a location parameter, `ancillarity' tells us that in the Frequentist approach, we should obtain CIs for 0 by pretending that 0 is a random variable 0 with pdf 7401 y) proportional as a function of 0 to ihd(0;y1,Y2, ...,y.) = IV (yklo); in other words, by 'taking the Bayesian posterior pdf corresponding to a constant prior density'. We therefore have 740 I y) = 1-1 H gYk 1 0), where / = f +cc) Hi (Yk 1 OdSo• \u0000 (E2) The calculation of such integrals is a problem, especially in higher-dimensional situations. We shall see later how Markov Chain Monte Carlo methods avoid the difficulty. How does this compare with 194(E1)? We can get an idea about this by simulation. A sample of size 40 was taken from the Cauchy distribution with 0 = 0. Figure E(i) compares the `Bayesian posterior' density with the normal curve obtained from 194(E1), based on the first n observations for n = 3, 15, 40 successively. The first three y- values on this simulation were 5.01, 0.40 and —8.75 — a bit spread out, but the Cauchy distribution does have large tails. We see that, for this sample, for large n, the posterior density is more concentrated around the mean than the normal, but the normal must eventually tail off more quickly, of course. Eb. Exercise: Simulating from the Cauchy distribution. Show that if U has the U[0, 1] distribution, then Y =- tan7r(U — 1) has the Cauchy density with 0 = 0. Here's an alternative method. Show that if we repeat choosing IID U[-1, 1] variables V and W until V2 + W 2 < 1, then Y = V/W has the Cauchy density with 0 = 0. n = 40 n = 3 n = 15 196 \u0000 6: Confidence Intervals for one-parameter models Figure E(i): Parts of posterior pdfs for Cauchy sample. The normal curve is for N(Oobs, 2/n). ► Ec. Important note on use of median. We know from Theorem 165La that, for large n, the median of Y1 , Y2, • • • , Ye., has approximately the N(0, '-,e”:) distribution, and this gives an easy way of obtaining approximate CIs for 0. One would have a 95% CI (m — r, m r), where r = 1.96 * 7r/(2Vii). For the 'n = 40' sample illustrated in Figure E(i), the true degree of confidence of the interval arrived at in this way is 97.3%, not 95%. So things are only very rough guides. ►► F. 'Randomness', entropy, approximation of pdfs, etc. The proof of 6.5. Maximum-Likelihood Estimators (MLEs) and associated CIs \u0000 197 the Consistency of the MLE in Subsection 193C utilized ideas associated with entropy. I'll just say a brief word about this concept. \u0000 We know that if Y1, Y2, \u0000 , Yr, are IID each with pdf/pmf f , then the Strong Law implies that the associated likelihood satisfies 1 \u0000 — n lnlhd(f ; \u0000 172) ...,Yn) := n — 1 \u0000 f (Yk) E f ln f (Y) = f f (y) In f (y) dy, the subscript f in E f signifying that the expectation is associated with the pdf/pmf f . We define the entropy associated with the pdf f as Ent( f) := — f f (y) In f (y) dy. \u0000 (F1) We see that, very roughly speaking, the likelihood of the observed outcome is likely to be roughly of order of magnitude e —n Ent(f). The larger Ent (f), the less the likelihood of the typical outcome, and thus the greater the 'randomness'. Be careful about sign conventions and normalization conventions in regard to entropy; in Shannon's information theory, log2 is used rather than In (because information is thought of in terms of bits, bytes, etc). However, entropy was, of course, studied much earlier by physicists, especially Clausius and Gibbs, in connection with the Second Law of Thermodynamics: 'entropy increases'. If we write Yk = h(Zk), where h is a smooth strictly increasing function, then Ent (f) and Ent ( fz) are different: the concept of entropy does not have the desired invariance property. We have f z (z) = f (h(z))h' (z) (see 55(F)) from which it follows (check!) that Ent (f z ) = Ent (f) f f (h(z))h' (z) ln h' (z) dz. In spite of this lack of invariance, entropy is rightly a widely used concept. The key step in the proof of the Consistency of the MLE was that if Yi, Y2, • .. , Yfl, are ILD with pdf f , and g is another pdf, then with Pf probability 1, 1 \u0000 lhd(g;Y-1 \u0000 ,Y2, • • • ,Yn) \u0000 g(Y) —In \u0000 —> E f ln \u0000 < 0' n lhd(f; Y_ 1, Y2, ... , Yn) \u0000 AY) — with equality if and only if f and g determine the same distribution. We can clearly use 101 \u0000 f hi ( AY) g(Y) \u0000 J \u0000 \\g{Y) f(Y) clY (F2) as a measure of how badly f is approximated by g. This is the Kullback—Leibler `relative entropy'. 198 \u0000 6: Confidence Intervals for one-parameter models Fa. Exercise. Check that App(f \u0000 g) is invariant under a one-one transformation Y = h(Z). Fb. Exercise. Check that if f is the pdf of N(pi , 1) and g that of N(p2, 1), then APP(f 9) = \u0000 — 12)2. By extending the above exercise, we can get a really good understanding of Fisher information. \u0000 ►► Fc. Exercise. Consider the usual situation where Y1, Y2, \u0000 , Yn, are IID, each with pdf/pmf f (y 0). Prove (for nice situations) that, as yo —> 0, App (f (• 10) \u0000 f (• I cp)) = Z (go — 0) 2 f . (0) + 0 ((so — 0)3) Now explain the good sense of Fisher information. ► G. Lemma: Maximum-entropy distributions. (a) Amongst all pdfs on R with given mean p and variance 52, that of the N(p,, o-2) distribution has greatest entropy. (b) Amongst all pdfs on [0, co) with given mean p, that of the E(mean distribution has greatest entropy. (c) The value of p which maximizes the entropy —pinp — (1 — p)ln(1 — p) of the Bernoulli(p) pmf is 2. Proof of Part (a). All three parts are proved similarly. I do Part (a), and leave Parts (b) and (c) as exercises for you. Let f be a pdf on 118 with mean u and variance o-2. Let g be the pdf of the N(p, o-2) distribution. Then 0 < f In ( gf ((:))) f (y) dy = f ln(f (y))f (y)dy — f ln(g(y))f (y)dy 0_211 )2 1 (y) = —Ent( f) + f in (o-Nr) f (Y) dy f (Y2 \u0000 dy = —Ent( f) + In (o-N7r) + with equality if and only if f = g. Ga. Exercise. Prove Parts (b) and (c) similarly. 111 ►► H. Entropy and Fisher information for location parameters. \u0000 A natural question now arises: Can the maximum-entropy property of the normal distribution explain its role in the Central Limit Theorem? It is a question which 6.6. Bayesian Confidence Intervals \u0000 199 had been considered by many authors prior to its definitive affirmative solution by Barron [11]. Let Y be an RV with location-parameter density f (y 0) = h(y — 0) where h is a pdf on R with associated mean 0 and variance 1. Since ao f* (y 0) = — hi(y — 0), we have rpm = E (fat9/(Y I 6)/ 2) = E0 ({hh' ((yY ))}2) \u0000 ./(Y) (say) /(Y) being independent of 0, and E 0 referring to the '0 = 0' situation. Consider also (see Exercise Ha) J(Y) := E 0 ({ \u0000 h(Y) \u0000 co(Y) } 2) \u0000 1.(7) 1, \u0000 (Hl) where yo is the pdf of N(0, 1). Barron's remarkable identity is that if X is a 'continuous' RV with positive pdf on (0, oo), then 17r App( f co) = f J(X cos° Z sin 0) tan° dt, Z being independent of X with Z \u0000 N(0,1). Barron uses this in proving a strong version of the CLT (with `Scheffe' convergence of densities) if the typical summand X has continuous pdf. The method will not deal with discrete RVs or with 'fractal' ones, but it is very illuminating even so. Ha. Exercise. Prove equation (HI), and show that if J(Y) = 0, then Y N(0, 1). You may assume that yh(y) 0 as oo. Since J(Y) > 0, it is clear that /(Y) > 1. Show that this is the Cramer—Rao MVB result for the location-parameter situation and that 0 H N(0, 1) is the only location-parameter family with mean 0 and variance 1 and with an MVB Unbiased Estimator. 6.6 Bayesian Confidence Intervals I should reiterate my thanks to David Draper here. This section is much improved due to his wise advice. See towards the end of Subsection 169A for notes on my deliberate misuse of orthodox Bayesian terminology (of which misuse David does not approve). Let's see how the Bayesian machine works before getting involved with the philosophy. Posterior density oc Prior density x Likelihood, 7r(0)Ihd (0; yobs) or, in symbols, (Al) (A2) 200 \u0000 6: Confidence Intervals for one-parameter models Suppose that I give you a newly-minted coin, that you throw it 5 times and get 1 Head and 4 tails. The Frequentist 'best estimate' of the probability p that the coin falls Heads would be 1/5, an absurd estimate because we know that the true value of p will be close to 1/2. (But, of course, the Frequentist Confidence Interval would be very wide.) How should we build in prior information? We can only think sensibly about this after we have our machine running. ► ► A. THE formula again. From one point of view, Bayesian Statistics consists of just one formula already discussed in subsections 77F-78G with the 'constant of proportionality', a function of yobs, ensuring that 7r (9 yobs) is a proper density — a true pdf — in that fR 7r (0 I yobs) do = 1. Because of the proportionality at (A2), we need only specify the prior 740) `modulo constant multiples', and we can also absorb functions of y obs alone into the oc sign, being free to omit them from lhd (0; yobs). The posterior density 7r (0 I y obs) is meant to indicate 'degree of belief' in the whereabouts of 0 after the experiment is performed with result yobs. Thus a C% Bayesian Confidence Interval for 0 after the experiment is performed is an interval I such that 7r (0 I yobs) d0 = C%. \u0000 (A3) To a Bayesian, the posterior density itself provides the correct way of describing inference about 0, not the less informative C% Confidence interval derived from it. Of course, knowledge of the posterior density is equivalent to knowledge of all C% CIs for every C. In strict Bayesian theory, there is the obvious analogous interpretation to (A3) of the prior density 7(0) as expressing degree of belief in the whereabouts of 0 before the experiment is performed. As is discussed later, Bayesian degrees of belief are subjective: each person is allowed to have his/her own prior density, though he/she needs to justify its choice in some way; and that person's prior density will result in his/her posterior density. Somewhat controversially, Bayesians treat 'degree of belief' densities in exactly the same way as they do pdfs. 6.6. Bayesian Confidence Intervals \u0000 201 For the purposes of Your calculation, therefore, it is as if, as we did in the Rev'd Thomas Bayes story in Subsection 78G, You regard 0 as the value of a Random Variable 8 chosen by God according to the pdf 74•), where 7r(•) is Your prior density. Your posterior density 7F • I Y thS) is then the conditional pdf of 0 given that Y = y°138 in a sense already intuitively clear from Subsection 78G and clarified further in our discussion of conditional pdfs in the next chapter. But I also discuss it now, so that you will feel secure. The fundamental postulate is that conditionally on 0 =. 0, the Variables Y1/ Y21 • - Yn. are HD each with pdf f So, heuristically, IP(Y E dy I 8 E dO) = P(171 E dyi; • • • ; Yn E dyn I 0 E dO) P(Yi E dyi e Ede) \u0000 E dyri IO c d0) = f (yi I C)dyi . • • f (yn e)dyn = 1hd(0; yl, - • • Yr)dyi • • • dyri = lhd(0; y)d3r, and so P(8 E c10;Y E dy) = P(0 E dO)P(Y E dy 10 E dO) = 71-(0)d0 lhd(0; y)dy. Hence P(Y E dy) \u0000 f 71-(0) lhd(0; y)dO} dy, \u0000 (A4) and P(0 E dO I Y E dy) = \u0000 ip(y E dy) ir(0) lhd(0; y)d0 f 7r(v) lhd(yo; y)dcp or 740 I y) oc ir(0) lhd(0; y). Note that Y1, Y2, • • • , Y,,, though conditionally independent given 0, will generally not be absolutely independent because the integral at (A4) mixes up the multiplicative property which would hold if 0 were (as in the Frequentist approach) fixed at a deterministic value. P(8 E d0; Y E dy) 202 \u0000 6: Confidence Intervals for one-parameter models (Historical Note. The 'God and the Rev'd Bayes' story does not represent (as Statistics, I mean, not as Theology) the way that Bayes thought of things.) Especially to express the idea of no prior information, or of vague prior information, about 0, improper priors such as 7r(0) = 1 (0 E R) or ir(0) = 1/0 (0 e (0, oo)) are often used. These priors are improper in that they satisfy f 7(0) dO = oo, so they cannot be normalized to a proper pdf by dividing by a constant. The only sense we can make of a uniform prior 74.) = 1 (for example) in terms of God's experiment is that, conditional on the fact that the number chosen by God lies in the interval [a, b], it has the U[a, I)] distribution. Warning. Using 'conditional probabilities' in settings where the underlying measure has infinite total mass can be very dangerous. It really is best to use proper priors. For the time being, I shall speak of 7r(•) as 'our' prior, pretending that You and I have agreed on it. ►► B. Recursive Bayesian updating. We have already seen in Subsections 82M — 840 how the posterior pdf after one experiment can become the prior for a second experiment; and so on. This is a particularly nice feature of Bayesian theory. ► Ba. Pre- priors and Pre- experiments. We have to see this topic in practice (as we shall do later) to understand it properly. It can sometimes help in achieving a sensible balance between prior information and information from our experiment to imagine that Zeus first chooses 9 according to a 'Pre- prior' 71-z •, then tells Tyche the function 71-z (•) but not the value of 0, that Tyche performs a first experiment (a pre-experiment to ours) to determine 0, and that Tyche's posterior density 71-7, • is then revealed to us and becomes our prior g•). Note that (heresy, but one with some appeal to Fisher and to K. Pearson) it suggests a way of building prior information into Frequentist theory. It is helpful to think of Zeus as using a 'vague prior', widely spread and therefore giving little information about the whereabouts of O. The number of observations made by Tyche is then the effective prior sample size, a good indicator of how heavily we are weighting prior information. In the next subsection, however, Tyche does not do an experiment. ► C. 'Vague prior' Bayesian theory for a location parameter. Suppose that Y1, Y2, • • - , Yn are IID each with pdf/pmf of the form f (y 10) = h(y — 0). If we have no prior information about the whereabouts of 0, it is natural that our prior should be invariant under any shift 0 i--> 0 + c where c is a constant. This implies that 7(0) = ir(0 + c), so that 7• = constant, and the constant may as 6.6. Bayesian Confidence Intervals \u0000 203 well be 1: we say that we use the uniform prior. [It is impossible to conceive of a situation where we have no prior information: we may for instance know that 101 < 106; but ignore this for now. Considerations of robustness are important here.] As already explained, the posterior density 71 • I yobs) then agrees with the Frequentist 'posterior density' obtained by using ancillary statistics. Or perhaps one should be cynical and say that ancillarity was used to cook the Frequentist answer to agree with the Bayesian with a uniform prior. You can see why I refer to the uniform prior for location parameters as the Frequentist prior. If each Yk is N(0,1), then, since 7r(0) = 1, we find from 182Ca that (0 1 yobs) a 1hd(0; y'bs) oc exp {-ln(0 — Yobs )2 } functions of yobs being absorbed in the cx signs. Thus, ) 7r (• I y obs) must be the pdf of N(yobs, 1/n), in agreement with the discussion at 175Ba. D. The improper 'vague prior' for Exponential E(rate A). Suppose that each Yk has the E(rate A) distribution. Write Yk = exp(Wk) and A = exp(—a). Knowledge of Y is the same as that of W. The density of Wk in terms of a is given by f (w I a) = h(w — a), where h(r) := exp fr. — el-. Thus a = — In A behaves as a location parameter for WI., W2, . W n; and the uniform prior for a translates into a prior 7r(A) = 1/A for A. Naively, 'dal = Ida I/A. Then, 7ro y ob.) oc 1 Ane —Asob \u0000 An—ie —Asobs A so that 7r(A I yobs ) ) is the pdf of the Gamma(n, rate sobs) distribution, in agreement with the discussion at 176Ca. This is my justification for calling 7r(A) = A-1 prior the Frequentist prior here. ►► E. Sufficiency and ancillarity in Bayesian theory. If t \u0000 7-(y) is a sufficient statistic in that the factorization 183(D1) lhd(0; y obs) = g(0;t)h(Y 3bs) holds, then, for any prior 7•, we have 7r (e I yobs) cc 7r (0)g(0;ebs) h(yobs) oc 7r(0)g(0;t'bs) SO that 7r (0 I yobs) ) depends on y obs only via eths: \u0000 7r (o I yobs) \u0000 7r(0)g(0;Obs) f 7 (y:)0 (w; tobs) dc . 204 \u0000 6: Confidence Intervals for one-parameter models Though we have not studied ancillarity, I remark that since we condition on the whole information in yobs in Bayesian theory, we automatically condition on ancillary statistics, so there is no more to say. This is indeed all much simpler than the Frequentist version. ►► F. Asymptotic normality in Bayesian theory. Imagine for a moment that we work with a uniform prior 7(0) = 1. Then 7(9 I yobs) CC Ihd(e; yobs). We take a Taylor expansion of 409 ; yobs ) around Bobs (at which point, aet(o; yobs) = 0) to obtain (0 ; yobs) \u0000 £ (gobs; Yths) \u0000 (a — eobs) 2 2 90 f(0; y obs) B= Bobs However, the Strong (or Weak) Law shows that am obs \u0000 e; y ,' \u0000 _ E f . (vb. ) k \u0000 0 0=e0b. io=eobs is likely to be close to nIE f*(Y 10) = — nI f.(0), whence, as a function of 0, \u0000 7(0 I yobs) cx lhd(B; yobs) \u0000 ) function(yobs, exp — 1(0 — oobs) 2 h(tjobs)} whence, 740 yobs ) must be close to the density of N (oobs , 1/-it ( 'jobs)) • If some other prior density 7r(0) is used, then 7(9) will be approximately constant near Bobs, so since the likelihood will fall off rapidly as 0 moves away from Bobs, we can assume that 09) is constant when n is large. For large n, the information in the current experiment swamps the prior information. Thus, again, ,n- (0 y obs) ) must be close to the density of N (kips , 1/1e( 'jobs)) • This is the Bayesian view of Fact 193B. Fa. Something to think about. Note that while the CLT was crucial to the Frequentist approach in Subsection 194D, it was not used in the Bayesian: in the latter, the normal distribution just appeared from Taylor expansion. ► G. Bayesian reference priors. How should we find the appropriate prior density to represent vague prior information in general? There is an elaborate theory of Bayesian reference priors (see Volume 1 of Bernardo and Smith [17]) which seeks to answer this problem. In commonly met cases, the answer provided by that theory agrees with Sir Harold Jeffreys' prescription: \u0000 740) cc if.(0) 2 \u0000 ( 'reference prior' ). \u0000 (G1) One version of the 'reference' theory is based on entropy and, very roughly speaking, the reference prior is the prior starting from which the experiment makes the biggest gain 6.6. Bayesian Confidence Intervals \u0000 205 in information about O. We see from Subsection 197(F2) that, for example, we might measure the gain in information about 0 made by taking an observation y obs by J ir(9 I Y°1' s)ln ir(O yobs) d0 7 (0) That's a small start on the route which you should follow up in Bernardo and Smith [17]. Entropy is 'Shannon information'. A case for the reference prior can be made on the basis of Fisher information, too. Suppose that a = u(0), where u is a strictly increasing smooth function. With the asymptotic-normality result in mind, we can try to make a behave as closely as possible to a location parameter by making the Fisher information for a a constant (say, 1) independent of a. Thus, we want (with usual misuse of information) f aaf: ( aa ) 2 gy I a) dy = 1. Note that the integral is with respect to y, not a. By the chain rule for differentiation, a If* (0) = f \u0000 f* ) 2 ao \u0000 f (y I dy = u i(0)2 f ( aafa* 2 \u0000 ) f (y \u0000 dy = u1(0)2, whence u' (0) = I f* (0)1 Since the uniform prior is appropriate for the pseudo-location parameter a, and da = u /(6)d0, it is appropriate to use the reference prior for 9; or at least that's the way this argument goes. ► Ga. Invariance of reference priors under reparametrization. Suppose that v is a strictly increasing function of 9, and let 9 = v(p). Then (p is a new way of parameterizing things. Now if we regard 9 and cp as Random Variables in Bayesian fashion, 0 having pdf r(0) and co having pdf 'Tr (co), then, by the Transformation Rule at 55F, we should have Fr(yo) = 7r(v(6))v'e,o)• Let the pdf of an observation in terms of co be 9(Y I (P) := f (Y I v(co))• Then the information 4• (co) about cp satisfies ag,, \\ 2 ig. 69) := f aep \u0000 g(Y I (P) dY = (0 2 f Cafe* )2 f (Y I 0) dY = (0 2 f. (0), and ig* ((p)1 = v'((p)I f * (v((p)) 2 . 206 \u0000 6: Confidence Intervals for one-parameter models In other words, the reference prior transforms correctly under change of parametrization, the strongest argument in its favour. The main dictate of Bayesian philosophy is that one must always behave consistently. Gb. Exercise. Prove that the reference prior density (for 9) for Bernoulli(9) is the proper density {9(1 — 0)} \u0000 illustrated on the left of Figure 2091(i) and for Poisson(0) is the improper density \u0000 However persuasive the Mathematics of reference priors has been for you, are you happy that the Bernoulli reference prior attaches so much more weight to 9-regions near 0 than to regions of the same length near ► Gc. Exercise. Prove that if 9 is a location parameter in that f (y 0) = h(y — 9) for some pdf h on R, then the reference prior for 0 is a constant function. ► Gd. Exercise. For our 1-parameter situation, -y > 0 acts as a scale parameter if for some known constant 00 and some pdf g on R, we have f (0,y) \u0000 1 g y 00) 7 ) Show that -y is a scale parameter for E(mean -y) and for N(0, 72). Prove that 1 (arr)(17 1'7) = - [1 + z(g*Y(z)1 where Z has pdf g, and deduce that the Jeffreys reference prior 71-(-y) for -y satisfies 74-y) a -y-1. Under this prior, ln(-y) has uniform density on R. The truth is that outside of location-parameter cases and scale-parameter cases, there is controversy about the appropriateness of the reference prior at 204(G1) as a prior. The multi-parameter situation — with its Riemannian geometry and its worrying features — is discussed at 379N. Discussion. If one is to make any sensible inferences about 0, then • either one must have reasonably good prior information, which one builds into one's prior, • or, if one has only vague prior information, one must have an experiment with sample size sufficient to make one's conclusions rather robust to changes in one's prior. There are, however, important cases where one has neither because data are difficult or expensive to collect, or because Nature only provides examples infrequently. 6.6. Bayesian Confidence Intervals \u0000 207 ► H. Conjugate priors. The idea is that in certain situations, if one chooses a prior 74.) in a family 'conjugate' to the form of the pdf f (y 10) of the model, then the posterior density R- (. y obs) will also be in that conjugate family. `Conjugate priors' are mathematically very convenient; but this is not in itself a good Statistical reason for using them. However, it is usually the case that we can approximate any prior density arbitrarily closely (in the right sense) by mixtures of conjugate priors. See 213L for discussion of this. Ha. Normal conjugate for N(0, prec P0) model, po known. Recalling that precision is the inverse of variance, we write N(0, prec p) for N 0, IS)). ► APOLOGIES for using p for precision. The symbol p is usually reserved for correlation. It shouldn't cause any problems that I make both uses of p. No correlations feature until Section 8.4. I just ran out of symbols. Suppose that each Yk is N(0, prec po) where Po is a known constant. Suppose that we take as prior for 0 the density of the N(m, prec r) distribution. Then In r (0 yobs) = —ir(0 — m)2 — 1Pon(Yobs 0)2 + function (m, r, po, yobs) = —1rnew (0 — mnew)2 + function (m, T, Po , yobs) where rnew (HI) We see that if each Yk has the N(0, prec po) distribution, and 74.) is the density of N(m, r), then 7(• I yobs) obs ) is the pdf of N(mnew, prec rnew). The family of normal distributions is therefore conjugate for the N(0, ae,) model. Our prior estimate of 0 is in with precision r. In Frequentist theory, the estimate gobs is an estimate of 0 with precision npo. We see that, in going from prior to posterior, • precisions add, • the new mean innew is the probability mixture of m and yobs weighted proportionately to the corresponding precisions. 208 \u0000 6• Confidence Intervals for one-parameter models (a) THE Bayesian picture postcard (b) A Bayesian nightmare Figure H(i): Prior-likelihood-posterior pictures This makes good sense; and it can help guide us in our choice of a suitable prior when we do have prior information about O. Note that the effective prior sample size Npri is r/po, the effective complete sample size is Ncomp = rnew /Po, as is checked out by Nprim + ny Mnew = \u0000 • N pri Ti Formally, the uniform vague prior has prior precision r = 0 and any m. In strict Bayesian theory with proper priors, we might take as prior the density of N(m, prec po) with po very small. Figure H(i)(a) illustrates a good case of the 'normal conjugate for normal' situation. Figure H(i)(b) is a bad case in which prior and likelihood are in serious conflict and in which therefore we have no robustness: a small change in the prior could cause a large change in the posterior density. When prior and likelihood conflict, prior x likelihood is always small, so the 70 I yobs ) ) is obtained from something close to 0/0. You can see how lack of robustness can arise. Ncomp — Npri + n, 6.6. Bayesian Confidence Intervals \u0000 209 • • I. The Beta(K, L) distribution. The important family of Beta distributions acts as a conjugate family for the Bernoulli(0) model. For K > 0, L > 0, a Random Variable X is said to have the Beta(K, L) distribution on [0, 1] if it has pdf fx(x) = beta(K, L; x) xic-1(1 — x)L-1 (0 < x < 1) \u0000 (I1) := \u0000 where, of course, B (K, L) is the Beta function: B(K, L) 1 B(K, L) := f x -K —1(1 — x)L-1 dx = o F(K)F(L) F(K + L) . (I2) We saw a probabilistic proof of the F-function expression in the case of positive- 1 o \u0000 0 \u0000 0 K = 0.5, L = 0.5 \u0000 K = 1, L = 1 \u0000 K = 2, L = 4 Figure I(i): Pdfs for some Beta(K, L) distributions integer K and L at 107(L), from which a nice interpretation of the Beta(K, L) distribution follows. A probabilistic proof of the general case will be given later at 250(Cb). Illustration of some Beta pdfs are given in Figure I(i). Ia. Analytic proof of the r-function formula. This proof assumes familiarity with Jacobians, revision of which occurs in Chapter 7. We have O. \u0000 F(K)F(L)- \u0000 e-uuK- i dufv=0 e —vvL—ldv. fu=o 210 \u0000 6: Confidence Intervals for one-parameter models Write u v = s, u/ (u + v) = r, so that u = rs, v = (1 - r)s, and we have the Jacobian au 8u 3,1; \u0000 3z a, as -S 1 - r S. = Hence, 0. F(K)F(L) \u0000 f e- ssK-1sL-l sdsf r K-1(1 - r)L-l dr = r(K+L)B(K,L). Ib. Exercise. Show that the mode of X, the value where fx (x) is a maximum, satisfies mode(X) = K - 1 \u0000 if K + L 2 K > 1 and L > 1. Show that (compare Exercise 108La and the line preceding it) B(K + 1, L) E(X) \u0000 - \u0000 B(K, L) \u0000 K KL Var(X) = \u0000 (K L) 2(K + L + 1) . (K,L>0), Ic. The Beta family as conjugate for the Bernoulli model. If Yi , Y2, ... Y7, are IID each Bemoulli(0), and we take as prior density for 0 the Beta(K, L) density, then the posterior density is the pdf of Beta(K + sobs, L n - sobs), where, as usual, sobs = E yes . It is as if Zeus used the {0(1 - 0)}-1 prior, and Tyche's pre-experiment of K L trials produced K successes and L failures, and Tyche's posterior density becomes our prior. (Immortals are not restricted to integer values of sample size!) The reference prior for 0 is the (proper) pdf of Beta(1, 1) illustrated on the left of Figure 2091(i), but there are, as we have seen, rival arguments for using the improper formal Beta(0, 0) density R-(0) = {0(1 - 0)}-1 or, in some cases, the uniform prior pdf of Beta(1, 1). Id. Calculation of Beta distributions. This may be achieved in a way analogous to that used at 149Ea for Gamma distributions. Here's the relevant part of a program: double Fbeta(double x){ int n; int ans; double sum, frac, temp, logfrac, ratio, term; temp = - loggam(K) - loggam(L) + loggam(K + L); 6.6. Bayesian Confidence Intervals \u0000 211 if (x <= 0.5){ logfrac = K * log(x) + L * log(1-x) - log(K) + temp; frac = exp(logfrac); term = sum = frac; n=0; do{ ratio = x * (K + L + n)/(K + n+1.0); term = term * ratio; sum = sum + term; n++; }while((fabs(term) > htol) II (ratio > 0.55)); return sum; } else { logfrac = K * log(x) + L * log(1-x) - log(L) + temp; frac = exp(logfrac); term = sum = frac; n=0; do{ ratio = (1-x) * (K + L + n)/(L + n+1.0); term = term * ratio; sum = sum + term; n++; }while((fabs(term) > htol) II (ratio > 0.5)); return (1.0 - sum); } } ► J. A summary table of some conjugate families. Y1 7 Y2, • • • , Yn 7 IID each with distribution Prior density 7(0) is density of distribution Posterior density r(B 1 yobs) is pdf of distribution N(0, prec po) N(m, prec r) N rm+nP° ( r+npo \u0000 , prec r + npo) E(rate 0) Gamma(K, rate a) Gamma(K + n, rate a + sobs) Poisson(0) Gamma(K, rate a) Gamma(K + sobs, rate a + n) Bernoulli(0) Beta(K, L) Beta(K + Sobs, L + n — sobs) N(0, prec 0) Gamma(K, rate a) Gamma(K + 2n, rate a + 1E yz) Table J(i): Conjugate priors Table J(i) collects together some useful information. ■ • Ja. Exercise. Check the last four rows of the table. In each case say how the reference prior fits into the pattern. ►► K. Predicting new observations from current data. Making predictions about future observations is a key element of Statistics. 212 \u0000 6: Confidence Intervals for one-parameter models Ka. A coin-tossing example. Suppose that our experiment consists of tossing a coin with probability 0 of Heads n times, and that a Beta(K, L) prior is thereby converted to a Beta(K + sobs, L n — sobs) posterior. If we wish to estimate the probability that the next throw will produce Heads, then, as Chapter 9 will clarify, we have to find the posterior mean of 0, namely, E post (8) \u0000 K + sobs = f 07(0 yobs) dO K L n' as we know from Exercise 210Ib. If we wish to estimate instead the probability that the next two throws (after the nth) will produce Heads, we must find E po,t (82), namely, B(K + 2 + sobs, L + n — sobs) \u0000 K s obsx K sobs ± 1 B(K Sobs, L n — sobs) \u0000 K +L±n K+L±n±1 . There is good recursive sense in this: Bayesian predictive probabilities display a nice coherence. Frequentist theory is in some difficulty here. If you tell a Frequentist the results of your n tosses of a coin and ask \"What is the probability that the next two results will be Heads?\", he/she will say \"Well, 02, of course, where 0 is the unknown parameter. But I can give you an approximate CI for 02 (which, to make matters worse, does not take into account the fact that 0 is bound to be close to ).\" Not much recursiveness here! Kb. A 'normal' example. Suppose that for observations y (13.b S y s \u0000 ynobs, we use the N(0, prec po) model (P0 known) with N(m, prec r) prior, thereby obtaining an N(m,,, prec rnew) posterior. What would we say about Yn+i in this situation? Intuition (made precise in the study of joint pdfs in the next chapter) would say that, conditionally on Y = yobs, 1/72+1 is O + en+i where 8 is N (m„,, prec rnew), en+1 is N(0, 0-g) , where ug = 1/po, and O and En±i are independent. Thus we would say that given our current information (171, 172) • • • Yn), Yn-ki is N(Mnew, rnew ag). Kc. Exercise. Suppose that we take the `Frequentist' uniform prior for 0 in the 'normal' example just discussed. Explain that there is a certain sense in which a Frequentist might be happy about the Bayesian prediction for Yn±i because of the use of a Pivotal RV. Kd. Exercise. Suppose that Y1, - Y 2 7 • • • 7 Yn) Yn+17 Yn+2 are IID, exponential rate A. Suppose that es , es yobs have been observed, and that their sum is s = sobs. Show that a Bayesian using the `Frequentist' prior A-1 for A would say that 1 post (Ynd-i > tS) = (1 ± on ?post (Yn-F1 > tn+1.9; Yn+2 > tn+28) = 1 (1 +4,+1 tn+2)n 6.6. Bayesian Confidence Intervals \u0000 213 As will be clear from Chapter 7, and clearer still after our study of the Poisson process in Chapter 9, it is true for every A that, with S :=1/1. + • + 1771, 1 ED(yn±i > tS) = (1 \u0000 + P(Yn+1 > tn-F1S; Yn-F2 > tri+2S) (1 + tn+1 tn-F2) n which makes Frequentist and Bayesian views tally here. Ke. Exercise: Poisson model. Suppose that all nuclear power stations on Planet Altair 3 were installed at the same instant, exactly 1 year ago, and that accidents occur purely at random, that is in a Poisson process of rate 9 per year, as studied in detail in Chapter 9. This means that the number of accidents occurring in a time interval of length t has the Poisson distribution of parameter Ot independently of what happens in disjoint time intervals. The only information we have about 0 is that 1 accident has occurred in the 1 year. What is our best prediction of the expected number of accidents in the next year? Frequentists would say 1, of course. Show that the Bayesian reference prior would lead one to say 11. This example will be discussed further in Chapter 9. Kf. Exercise. Show that if 0 has posterior distribution Gamma(K, rate a), then our estimate that a Poisson 0 variable takes the value n is (n + K — 1)(n + K — 2) • • • (K + 1)K \u0000 a K (a \u0000 + 1)K-Fn for n = 0, 1, 2, .... Show that if K is a positive integer, then this is the probability that the Kth Head occurs on the (K + n)th toss of a coin with probability oil (a+ 1) of Heads. See Exercise 53Dc. We shall discuss this further in Chapter 9. ►► L. Mixtures of Conjugates. Suppose that our likelihood function has a conjugate family. Then we may well use a prior of the form P(Ork (0), (L1) where each rk• is in the conjugate family, and p(k) > 0, > p(k) = 1. Thus 740) is a probabilistic (p(k)-weighted) mixture of conjugate pdfs. In the 'God and the Reverend Bayes' picture, God could choose 0 directly using 7r(9). Alternatively, we could have a hierarchical model in which God does a two-stage experiment: first, He chooses N with values in {1, 2, ... , r} at random with P(N = k) = p(k), and then He chooses 0 with pdf 71-N. He tells Bayes all the values p(k) and all the functions ir k but keeps N and O secret. La. Exercise. Show that 1 n! 70 I Y) = \u0000 Ork(O J Y), \u0000 (L2) 214 \u0000 6: Confidence Intervals for one-parameter models where, of course, each irk (• I y) is in the conjugate family and where (Exercise!) the p(k y) satisfy 201Y) \u0000 E Ii *here Ik \u0000 P( k ) rk( hd(0; y) d0. \u0000 (L3) Of course, p(k y) > 0, E p(k Iy) = 1. Indeed, show that y(k y) \u0000 P(N = k \u0000 y) \u0000 (L4) in a sense which should be intuitively obvious and which is made rigorous in the next chapter. This formula is the key to Bayesian significance testing. We see that ir(• I y) is again a probabilistic mixture of conjugate pdfs. We shall see later in Subsection 217N and Chapter 8 how this may be used. In principle, the scope of the method is as wide as possible because (In the cases of interest to us' is understood here) \u0000 we can approximate ANY prior DF by a mixture of conjugates. \u0000 (L5) We know from our study of the CLT that this means: given any DF G, we can find DFs Gn, each with pdf a probabilistic mixture of conjugate pdfs, such that GT, (0) —> G(0) at every point 0 at which GO is continuous. The following proof of this fact is of independent interest. Proof of (L5). The idea is as follows: first (Stage 1) we prove that G can be approximated (in an even stronger sense) by DFs of RVs each of which takes only finitely many values. But the DF of an RV taking only finitely many values is a probabilistic mixture of the DFs of deterministic (constant on 52) RVs. Hence, to complete the argument, we need only show (Stage 2) that the DF of a deterministic RV may be approximated by a probabilistic mixture of DFs each with conjugate pdf. Stage 1. Let G be any DF. Recall that G is right-continuous, which is why we can use min rather than inf in the following definitions. Put 00 = —co, G(00) = 0. Let n be a positive integer. Define recursively for k > 1, 0k := min{0 : G(0) > G (0k_ i ) + 1/n} provided the 0-set on the right-hand side is non-empty. Note that we can define Ok for at most n values of k. Let a be the largest such k, and set 0a _Fi = co. Now define (for 0 E R) H(0) := G(0k) if Ok < B < 0k±i• Then 0 < G(0) — H(0) < 1/n for every 0. This completes Stage 1. 6.6. Bayesian Confidence Intervals \u0000 215 Stage 2. It is certainly enough to show that a single DF with conjugate pdf can approximate the DF \u0000 of the constant variable O = c: 11,0) = {0 if 0 < c, 1 if 0 > c. Tchebychev's inequality shows that the DFs of ( N (c , 1 — , Gamma (n, mean n — c ) , Beta(nc, n(1 — c)), which all have mean c and which have variances 1 \u0000 c \u0000 c(1 — c) 71, n' \u0000 n + 1 respectively, all converge to H, at values of 0 other than the discontinuity point c of Remarks. It goes without saying that if we allow ourselves complete freedom in our choice of mixtures, we have essentially the same problem as that of choosing a prior from the set of all priors. We would normally use a mixture of just a few conjugate pdfs. ►► Lb. Hierarchical models. That the parameter O might itself be considered to be the result of a two-stage experiment, involving hyper-parameters (p(k)) and (7rk (-)) in our example, is part of the theory of hierarchical modelling. Draper [67] is one of the places where this is presented with clarity and good sense. Of course, the p(k) could themselves be considered random if we take another level in the hierarchy. The idea of 'adding levels' will feature in different ways in this book: in modelling coin-tossing in Subsection 217N, in the Gibbs-sampler analysis of a Cauchy location parameter in the next chapter, in models in Chapter 8; and it does in a way form a central part of de Finetti's theorem in Subsection 220P. But the main uses of hierarchical models feature in Chapter 8. ►► M. First thoughts on choosing Your prior. The points raised here will be illustrated in later subsections. ► Ma. Subjectivity. Bayesian Statistics is a subjective theory: Everyone is entitled to his/her own prior. You have Your prico; I have Mine. But this puts a real responsibility on You to say how you arrived at Your prior. 216 \u0000 6: Confidence Intervals for one-parameter models You may choose the very popular route of choosing one of the standard `vague' priors or something close to it. Or you may — and whenever possible, should — take the business of choosing Your prior very seriously, arriving at one with characteristics approved by people working in the field of application. Those people may suggest either a prior mean and variance or a 95% prior CI, and You might choose as Your prior a conjugate prior to fit their beliefs. Draper [67] has examples you should study. See also the use of imaginary results in the next subsection. ► Mb. Comparing 'pre' and actual sample sizes. Suppose that there is a natural measure of effective prior sample size, the number of experiments that Tyche did before she handed over to you. (For example, for large K and L, a Beta(K, L) prior has effective sample size K + L, modulo irrelevant quibbles over vague priors.) If the effective prior sample size is much larger than the experimental sample size, then there is little point in doing the experiment. At the other extreme, if the experimental sample size is much larger than the effective prior sample size, then one might as well use a vague prior. That leaves important cases where the two sample sizes are 'comparable', but it raises other points as well. ► Mc. Robustness and sensitivity. As mentioned earlier, one can only be happy about one's conclusions if they are relatively robust in that small changes in the prior result in small changes in one's conclusion. (This assumes that one is happy with the model, that is, with the likelihood function.) We lose robustness in cases where the likelihood function lhd(.; yobs \\ ) is in conflict with the prior density function 7(•) in the sense that whenever one is large, the other is small. Md. Cromwell's dictum. Bayesian statisticians all quote Cromwell's dictum, a sentence in a letter (3 August 1650) from Oliver Cromwell to the General Assembly of the Church of Scotland begging them to consider the possibility that they might be wrong. (The actual sentence, \"I beseech you, in the bowels of Christ, think it possible that you may be mistaken\" reads very strangely in terms of modern language. However, it's not so much language that has changed as our knowledge of the way we function: in the time of Cromwell, the bowels were thought to play a much more important part in our lives than merely that of helping process food! The brain was very under-rated! I guess that `bowels' in Cromwell's sentence might these days read 'innermost being' or something similar. It's interesting that we have not fully escaped the old ideas, as phrases such as `gut reaction' demonstrate.) Bayesians usually quote Cromwell's dictum to state that one's prior density should never be literally zero on a theoretically possible range of parameter values. But we should regard it as a warning not to have a prior density which is too localized, and which is therefore extremely small on a range of parameter values which we cannot exclude. Consider ye therefore the possibility that a prior with very large effective sample size just might be wrong! (That your model might be wrong is something you must always bear in mind.) Using 'heavy-tailed' priors increases robustness. 6.6. Bayesian Confidence Intervals \u0000 217 ► N. Choosing a prior for coin tossing: use of imaginary results. How would you decide on Your prior for the probability 9 that a coin which you are just about to toss 100 times will fall Heads? What makes this case difficult is your very strong prior belief that 9 is very close to 1. But there are points in the following discussion which are relevant much more generally. Suppose that you are 95% confident that 9 E (0.49, 0.51). \u0000 (N1) You might decide that you will use a Beta(N, N) distribution with large integer N. Such a distribution has mean 0.5 and is strongly concentrated about that point. It is as if in the Pre-experiment, Zeus used the density {9(1 — 0)}-1 (the 'Frequentist prior') and that Tyche then tossed the coin 2N times getting N heads. But we know from our 'voting' discussion that Tyche's posterior density is approximately normal Ng- , TM, so that, with 2 for 1.96 as usual, we must have (as nearly as makes no difference) \u0000 2'4 x \u0000 2N \u0000 '\\/ 1 \u0000 = 0.01, whence N = 5000. The effective prior sample size, the size of Tyche's experiment, is therefore 10000, and if we really are going to stick to the Beta(5000, 5000) prior, then there is no point in doing the '100 tosses' experiment because the effective prior sample size would dwarf the experimental sample size. If you are determined to stick to the Beta(5000, 5000) prior, you need to do an experiment with a sample size of many thousands for the experiment to be worth doing. But you may think to yourself: \"But what if I were to toss the coin 100 times and it fell Heads only 30 times? Surely, I should begin to think that the coin is biased?!\" Cromwellian doubt about your prior is setting in. You might allow a small probability c (the 'Cromwell factor') that something in the coin's manufacture (or in its history since then) makes (say) a Beta(M, M) distribution appropriate, where M is much smaller than N. Hence, recalling that beta(K, L; •) denotes the density of the Beta(K, L) distribution, you might choose to use as Your prior density the function (mixture of conjugate pdfs) \u0000 (1 — c) beta(N, N; 0) + cbeta(M, M; 0). \u0000 (N2) For really small c, you will still have (N1) — very nearly. Na. Exercise. Let 1/1, T1, H2, T2, h, t be integers. Show using equation 213(L2) that if you use a prior (1 — c) beta (Hi, ; 0) + c beta (H2, T2; 0) , and the coin is tossed h + t times, producing h Heads and t Tails, then Your posterior density for 0 is given by \u0000 (1 — a) beta (H1 + \u0000 + t; 0) + beta (H2 h, T2 ± t; 0) , where = A(1 — c) + c' 218 \u0000 6: Confidence Intervals for one-parameter models where (in a form for easy computer implementation) ln A = ln B (Hi + h,Ti + t) - ln B (11-1,Ti) - ln B (H2 + h, T2 ± t ) ± ln B (H2,T2) • Hence show that Your posterior mean 0, of 0 (Your estimate that a further toss of the coin will produce Heads) is VA(1 - c) ± Sc 0, = A(1 - c) ± c ' where 111 ± h \u0000 H2 ± h V = \u0000 s - Hi + h ± Ti + t' If A is very small, then the derivative dB, \u0000 A(S - V) \u0000 de \u0000 {A + c(1 - A)}2 is huge at c = 0 and becomes very small very rapidly as c moves away from 0. Use a computer to show that Your estimate of 0, takes the values shown in Table N(i) [respectively Table N(ii)] when Hi = Ti = N = 5000, H2 = T2 = M, h + t = 100, \u0000 and M = 4 [respectively, M = 6]. \u0000 ❑ Value of c h A 1 0.1000 0.0100 0.0010 0 20 0.0000001 0.2222 0.2222 0.2222 0.2222 0.4970 25 0.0000225 0.2685 0.2686 0.2690 0.2736 0.4975 30 0.0019177 0.3148 0.3179 0.3440 0.4352 0.4980 35 0.0554319 0.3611 0.4068 0.4773 0.4961 0.4985 40 0.5856032 0.4074 0.4844 0.4975 0.4989 0.4990 45 2.3684058 0.4537 0.4975 0.4993 0.4995 0.4995 50 3.7630309 0.5000 0.5000 0.5000 0.5000 0.5000 Table N(i): Posterior means 0, of 0 when M = 4 Note that the strange top rows of these Tables tally exactly with the discussion around (N3): for results with h far from 50, there is extreme sensitivity to c for c near 0. Table N(iii) gives the values a for various (h, c) pairs when M = 6. This subsection has illustrated the 'method of imaginary results' in which You look at posterior densities (or at summary statistics such as the posterior mean, as we have done) for hypothetical results, both to try to assess how strongly You 112 + h + T2 ± t • (N3) Prior value of c h 1 0.1000 0.0100 0.0010 0 20 1.0000 1.0000 1.0000 0.9999 0.0000 25 1.0000 0.9997 0.9971 0.9711 0.0000 30 1.0000 0.9814 0.8273 0.3220 0.0000 35 1.0000 0.6754 0.1591 0.0184 0.0000 40 1.0000 0.1771 0.0192 0.0019 0.0000 45 1.0000 0.0531 0.0051 0.0005 0.0000 50 1.0000 0.0346 0.0033 0.0003 0.0000 6.6. Bayesian Confidence Intervals \u0000 219 Value of c h A 1 0.1000 0.0100 0.0010 0 20 0.0000001 0.2321 0.2321 0.2321 0.2322 0.4970 25 0.0000298 0.2768 0.2768 0.2774 0.2832 0.4975 30 0.0021079 0.3214 0.3247 0.3519 0.4412 0.4980 35 0.0534058 0.3661 0.4091 0.4774 0.4961 0.4985 40 0.5161140 0.4107 0.4834 0.4973 0.4988 0.4990 45 1.9823827 0.4554 0.4972 0.4993 0.4995 0.4995 50 3.0968888 0.5000 0.5000 0.5000 0.5000 0.5000 Table N(ii): Posterior means Oc of 0 when M = 6 Table N(iii): Posterior values c of c when M = 6 feel about certain aspects of Your prior and to assess sensitivity. You can present people in the field of application with tables based on imaginary results and let their reactions guide you in Your choice of prior. In the coin-tossing example, if You do decide to use a prior of the form 217(N2), then you could look at several possibilities for the pair (c, M), but you would need to be very cautious about interpreting Your results if h did turn out to be far from 50 because of the pathological sensitivity of 0, to c near c -= 0. Our coin-tossing example is too close to the notorious Bayesian significance testing of a sharp hypothesis discussed in Subsection 2341 below. And yet it is the case that we would have very strong prior belief that 0 is very close to 2. ► 0. Philosophy of priors. The controversial questions: • Can we really measure 'degree of belief' quantitatively; and if so, is there any reason why we should be able to apply the rules of Probability to these measurements? 220 \u0000 6: Confidence Intervals for one-parameter models have had endless debate. Many Bayesians would say that logical coherence forces us to answer yes to both questions. Their betting argument for this is that a set of questions of the form 'Would You be more willing to bet on this than on that?' can determine Your prior. I am not at all persuaded by the 'betting argument', many forms of which come very close to tautology; and since I cannot present it with any conviction, I skip it. A second argument is based on exchangeability. I am not convinced by this argument either; but it is an interesting argument based on a remarkable result, de Finetti's Theorem, the subject of the next subsection. To become obsessed with defining real-world things is to miss completely the subtlety of Mathematics. We saw that it is both practically and logically impossible to define probability in terms of long-term relative frequency. In all Applied Mathematics, we build models for the real world, and we study the models, not the real world. We investigate how useful our models are as analogues of the real world by testing how well predictions made in the model transfer to the real world. Testing predictions is more difficult in Probability and Statistics than in many other branches of Applied Maths, but there is a lot of evidence that, used sensibly, Bayesian Statistics is very useful as a model for the (everyday, 'non- quantum') real world. (We know that all of the ideas we have seen so far in this book fail for quantum mechanics, but that there Quantum Probability works supremely well.) The fact that sensible Bayesian Statistics generally works is good enough for me, and cutting the discussion of philosophy there (except for a discussion of de Finetti's Theorem) is my contribution to world ecology. Enough paper has already been devoted to the topic. It would, however, be dishonest of me to pretend that after decades of controversy, all conflicts between Bayesian and Frequentist schools are now resolved. Inevitably, some conflicts feature in later sections. ► P. Exchangeability; de Finetti's Theorem. We study only the simplest case of variables 171, Y2, Y3, ... taking values in the two-point set {0, 1}. An infinite sequence Y1, Y2, Y3, ... of RVs taking values in {0, 1} is called exchangeable if whenever T is a permutation of N = {1, 2, 3, . .} which leaves all but finitely many elements fixed, then the sequences Y1, Y2, Y3, • • • and Iry(i), Y7.(2), Yr(3), ... have the same probabilistic law. This means that for every m E N, for yj E {0, 1} for 1 < k < m, and for every permutation p of {1, 2, ...,m}, P(Y1 = Yl; Y2 = Y2; • • • ; Ym = Ym) P(Yp(i) = Yi;Yp(2) = y2; - • ; Yp(m) = yam,) • A Bayesian might say, \"If I want to model coin-tossing or indeed any situation in which a Frequentist would use an `IID Bernoulli(B) model', then in My model, I must surely assume that the finite-length experiment actually performed has the structure of the 6.6. Bayesian Confidence Intervals \u0000 221 first part of an infinite exchangeable sequence Y1, Y2, Y3, . ..; it would be illogical to do otherwise.\" ■ ■ Pa. Fact: de Finetti's Theorem. Let Y1, Y2, Y3, ... be an (infinite) exchangeable sequence with values in 0,11 in the sense described above. Then, with probability 1, the limit 0 := lim Yi + Y2 + • - • + Yn n exists (in [0, 1[); and conditionally on 0 = 0, the variables Y1, Y2, Y37 . .. are HD each Bernoulli(0). Thus, for example, for m e N, for yi , y2, ... , ym E {0, 1} with sum r, 1 11(Yi = yi; • • • ; Yr = Yr) = I. O r (1 — Or-r P(CI E (10). 0 Do note however, that for de Finetti's Theorem, we must have an infinite exchangeable sequence. See Exercise Pb below. So, a Bayesian might say, \"I must assume 'infinite' exchangeability, so the Random Variable 0 must exist, 0 plays exactly the role of the Frequentist's probability of success, and the distribution of 0 is what I mean by My prior distribution.\" Whether or not de Finetti's Theorem justifies the existence of a prior distribution, it clearly does not help You decide what to take for Your prior. We shall see later that de Finetti's Theorem and the completely general Strong Law of Large Numbers follow from the same use of the Convergence Theorem for Reverse Martingales. This reasoning will make it clear why an infinite exchangeable sequence is necessary for de Finetti's Theorem. Pb. Exercise. Suppose that Y1 and Y2 are RVs with values in {0, 1}, exchangeable in that for some non-negative a, b, c with a + 2b + c = 1, P(Yi = Y2 = 0) = a, P(Y1 =1, Y2 = 0) = 11D(Y1 =0, Y2 = 1) -= b, P(Yi = Y2 = 1) = c. Why need it not be the case that there exists a measure p such that whenever yi , y2 E {0,1} with sum r, 1 P(Y1 = Yl; Y2 = Y2) = f Or (1 — 0)2-r ft(d0)? 0 You do not need to know about measures. What is the simple idea needed here? 222 \u0000 6: Confidence Intervals for one-parameter models ► Pc. Exercise. Let Yri = 1 if the ball drawn from Polya's urn (76D) just before time n is Red, 0 if black. Prove that Y1, Y2 , Y3 . . . is an exchangeable sequence. What is the distribution of O when de Finetti's Theorem is applied to this case? Q. Remark. The Bayesian theory of CIs continues at a higher level in Chapters 7 and 8. A discussion of Bayesian Hypothesis Testing features in the next section. 6.7 Hypothesis Testing — if you must In addition to discussing Hypothesis Testing, this section also includes at Subsection 236J a first discussion of Model Choice. For a long period, Frequentist Hypothesis Testing (FHT) and its associated p-values, etc, dominated Statistics; and this alone requires that we take a look at the topic. How can we not look at the F-test in Analysis of Variance?! For simple situations, FHT then went out of favour with many statisticians, for reasons I'll explain. The main point is that, for simple situations, Cis usually provide a much better way of analysing data, and computational advances mean that CIs are available in many contexts. However, FHT perhaps retains value to all as a diagnostic aid, something which might be used to decide on whether further investigation would be of value. And it might be said to retain its value in regard to decision making, though even there it is hard to see why CIs are not better. I have not the space (and perhaps not the enthusiasm) to have a chapter on Decision Theory, a topic which I agree to be of importance. See Appendix D for references. I have stressed that for simple situations, Hypothesis Testing is best replaced by the use of CIs. However, as I have earlier emphasized, for investigating interactions in complex ANOVA tables, for testing independence in contingency tables, for questions of goodness of fit, etc, Hypothesis Testing remains essential. Where some Bayesians commit in their testing of sharp hypotheses the very error for which Bayesians have long criticized Frequentist theory, I shall say so. In this context I hope to lay to rest claims that Bayesian and Frequentist views are sometimes in extreme conflict for large sample sizes. I begin by describing Frequentist Hypothesis Testing as clearly as I can. I present the full story of the Likelihood-Ratio (LR) Test first, and then try to bring it to life via examples. We shall see in Chapter 8 that the t test, x2 tests, F tests, are all special cases of the LR test. 6.7. Hypothesis Testing — if you must \u0000 223 The LR Test makes Hypothesis Testing well structured mathematically, not the shambles of ad hoc methods it used to be, though, of course, the brilliant intuition of the classical figures in Statistics had led them to the right answers without a general theory. So sensible is the LR Test that a cynic might say that statisticians have struggled hard to 'prove' that it is the best test by cooking up for each situation a criterion for 'best' which ensures that the LR test is best. A celebrated result, the Neyman—Pearson Lemma (not discussed in this book), does prove that the LR Test is unquestionably best (`most powerful') in a certain extremely simple situation of little practical importance. In other situations, the LR Test might be Uniformly Most Powerful (UMP) Unbiased, or UMP Invariant, or UMP Similar or UMP whatever. I take the attitude that if one has to test a hypothesis, then the LR Test is manifestly a good way to do it, and leave it at that ... except that I think that the form of the LR Test can sometimes be improved even within the terms of Frequentist theory. ► ► A. The LR Test: fundamental terminology. Recall that we are adopting the Frequentist viewpoint for a time. If you wish, suppose that we have very little prior information about the whereabouts of 0. Everything in this subsection is worded so that it applies without modification to the case when 0 is a multidimensional parameter or, in other language, when there is more than one parameter. One has to think of Hypothesis Testing as follows. We have a Null Hypothesis Ho which we shall only reject if there is rather strong evidence against it. We set up an Alternative Hypothesis HA which indicates what kind of departure from the Null Hypothesis we are looking out for. It is important to realize straight away that in Hypothesis Testing, the Null and Alternative Hypotheses are not placed on an equal footing: the Null Hypothesis is 'innocent unless proven guilty' in the 'Ho versus HA' trial. We behave in a way that justice is seen to be done to the Null Hypothesis: even when we reject it, we must have given it a very fair hearing. (Note. The asymmetry between Ho and HA is inappropriate in many practical situations, but ignore this fact for now.) For the moment, we consider the situation where the sample size n is somehow predetermined. The Null and Alternative Hypotheses will take the form He 0 €: Bo, \u0000 HA : 0 E BA, 224 \u0000 6: Confidence Intervals for one-parameter models where Bo and BA are disjoint subsets of the parameter space. We define mlhd (Ho; yobs) := sup lhd (0; yobs) OEBo mlhd (HA; yobs) := sup lhd (0; yobs) eeBA so that mlhd (Ho; y y consistent with Hypothesis Ho. Yes, 'maximum' should be `supremum' . We define the likelihood-ratio mlhd (HA; yobs. ir(Yths) \u0000 Ir (HA :110 yobs) := mlhd (Ho; yobs) which measures how much better an explanation of the observed data HA is than Ho. The Likelihood-Ratio Test takes the form: Reject Ho if 11.(y°1') > ic \u0000 (A2) for some suitably chosen constant k. Thus, we reject Ho if HA provides at least it times as good an explanation of the observed data. Sensible so far — yes?! (Well, actually, not always; but stick with it for now.) Now, this strategy could lead us to reject Ho when Ho is in fact true. To consider this, we need to think about the Pre-Statistic LR(Y) which crystallizes into the actual statistic 1r(y°1'). The size or significance level of the TEST is defined to be significance level of test := size = sup P(LR(Y) > nj0), \u0000 (A3) oEno the maximum probability consistent with Ho's being true that we reject Ho. To protect Ho, we insist that the size is small. Tradition has tended to set the significance level at one of the values 5% or 1%. When we have decided on the significance level of the test, we can find the appropriate constant it, as we shall see. In discrete situations, we normally cannot fix the size to be exactly what we want for the same reason that we could not fix the exact level of a CI in discrete situations. I am going to ignore this problem, and pretend that we can always fix the size to be a given value a. I therefore avoid randomization tests and the like. A related concept is the following. We define the p-value of the observed data, also called the significance level of the DATA by p-value(y'bs) := sup P( LR(Y) > \u0000 s) 10) BE-Ito (A4) ohs) is the maximum likelihood of obtaining the observed data (Al) 6.7. Hypothesis Testing — if you must \u0000 225 the maximum probability consistent with Ho's being true that evidence against Ho as least as strong as that provided by the data would occur by Chance. (A convoluted concept!) We see that we reject Ho if and only if the p-value(yths) < size. Remember that it's a SMALL p-value which represents strong evidence against Ho. Traditionally, a p-value of less than 5% [respectively, 1%, 0.1%] has been said to constitute significant [respectively, very significant, extremely significant] evidence against Ho. I stress that the size of the test and the p-value of the data are calculated on the assumption that the Null Hypothesis Ho is true. Protecting the Null Hypothesis in the way we do certainly gives the theory a lopsided character, which we must abandon when we come to study model choice. ► The power function. The p-value alone does not contain enough information. We can only achieve adequate understanding of the operation of the LR Test via consideration of the power function power(0) \u0000 P(reject Ho I 0) = P(LR > i 1 4). \u0000 (A5) We clearly want the power function to be 'small on Bo and large on BA', even though these regions will usually have a common boundary across which the power function is continuous! We note that size = eseuBp. power(0). When we plan a Hypothesis Test, we need to choose BOTH sample size and value of lc to make the test operate reasonably. We see this in the next subsection. ►► Non-rejection is not acceptance. If we do not reject Ho, then all that we are saying is that there is not strong evidence in data to reject that Ho might be a reasonable approximation to the truth. This certainly does not mean that there is strong evidence to accept Ho. I shall always use 'Reject' or 'Do not reject', never `Accept'. ► Test statistics. In practice, with likelihoods arising from the exponential family, we rarely calculate the likelihood ratio. Instead, we find a SIMPLE (sufficient) test statistic r bs such that the rejection criterion {1r(yobs) > K1 takes the form {rips > C} or {Irbs > c} for some constant c. However, there is a remarkable result that says that, under certain circumstances, for large samples, 21n LR(Y) has approximately one of the x2 226 \u0000 6: Confidence Intervals for one-parameter models distributions. We shall see how to prove the one-dimensional case in Subsection 230D. Aa. Deviance. Because of the importance of 21n LR(Y), we have a special name and notation for it: we refer to the Deviance Dev(Y) \u0000 Dev( IIA, Ho; Y) \u0000 2111 LR(Y) or deviance dev(y) = 2 1n1r(y). ► Ab. Type I and Type II errors. (You must be made aware of this terminology.) If we reject Ho when it is true, we commit a 'Type I' error. If we fail to reject 1/0 when it is false, we commit a 'Type II' error. Type I error probabilities are therefore described by the power function on B0, and Type II error probabilities by 1—(power function) on BA. The size of the test is the maximum Type I error probability. ► B. Hypothesis Tests and Confidence Regions. This linking of Hypothesis Tests and CRs will make more sense in the concrete context of the next subsection where it all works well. But here's the general story, in which we consider that the sample size n is fixed in advance. It will force us to consider modifying the LR Test (as we do in Subsection 232F) in certain circumstances. The size a test of 0 = (19 against 0 0 cp has the form Reject 0 = (p if lhd(eobs; Y°bs) \u0000 N \u0000 (B1) lhd(w; yobs) eP)' where &A), is the mle of 0 and /c(v) is the appropriate constant. The set CR of (p such that we do NOT reject 0 = (p against 0 co at size a has the form CR = { p : lhd (V); yobs) > A(c0, yobs)} \u0000 where \u0000 (B2) p, yobs) \u0000 n(co ) —1 lhd(Oobs; yobs). For any cp, P(co E CR I co) = F (lhd(co; Y) > A(v, Y) I (p) = P(we do NOT reject 0 = co I (p) = 1 — a, so that CR is a 100(1 — a)% Confidence Region for 0. This fact is often pointed out in the literature, but numerous difficulties attach to it. I mention one now, and discuss another in Subsection 232F. If NO is a constant function, in which case A Op, yobs) ) depends only on yobs, not on co, then CR at (B2) has the sensible property that if a E CR and lhdo; yobs) > lhd(a; y °135), then E CR. If Ic(•) is not constant, we have inconsistency with the intuitive idea of the LR Test. 6.7. Hypothesis Testing — if you must \u0000 227 Ba. Lemma. (a) If 0 is a location parameter so that f (y 0) = h(y — 0) for some pdf h on I1, then the function n(•) is constant. (b) If -y is a scale parameter in that f (y y) = -y-1 f (-Cly I 1), then the function KO is constant. Proof of (b). If Y has the lhd((p; -) density, then Z = co—lY has the lhd(1; -) density. But (with the middle equality 'just logic') sup lhd('y; Y) = sup lhd(y; (pZ) = sup lhd(rp; cpZ) = co' sup lhd('rp; Z) -Y \u0000 7 \u0000 -Y while lhd(cp; Y) = lhd(cp; (pZ) = (p —nlhd(1;Z). Thus, sup71hd(y; Y) > k(1)1hd(cp; Y) if and only if sup,), lhd(-y; Z) > k(1)1hd(cp; Z). Since Z has the P(• I 1) law of Y, P( sup lhdey; Y) > K(1)1hd(cp; Y) co) -Y ]P \\ sup lhd(y; Z) > ic(1)1hd(co; Z) 7 = P(sup lhd(y; Y) n(1)1hd(cp; Y) and so n(y) = n(1). \u0000 ❑ Exercise. Prove (a), which is much easier to do. C. Application to the N(0, 1) case. We look at both two-sided and one-sided tests, though CIs are better! Ca. Two-sided test. Consider testing Ho : 0 = 0 against HA : 0 0 based on an IID sample Y1, Y2, \u0000 , Yn from an N(0, 1) population. Using the fact that the MLE of 0 is Y and the Parallel-Axis result at 182Ca, we find that (you check!) Dev(Y) = 21n LR(Y) = nY2. \u0000 (C1) We shall therefore reject Ho if 1Y1 > c, where c = -V2n-1 In n, which is very sensible. Suppose that we want a power function such that size = power(0) = 5%, power(0.4) 90%, c = \u0000 i , 0.4— c am \u0000 Vn \u0000 Vn 1.96 \u0000 1.28 \u0000 3.24 0.4 T-1, 228 \u0000 6: Confidence Intervals for one-parameter models Reject Ho Do NOT reject Ho Reject Ho —0.24 5% power at 0 _....A hi..._ 0.24 90% power at 0.4 0 0.4 Power Function \u0000 90% 5% r- 0 \u0000 04 Figure C(i): Power function of a certain test so that in particular we shall be about 90% certain of rejecting Ho if the true value of 0 is 0.4. We therefore want (look at Figure C(i)) Y /.1 whence n = 66 and c = 0.24. We therefore take n = 66 and reject H0 at the 5% level if the 95% CI C1.96 — 1.96 Nrn ) does NOT contain 0. This is a sensible tie-up between CIs and HTs, as we knew it would 6.7. Hypothesis Testing — if you must \u0000 229 be because 0 is a location parameter. ► Cb. The resolution at which we examine things. The numbers 0, 0.24 and 0.4 in Figure 228C(i) are the 'actual real-world' numbers. But it is important to realize that the natural unit of length for the situation is 1/VTi (with n = 66) so that the three numbers are, as we have seen, Offri, 1.96krn and (1.96 + 1.28)/ N/Ti. Cc. Chi-squared formulae for Dev = 21n LR. Suppose again that we are working with an HD sample Yl , Y2, • • , Yn, where n need not be 66. Then if Ho is true then N(0, 1), so that, from 227(C1), if 9 = 0, then Dev(Y) = 2 hi LR(Y) istributi Recalling the point about the resolution at which we examine things, we consider power(76) where 6 = n 2 . If 0 = yb, then Dev(Y) = 21n LR(Y) = (n117)2, where nzY N(-y, 1). Hence power(-6Ft) is obtained exactly from the result \u0000 if 9 = yI \\in, then Dev(Y) = 21n I,R(Y) \u0000 Xi (1.2). The normal case is obeying exactly what (as we shall see in the next subsection) other distributions obey approximately for large sample sizes — the usual story. Cd. Warning. In the above discussion, H0 : 0 = 0 was a sharp hypothesis in which 0 is specified exactly. The problems which arise because of the absurdity of making sharp hypotheses are discussed in Subsections 232G and 2341. Ce. One-sided test. Consider the (now-sensible) question of testing Ho : 0 < 0 against HA : 0 > 0 again based on an HD sample Ili, Y2, \u0000 , Yn from an N(0,1) population. This time, 2 Dev(Y) = 21n LR(Y) \u0000 { — nr2 if 17 < 0, +nY if Y > O. Again suppose that we want a power function such that size = power(0) = 5%, power(0.4) 90%. We therefore want 1.645 \u0000 1.28 \u0000 0.4— c am \u0000 •VT/, \u0000 -\\FL so we take n =- 54 and c = 0.22. Exercise: Sketch the diagram corresponding to Figure 228C(i) for this case. c = 230 \u0000 6: Confidence Intervals for one-parameter models ► ► D. A )(2 approximation for the distribution of Deviance. Suppose that n is large and that Yi, Y2, ... , Yom, is a sample from a distribution with pdf f (y 0) where 9 is a one-dimensional parameter. Consider testing Ho : 0 = cc) against HA : 1 5 \u0000 (P. Under 1/0, we have (compare the discussion in Subsection 194D) Dev(Y) = 21nLR(Y) = 2 {.(6, Y) — t(co,Y)} —289.46,10(o — cp) + ap(o,y)(6 — co)2 = aP(6,Y)(6 — \u0000 MO(6 — 02 ,.„ \u0000 2 Here we have used the facts that aoi(6,Y) = 0, O — cp \u0000 N (0, 1 It (0) ) Summarizing then, 21nLR(Y) \u0000 xi , under III, a special case of a general result which we study later. It is interesting to note that, from the Normal Table, p515, or the A Table, p517, we have for 5% significance: 21n LR > 1.962 = 3.84, so LR > 6.82, for 1% significance: 21n LR > 2.582 = 6.64, so LR > 27.6. This gives some idea of the size of n for the LR Test. ►► Da. Power function and resolution. The non-central x2 approximation. In considering the power function, we must — as at 229Cb — examine the resolution at which we measure things. Here the natural scale of measurement is = {Ii(V)} \u0000 Inif*MY and we must consider power(co + - y8). We have 21nLR(Y) \u0000 h(co + -y6)(6 — cp)2 \u0000 h(co)(6 — yo)2 = {s —'(ö_)}2 But O — cp N (ryo, 62). Hence power(yo + - y8) is obtained approximately from the result if 0 = + ^yS then Dev(Y) = 21n LR(Y) R..-1 non-central X?(\"Y )- 6.7. Hypothesis Testing — if you must \u0000 231 ► E. Application to coin tossing. Let's discuss an example rather informally before we go on to the theory. Ea. Example. Suppose that Jane tosses her coin 100 times and obtains the result HHTH T \u0000 (El) with 54 Heads in all, while John tosses his coin 100 times and obtains the result THHT H (E2) with 61 Heads in all. Do note that in each case we are given the full result of all individual tosses, not just the total number of Heads obtained. (I know that I have not typed out all the details for you, but pretend that I have.) If we make the Null Hypothesis (a sharp one!) that Jane's coin is fair, and the Alternative that it is not, then the chance of Jane's getting the detailed result at (El) under the Null Hypothesis is exactly 2 x 2 x = 2 —loo. The best value of p for the Alternative Hypothesis for Jane's coin, p = 0.54 (corresponding to the mle), does not provide significantly better explanation than the Null Hypothesis, and the p-value of the data is 0.484 as we shall see shortly. We would certainly not reject at the 5% significance level the Null Hypothesis that Jane's coin is fair. If we make the Null Hypothesis that John's coin is fair, then the chance under the Null Hypothesis of John's getting the detailed result (E2) is again exactly 2—boo. Thus if both coins are fair, John's detailed result is no more unlikely than Jane's. However, as we shall see, the statement within the Alternative Hypothesis that John's coin has p = 0.61 does provide a significantly better explanation of John's result, and the p-value of the data is 0.035. Note that, with lots of entropy floating around, lr(yobs) \u0000 (0.61)61(0.39)39 2 ln Ir (y°bs) = 4.88 > 1.962. 2—too We would therefore reject at the 5% significance level the hypothesis that John's coin is fair. Saying whether or not we would reject the Null Hypothesis that the coin is fair is not the best way to analyse these data. Plotting the posterior pdf 7(• I yobs) for the Frequentist prior 9-1(1— 9)-1 (or the reference prior 0-12- (1 — and giving C% CIs for a range of C values would be MUCH better. Eb. The theory for the Bernoulli case. So, suppose that Y1, Y2, \u0000 , Yn are IID each Bernoulli(9). Then for testing 0 = yo against \u0000 cp, we have Dev(Y) = 21nLR(Y) = nh(Y), where h(y) = y In y + (1 — y) ln(1 y) — y In cp — (1 — y) 111(1 — (,o). 232 \u0000 6: Confidence Intervals for one-parameter models Now, 1 +ve if y > co, — co hi(Y) = \u0000 y \u0000 = 0 So, LR(Y) > ic is equivalent to Y cl [a, b] for some interval [a, b] around co. One needs a computer to carry out the test accurately. For large, or even moderately large, n, we effectively reject Ho at size a if the 100(1 — cE)% CI at 178(E1) does not contain co. Exercise. Use this to check the p-values for Jane and John. The ones listed above were calculated to 3 places on the computer. The CLT with integer correction is here spectacularly good because of the symmetry of the binomial pdf when co = 2. F. Frequentist priors and the LR Test. We have seen that Frequentist CIs are often based on Frequentist priors which are not constant. For example, the Frequentist prior for E(mean 9) or for any scale parameter is Surely, in such a case, the correct form of the LR Test for Frequentists is 49 \u0000 if = 49, —ye if y < co. suPeEPn AO ft> SlIpe,E Be r (9 I rbs) where we use the Frequentist prior. Only in this way will we get the correct tie-up between the LR Test and Frequentist CIs, and then only in cases where the n(•) function in Subsection 226B is constant. But at least, both location and scale parameters would be covered. For Bernoulli(9), where the Frequentist prior is 0-1(1 — O) —', we are in trouble if there are either no successes or no failures. The reference prior 9- 1 (1 — 0)- 1 does not share this difficulty. It really is impossible to obtain a tidy all-embracing Frequentist theory, except for asymptotic large-sample results. ► G. Discussion of Frequentist Hypothesis Testing. This discussion will be continued at later stages. Ga. Having to make decisions. One of the areas in which FHT is often still considered of value is that in which Yes/No decisions have to made. Suppose that a drug company (or a company making agricultural fertilizers) has to decide on the basis of an experiment on a proposed new treatment whether to switch from an established product to the new one, the switch entailing substantial costs. It knows how effective the established treatment is. It may well formulate this as 'testing 9new < Bola against 9new > Bold' where 9new represents the effectiveness of the new treatment. The company will however, if it is sensible, pay close attention to the power function by asking 6.7. Hypothesis Testing — if you must \u0000 233 `how much better' makes the switch worthwhile. So, to all intents and purposes we are back with the more informative CIs. I really should again mention Decision Theory, utility, loss functions, etc, at this point. I am not fond of that material, and I leave its presentation to others. See Appendix D. Gb. Sharp hypotheses. A sharp Null Hypothesis for the one-dimensional situation which we are considering states that 9 takes some definite value 9o: 9 = 9o. Now the point about a sharp Null Hypothesis is that (in every case of interest to us) it is false. (Newton's theory is immensely more precisely correct than any Null Hypothesis in Statistics; but it is wrong.) If we set a fixed size a for our Test of H0 (against the Alternative that Ho is false), then if the sample size is large enough, we shall definitely reject the false hypothesis Ho. So, why bother to take a small sample? This objection has continually been raised by Bayesians. However, sensible Frequentists know that they are only testing whether there is strong evidence that Ho is not an acceptably good approximation to the truth. (After all, as remarked earlier, Newton's theory is good enough for manned flights to the Moon and back.) The power function specifies what one means by `acceptably good'. Again, everything hinges on the resolution at which we look at things. The discussion in Subsection 227C where we chose the sample size to achieve desired power at a certain level of deviation from the Null Hypothesis shows the ideal way to do things. We have to be particularly careful not to reject an acceptable Null Hypothesis these days when vast data sets are available on computers. One also needs to be careful that one might reject an exactly true conjecture in Probability because one takes a vast simulation (affected by the inevitable approximations computer methods entail and perhaps defects in even good random-number generators) and very likely uses tests which are much too sensitive. Computer simulation once persuaded me that a result is false, but I later proved that it is true. We take up the problem of sharp hypotheses again in Subsection I and later in multidimensional situations where it is more difficult to deal with. Note. In my first draft, I deliberately omitted mentioning Absence of Extra-Sensory Perception, which seems to some of us a sharp Null Hypothesis which may be true. Then David Cox mentioned it in his comments. We do not, however, regard this as disproof of the Null Hypothesis. Gc. When Hypothesis Testing is definitely inappropriate. \u0000 In the example of Subsection 178F, there is no reason whatever to protect a Null Hypothesis that pA = pB. That is why the x2 test is totally inappropriate. Gd. When Hypothesis Testing is helpful. I have kept mentioning cases — for complex ANOVA models, for contingency tables, etc — where Hypothesis Testing serves a very useful purpose. See especially Chapter 8. 234 \u0000 6: Confidence Intervals for one-parameter models H. Bayesian Hypothesis Testing (BHT). Bayesian Hypothesis Testing is obvious. In Bayesian theory, we can speak of the (posterior) probability that a Null Hypothesis is true: it is just IED(Ho is true yobs) \u0000 f \u0000 (0 yobs)do OEB0 That's the end of the story! The difficulty for the theory is, as usual, caused by the beginning of the story: how does one choose one's prior? There are, of course, often serious difficulties in implementing the formula in practice, though to a large extent these are now overcome by MCMC methods. ►► I. Bayesian significance testing of a sharp hypothesis: a plea for sanity. As stated at 233Gb, it was Bayesians who correctly emphasized that if the sample size is large, Frequentists will inevitably reject a sharp hypothesis at the 1% level because a sharp hypothesis will not be exactly true. It astonishes me therefore that some Bayesians now assign non-zero prior probability that a sharp hypothesis is exactly true to obtain results which seem to support strongly Null Hypotheses which Frequentists would very definitely reject. (Of course, it is blindingly obvious that such results must follow.) Suppose that Y1, Y2, \u0000 , Yn is an HD sample from an N(0,1) population. We are again interested in testing Ho : 0 = 0 against HA : 0 0 O. However, now, we think like this. Let us assume that God tossed a coin with probability Pn of Heads, chose 0 = 0 (so Ho is true) if it fell Heads, and chose 0 according to the N(m, prec r) distribution (in which case HA is true (with probability 1)) if the coin fell Tails. Then using 213(L2), we find that P(Ho \u0000 = _To + IA ' where Io = p„, (- 27 \u0000 exp \u0000 k) 1 ) ln IA = — pm) f C7r ) exp(—P-02) ( Tn. \u0000 exp \u0000 ( — 9)2 } d0. 0 2 Now use the Parallel-Axis result E(yk - )3)2 = E (yk - y)2 + \u0000 - Y)2 Io 6.7. Hypothesis Testing — if you must \u0000 235 with 0 = 0 and 0 = 0 to obtain -1-A \u0000 1— pr., \u0000 r pn exp (-1nY2) \u0000 27) exP(- 1r192) exP {- 171( t — V)2} d0 Io \u0000 — \u0000 Th e \u0000 exp{ \u0000 r \u00001 Pn 2(1 + r/n) v r + You can work out the integral now if you wish, but we shall see why the calculation is correct at 251(Da) below. If we allow n to tend to infinity and keep per, fixed at p, and if y = c/ n, then for fixed c, IA \u0000 1—p exp \u0000 c2 \u0000 1 \u0000 r \u0000 constant 2(1 r / n) j V r +n \u0000 VT/ • So, in investigating what happens as n \u0000 oo, if we make what I regard as the serious mistake of keeping pn fixed, then for y = c/Vii, (which Frequentists would regard as \u0000 significance evidence against Ho if c > 1.96), we would have P(Ho I y) \u0000 1. Let me explain why I think that keeping pn fixed as n —> oo is a serious mistake. Do remember that we are considering the situation where n —> oo. We just do NOT really believe that 0 is EXACTLY 0. If we keep per, fixed at p as n —> oo, then we are saying that we have a prior probability p that 0 is EXACTLY 0; and this is plain silly. Think of the resolution at which we are looking at things, remembering the discussions at 229Cb and 230Da. Doing an experiment of sample size n is looking at things measured on the scale where the natural unit of length is 1/ AFL What we should say therefore is that Ho represents the hypothesis that 6 belongs to the little atom of natural length 1W1, centered at 0. Thus it is natural to take pn = a/ AF-1, for some a, and then for y = ck/T-t, 1 P( Ho Y) \u0000 1+ (N, \u0000 exp(c2/2)' and sanity returns, with close agreement between Bayesian and Frequentist theories. Note that a has dimension (length)-1 and that r as the inverse of variance has dimension (length)-2 whence \\Fria is dimensionless as it should be. What I am saying is that we CANNOT ignore the degree of resolution of the experiment when choosing our prior. Bayesians normally claim that one's prior must not depend on the nature of one's experiment, but many of them are willing to compromise this when it comes to reference priors. This point will be explained in Chapter 9. I hope that I have persuaded some that they must adapt their prior to resolution too. I regret having felt it necessary to take up so much of your time with a situation where there is only one sensible thing to do: Give a Confidence Interval. I am not claiming any originality here (or anywhere else in regard to Statistics). See, for example, the discussion in Cox and Hinkley [49] for similar 236 \u0000 6: Confidence Intervals for one-parameter models remarks on sharp hypotheses. Bartlett may have been the first to counter rather silly claims made by some Bayesian statisticians who on all other matters have shown great wisdom. But what about ESP, you say! I knew you would. ►► J. Frequentist Model Choice: parsimony, AIC and all that — in the simplest case. The Principle of Parsimony encourages us to choose an acceptably good model with as few parameters as possible. There are two reasons for this. Firstly, a model with more parameters leads to predictions with less bias but with higher variance, and we have to balance one against the other. Secondly, a model with more parameters tends to be more sensitive to small changes in the data. The first point will be illustrated in this subsection, and the second mentioned in Chapter 8. Because a good model will yield a high maximum likelihood for the data, it is becoming ever more common to choose a model which maximizes the value of 2 ln(maximum likelihood under model) — function(sample size, number of parameters in model). We have the usual fascinating situation for Statistics. There are no completely satisfactory answers: each of the various proposals for the 'function' has advantages and disadvantages. A popular practice is to choose a model which maximizes the Akaike Information Criterion (AIC) AIC := 2 ln(maximum likelihood under model) —2(number of parameters in model). \u0000 (J1) I am a great believer in Dynkin's advice: 'Always discuss the simplest case'. In this subsection, I discuss an almost ridiculously simple case (truly the simplest possible). I use it to illustrate the point about balancing lower bias against increased variance, and to present two arguments which help motivate the AIC criterion, one based on Decision Theory, the other (Akaike's own) on the Kullback—Leibler measure of how badly one pdf approximates another. Our simple situation. Suppose that a Random Variable Y has the N(it, 1) distribution for some p. Let Mo be the model that it = 0 and MA the 'full' model that it E R. We take the view that how good a model is depends on how well it makes predictions about a Random Variable X, independent of Y, and with the same distribution as Y. 6.7. Hypothesis Testing — if you must \u0000 237 Suppose that yobs is the observed value of Y. On the basis that Mo is true, we predict that X rs, N(0, 1), a biased prediction in that X has mean 0 rather than p. On the basis of MA, we predict that X , N(yobs ,-..t ) , which is 'unbiased' in the sense that on average yobs gives the correct value p. ► A Decision-Theory argument. Concentrate first on the observed value yobs rather than Y. The mean squared error in predicting X incurred by using the prediction p = 0 associated with Mo is E { (X — 0)2} = /12 + Var(X) = 112 + 1. The mean square error in predicting X incurred by using the prediction p associated with MA is E { pc y obs)2} = (12 yobs)2 + 1 \u0000 p2 + 1 + (yobs)2 _ zuyobs. We now average these over possible values of yobs leading to results /12 + 1 for Mo, ii2 + 1 + E (Y2) — 2ptE (Y) = (p2 + 1) + (it2 + 1 — 2/12) for MA. Yes, we could simply have used independence to say that E { (X — Y)2} = Var(X — Y) = Var(X) + Var(Y) = 2 (!). On average, therefore, using MA rather than Mo will reduce the mean square error by p2 — 1. This value (unknown to us) is the 'average amount by which MA is better than Mo' in terms of Decision Theory with mean square error as loss function. Now concentrate only on the Y experiment. Twice the maximum-likelihood of obtaining a result y under Mo is while Hence, 21n mlhd(Mo; y) = — ln(27r) — y2, 21n mlhd(MA; y) = — ln(27r) — 0. E Dev (MA, Mo; Y) := 1E2 ln mlhd(MA; Y) — E 21n mlhd(Mo; 17) E(y2) \u0000 p2 + 1, = yobs 2 more than p2 — 1. On average, therefore, the Deviance is favouring MA over Mo by 2 more than the Decision Theory suggests is appropriate. On average, to 238 \u0000 6: Confidence Intervals for one-parameter models get consistency with Decision Theory, we need to take 2, which is here twice the difference in number of parameters, from the deviance; and this is in agreement with AIC. We note that AIC suggests that if (yobs)2 < 2, we choose Model Mo; otherwise, we choose Model MA. (We ignore the case when (yobs)2 = 2.) ► Akaike's idea. Akaike arrived at what he called An Information Criterion by the following argument which again involves making predictions about X, but now the prediction of the whole pdf of X. See [2] for a more recent paper by the man himself. Let (pi, be the pdf of N(p, 1). Then, as we know from Exercise 198Fb, A139(49/20 \u0000 Wo(')) = 412, App (cpp, (.) +- coyobs .)) = (it — yobs)2. The difference between these two expressions measures how much better we bs (the estimate from MA) describes the true (PA, than does cOo (the estimate from M0). But in this simple case, this difference is exactly the same as the difference we had between the mean square errors; and the rest of the argument is now identical. We have, however, replaced the quadratic loss function, which is specially related to normal distributions, by a Kullback—Leibler loss function which applies in much more general cases. ► Ja. Exercise. Do the n-variable case where Y1, Y2, • • • ) Yr, are IID each N(p,1), X1, X2, ... X n are also IID each N(tt, 1), and independent of Y1, Y2, ... , Yn, and where the loss function for the Decision-Theory approach is E(x.,- /-1)2, whereµ is an estimate of p. Show that AIC would suggest choosing Mo as our model if 1402bs < 2. Show that if Mo is indeed correct, then the probability of accepting it as our model does not change with n, whereas we would have wished it to tend to 1 as 7/ —) 00. Jb. Discussion. Other criteria, for example that of Schwarz [209], do not show the 'lack of consistency' described in the last sentence of the above exercise. But all the familiar problems associated with sharp hypotheses surface here; and remember that, whatever the context, a lower-dimensional hypothesis will be sharp. In our simple case, Mo will not be exactly true, so (check!) if we take a large sample, Akaike's criterion will lead us to choose MA. Why then bother to take a small sample? Here we go again. To make sense of things, we have to think of alternative hypotheses of the form p = 6n — 2 , working in the natural scale for the sample size. You will not want me to go through all this again in the new context. I should not need to add that the only sensible thing to do in the situation described by our simplest case, is to give a Confidence Interval for pt. 6.7. Hypothesis Testing — if you must \u0000 239 K. It's a complex world. The point I wish to make here is that: real Statistics is not primarily about the Mathematics which underlies it: common sense and scientific judgement are more important. (But, as stated earlier, this is no excuse for not using the right Mathematics when it is available.) Suppose that it has been reported that a certain town has more sufferers from a certain disease than would be expected in a place of that population size, and it is thought that a nearby industrial site of a certain kind is the cause. One might set up a Null Hypothesis that the number of cases is Poisson(0o) where 00 is a 'national average' for a population of the given size, and test this against an Alternative that the number of cases is Poisson(0) where 0 > 00. But an enormous number of questions arise. Shouldn't we look at the total population living near this and other similar industrial sites? Should we then compare these results with 'control' regions similar in many ways, but away from such industrial sites? Were there perhaps always more cases of the disease in the particular town? Did a journalist discover this case by looking through the Atlas of Cancer Mortality, finding a 'cluster', realizing that there was an industrial site near, and sensing a story? And so on. 7 CONDITIONAL pdfs AND MULTI-PARAMETER BAYESIAN STATISTICS I have deferred the topic of joint and conditional pdfs for as long as possible. Since we are now equipped to appreciate at least some of its usefulness, its study is now much more interesting than it would have been earlier. The fundamental 'F' and T distributions of Statistics are introduced in this chapter. Bayesian Statistics is extended to cover multi-parameter situations, and we see how it may be made effective by Gibbs sampling. One of the reasons for my deferring the topic of joint pdfs is that it can look a bit complicated at first sight. It isn't really. Amongst the theory as it applies to Probability and Statistics, only the `Jacobian' Theorem 249C has any substance; and you can, if you like, take that for granted. (Gibbs sampling certainly has substance too!) What is a little offputting is the notation. Wherever possible, I simplify the appearance of things by using vector notation. Note. We defer the study of the most important joint pdf, that of the multivariate normal distribution, until Chapter 8. \u0000 ❑ We start with the simple situation for joint pmfs, which proves a valuable guide to that for joint pdfs. 7.1 Joint and conditional pmfs ► A. Definition of joint pmf. Suppose that each of X1, X2 , \u0000 Xn takes values in Z. The 'joint' pmf of X = X1, X2, \u0000 Xn, ) is the function px = Pxi,x2,...,x,, on Zn , where, for x = (xi, x2, . . . , xn) in Zn Px (x) \u0000 pxi, x, ...,x (xi i; — -; Xn Xn) • P(X \u0000 P ( 7.1. Joint and conditional pmfs \u0000 241 Really, px(•) is the pdf of X and Pxi \u0000 ...,x ( ) is the joint pdf of n • • • X1, X2, . X n. Of course the pmf pXk (•) of an individual Xk, now often called the marginal pmf of Xk, is obtained as px,(x) = E PX1,X2, ...,Xn (21, • • • f ik—i x , 2k+17 • • • 7 in), the sum being over all possible (n — 1)-tuples (ii, \u0000 , ik-17 ik+1) • • • in)• Aa. Exercise. This concerns the same situation as at Exercise 52Da. A fair coin is tossed twice, the number N of Heads noted, and then the coin is tossed N more times. Let X be the total number of Heads obtained, and Y the total number of Tails obtained. We have Table Aa for the joint pmf px,y of X and Y in units of is . Thus, for example, px,y(3, 1) = 2/16. 0 y 1 2 0 0 0 4 4 1 0 0 4 4 x \u0000 2 0 4 1 5 \u0000 px (x) 3 0 2 0 2 4 1 0 0 1 6 9 PI' (Y) Table A(i): Joint and marginal pmfs in units of 16 You should check the table. Note the appropriateness of the term 'marginal'. ► B. Independence. \u0000 Suppose that X = (X1, X2, . X n) has joint pmf px = Px1,x 2, ...,xn. Then, X1, X2, . \u0000 X n are independent if and only if PX1,X2, ...,x,,, ( x1, X21 ... , x) = PX1 (X1)PX2 (X2) • • • PXn (Xn). Ba. Exercise. Suppose that, for some functions gl , g2, . . . , gn each on Z, we have Px,,x 2,...,xn (xi, x2, ..., xn) = g1(x1)92(x2) • • • gn(xn). Prove that X1, X2, • • • , X n are independent, Xk having pmf gk•lGk, where Gk E. 9k (x)• Bb. Example. Consider an experiment which consists of two parts. First an integer N is chosen at random in Z+ with pmf pN. Then a coin with probability 0 of Heads is thrown N times, producing X Heads and Y Tails. Then, for non-negative integers x and y, P X,Y (X Y) = P N (X + Y) ( X x Y ) e x (1 — 0)Y . 242 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' [Note that if pN(•) is known, then if we wish to make inferences about 0, then N is an Ancillary Statistic, so we must condition on the value of N in the Frequentist theory. The same is true if pN(•) is not known, but we can be sure that 'the choice of N cannot influence 0'. In other words, if we obtain x Heads and y Tails, we must behave exactly as if the number of tosses had been fixed at x + y in advance.] We know from the 'Boys and Girls' Problem 100I that if N has the Poisson(A) distribution, then X and Y are independent, X having the Poisson(AO) distribution and Y the Poisson(A (1 — 0)) distribution. ►► C. Conditional pmfs. Suppose that X = (Xi, X2, . , X r ) takes values in E. and Y = (Y1, Y2, . \u0000 Y8) takes values in Zs. Then the 'joint' vector (X, Y) = (Xi, X2, . \u0000 X r , /71, 172, . \u0000 Y8) takes values in Zr+s; let px,y be its pmf. Then the conditional pmf of X given Y is the function px ly (• •) defined via px iy(x I y) \u0000 P(X x Y y) ;Y=y) P( How this is defined when py (y) = 0 in which case pxx (x, y) is also 0, does not matter here. In Exercise 241Aa, px iy (x12) takes the values t, t, e, 0, 0 for x = 0, 1, 2, 3, 4 respectively. In Example 241Bb, we used intuitively P(X = x; Y = y) = P(X -= x;Y y; N = x + y) = IP(N = x y)P(X = x; Y = yIN = x + y). Of course, P(X E A) = \u0000 P(X E A; Y = y) = E E py(y)px,y(x I y), \u0000 (C2) xEA y the y-sums being over all possible values of y. It is all very similar to what we have seen before in connection with Bayes' Theorem in Probability. D. Conditional independence for discrete Variables. \u0000 Let Xi, X2, . X n, Y be discrete RVs. \u0000 Then X1, X2, . X n are called conditionally independent given Y if PX1,X2, • • •,Xn IY \u0000 PX1 I YPX21Y • • • PXn I Y • (Cl) Y 7.2. Jacobians \u0000 243 Da. Exercise. (a) Give an example of three RVs Xi , X2 7 Y where X1 and X2 are independent but are not conditionally independent given Y. (b) Give an example based on the experiment of tossing a fair coin once(!) of three RVs X1, X2, Y where X1 and X2 are conditionally independent given Y but not independent. 7.2 Jacobians Here we prepare for Theorem 249C on the behaviour of pdfs under transformations. If you know about Jacobians, or are willing to take that theorem completely on trust, you can jump on to the next section. Meanwhile, I am going to explain the heuristic idea which underlies the role of Jacobians. A. Oriented parallelograms. Let a = (al , a2) = a l i + a2j and b be vectors in the plane R2, i and j being unit vectors along the x-axis and y-axis as usual. Let OrPar(a, b) be the oriented parallelogram which is described by a particle which moves (in a straight line) from 0 to a, then moves from a to a + b, then from a + b to b, then from b to 0. See Figure A(i). The parallelogram is oriented in that 'it has an arrow going around it' showing the 'sense' in which the particle moves. The unoriented parallelogram a + b 0 \u0000 i Figure A(i): The oriented parallelogram OrPar(a, b) which is the track of the particle is denoted by Par (a, b). 244 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' The signed area of OrPar(a, b) is +Area (Par(a, b)) if OrPar(a, b) is traversed anticlockwise (as it is in Figure A(i)), —Area (Par(a, b)) if OrPar(a, b) is traversed clockwise. An anticlockwise 'arrow' thus corresponds to positive orientation, a clockwise arrow to negative orientation. Let 0 be the angle needed to rotate the vector a anticlockwise until it lies along b, as shown in Figure 243A(i). Introduce the vector product k = i x j, so that k is a unit vector which projects out from the page towards you. You know that the vector product a x b has the expressions ) i \u0000 j k al \u0000 ( b1 b2 a2 0 \u0000 = det al az bi b2 0 a x b =(Ilall Ilbll sin o) k = det k , so that ( SignedArea (OrPar(a, b)) = det ab: a2 Note. Vector products work only in 1183 . However, there is a theory of exterior products which gives generalizations to all dimensions of such results as equation (Al). B. The Jacobian formula in two dimensions. Suppose that we have a one- one map 0 mapping a region V of I182 onto a region X of 1R2. In modern parlance, 0 is a bijection of V onto X. We assume that if (x, y) = 0(v, w), then, within V, the partial derivatives 2, 2, X. exist and are continuous. We develop a heuristic proof that, for any nice function h, ) (Al) f h(x,y) dxdy = f fv h (0(u, v)) J x y v w ) dv dw, \u0000 (B1) where we have the following formula for the Jacobian J: J x \u0000 ax ay = det \u0000 aa xv \u0000 , w a. ow and I J(*) I is the absolute value of JO as usual. In comparing definitions of Jacobians in different books, remember that the determinant of a matrix is invariant under the operation of taking transposes. Imagine splitting the region V into lots of disjoint positively oriented rectangles, the typical rectangle (see the left-hand side of Figure B(i)) having corners 1, 2, 3, 4 with coordinates 1 : (v, w); 2 : (v + 8v, w); 3 : (v + 6v, w + ow); 4 : (v, w + Sw). This is the oriented parallelogram OrPar ((by)i, (bw)j) based at (v, w) rather than at (0, 0). 4 1 \u0000 > \u0000 2 OrPar ((6v)i, (80j) M(v, w, v + 8v, w + 8w) 3 A 7.2. Jacobians \u0000 245 Figure B(i): Effect of map on small rectangle Under 0, the oriented parallelogram just described will map into a miniregion M(v, w, v + 5v, w + 5w) closely approximating OrPar(a, b) based at (x, y) = (v , w) (see the right-hand side of Figure B(i)) where a = 0(v + bv, w) — 0(v, w) \u0000 (8v) (Z2) , b = 0(v, w (5w) — 11)(v ,w) \u0000 (bw) (99: , :tyv ) Thus, SignedArea (M(v, w, v + bv, w + Sw)) \u0000 (6v) (8w) J ( x v w Obviously, Figure B(i) refers to a situation where J is negative. The integral f f h(x, y)dx dy ^s EE h(x, y)Area (M (v , w , v + 8v, w + bw)) , summed over all the little miniregions in X, EE h (0(v, w)) J (X y \u0000 by bw, v w summed over all the little rectangles in V. The good sense of result 244(B1) is now clear. 246 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' ►► C. Fact: Jacobian Theorem. Let be a injection from a subset V of Rn to a subset X of Rn, with continuous partial derivatives in that each Zz exists and is continuous on V. Then L h(x)dx f h(P(v dv, where JIV J = det ( axi ax2 \u0000 ax„, av1 axi axe \u0000 ax„ 07.'2 \u0000 av2 \u0000 81,2 • axi axe \u0000 axn avn avn \u0000 avn We have seen the heuristic reason for this result when n = 2, and we have long known it as the 'Chain Rule' when n = 1. We assume the general case. 7.3 Joint pdfs; transformations ►► A. Definition of a joint pdf. Let X = \u0000 X2, ... X n ) be an RV with values in W. Then X is called 'continuous' with a pdf fx (a joint pdf of X1, X2, . \u0000 Xn) if, for every nice (Borel) subset A of Rn, P(X e \u0000 f fx (x) dx A f f • • • f f.X1,X2, . ,Xn (X1 X2, \u0000 Xn) dX1dX2 \u0000 dxn; or, in shorthand, P(X E dx) = fx(x) dx. Then, for example, fl (X) := \u0000 • \u0000 fX1 X2 \u0000 X (X, x2, X3, • • • , Xn) dX2dX 3 . . . dxn Z 2 13 \u0000 x,t \u0000 ,•.., is a pdf for X1. Moreover X1, X2, . , Xn are independent if and only if f x (x 1) f x2 (x2) • • • f x,,.(Xn) is a joint pdf for Xi , X2,... , X. 7.3. Joint pdfs; transformations \u0000 247 Aa. Discussion. \"What's all this 'a pdf'? Why not 'the pdf ?\", you say. Well, it is just to emphasize that we have to remember that a pdf is not uniquely defined: it is only defined 'modulo sets of measure zero'. If, for example, a point with coordinates (X, Y) is chosen uniformly in the unit disc of radius 1 centred at the origin in the plane, then we would set ix,y(x,y) = { 7r -1 if x2 + y2 < 1, 0 if x 2 ± y2 > 1, but the value of fx,y on the circle x2 + y2 =1 is arbitrary. (Yes, purists, we could change fx,v on other sets of measure zero, too.) If (X, Y) is instead chosen uniformly in the unit square [0, 1] x [0, 1], then I[0 1] acts as a pdf for each of X and Y, but /(0,1) x (0,1) is a perfectly good pdf for (X, Y) and 1(0,1) x (0,1) (x, y) \u0000 /p oi (x)/[04] (y) on the boundary of the square. But X and Y are most certainly independent, and (x ,y) H 1[0 ,n(x)i[o ,1](y) = I[oo]x[0,1](x , y) is a pdf for (X, Y). Note, while we are discussing such points that if (X, Y) is chosen uniformly at random on the circle {(x, y) : x 2 + y2 = 1}, then (X, Y) is not 'continuous': it has no pdf on R2. Of course, each of X and Y has a pdf, namely 7r-1(1— x 2)A (Exercise!). [You can see why the 'continuous' terminology is silly. In this example. we would take = {(x,y) : x 2 + y2 = 1}, X(x, y) = x, Y (x,y) = y. Then (X, Y) is a continuous function on 52, so a continuous RV, but not a 'continuous' RV!] Ab. Example. Problem. Two people A and B plan to meet outside Bath Abbey. Each is prepared to wait for 10 minutes for the other. The arrival times of A and B are independent RVs, each with the uniform distribution on [0, 1], 0 signifying 12.00 noon and 1 signifying 1 pm. What is the chance that A and B meet? Solution. Let X be the arrival time for A, Y that for B. Then (X, Y) is uniformly distributed in the unit square, so the answer is the area of that portion of the unit square corresponding to 'A meets B', namely, the portion \u0000 {(x,Y): 0 < x < 0 < y < 1; \u0000 — YI < This is the shaded area in Figure A(i). The two unshaded triangles would form a square of side g, so the answer is \u0000 - ( t) 2 = a. Ac. Exercise. Two points are chosen independently on [0, 1], each according to the uniform distribution. Imagine that the interval [0, 1] is cut at these two points making three pieces. Show that the probability that these three pieces can be made to form the sides of a triangle is 1/4. Hint. Shade in the relevant region in the unit square. 248 \u0000 7: Conditional pdfs and `multi-parameter Bayes' 5 6 1 6 5 6 0 1 6 1 Figure A(i): Shaded region corresponds to 'A meets B' Ad. Exercise. \u0000 (Compare Exercise 241Ba.) Suppose that, for some functions gi , 92, \u0000 gn each on R., gi(x1)92(x2) ... ga(xa) acts as a pdf for X = \u0000 X2, ... X n ). Prove that Xi , \u0000 , X a are independent, Xk having pdf gk(•)IGk, where Gk := fxciR gk(x)dx. B. Example. Suppose that we have the situation where X and Y are independent RVs, X E(rate A) and Y E(rate it). Let T := min(X, Y), Z = max(X, Y), A = Z — T. Let us find IP(A > a). Solution. We have A > a if and only if (X, Y) belongs to one of the shaded triangular regions in Figure B(i). Looking at the top region, we see that P(A > a; Y > X) = °° r P(X E dx, Y E dy) fx=o y=a+x 00 f 00 \u0000 00 \u0000 00 =- \u0000 Ae—Aspe —µy dxdy = \u0000 dx .fie—ax \u0000 µe—µy dy x=0 iy=a-Fx \u0000 x=0 \u0000 fy=a-Fx f co \u0000 oo \u0000 .fie—µa \u0000 AC AXC A(a+X) dX = Ae -ila \u0000 e -(A+A)s dX = \u0000 Jx=o \u0000 x---0 \u0000 A + /..t Combining this with the corresponding formula for the lower triangle, we have Ae—Na P(A > a) = + 7.3. Joint pdfs; transformations \u0000 249 a ■• • Figure B(i): The region {A > a} Ba. Exercise. A man and woman go into two next-door shops at the same time. The times they spend in the shops are independent and are exponentially distributed with MEANS 5 minutes for the man, 10 minutes for the more discerning woman. (a) How many minutes on average until the first emerges from a shop? (b) What is the probability that the man emerges from his shop first? (c) Given that the man emerges from his shop first, how long on average does he have to wait for the woman? (d) How long on average does the first have to wait for the second? (e) Is the time which the first has to wait for the second independent of the time when the first emerges from a shop? 11411111MMOMMIErr C. Theorem: effect of transformation on pdfs. Suppose that X is a `continuous' ir -valued Random Variable with values in some (Borel) subregion X of Jr and with a pdf fx (zero off X). (Of course, X may well be the whole of Rn.) Suppose that ti) is a bijection from a (Bore!) subset V of onto X with continuous partial derivatives within V. If we write X \u0000 tk.(V), so V \u0000 0-1(X), then fv(v) \u0000 (0(v)) is a pdf for V on V. `Conversely', if V has pdf fv. on V and we put X tp(v for X ort X, is a pdf 250 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' Proof. Suppose that V0 C V (V0 Borel), and write X0 =- O(V0)• Then, using Fact 246C, P(V E Vo) = P(X E X0) = fxo fx (x) dx = f fx(0(v)) J ( 3vc \u0000 dv; and this clinches the direct part of the theorem. The 'converse' part follows similarly. ❑ Ca. Example. Let X,Y be IID, each E(1). We wish to find the joint distribution of (V, W), where V = X/(X + Y) and W = X + Y. (We already know that the marginal distribution of W is Ganuna(2,1).) First, F(X = 0) = P(Y = 0) = 0, so we can take X = (0, 00)2. We take V = (0,1) x (0, oo) and (since we want v = x/(x + y) and w = x + y) we take 0(v, w) = (x, y), where x = vw, y = w(1 — v). That is a bijection from V to X is now obvious. We have ax ay det ( g pi = ( v 1- v = w› 0, aw Ow ) \u0000 W —W so IJ•I = w. Thus, since fx,y(x, y) = e —(x+Y) on X and x + y = w, we have as a pdf for (V, W): fv,w (v , w) = we'I( 0,00)(w)I(0,1)(v). Thus, V and W are independent, W Gamma(2, 1), V — U(0,1). ►► Cb. Exercise: Relation between Gamma and Beta variables. Generalize the above Example to show that if X Gamma(K, 1), Y Gamma(L, 1), and X and Y are independent, and if we put V X + Y W=X+Y, then V — Beta(K, L), W Gamma(K + L, 1), and V and W are independent. This gives a probabilistic interpretation to the proof of the relation between Gamma and Beta functions given at 209Ia. It also gives a method for simulating Beta RVs. ► Cc. Exercise: 'Polar' method of simulating N(0,1) variables. Suppose that X and Y are IID RVs, each with the N(0, 1) distribution. 7.3. Joint pdfs; transformations \u0000 251 The probability that (X, Y) \u0000 (0, 0) is zero, and we ignore this possibility. So, take X = R2 \\ {(0, 0)}, the plane with the origin removed. Take V = (0, co) x [0, 2g), and write x = N5 cos w, y = Nru sin w, so v = x 2 + y2, w = arctan(y/x). Prove that Show that if then x y 1 . 1 w \u0000 2 X = -‘1V- cos W, Y = ✓-17 sin W, V and W are independent, V ti E(mean 2), W U[0, 27r). Hence, if U1 and U2 are IID each U[0, 1], and we set := V21n(1/U1) cos(27rU2), rl := V21n(1/U1) sin(27rU2), then and n are IID each N(0, 1). This is a widely used method of simulating normal variables. That it has led to difficulties in the past is explained in Ripley's book. ► ► D. Pdf of a sum. Let X and Y be RVs with joint pdf fx,y. Let V = X and W = X Y. Then x = v, y = w — v, and x y \u0000 ( 1 —1 = det v w \u0000 0 1 so that fX,X+y (V, w) = fX,Y(v, W — v), \u0000 fx+Y(w) = f fx,y(v,w — v)dv. In particular, if X and Y are independent, then we have the convolution formula: fx y(w) = (fx fy)(w) := I fx( )fy(2ta — v)dv, \u0000 (D1) fx * fy being the so-called 'convolution' of fx and fy. Da. Exercise. We know from our study of MGFs that if X and Y are independent and X N(p1, \u0000 Y N(p,2, o) then X +Y N(pi ±,u2, oi+o-3). Combine this with equation (D1) to evaluate the integral at 235(11). E. Pdf of a ratio. Let X and Y be RVs with joint pdf fx,y. Let V = X and W = X/Y. Then x = v, y = v/w, and (Check!) I JI = Iv/w21. Hence, \u0000 I \u0000 V \u0000 V \u0000 w2 \u0000 fw(w) = f fx,y (v,— w)1-- w2d dv. v fx,x/y(v,w) = fx,y (v,—) = 1, 252 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' Ea. Exercise. Check that if X and Y are IID each E(1), then X IY has pdf (1 + w)-2 (0 < w < oo). Note that E(X) = E(Y) = 1, E(X/Y) -= oo. Do remember that if X and Y are independent, then \u0000 1 \u0000 E (X) E (—X ) = E(X)E (—) , NOT E (v.X ) E (Y) ►► F. The Fisher \u0000 distribution. The F distributions of this subsection and the t distributions of the next are amongst the most fundamental distributions for Statistics. Suppose that X — xr2, Y \"' \u0000 X and Y are independent. Then the \u0000 distribution (with r degrees of freedom in the numerator, and s degrees of freedom in the denominator) is the distribution of X/r sX \u0000 W := \u0000 = — Y s \u0000 rY We use the mnemonic (aid to memory) Xir F,,, \u0000 , Mum, Den indepen Xsis Now, 1X Gamma(ir, 1), 2 Y — Gamma(2 s, 1), so that :W + 1 \u0000 X +Y \u0000 Beta(ls, 1r). Thus the computer can calculate the Fr,., distribution function from that of a Beta distribution; and that's the way the F-table, p518, was done. We have (with beta as the pdf of the Beta distribution and B as the Beta function) r fWMThsr to 2 beta Os, 1r; 1 S (1 \u0000 J-M5W (rls)rwr -1 B(0,10 (1+ rw)Er+s) • s Since X2 — E(mean 2), it follows from Exercise Ea that the F2,2 distribution has pdf (1 + w)-2. This checks out with the above formula. Note that if W F2 ,2, then P(W > 19) = 1/20 exactly. This tallies with the table on p518. 1 (F1) 7.3. Joint pdfs; transformations \u0000 253 ►► G. Student's tv distribution. (`Student' was the pseudonym of W S Gosset, who, as an employee of the Guinness brewery, had to publish under a pseudonym.) Suppose that X N(0, 1), Y x,2, and X and Y are independent. Then Student's tv distribution with v degrees of freedom is the distribution of T := X \u0000 N(0, 1) t VY/v \u0000 1/X12)/v NUM, Den independent. \u0000 (G1) Now, T is clearly symmetric about 0, so fr(—t) = f T (t). Also, since X2 A and is independent of Y, we have S := T 2 — F(1, v). This allows us to calculate the t-table on page 516. With the first 2 in (G2) taking into account the fact that T is symmetric and (—t)2 = t2, we have for t > 0, 1 \u0000 t2 \u0000 101+1) fT(t) = 2.2tfs(t2) = \u0000 1 —I \u0000 (G2) \u0000 v2 \u0000 U ) \u0000 v This also holds, of course, for t < 0. In particular, the t1 distribution is the standard Cauchy distribution with density 7r-1 (1 + t2)-1. As v \u0000 co, the distribution of Y/v concentrates heavily about 1 (by the Strong or Weak Law), so the too distribution, the limit of t,, as v -+ co, is the N(0, 1) distribution. Hence, ti = Cauchy, \u0000 t = N(0,1). Ga. Exercise. Show that if v > 3, T t,,, and F Fro, (r E N), then \u0000 E (T) = 0, Var(T) = \u0000 v 2' E(F) v v Hint. Your calculations should include E (T 2) = E (X 2) E ( yv , where Y ti Gammaav, rate \u0000 and you should employ the 'Change pdf' trick, considering the Gamma(z v — 1, rate 1) density. \u0000 ► Gb. StandT, (v > 3), Standardized tu. \u0000 We define the standardized t,, distribution StandT, (v > 3) to be the distribution of {(v — 2)/v}1Tv where t v. Then the StandTv distribution has mean 0 and variance 1. Figure G(i) plots the pdf of StandT4 in black with the pdf of N(0, 1) in grey. Gc. Importance in modelling. The t,, densities for finite v tail off much more slowly than the Normal, and are important in modelling the many situations where the very fast Normal tail off is unrealistic. 254 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' -4 \u0000 -3 \u0000 -2 \u0000 0 \u0000 2 \u0000 3 \u0000 4 Figure G(i): Pdfs of StandT4 (black) and N(0, 1) (grey) ► Gd. Simulating from t distributions. It is important to be able to simulate from t distributions for the modelling reasons described above. The Kinderman–Monahan method is as follows. Let U U[0, 1]. Define X – { 1/(4U – 1) if U < 4U – 3 if U > 2. Then (check!) X has pdf min(1, x-2 ). By the rejection-sampling idea, we therefore accept X with probability –1 (1 + — X 2 )2 max(1, X 2). But, of course, we have to check that this expression does not exceed 1. I skip this. The above idea is incorporated in the SimGamT programs in the next subsection but 7.3. Joint pdfs; transformations \u0000 255 one. A 'speeding-up' trick (due to K&M) utilizes the idea that X-2 \u0000 2 , 1— 11X1 < (1+ — which allows quick acceptance in a good proportion of cases. Test of KMStudT() at various quantiles of t(4) distribution for run of length 100000: exact \u0000 0.50000 0.70000 0.90000 0.95000 0.99000 0.99500 empirical 0.49985 0.70141 0.90090 0.95052 0.99026 0.99493 Thus if P(t4 < x) = 0.7, then 70141 of the simulated values were less than or equal to x. ► Ge. Exercise. Show from 253(G1) that we could also simulate a Y-value from the distribution as follows. Choose R Gamma(2v, rate 2 v), and then Y N(0, prec R). Note that we can simulate a Y-value from the StandT, distribution via R Gamma(2 v, rate (2 v — 1)), and then Y ti N(0, prec R). ► H. Ratio method of simulation (Kinderman—Monahan). A very useful method of simulation is based on the following lemma. Ha. Lemma. Suppose that h is a non-negative function with finite integral H over Et. Let \u0000 A := {(x, y) : 0 < x < h(y I \u0000x)} . Suppose that (X, Y) is chosen uniformly over A, so that .fx,r (x, y) = IA (x, y) acts as a pdf for (X, Y). Then V := Y I X has pdf 11-1h(v). Use. Suppose that A C B where B is a region from which it is easy to pick a point uniformly at random. We obtain points uniformly within A by selecting those chosen independently and uniformly within B which lie in A. Proof Let u = x, v = y/x, so that x = u, y = uv. Then, by the Jacobian result, uv) fu,v (u, v) = /A (u, \u0000 u, Area(A) Area(A) 256 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' and fv(v) = Area(A)-1 \u0000 udu = 2Area(A)-1h(v). jr,Vmo u=o But 1 = f fv(v)dv = -1-Area(A)-1H, so the desired result follows. \u0000 ❑ ► ► I. Simulation of Gamma and t variables. Here now are the programs I use to simulate t and Gamma variables. The KMSTudT part follows the K-M method of 254Gd. The AD parts relate to the Ahrens-Dieter algorithm from Subsection 150F. After stating the programs, I explain the relation of the CF parts (the Cheng-Feast algorithm) to the above Lemma. Here's a header file in which the 'set' and 'prepare' bits assign values to static variables: /* SimGamT.h \u0000 DW and William Browne for simulation of Gamma(K, rate alpha) and t(nuT) variables */ #if defined SimGamT_h #else #define SimGamTh void setKalpha(double KK, double aa); void ADprepare(); void CFprepare(); double ADgam(); double CFgam(); double rGrate(); /* for doing many simulations with the same (K,alpha); */ double rnewGrate(double KK, double aa); /* for dealing with a different (K,alpha) each time */ void prepKM_T(int nul); double KMStudT(); #endif And here's the program file, partly based on a program of Bill Browne's: /* SimGamT.c for simulation of Gamma(K, rate alpha) and t(nuT) variables cc -c SimGamT.c RNG.o -o SimGamT.o -lm \u0000 to compile */ #include <stdio.h> #include <math.h> #include \"RNG.h\" #include \"SimGamT.h\" static int nuT; static double K,alpha,a,b,c,d,sqrtK; void setKalpha(double KK, double aa){ K = KK; alpha = aa;} 7.3. Joint pdfs; transformations \u0000 257 void ADprepare(){a. = E/(K+E); b = 1/a;} void CFprepare(){a. = K-1; b = (K - 1/(6*K))/a; c = 2/a; d = c+2; sqrtK = sqrt(K); } double ADgam(){ int accept = 0; \u0000 double X,U; do{U = Unif(); if (U>a){ X = - log(b*(1-U)/K); if (Unif() < pow(X, K-1)) accept = 1; } else{X = pow(b*U,l/K); if (Unif() < exp(-X)) accept = 1;} }while (accept == 0); return X; } double CFgam(){ int done; double U,U1,U2,W; do{ if (K>2.5){ do{Ui = Unif(); U2 = Unif(); U = U1 + (1 - 1.86*U2)/sqrtK; }while ((U >= 1)II(U<=0)); } else{U1 = Unif(); U = Unif();} W = b*U1/U; done = 1; if (c*U + W + 1/W > d){ if (c*log(U) - log(W) + W > 1) done = 0; }while (done == 0); return a*W; } double rGrate(){ if (K==1) return -log(Unif())/alpha; if (K>1) return CFgam()/alpha; else return ADgam()/alpha; } double rnewGrate(double KK, double aa){ setKalpha(KK,aa); if (K==1) return -log(Unif())/alpha; if (K>1) {CFprepare(); return CFgam()/alpha;I else \u0000 {ADprepare(); return ADgam()/alpha;} } void prepKM_T(int nul){ nuT = nul; double KMStudT(){ int done = 0; double U, U1, V, X; do{ U = Unif(); Ul = Unif(); if (U < 0.5) {X = 1.0/(4.0*U - 1.0); V = U1/(X*X);} else {X = 4.0*U - 3.0; V = U1;} if (V < 1.0 - 0.5* fabs(X)) done = 1; else if (V < pow((1.0 + X*X/nuT), - 0.5*(nuT + 1.0))) done = 1; }while (done == 0); return X; 258 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' The 'if K>2 . 5 ...' part of the Cheng—Feast algorithm cleverly trims down the 'B' region. See Ripley [196] for pictures. If V = aW, then c In U — ln W + W — 1 < 0 is equivalent to the Kinderman—Monahan condition U < -VVK —le — vfunction(K). For any R > 0, ln r < r — 1, so cln U + ln(l/W) + W — 1 < cU — d + W + 1/W. Hence if the quickly-checked first condition within the last 'if' in CFgam fails to hold, V = aW is returned. 7.4 Conditional pdfs ► ► A. Definition. Let X = (Xi, X2, . X T) be an P.r -valued RV and Y = (171, Y2, • • • , Ys) an R8 -valued RV such that (X, Y) is a 'continuous' RI-Fs-valued RV with pdf fx,y. Intuitively, fx(x)dx = IP(X E dx), the probability that X will lie in a little volume dx around x. In analogy with the discrete case 242(C1), we want fx ly(x I y),dx P(X E (Lx1Y E dy) IP(X E dx; Y E dy) \u0000 fx,y(x, y) dxdy P(Y E dy) \u0000 fy (3)4 so that we define the conditional pdf f x ly (.1.) of X given Y as fXIY I Y) \u0000 fx Y (x) fY (Y) (Al) (We need only bother about situations where fy (y) > 0.) Then the heuristic idea that for a nice (Borel) subset A of P(X E \u0000 E dy) = f fxiy (x 3,r) dx, f = f P( X EA I YE dy)P(Y E dy) 3rEB L'EB ✓ cEA fxiy (X y) dx fy (y) dy fyEB IcEA fx,y(x, y) dxdy (correct!). 7.4. Conditional pdfs \u0000 259 tallies in the following calculation (where B is a nice subset of 1118 ): P(X E A; Y E B) = f IP( C C A; Y c dy) yEs For real-valued RVs X and Y for which (X, Y) is 'continuous', P(X E A I Y E dy) = \u0000 P(X EAIYE (y \u0000 , \u0000 (A2) except perhaps for boundary effects as discussed at 247Aa. Note Bayes' formula: for fixed y, as functions of x, fxiy(x I Y) = fY (Y) fx(x)fyix(Y I x) cx fx(x)fyix(Y ix), \u0000 (A3) always with a proviso that fx ly(x I y) and fx,y(x, y) are only defined modulo (x, y) subsets of measure zero. Aa. Convention. Unless there is some point to be made, I am not going to continue mentioning this 'modulo sets of measure zero' business: henceforth, it is understood. We shall therefore blur the distinction between 'a pdf' and 'the pdf'. Very often, there is an obvious smooth canonical choice of fxiy (x 1 y) away from boundaries and given by (A2). Ab. Exercise. Suppose that X and Y are independent, X E(rate A), Y E(rate p), where A 0 p. Let V = X and W = X + Y. Prove that (p — A)e(A-A)v fviw(v I w) = e(p-A)w ___ 1 'Awl (v)• Note (and discuss) the contrast between the cases p > A andµ < A. Continuity correctly suggests that if A = p, then fviw (v 1 w) = w-1/[0,,„] (v). Ac. Exercise. Suppose that M and E are independent RVs, M N(0, a 2) , E N(0, 1). Let V = M, W = M e. Prove that fv iw (•Iw) is the pdf of the N(/w, 8) distribution, where 02 = a2/(a2 + 1). We write Shorthand notation: (V I W) \u0000 N(; W,(3) \u0000 (A4) 260 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' to summarize this. Of course, (W I V) \u0000 N(V, 1). The full multivariate-normal generalization of these results will feature later. Explain why this exercise could have been deduced from the table 211 J(i) of conjugate distributions. Ad. Exercise — Warning! Consider the following modification of the problem in Example 247Ab. Two people A and B plan to meet outside Bath Abbey. Each is prepared to wait for 10 minutes for the other. The arrival times X and Y of A and B are independent RVs, each with the uniform distribution on [0, 1], 0 signifying 12.00 noon and 1 signifying 1 pm. What is the chance that A and B meet given that A arrives before B? You must correct the following wrong argument. Wrong argument. If A arrives before B, then the pdf of X given Y is y-1 /[0,y]. So, if we know that Y E dy, then the chance that A and B meet is 1 if y < 1/6, (6y)' if y > 1/6. Hence, since Y is uniform on [0, 1], the answer is 1/6 1 1 dy + f (6y) — \u0000 = + In 6. Wrong! 1/6 Note. The answer has to be wrong because the areas of triangles with rational coordinates cannot possibly involve the transcendental number In 6. You know that the correct answer is 11/36. Your task now is to make the above idea into a correct argument. ► B. (Further) Need for care with conditioning. Suppose that (X, Y) is a `continuous' RV in R2. It is usual to write P(X E dx Y = y) = /x i y (xly)dx, \u0000 (B1) so that fx,y(• I y) is read as the conditional pdf of X given that Y = y. But though this notation is sound (modulo sets of measure zero), one needs to handle it with a degree of care. The better notation fx iy(x I y)dx = P(X E dx I Y E dy) correctly suggests the 'limit' interpretation at 259(A2). As an event, 'Y = y', namely, {w : Y (w) = y}, has probability zero; and we cannot condition on an event of probability zero. If E and F are events, then lP(E I F) = IP(F n E)/IP(F) makes little sense if JP(F) = 0, in which case JP(F fl E) = 0 also. The potential for creating 'paradoxes' by working out 0/0 in various ways clearly exists. Suppose that (X, Y) is chosen uniformly at random within the quarter-disc Q := {(x, y) : x > 0, y > 0, x2 + y2 < 1}, so that (X, Y) has pdf 4/7r on Q. Let V = X, W = Y — X. Then J = 1 and V, W has pdf 4/7r on the region {(v, : 0 < v < —v < w < \u0000 + -V1 — v2} . 7.4. Conditional pdfs \u0000 261 Figure B(i): Q n {-6 < Y — X < 6} and Q n {i - 6 < (Y I X) < 1 + 6} The canonical conditional pdf of V given W when W = 0 is -■ /(01/.v) ; and this is the pdf of X given that Y — X = 0. However, if we put S = X and T = Y/X, then (check via pdfs!) the canonical choice of pdf of S given T when T = 1 is 4s on (0, 1/A; and 4x is the pdf of X given that Y/X = 1. Thus the pdf of X given that Y — X = 0 is different from the pdf of X given that Y/X = 1. Figure B(i) shows the regions Q n { — 6 < Y — X < 6} and Q n fi - 6 < (Y I X) < 1+ 81. We see that as S J. 0, the pdf of X given that — S < Y — X < S will tend to a constant, and that the pdf of X given that 1 — S < (Y/X) < 1 + S will tend to a constant multiple of x (and since we know that in each case the range of X is (0, 1/A, the constants take care of themselves). What emerges then is that it is meaningless to speak of IP(X E dx I X = Y) because we rightly get different answers according to whether we interpret it as IP(X E dx IY — X = 0) or P(X E dx I YIX = 1) or ... . Ba. Repeated Warning. Bayesian theory with improper priors uses 'conditional probabilities' where the underlying measure is not finite. This can cause paradoxes, and has led me into serious error on at least one occasion. ► C. Sufficient Statistics, 2. Please re-read Subsections 183D and 185F. As usual, we assume that Y1, Y2, ... , Y. are IID, each with pmf/pdf f (Y I 9), where 9 is a vector-valued parameter. The usual definition of a (vector-valued) Sufficient Statistic T for 9 in Frequentist theory is that fyi T (y I t, 0) does not involve 0; 262 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' in other words, this conditional pdf is the same for all 0. There are technical difficulties here in the 'continuous' case, because T is a function T = r(Y) of Y, and fyI T(y I t, 0) is not a pdf on IR' because it is concentrated on the submanifold {y : r(Y) = t}. Example 185Fa illustrates this. So, fyiT (- I t, 0) is what is called a Schwartz distribution. If T = Y, for example, then fyiT (• I t, 0) is the Dirac delta function concentrated at t. Still, the intuitive idea that fY (Y1 0) -= .h(t I 0).TYIT(Y I t, 0 ) = .fT(t I 0).TYIT(y I t) can be interpreted sensibly, and it obviously corresponds closely to the factorization criterion (see 183Da) that fy(Y 10) = g(t, 0)h(Y)• Formal proof of the analogue of Fact 184Db has to be skipped. Ca. Exercise. Suppose that pc, v are unknown parameters, and that Y1,12, \u0000 , Y7, are IID each N(//, v). Show that the pair T =- (Y, R2), where R2 = \u0000 — Y)2 = \u0000 - nY2 is sufficient for the pair (it, v). Important Note. After Zeus chooses the value t°1's of T in a way which we shall understand fully in Chapter 8, Tyche chooses yobs uniformly on the (n — 2)- dimensional sphere in which the hyperplane through yobsi perpendicular to 1 cuts the (n — 1)-dimensional sphere {y E J1 : E(yk - Yobs)2 = robs}. Wait for 308Bd where this is discussed again. ► D. Minimal sufficiency. Continue with the general situation of the preceding subsection. Recall that the crucial point about a Sufficient Statistic T is that it contains all information in the sample Y relevant to making inferences about 0. We clearly want T to be of as low a dimension as possible. Taking T = Y does not achieve much, though it is the best we can do in the generic case (that is for a case without some special structure). A Sufficient Statistic T is called Minimal Sufficient if it is a function of every other Sufficient Statistic. When I say that T is a function b of U (say), I mean of course that T(co) = b(U(w)) for every w. Minimal Sufficient Statistics S and T are therefore functions of each other: in this sense, a Minimal Sufficient Statistic is essentially unique. The 1-dimensional Sufficient Statistics at 184E are clearly Minimal Sufficient, and it is 'obvious' that the pair (Y, R) in Exercise Ca is Minimal Sufficient. 7.4. Conditional pdfs \u0000 263 ► E. Ancillary Statistics. Suppose that Y is a vector of observations and that (T, A) is a Minimal Sufficient Statistic for the parameter 0. Suppose further that the distribution of A does not involve 0, and that A is maximal in this regard in that any function of (T, A) the distribution of which does not involve 6, is a function of A. Please note that such a pair (T, A) may not exist. If the pair does exist, we call A an Ancillary Statistic for 0 for Y. Assume that a pair (T, A) with the above properties exists. Then, fY(Y1 0) = fir,A(t, a I e)h(Y I t, a, 0) = fA(a 0)/TIA(t I a, 0)h(Y I t, a) = fA(a),frIA(t I a, O)fY(Y I t, a). We are using the last density heuristically: it will generally be restricted to a submanifold of Ir. So, we can envisage the following situation in which Zeus knows the value of 6, but Tyche does not. Stage 1: Tyche chooses the actual value ebs of A with pdf fA (a) without knowing 6. Stage 2: Zeus then chooses the actual value Vbs of T according to the pdf fTIA(t I ebs, 0), and tells Tyche the chosen value. Stage 3: Finally, Tyche chooses yobs according to the density fy(y I t°19s, acths) which does not involve 0. The only stage at which 0 was involved was in Zeus's choice of e bs given the value ebs of A. Hence we should base our inference on that stage, that is on the function t H fTIA (t I aobs, 0). ► Ea. Exercise*. This was discussed at l 87Hb as an illustration of the general rule for location parameters (for derivation of which, see Eb below). We now give a more careful study of this case and its special features, but arrive at the same answer. Suppose that Y1 , Y2, \u0000 , Yn are HD, each with the U[9 — \u0000 + I] distribution. Let T = min yk and A = max Yk — mM Yk. Show that (T, A) satisfies the conditions described above. Show that fA(a) = n(n — 1)e-2 (1 — a)/[01](a). Explain exactly how Zeus and Tyche perform Stages 2 and 3 for this example. For calculating Confidence Intervals, we may regard 0 as having the uniform U [max Yk — min Yk fl distribution. 264 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' ► Eb. Ancillary Statistics and location parameters. \u0000 Suppose that B is a location parameter, so that f (y 0) = h(y — 0). Assume that f is 'generic', so that Y is Minimal Sufficient for 6 for Y. Recall Order-Statistic notation (Subsection 107L) and let A = (U2, U3, . , \u0000 ) where Uk \u0000 Y(k) \u0000 —1)) \u0000 (El) and T := Ul \u0000 Y(1). Clearly, fY(1),...,Y(n) (Y(1), • • • Y(n)) = n! .f (Y(1) I 0) \u0000 . f (y(n) 19) iy(i ) <---<y(„.) • So, since y(k) = u1 + • + uk and J = 1, (1/1, 1/21 •- \u0000 Un) = nlf (ui I 0) f (fli + u21 0) • • • f (ui \u0000 u2 + • • + un I 0) I{.k>o, k>2}• Now, the integral of the right-hand side relative to u1 is n! f h(ui — 0)h(ui + u2 —19) h(ui + • • + un — 0)dui ui = n! f h(x)h(x u 2) . h(x u 2 \u0000 • un)dx xER =: i(U2, \u0000 U m), which is independent of 0 — the whole point about Ancillary Statistics! If we condition on U2, \u0000 , Un, then we make our inferences about 0 depend only on fu11 u2,...,u„ (ui I u2, \u0000 , un) = n!/(u2, \u0000 , un)-11hd(0; y), as claimed in Subsection 186Ha. Tyche produces a sequence with the same probabilistic law as that at (El) without knowing the value of 0 (but, of course, knowing the function h). Given Tyche's results, Zeus can choose the value y`(1) of Y(1) to be observed (or else, for example, the value gobs of Y to be observed) according to the conditional pdf of Y( i) (or of Y) given the actual sequence `.\" \u0000 Y(2) \u0000 Y(1) Y( 3) \u0000 Y(2) \" \u0000 Y(n) \u0000 Y(n -1 )' .,obs „,obs \u0000 „,obs „,obs \u0000 „,obs \u0000 „,obs \u0000 (E2) produced by Tyche; and then the entire actual sequence y(1.7, y(47 , . y''7113 of order statistics is determined. Utilizing a random permutation of these, Tyche finally produces the actual set y?bs , Abs, ynobs of observations. We therefore should condition on the sequence at (E2) in producing a CI for 0. The random permutation employed by Tyche can never be known to us. ► Ec. Important comment. \u0000 Of course, when the pdf f (y 10) is of a special structure which supports lower-dimensional Sufficient Statistics, we can approximate this pdf arbitrarily closely by a 'generic' one. So, if any kind of continuity applies, it cannot matter what the Minimal Sufficient Statistic is for a 7.5. Multi-parameter Bayesian Statistics \u0000 265 location-parameter model: we must get the right answer from the formula for the generic case. Note that in all contexts, not just location-parameter ones, this point gives Bayesian Statistics a great advantage over Frequentist in that Bayesian theory handles every distribution the same way, not being concerned with fragile properties such as possessing a sufficient statistic, properties lost under more-or- less any small perturbation of the model. People would wish me to remind you again at this point of the 'problem of priors', always the difficulty with the Bayesian approach. ► F. Conditional independence for 'continuous' Variables. Suppose that (X1, X2, ... \u0000 Y2, ... Ys) is a 'continuous' WI' -valued RV. We say that X1, X2, . \u0000 Xr are conditionally independent given Y if f X1,X2,.••,Xrly(x1, x2, . \u0000 , xr I 3') = f \u0000 (X1 I Y)fX21Y (X2 I y) • • • fxrly(x, I 3)• Fa. Exercise. \u0000 We take Y to be 1-dimensional for simplicity. Show that we can easily recognize conditionally independence because it arises if and only if we have a factorization of type fx1,x2,...,x,,y (xi, x2, \u0000 , xr I Y) = \u0000 (xi, Y)92 (x2, y) • .. gr (xr, y)h(Y) for some functions g1 , g2, \u0000 , gr , h. (I do know that the h can be absorbed.) Show that then , fx,,IY(xk Y) = gk(xk,Y) Gk (Y) where Gk(y) = f gk (X k Y)dX k and fY(Y) = G1(Y)G2(Y) • • • Gr(Y)h(Y)• 7.5 Multi-parameter Bayesian Statistics From one point of view, theoretical Bayesian Statistics still consists of just the one formula, even in the multi-parameter context. I shall explain Gibbs sampling as one important technique amongst a number of brilliant Markov Chain Monte Carlo (MCMC) techniques which may be used to put THE formula into practice, allowing us to estimate the posterior distribution of a multi-dimensional parameter 0. I shall also describe the impressive BUGS program which gives easy access to the use of such techniques. This program allows one to study models of almost limitless complexity, something I find as disturbing as it is exciting. But there are new factors to be borne in mind when dealing with multi-parameter situations; and these alert one to dangers of misuse of programs. 266 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' ►► A. THE formula, again. \u0000 The situation is this. We have Pre-Statistics 2, \u0000 Yr, which in the Frequentist approach are IID RVs each with pdf f (. 10), where 0 = (01, 02, . . . , 0,) is a multi-dimensional parameter. (For example, Y1, ,112 might be IID each N(ii, prec p) and 0 = (p, p).) In the Bayesian approach, we consider the parameter as being an RV O with prior pdf 7(0), and use the model that, conditionally on the value 0 of 0, Y1, Y2, • • • , Yn are IID each with pdf f (y 0), so that fy1e (y10) = lhd(0; y) := H f (yk 1 0 ). Then THE formula, a special case of 259(A3) asserts that 7(0 I y) cx 740) lhd (0; y). \u0000 (THE formula!) \u0000 (Al) But now, we understand that ir(0 1Y) \u0000 fe1Y(0 IY)- All this was discussed in Subsection 200A. The difficulty in implementing this idea is that the 'constant of proportionality' (actually, a function of y) involved in (Al) is 1/K(y), where K(y) = fo 7(0) lhd(0; y) dO, (what physicists call the partition function); and this integral might be very hard to evaluate — even numerically — in complex cases. Gibbs sampling and other similar techniques allow us to estimate fr 7r(0 I y)d0 for any nice set P without evaluating the integral K (y). Of course, K(y) is the 'prior predictive pdf' for Y, the absolute pdf for Y if 0 were indeed chosen by God according to the pdf 7r(•). We look at a number of examples — in this chapter and the next. Aa. Sufficient Statistics in Bayesian theory. Suppose that 0 is a multi-dimensional parameter, that 7- : \u0000 —> \u0000 and that for y E Ir (compare 183D1) lhd(0; y) = g (0 ,t)h(y) = g (0 , r(y))h(y), where t = r(y) and where h(y) does not involve 0. Then, with functions of y being absorbed into the oc, /0 I Y (61 Y) CC 740) lhd(0; y) oc R-(0)g(0,t), so that the conditional pdf of 0 given Y depends on y only via t. Our wish to formulate that fe I Y (° I Y) = f0 I T(6/ t) is frustrated by technical obstacles related to those mentioned at the start of Subsection 261C. These can be overcome — but not here! 7.5. Multi-parameter Bayesian Statistics \u0000 267 ■ B. The N(p, prec p) case when p and p are both unknown. (The Frequentist study of this case needs extra technology and has to wait until the next chapter, though our use of the Trequentist prior' will tell us what results to expect.) We take the Model: Y -1, - Y 2, \u0000 , Y,„ are IID, each N(p, prec p). Then, lhd (p7 p7 yi:)_bs , yips , \u0000 ynobs) (270 -1n O n exp - p E (yes _ 0 21 \u0000 = (27)-2 np2n exp \u0000 n (Yobs - /1)2} exp p q( y obs y obs)} where \u0000 q(3r \u0000 Yobs) 2 \u0000 (B1) This is part of a systematic notation we shall develop for 'quadratics'. Let M and R be ea and p, viewed as RVs in Bayesian fashion. Let us take as Prior: M N(m, prec r), R Gamma(K, rate a), M, R independent. Then, the posterior pdf 71(p p gobs \\ ) is proportional as a function of (p, p) to m )2} p K—l e —ap \u0000 w bs 0 2} exp \u0000 (p, \u0000 X 19 n exp \u0000 (B2) Ba. Taking the Frequentist prior. If we take the Frequentist prior with m = r = K = a = 0, then (p , p I Yobs) cc p2\"-1 exp \u0000 (yobs 102 - /34(Y°bs, yobs)} \u0000 (B3) Integrate out (keeping the N(yobs, prec np) pdf in mind) to see that 7r(p y obs) _ f 71.01, p 'yobs) dati \u0000 (n-3) exp \u0000 p(i(yobs, yobs)} This gives the following result (corresponding to a central result in the Frequentist theory of the next chapter) q(y°1-1', y'bs)R \u0000 x2„ 1 = Ganima( (rt - 1), rate A). \u0000 (B4) If we integrate out p in (B3) (with the pdf of the Gamma(in, rate 2 E(yibs p)2)) distribution in mind), we obtain 7r(it yobs) oc \u0000 n (gobs - /02 q(yobs yobs)} \u0000 7 which, as looking at 253(G2) makes us realize, states that (M -Yola0 a- n-1 1, \u0000 (B5) (yes 11)1 \u0000 (C2) [a 268 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' where el-ri`ths, is the estimate of standard deviation a (of which more later): obs (B6) Results (B5) and (B6) give us the CIs associated with the standard 't' method, of which more in the next chapter. ► ► C. The Gibbs sampler for this case. The function at 267(B2) is a little complicated, and we would need to integrate it over (it, p) to get the normalizing function of yobs However, on looking at 267(B2), we see that, with obvious notation, Conditionally on R = p (and the known yobs), ( r rm nPYobs M \u0000 N \u0000 , prec (r rip)) (C1) Compare the top row of Table 211 J(i). Also, Conditionally on M = ,u,, R ti Gamma (K \u0000 rate Compare the last row of Table 211 J(i). Thus, the conditional probabilities are rather simple. The idea of the Gibbs sampler, as for all MCMCs, is that being given a sample with Empirical Distribution close to a desired distribution is nearly as good as being given the distribution itself. For our present case, the Gibbs sampler works as follows. Choose an initial value p[0] arbitrarily, say p[0] = K/a. Set c = 0. REPEAT Choose p[c + 1] according to the \u0000 p[c]) density described at (C1), Choose p[c + 1] according to the 74• I p[c 1]) density described at (C2), UNTIL c = C. Here, we have always 'updated' p before p. We could randomize the order of updating. Since we have 'C' programs for simulating both Normal and Gamma variables, it is easy to write a bare-hands 'C' program to implement this Gibbs 7.5. Multi-parameter Bayesian Statistics \u0000 269 sampler. Such bare-hands programs are given for other examples later. You can return and do this one as exercise. ►■ • Ca. Fact. For a turn-in' b and a `gap' g, the Fxnpirical Distribution of {(1.t[c], p[c]) :c=b+tg, t. 1, 2, ... , NI is close to the distribution with pdf 7r(p,, pj y° 5). More precisely, the following ergodic property holds: for a nice function h, we have with probability I as N -› oo, h(p[b + tg], p[b + tg]) \u0000 f h(,a, p) {µ, pj yths)ditdp. See the introduction to Gibbs sampling in Norris [176] and the articles by Roberts and by Tierney in Gilks, Richardson and Spiegelhalter [94]. Ideally, of course, we want a result which gives with probability 1 the required convergence simultaneously for a wide class of functions It; and the appropriate `ergodic' theorems exist. The burn-in time is meant to arrange that the sampler is close to reaching the desired 'steady state'. A nice feature of the Gibbs sampler is that if our primary concern is with pc (say), in which case p is a nuisance parameter, then the Empirical Distribution Function of {p[c] : c = b + tg, t = I, 2, ... , N} will be close to the DF with 7r(pt yobs) as pdf. The idea of the gap is to stop a high degree of correlation between successive values of the pair (p[c], p[c]), but it usually does not matter if we take g = 1. Diagnostics from the theory of time series are often used to assess the degree of correlation between (A[ci ], p[ci ]) and (p[c2], p[c2]) when c1 and c2 are moderately spread apart. The real point however is that we want to be sure that the values (p[c], p[c]) wander over the full range of reasonably likely values. We shall see later that we can fail to get the desired `wandering' even for simple situations. MCMC theory, originally developed by physicists, has blossomed into a huge field within Bayesian Statistics. The seminal papers are those by Metropolis et al [164] and by Hastings [110]. We take only the briefest look to get the flavour, and we only consider the (perhaps most intuitive) Gibbs-sampler algorithm, a special case of the Metropolis— Hastings algorithm, use of which in Statistics owes much to Ripley [195] (utilising work of Kelly and Ripley, and of Preston) and to Geman and Geman [91]. I have neither the space nor the expertise to give you a full treatment of Gibbs sampling here. We shall look at a number of examples, and learn via them. 270 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' ► D. WinBUGS applied to our N(.L, prec p) model. \u0000 The BUGS program (Bayesian analysis Using the Gibbs Sampler) was developed by a team (led by David Spiegelhalter) at the Medical Research Council Biostatistics Unit at Cambridge University, and is now being further developed by them in collaboration with the Department of Epidemiology and Public Health, Imperial College School of Medicine, London. Numerous people (DS, Wally Gilks, Nicky Best, Andrew Thomas, David Clayton, ...) have contributed significantly to the program. As mentioned in Appendix D, the program is structured along the lines of graphical models and Markov random fields. Since (I make no comment!) Windows is currently the most widely used operating system (though not the one I use), I describe WinBUGS , the version for Windows which may be downloaded — FREE when I got mine, but possibly not by the time you read this — via [238]. WinBUGS likes to simulate from a density (of one parameter given the data and all other parameters) which is logconcave, so that it can utilize the Gilks—Wild Adaptive Rejection Sampling technique mentioned at 139Ib. But the program uses a variety of other sampling techniques, including Metropolis sampling. Brilliant as WinBUGS is, it is a good strategy to try out your programs first on data simulated using the model — where you know what the answers are. This will indicate if something is going wrong. ► Two other good things about WinBUGS . An excellent set of examples comes with the package. The package also enables you to join a User Group. If you do so, you can see problems posed by others and their solutions, and, if necessary, you can pose your own problems. As always you should do this last only after studying the online manual and examples carefully. Let's get the computer to simulate a random sample of size 10 from a N(0, 1) distribution: 1.21, 0.51. 0.14, 1.62, —0.80, 0.72, —1.71, 0.84, 0.02, —0.12, \u0000 (D1) which we shall use as data. I am taking a proper prior 'close' to the improper Frequentist prior for which m=r=K=a= 0, choosing my values of these parameters as suggested by the 'online' WinBUGS manual. What you type in is in typewriter font; so are the results. Comments made by the machine are in italic font. Things you do with mouse are underlined. I have put some headings in bold font to structure things. It is always possible that slight modifications of my instructions will be needed for more recent versions of WinBUGS. Model: Do File — New to create a new window. In it, type model{ for(i in 1:n){ y[i] \u0000 dnorm(mu, rho) \u0000 N(p,, prec p) 7.5. Multi-parameter Bayesian Statistics \u0000 271 } mu - dnorm(m, r) \u0000 M — N(m, r) rho - dgamma(K , alpha) \u0000 R ,---, Gamma(K, rate a) (Independence of M and R under prior assumed by machine) } Model - Check model (model is syntactically correct) Data: Make another new window in which you type list(n=10, y=c(1.21, 0.51, ... , 0.02, -0.12), m=0.0, r = 1.0E-6, K=1.0E-3 , alpha = 1.0E-3) So, n = 10, y is as at 270(D1), m = 0, r = 10-6, K = a = 10-3. Do Model - Data (data loaded) Compilation: Model - Compile (model compiled) Every 'stochastic element' described in a 'name d . . . ' statement and not already initialized in the 'Data' statement, must now be initialized. Initial values of parameters: Make a new window, and type \u0000 list (mu = -1, rho = 2) \u0000 (arbitrarily chosen) Model - Inits (initial values loaded: model initialized) (Note: The machine can be asked to carry out the 'non-data' initialization step automatically via Gen inits.) Do a burn-in: Model - Update \u0000 (updates =) 1000 (refresh=) 100 \u0000 (refresh explained below) update Note: Update Tool window remains in view Say what to monitor: Statistics - Samples (node =) mu (in dialog box) set Notes: You can ask WinBUGS also to monitor other nodes simultaneously. Sample Monitor Tool window remains in view. Begin the monitored sampling used in the analysis: Model Update (updates =) 10000 (refresh=) 100 update stats (not all of which I quote) 272 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' node mean \u0000 sd \u0000 2.5% 97.5% start sample mu 0.239 0.356 -0.4607 0.9469 \u0000 1001 \u0000 10000 The mean is the mean of the sample of the p[c], and the 2.5% and 97.5% percentiles are obtained from the empirical distribution of the ,a[c] values. Thus BUGS is suggesting the 95% Confidence Interval (-0.4607, 0.9469) for p. The Frequentist CI obtained by the famous 't' method explained in the next chapter (already obvious from 267(B5)) is (-0.456, 0.942) which may be obtained from Minitab. The Minitab commands are MTB > set ci \u0000 Put in column 1 the following data. DATA> 1.21 0.51 0.14 ... 0.02 -0.12 DATA> end MTB > tint c1 \u0000 Obtain a 95% CI for p using 't' method. N MEAN STDEV SE MEAN 95.0 PERCENT CI 10 0.243 0.976 \u0000 0.309 (-0.456, 0.942) Minitab's STDEV is the value 61-7,thsi on the left of 268(B6), and SE MEAN denotes -aobsi k/71. You could also use R as follows: x <- c(1.21, 0.51, ..., -0.12) ttest(x) to get the same results. WinBUGS will show you the evolution of the sampled values of p[c], p[c] in windows of 'width' refresh. Try different values of this parameter. WinBUGS will plot the estimated posterior pdf for p or for p, etc, etc. For people who wish to study correlations, etc, in detail, BUGS may be used in conjunction with the CODA diagnostics package which may also be downloaded via the Internet (same address). This package is designed to work with R and/or S-PLUS. Da. The MLwiN package. The very useful package MLwiN mentioned in the Preface is available from [166] - at a cost. E. WinBUGS for our Cauchy example. The 'n = 3' case in Figure 196E(i) shows a tri-modal (three-peaked) posterior pdf for the Frequentist uniform prior 7T(9) = 1 for the Cauchy density f(y I 0) = \u0000 (1+ (y — 0)2)1 1 - \u0000 (El) It is an interesting case on which to test out WinBUGS , to see how soon the program recognizes that there are three peaks. Now, my current version of WinBUGS doesn't seem to accept the Cauchy (or t1) distribution. However, we can utilize Exercise 255Ge, with v = 1. We can produce a variable Y with density as at (El) via R Garnma(1, rate 1), \u0000 (Y1R, 0) N(0, prec R). So, we set up a WinBUGS model 7.5. Multi-parameter Bayesian Statistics \u0000 273 model{ theta - dnorm(0.0, 1.0E-6) \u0000 (Vague prior) for (i in 1:N){ dgamma (0 . 5 , 0 5) dnorm (theta , R [i] ) Ri Gamma(i, (Y 1Ri, 0) — N(C), prec Ri) } with data list(N = 3, Y=c(-5.01, 0 4, 8.75)) and initial values list(theta = 0, R = c (1, 1, 1)) We use WinBUGS exactly as in Subsection 270D. The tri-modal nature of the posterior pdf very soon emerges. Ea. 'Bare-hands' Gibbs sampling in 'C'. Suppose that we take the improper prior 740) = 1 for O. Then (you check!) fe,R,Y (0, r, y) = \u0000 { (270-1 exp \u0000 [1 (yi — 0)2])1 , so that given O and Y = y, the variables R1, R2, R3 are independent, (Ri 0, Y) \u0000 E (rate 2 [1 \u0000 — 0)2]) , and (0 I R, Y) N F rzYi , prec E r1) You should check that the following will therefore work. (Recall from Subsection 136H that Expl (lambda) simulates an E(rate A) variable, and, from Subsection 160Ca that PRECnormal (mu, rho) simulates an N(µ, prec p) variable.) theta = 0.0; for(c=1; c<=Nobs + 100; c++){ rysum = rsum = 0.0; for(n=0; n<3; n++) r [n] = Expl(0.5 * (1.0 + (y En] - theta)*(y [n] - theta))) ; for(n=0; n<3; n++){ rysum = rysum + r [n] *y [n] ; rsum = rsum + r [n] ; } mu = rysum/rsum, theta = PRECnormal(mu, rsum); 274 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' Sample size 1000 1111111111lialla11111111111111).,. Sample size 100000 Figure E(i): Tare-hands in C' Gibbs sampler for a Cauchy model the various theta values for c > 100 giving the desired sample. The results for a sample of size 1000 with a 64-bin histogram and for a sample of size 100000 with a 128-bin histogram are shown in Figure E(i). The curve shows the exact posterior pdf for the uniform prior. (The pictures are from my C-to-Postscript converter.) Even the '100000' case took at most 3 seconds on a 200Mhz PC. As we saw at Figure 138I(i), the end bins of the histogram carry estimated tail probabilities. If concerned with just a few parameters, I prefer to watch histograms evolving than to look at the time series which show the successive simulated values of parameters. 7.5. Multi-parameter Bayesian Statistics \u0000 275 ► Eb. Important Warning. Before you become too excited about the success of Figure 274E(i), consider the following. We have added as many nodes Ri to the sampler as there observations YZ, an extremely 'non-parsimonious' strategy in general. We should not therefore be surprised if there are problems even for moderately large sample sizes. See 353Hf below. F. Exercise. Suppose that X1, X2, . \u0000 Xn Poisson(A), Y2, • • • , Y. '-'-' Poisson(P) 7 X1, X2, . \u0000 Xn , Y1, Y2, \u0000 Y. being independent. Assuming the reference priors for A and p (you can only do this to a high degree of approximation), find an approximate 95% CI for A/p on WinBUGS . Looking at Exercise 100Ia for a clue, determine the distribution of SAS T), where S = E xi and T = E Y. Does that give you a Frequentist method of dealing with this problem? ►► G. Advantages of, and problems with, MCMC methods. One of the great advantages of MCMC methods is that they can deal with much more realistic models than will the classical methods. However there is still value in the old idea of using parsimonious models: models with no more parameters than is really necessary: there is nothing clever in introducing loads of new parameters just for the sake of it. (And you have already been warned about the difficulties which can arise if you introduce lots of new nodes to implement the Gibbs sampler.) On the other hand, in image restoration for example, there just have to be loads of unknown parameters (the colours of all the pixels. etc). An obvious problem with MCMC methods is that they are simulations, not exact answers. In some situations, convergence to the posterior distribution can be very slow. Worse, in others, it is quite possible for the system to appear to have converged to some distribution, but the wrong one! This is when the simulated values of 0 get stuck within some region of the parameter space. One should always do a number of runs — with different initial values and different seeds. One should also always investigate how robust things are to small changes in priors. See Gilks, Richardson & Spiegelhalter [94], especially the article by Gelman, and Besag [18] (the text of a well-attended lecture to the Royal Statistical Society). In fact, there is a remarkable new technique, called perfect simulation, discovered by Propp and Wilson at M.I.T., which can produce one value of 0 definitely chosen from exactly the correct posterior distribution. When it works acceptably quickly (which it does not always do), it certainly resolves questions regarding burn-in time in MCMC methods. But it is usually hardly feasible to run the whole Propp-Wilson algorithm independently a huge number of times to get 276 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' the desired perfect sample. Nor can I see how it avoids the 'sticking' phenomenon: what matters is ergodicity rather than correctness of distribution of an individual element of a sample. See Propp & Wilson [189], and the informative site http : //dimacs.rutgers.edu/ dbwilson/exact.html/ As mentioned previously, Gilks, Richardson and Spiegelhalter [94] and the series [16] edited by Bernardo, Berger, Dawid, and Smith make for excellent reading of theory and practice. ► H. Warning: priors for precisions. \u0000 In hierarchical (or random- effect) models, the usual parameters are themselves regarded as 'random' with distributions determined by hyperparameters which themselves have prior distributions. These hyperparameters are therefore two steps away from the Observations; and we have to be careful about assigning vague priors to hyperparameters in the way we have done for parameters. It is fairly standard to use 7r(p) = p-1 for the vague prior for a parameter representing a precision, so that In(p) is uniform on R. However, assigning prior 7r(r) = 7--1 to a hyperparameter representing a precision can lead to improper posterior densities, something we certainly wish to avoid. Of course, we should never use improper priors. However, it is as well to be aware of situations where posterior densities can have similar tail-off to prior ones, and which could lead to undesirable behaviour of the Gibbs sampler in certain (probably rather extreme) cases. Even when our chief concern is with parameters, we should monitor what is happening to hyperparameters. In the following discussion, I shall treat certain parameters as Random Variables, using 0 rather than 0. This has the effect of promoting hyperparameters to parameters. Let's first consider a ridiculous, but instructive, situation. \u0000 Suppose that an Observation Y is given by Y = 0 ± E, \u0000 prec T), E N(0, 1), 0 and E being independent. Suppose that the prior 71-(it, r) for (ii, r) has the form 71-(7) so that IL has uniform prior. Since we cannot then hope to pick up any information about precision from one observation, we would expect 71-(T Y) = 71-(T). You calculate 7r(it,T y) and confirm our conjecture. In particular, if 7r(r) is improper, so will be 71-(7- y). Here, the true precision of the Observation Y is r/(7- + 1), so it is not surprising that the reference prior for r is {r 1)} -1, corresponding to the fact that ln(r/(r + 1)) has uniform density on (—oo, 0); and this reference prior leads to a proper posterior density. 7.5. Multi-parameter Bayesian Statistics \u0000 277 ► Ha. Exercise. Consider the very important model (where 1 < j < J and 1 < k < K) Yjk = e j + Ejk, \u0000 ej ^ N(p, prec T), \u0000 Ejk — N(0, prec p) \u0000 (111) where all variables Oi (j < J), E jk (j < J, k < K) are independent. Prove that for the situation described at (H1), the maximum-likelihood estimate (ft, f-, 0i , p) corresponding to possible observations (yjk : 1 < j < J,1 < k < K) satisfy the unsurprising relations: = V**, T= EA —11)2, p 1 = J K EE(y ;le — ei)2, K N J* 1- + K but these have to be solved numerically. Prove that if we take a prior 71-(A, T, p) = 71-(T, p), then (7-p, + Kpy3. \u0000 (03 I p, T, p, y) \u0000 N \u0000 kp , prec (7- + K p)) , TK p \u0000 (t I r,P,y) \u0000 N \u0000 Prec T + K p ) 3_1(.1-1) i(Jx—i) \u0000 T K p \u0000 7r(T, P I Y) \u0000 (T K p)1(-1 7rer, P) \u0000 —1) exp \u0000 T+ \u0000 Kp B (y) — pW (y)} where W(Y) := EE(y.,k — v„)2, \u0000 B(y) := \u0000 —v**)2. Note that we will not obtain a proper posterior density if we take R-(T, p) of the form 7--17r(p). This is because as T -) 00, Ix(r, P I Y) \u0000 function(p, y), 71(T, p) so that f 71- \u0000 y)dr will be infinite. Remark. In analogous situations to this, the standard (Jeffreys) reference priors can cause substantially worse difficulties even for parameters (rather than hyperparameters), as we shall see later. ► I. Simultaneous versus individual Confidence Intervals. \u0000 Many packages, including BUGS, will produce 95% CIs for each of many parameters separately: we refer to 'individual CIs'. This 'separately' must be remembered. If for i E {1, 2, .. . , rt}, we are 95% confident that parameter 0, is in interval h, then we may have nothing like 95% confidence in the 'simultaneous' statement that '0, E h for all i'. (If the statements 0, E h may be regarded as 'independent', our 278 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' confidence in the 'simultaneous' statement is only (0.95)n; but, in theory, without independence, it could be as low as 0 for n > 20 — Yes?!) On those occasions when we require a Confidence Region for a vector parameter 0 = (01, 02, . , 0,) there are two serious difficulties. Firstly, it can be rather expensive in computer time and memory to calculate such a CR. Secondly, it is usually difficult to convey the results in a comprehensible way. The best CR will have the form 7r(e I yobs) > or and this will not have the comprehensible product form {0 : Oi E /i, i = 1, 2, ... ,n}. \u0000 (I2) Since product forms, simultaneous CIs, are the only easy-to-understand ones, we often use non-optimal CRs of product form. As mentioned earlier, if we only have two parameters 0 = (01,02), then if we find 97.5% CIs I1 and 12 for 01 and 02 separately, then II. x 12 is an at-least-95% CR for 0. So, we can get conservative CRs this way. Simultaneous CIs will be mentioned in the next subsection and discussed in the next chapter. ► J. Extreme data viewed after an experiment. Suppose that M1, - M 27 are 'performance indicators' which represent on some scale how good hospitals in cities 1, 2, ... , n are. (Well, ... , assume that such a concept makes some sense. Of course, a performance indicator must take into account the severity of cases brought to the hospital in question, etc.) Quantity Mi is subject to a measurement error e„ so that the observed value (measurement) for city i is 172, = \u0000 +ei. The e, might represent the fact that people may recover or die for reasons which have little to do with their treatment. A journalist might focus on the hospital with the lowest measurement (which is not necessarily the one with the lowest M-value). Ja. One aspect. Just suppose that God knows that A = 0 for 1 < i < n, and that, as we assume, the E y are IID each N(0, I). Then, P(17, < —1.645) = 0.05 for every i, but P(min Yi < —1.645) = 1 — (0.95)n, which will be close to 1 for large rt. In this case we would clearly be unfair if we choose the hospital with the lowest measurement and treat it as if it were the only hospital. (We do not know that all Mi are zero.) 7.5. Multi-parameter Bayesian Statistics \u0000 279 Jb. An inappropriate use of 'absolute probabilities'. Suppose instead that experience suggests that we use a model where M1, M2 --)Mn1 61)E21 • ,en are independent, /1//2 N(0, a2), ei — N(0,1), a2 being known. Let Y, = Mi + ei, and let W be the value of i which produces the minimum measurement: Yw = min Yi We want a 95% CI for mu, = m w(wact ) of the form (—co, c). Here is one way to do this. We can evaluate a constant c such that IP(Mw < Yw c) = 95%. Then there is no argument about the fact that (—co, yobs c) is a 95% CI for mw. But is it the one we should use? At first sight, it seems to get round the 'unfairness' mentioned in regard to example 278Ja by the use of a kind of simultaneous CI. But it is not the appropriate thing to do. 10. Jc. 'Conditional' study of the last example. Bayesian theory tells us that we should condition on the fact that Y = yobs. If we know yobs, then, of course, we know the corresponding w and the lowest observation yuo,bs. From the Frequentist point of view, the fact that Y1, Y2, ... Y. are IID RVs each with the N(0, 1 + a 2) distribution which does not depend on the values of M1, M2, , M n might incline us to regard Y1, Y2, , Yn as Ancillary Statistics and (as in the Bayesian theory) condition on their observed values. See Subsection 263E. Now the pairs (M1, Y1), , (Mn, Y.) are independent, and for each k, (Mk I Yk) N (On, 0), 3 := a2 I (a2 + 1); \u0000 (J1) see Exercise 259Ac. It follows that p (w = k; Mk < Oyes + 1.6450 Y = yobs \u0000 0.95/{y72'<y9', 300 . Summing over k yields (Mw < ,(34'bs ± 1.6450 Y = yobs = 95%. \u0000 (J2) Thus, the conditional approach tells us to use (—co, OyZbs + 1.645 as a CI for mw, exactly as if the lowest observation had been the only one!! Do note however that the amount oywobs yobs by which yobs is increased to estimate the mean of Mw is maximized at the lowest observation. 280 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' The discussion at 278Ja may make us somewhat worried about the conclusion just obtained. But remember that in the example there, we essentially had a = 0, and the conditional method is giving us (—co, 0] as a CI for mw and [0, 0] for a two-sided CI. As we shall better appreciate in Chapter 9, result (J2) tells us that we have the absolute-probability result: P (Mw < /3Yw + 1.6450) = 95%, and this is the 'absolute' result we should use, agreeing with the 'conditional' approach to getting CIs. In case you are worried, you can do a cross-check when n = 2. The independence structure yields the fact that (mi., M., Y2) = .fY2 (Y2).fYi (YON,. I Yi and result 279(J1) shows that f fAfilYi (mliY1) dmi = 95%. ml<13yi +1.645.0 Thus, we have P (Mw 5 )(3Yw + 1.6450) = 21P (M 1 < Yl + 1.6450; Yl < Y2) iY2 (Y2)fyi (Yi)fmi (mi 1M.) thrttdYidY2 2 f92 91 <92 1111<fl91±1.645 .0 2 f dY2 fy2(Y2) f dm. fyi(m.) dmifmim(milYi) 92 Y1 <92 Li <Ry,. +1.645.0 2 f CIY2 fY2 (Y2) f dyl 92 Y1 <92 .fYi (M.) x 95% = 2P(Yi < Y2) x 95% = 95%. Part of the correct intuition is that if Yw is significantly less than 0, then it is likely that Mw is roughly /3Yw and ew is roughly (1 — 0)Yw. ► Jd. Important Warning: a `Bayesian—Frequentist conflict'. We can think of the situation just described as corresponding to that in which Y Miti, 1) and we do a Bayesian analysis supposing that each µi is a Random Variable A, the prior-distribution statement being that the Mi's are IID, each N(0, a2), where o-2 is taken as known from experience. Now, we know that, for any fixed i, the Bayesian CI for pi will, as a2 —> oo, converge to the Frequentist CI for Ai : as a2 \u0000 oo, we have —> 1. However, the Bayesian CI for /.tw = m w will converge to \u0000 (_ 00, gobs + 1.645] 7.5. Multi-parameter Bayesian Statistics \u0000 281 which is most certainly not a Frequentist CI. The `Frequentist' probability P (i-tw < Yw + 1.645 I ,k62, • • • An) is a horrible function of (pi , p2, \u0000 , pn) equal (as we saw earlier) to 0.95Th when all pi's are zero. So, what is going wrong? Well, letting a2 \u0000 oo for the Bayesian prior is tending to pull the pi's far apart from one another, something which does not correspond to what the Frequentist or sensible Bayesian wants. Though this does something to resolve the `Bayesian—Frequentist conflict', the problem still leaves me with a feeling of some unease about the `Bayesian' result for fixed a2, not so much for this example as for its implications for other situations. Of course, we should estimate a2 from the data and what reliable prior information we have. This remark does not fully resolve things by any means; but to take this discussion further and to extend to more complex situations is outside the scope of this book. You are alerted to the need for care. ► K. A perplexing Exercise: more `Bayesian—Frequentist conflict'. Suppose that 71> 3 and that Y1, Y2, \u0000 Yr, are HD each E(rate 0). Let S := Y1 + Y2 + + Yom,. Show that (a) the MLE for 9 is n/S, (b) the estimator (n — 1)/S is unbiased for 0, (c) the value a which minimizes the mean-square error E { (0 — alS) 2 I 0} is, for every 9, given by a = n — 2. According to what you have so far been led to believe, if we use the vague prior 0-1, the Frequentist prior(!), for this example, then Bayesians and Frequentists agree. This is not entirely true. Show that if we view 0 as a 'random variable' 0 in the usual Bayesian sense, then, conditionally on Y, 0 has the Gamma(n, rate S) distribution of mean n/S and variance n/S 2. Hence (see Lemma 691a) a Bayesian must regard n/S as the best estimator of 0 in the least-squares (lowest mean-square error) sense. Let us take 71 = 3 for definiteness. Then a Frequentist will say: E {(0 --1 ) 2 0} 1E { (0 3)2 0} S ) for every 0, so that 1/S is 5 times better an Estimator of 9 than 3/S in the least-squares sense. A Bayesian using the Frequentist prior will say that s} for every S, 282 \u0000 7: Conditional pdfs and 'multi-parameter Bayes' so that 3/S is 2A times better an Estimator of 0 than 1/S in the least-squares sense. Check out these calculations. But surely, you say, something must be wrong here: things just do not tally. We discuss this in Chapter 9. Use of improper priors is always prone to this kind of 'paradox'. However, there is more involved than improper priors as the Two-Envelopes Problem will illustrate spectacularly. L. Final Remark. I just wish to reiterate that study of multiparameter Bayesian Statistics, WinBUGS, simultaneous CIs, etc, continues within the next chapter. 8 LINEAR MODELS, ANOVA, etc 8.1 Overview and motivation The main theme of this chapter concerns a completely different aspect of the perfection of the normal distribution from that provided by the CLT. Classical Linear-Model theory hinges on the fact that • the normal distribution, • independence, and • Pythagoras's Theorem are linked in a fascinating way. Let's meet a simple case first. ► A. The Orthonormality Principle for two dimensions. (This story is probably due to Gauss and James Clerk Maxwell.) Let us suppose that X and Y are IID Random Variables each 'continuous' with pdf f which we assume continuously differentiable and strictly positive on R. Suppose that the law of (X, Y) is invariant under rotations about the origin in that for any such rotation T, T((X, Y)) has the same pdf on R2 as (X, Y). It is intuitively clear that the joint pdf f x,y(x, y) = f (x)f (y) must be a function of the distance from (x, y) to the origin, whence f (x) f (y) = g(x 2 + y2). But then, f (x) f (0) = g(x 2), 1(0) 2 = g(0), and h(x 2)h(y2) = h(x 2 + y2), where h(x) = g(x)/g(0). 284 \u0000 8: Linear Models, ANOVA, etc However, since h(r)h(s) = h(r s) for r, s > 0, then, differentiating with respect to r keeping s fixed, we have he(r)h(s) = (r s), whence h' (r \u0000 s) \u0000 h' (r) = h(r s) \u0000 h(r) \u0000 constant = c (say). (Technical Note. I have assumed that h is differentiable, but by the theory of one- parameter semigroups, this follows already from the measurability of h.) We have shown that h(r) = h(0)e\", and f (x) = f (0)ecx2, whence f must be the pdf of the N(0, a2) distribution for some a > 0. Conversely, if (X ,Y) are HD each N(0,0-2), then the law of (X, Y) is invariant under rotations about the origin. We require various n-dimensional generalizations of this Orthonormality Principle. All relevant algebra and geometry are revised/explained as they are needed. ► ► B. What we study in this chapter. The theory of Linear Models is the central part of traditional Statistics culture. The Orthonormality Principle allows us really to understand (Captain Kirk: `to really understand') the 't' result derived via Bayes' formula in Subsection 267Ba. It also allows us to study all the regression/ANOVA models mentioned in Subsection 28C. As stated there, the Orthonormality Principle marks the perfect tie-up which often exists between likelihood geometry and least-squares methods. We shall study ANOVA tables and the associated tests, but we shall be more concerned with Confidence Intervals, 'individual' and 'simultaneous'. Of course, we look at the Bayesian theory too; and, of course, the geometry remains relevant. We look at how Minitab and BUGS do the calculations, and at how MCMC methods allow more realistic models to be studied, though I have to add that things do not always work out as we might hope. We give some thought to assessing whether our models are reasonable, and, because of this, look at goodness-of-fit methods: `chi-squared', quantile-quantile plots, and Kolmogorov(—Smirnov) tests; I prefer the chi-squared amongst these. We study the Multivariate Normal (MVN) Distribution and correlation as a measure of association. Where the tie-up between likelihood geometry and least-squares methods breaks down, and unfortunately it does do so, our intuition takes a savage jolt. The most celebrated example, the shrinkage phenomenon discovered by Charles Stein, receives illuminating discussion in Stigler [218]. The Mathematics of ANOVA and of the MVN is by far the best way to understand finite-dimensional inner-product spaces. It is sad that courses on 8.1. Overview and motivation \u0000 285 Linear Algebra so often ignore tensor products, partitioned matrices, etc. I want to emphasize that in this chapter we use Linear Algebra only as a language. No deep result from Linear Algebra is needed here — not that there are that many in that subject! The 'Extension to Basis' Principle, assumed as a fact, is the deepest result used until we need the diagonalization property of symmetric matrices for the 'Multivariate Normal' section. The algebra which we learn here will equip us to take a quick look in Chapter 10 at the really mind-blowing situation related to Bell's inequality and the Aspect experiment. It perplexes me that Probability at the level of elementary particles requires very different methods of calculation from those which we have so far studied; and it should worry you too. Conventions. Henceforth, all vectors are column vectors, T stands for 'transposed', 1 stands for the vector 1 := (1,1, ... ,1)T and II v II stands for the length of a vector v: I I v l l 2 = v?-Fv3+--•+ v7,2 for v E \u0000 . ► C. Providing more motivation. Many people have suggested that before launching into the (easy but 'abstract') Linear Algebra, I should provide more motivation: that some numbers and pictures would help. David Cox made the good suggestion: \"Why don't you consider, for example, measurements on the three angles of a triangle?\" So, here's my set of 'trailers' for what's coming. The best bits are in the 'main features', though the trailers contain some nice geometry not studied later. In Subsection 28C, we considered the Model: Observation = (true mean) + (True Error) where the true means are structured in some way, and where the Errors corresponding to different Observations are IID, each N(0, u2) for some unknown parameter o-2. As explained in that subsection, after the experiment, we mirror this with observation = (fitted value) + residual where the fitted values are chosen on the basis of likelihood considerations. The assumption of IID normal Errors means that the likelihood geometry and least- squares methods tie up perfectly and that the Orthonormality Principle holds. Here are some key points: • the fitted values reflect the algebraic structure of the true means (in a sense which will become clear later); and subject to that, are chosen to minimize the residual sum of squares, rss, the sum of the squares of the residuals; 286 \u0000 8: Linear Models, ANOVA, etc • we estimate a2 by dividing rss by the associated 'number of degrees of freedom' (which notorious concept will be nailed down thoroughly by our treatment); • by combining these ideas with others, we obtain Confidence Intervals or Confidence Regions for the various parameters. Please re-read Subsection 28C now. Do not miss out the trailers, to which we shall be referring back. ►► D. A trailer for the Normal-Sampling Theorem. The simplest situation considered in this chapter is where we obtain a Confidence Interval for the mean p of a sample assumed taken from a normal distribution N(p,a 2), where bothµ and Q2 are unknown. Here every true mean is p, and the fitted values must all be equal. In Subsection 270D, we looked at the data 1.21, 0.51. 0.14, 1.62, —0.80, 0.72, —1.71, 0.84, 0.02, —0.12. I said that the Frequentist CI obtained by the famous 't' method is (-0.456, 0.942) which may be obtained from Minitab. The full theory of how this is done will be explained in Subsection 305B, but some pictures and remarks now might help indicate the way in which we shall be thinking. Diagram (a) in Figure D(i) shows the observations (•) and the fitted values (o), the latter forming a horizontal line y = Yobs since the value mobs which minimizes the E(yes in.bs ) 2 residual sum of squares \u0000 is yobs as we have often seen. Thus gobs is the 'best estimate' of p. The vertical lines represent the 'residuals', the discrepancies between the fits and the observations. We have rssobs := q (y obs y obs) \u0000 Em bs v)2 = ( E(oc bs)2) nvo2bs• Diagram (b) in Figure D(i) shows the way in which we must rethink Diagram (a). This diagram shows what is happening in R n . There, yobs is the vector of observations, Yobsi — (Yobs, Yobs, • yobs ) T is the vector of fitted values, and y °135 — yobs1 is the vector of residuals. The value yobsi is the value in'ths1 closest to yobs, that is, the perpendicular projection of yobs onto the line joining 0 to 1. In particular, the vector of residuals, y obs — yobs 1 is in the (n — 1)-dimensional space perpendicular to 1, and this helps explain intuitively why the appropriate number of degrees of freedom for the Residual Sum of Squares, RSS, is here n — 1. We therefore use := RSSQ(Y, Y) 1 E(Yk — Y)2 n — 1 = n — 1 \u0000 n — 1 as our Estimator of o-2. We know that nz (Y — p) N(0, 1), Q 8.1. Overview and motivation \u0000 287 y (a) The observations (•), fits (o) and residuals (verticals) pi (b) The plane in Rn containing 0, 1, yobs (c) The Pre-Experiment picture in Rn Figure D(i): Illustrations for the Normal-Sampling Theorem a fact that we could have used to obtain a CI for p had a been known. The appropriate modification for unknown a is, as we shall prove at 305B, t o-1, an-1(Y) Da. Exercise. Use this to confirm that Minitab did the calculation of the 95% CI for p correctly. There are t-tables on page 516. Diagram (c) in Figure D(i) is meant to depict the Pre-Experiment situation, where Y is the vector of observations as PreStatistics. By the Orthonormality Principle, the law of Y is invariant under rotations about the 'true mean' vector p1. On combining this with the obvious 'scaling' property, we see (do think about this carefully) that the distribution of the ratio n, (Y — p) 288 \u0000 8: Linear Models, ANOVA, etc IlY is the same as if Y were chosen uniformly on the sphere of radius 1 with centre ttl.: we just need the distribution of I tan Al for that case. Thus, we could calculate CIs from properties of spheres. We shall not take this route, except for the case of the 'angles of a triangle' problem in 292G below which will show how it could be done in general. ►► E. A trailer for Linear Regression. (Pedantic mathematicians might prefer the term Affine Regression' !) Here, the model is that we observe the response Yk (1 < k < n) to a deterministic, experimentally-set, `covariate' xk, and use the Model Yk = a + OXk Ek = ± 13 (Xk ±Y) Ek) where a,13, µ are unknown parameters related of course by the fact that it = a + [3.-±\", and where the Ek are IID each N(0, a2). (There are good reasons for the second formulation.) For the Model, the true mean corresponding to a covariate x lies on the true regression line y = a + Ox = µ + /3(x — Y). We want a fitted regression line to mirror this. A well-known case is where Yk is the race time for the great Welsh hurdler Colin Jackson when the supporting windspeed is xk (which could be negative). Now, the mean of Y is really a non-linear function of x, but the windspeeds when races were run were small compared with the athlete's speed, so as a rough first approximation, we can assume linearity. In this case, the windspeeds could not be set in advance by the experimenter: they are random. However, we can regard then as Ancillary Statistics, believing that they do not (much) affect the Ek values, and condition on them: hence, their randomness does not matter. Of course, the a, 0, µ refer to this particular athlete, and depend amongst other things on his mean running speed; and of course, his fitness at the time of a race and other factors are relevant, but if we only want a rough approximation, we can use the linear-regression model. What do we want to do for a general linear-regression model? The answer is provided by Figure E(i), In that figure, the top picture shows some data considered later and also the fitted regression line Y = Yobs + b obs (X - Y), and marks the residuals as vertical lines. The values gobs for the estimate of it and bobs for the estimated slope are obtained from the fact that the residual sum of squares has to be minimized. The bottom picture repeats the observations and fitted regression line in grey, and in black shows 'individual confidence bands for fit and for prediction'. With 95% confidence, the true mean ± 13(x — Y) corresponding to a single x-value x will lie between the inner curves. With 95% confidence, a new measurement Y corresponding to a single new value x will lie between the outer curves. I1Y1 \u0000 7111Y — I tan AI = All the theory is in Subsection 312D, and Figure 314D(ii) there will provide the 'likelihood geometry' picture corresponding to Figure E(i). 8.1. Overview and motivation \u0000 289 Regression line and residuals 95% confidence bands: inner for mean y + ,3(x - y) for a single x; outer for new observation for a single x-value x. Figure E(i): Linear regression ►► F. A trailer for ANOVA. The model we shall now consider is that in which we have a pair of 'factors' A and B (type of crop and type of soil, type of car and type of fuel, which workman/woman and which machine. There are J 'levels' of A and K of B. For 1 < j < J and 1 < k < K, we measure a response 17.7k (crop weight, output of pollution, number of goods produced, etc) when we use level j of A and level k of B. We assume that Yjk = µ + ai +15k + elk, where the Ejk are IID each N(0, o-2). Thusµ is an overall mean, al is the amount above the mean by which use of level j of A raises the mean, and i3k is described analogously. 290 \u0000 8: Linear Models, ANOVA, etc You can see that with this interpretation, we have Ea; = 0 = Ei3k. (That agricultural field trials are done differently in practice was mentioned in the Preface.) More refined models will be considered later. We stick to this simple case for now. Suppose that we obtain the 'actual' (but hypothetical, for ease of calculation) results shown in Table F(i). I want you to accept the appropriateness of the model for now to see the Arithmetic. The meaning of 'estimated A means' is obvious, each 'estimated 1 B 2 3 estimated A means estimated A effects 1 5 12 7 8 —2 2 13 12 17 14 4 A \u0000 3 13 12 14 13 3 4 5 9 13 9 —1 5 4 5 9 6 — 4 estd B means 8 10 12 10 estd B effects -2 0 2 Residuals: —1 4 —3 \u0000 1 —2 \u0000 1 2 —1 —1 —2 \u0000 0 \u0000 2 \u0000 0 —1 \u0000 1 Table F(i): Results for a two-factor ANOVA model A effect' ai is the corresponding mean minus the estimated overall mean m, and the estimated residual el k is given by elk = yik — ai — bk — m, so that the model is mirrored by yjk = rn + ai ± by + eik, as explained in Subsection 28C. All row sums and all column sums of the elk are zero. However, if all row sums are zero and all column sums except the last are zero, then the last column sum must be zero. 8.1. Overview and motivation \u0000 291 Thus the vector of estimated residuals satisfies J + K — 1 independent linear constraints and therefore lies in a subspace of W K. of dimension v := JK — + K — 1) = (J — 1)(K — 1); and this is the number of degrees of freedom for the residual sum of squares rss. In our example it is 8. We find that rss -= 48, whence the estimate of a' based on rss is Q„ = 48/8 = 6. We are going to be interested here only in the A factor. (The B factor may be handled similarly.) We find that the sum of squares of the estimated A effects is 46, with degrees of freedom 4. The estimate of the variance of the mean of an A row will be 46/4 = 11.5. But the variance of the mean of an A row will be a2/3, so that our estimate of a-2 based on A-means is 3 x 11.5 = 34.5 if the Null Hypothesis that all ai are equal (to 0) is true. (Note. The calculation would normally be done as (3 x 46)/4 = 138/4 = 34.5, as you will see in the Minitab output below.) Thus, it is natural to consider the estimate of a-2 based on estimated A means \u0000 34.5 \u0000 = 5.75, estimate of a-2 based on estimated residuals \u0000 6 and to reject the Null Hypothesis that all (xi are equal (to 0) if this ratio is too big. How we decide on 'too big' is explained later. More importantly, we discuss CIs, CRs and simultaneous CIs for the various parameters. Fa. Minitab study of our example. The following is an executable file myANOVA . MTB for studying our example. The command additive signifies 'no interaction'. name C1 'y' C2 'A' C3 'B' set 'y' 5 12 7 13 12 17 13 12 14 5 9 13 4 5 9 end set 'A' (1:5)3 end set 'B' 5(1:3) end print 'y\"A\"B'; format(3I3). twoway 'y\"A\"B'; additive; means 'A\"B'. 292 \u0000 8: Linear Models, ANOVA, etc When run via exec 'myANOVA ' it produces the output: Executing from file: myANOVA.MTB 5 1 1 12 1 2 7 1 3 13 2 1 etc ANALYSIS OF VARIANCE y SOURCE DF SS MS A 4 138.00 34.50 B 2 40.00 20.00 ERROR 8 48.00 6.00 TOTAL 14 226.00 Individual 95% CI A Mean ---+ + + + \u0000 1 8.0 ( * ) 2 14.0 ( * \u0000 ) 3 13.0 ( \u0000 * \u0000 ) 4 9.0 ( * ) 5 6.0 ( \u0000 * ) -+ + + \u0000 + \u0000 3.5 7.0 10.5 14.0 Individual 95% CI B \u0000 Mean \u0000 + \u0000 + \u0000 + \u0000 +- 1 \u0000 8.0 \u0000 ( \u0000 * \u0000 ) 2 \u0000 10.0 \u0000 ( \u0000 * \u0000 ) 3 \u0000 12.0 \u0000 ( \u0000 * \u0000 ) \u0000 + \u0000 + \u0000 + \u0000 +- \u0000 7.5 \u0000 10.0 \u0000 12.5 \u0000 15.0 Of course, you can get much fancier pictures in Minitab for Windows. Minitab ignores the 'Sum of Squares associated to the overall mean', as it is perfectly entitled to do. We shall study the table in Subsection 326L and see there why the half-length of an individual 95% CI is 3.27 for A means, 2.52 for 13 means, as is indicated in the pictures. ► G. Measuring the angles of a triangle. Suppose that y?.bs, yps , Abs are measurements of the true angles 01, 02, 03 of a triangle. We use the Model: Yk = ek + Ek, where Ei, e 2, E3 are IID each N(0, o-2). Of course, we need to discuss with the experimenter whether this Model is reasonable. Amongst other things, o must be small compared with the smallest angle measured. We 8.1. Overview and motivation \u0000 293 have the constraint 01 + 02 + 03 = 7r (in radians). Now, lhd(0; y) = (2702)-3/ 2 expf -- lie — Y112/a 2}, and the mle t°bs of 0 must minimize I I ebs —°y bs II2 subject to the constraint Eve. = 71\". Thus, t°bs must be the foot of the perpendicular from yobs to the plane {t : t1 + t2 + t3 = 74. This plane is perpendicular to 1, so that t = yobs + Ci for some cobs, and the sum constraint implies that Cobs = 3{7r (yy.bS + Abs + Abs)}, exactly as you could have guessed without knowing any Statistics. With this cobs, we use tabs _ y obs + cobs1 as our best estimator for O. However, we want a Confidence Region for 0, with Ic°bs I as our natural guide to accuracy. So, let Y — 0 have IID components, each N(0, u2). Let T be the foot of the perpendicular from Y to the plane {t : t1 + t2 + t3 = 74. Let F be the angle between 0 — Y and T — Y. Now we use the argument from Subsection 286D. Because the law of Y is invariant under rotations about t, and by obvious scaling properties, we see that the distribution of the ratio — I tanq = IlY 116 — T Til II is the same as the distribution of the corresponding ratio if Y were chosen on the sphere of radius 1. But, by Archimedes' Theorem (the one on his tombstone), U := cos' \u0000 rI has the U[0, 1] distribution, so, if A = tan2 F, then 1 = I cos F I = U \" U[0,1]. N/1 + A Hence, we can find a 95% Confidence Region for 0, namely, 110 — tobs 1 12 \u0000 K liyobs — tobs 112 < \u0000 ' where (1 + KO = 5%, so that K = 399. This agrees exactly with the F-distribution approach, as you will be asked to check later. The value of K is exactly twice the vi = 2, it2 = 1 point in the F-table, p518. We have found a 95% Confidence Region 110 \u0000 tobs il < 1/133 iyV3s + y :.11)s + y.,bs — 71, \u0000 i = 7r, \u0000 (G1) for 0, stating that 0 lies in a 2-dimensional disc centre t°bs with the described radius. We could calculate on the computer a number bobs such that we can be 95% confident that simultaneously for k = 1,2,3, we have le k — ckc bs 1 < bobs 294 \u0000 8: Linear Models, ANOVA, etc Direction of 1 Figure G(1): Illustration for triangle problem thus obtaining simultaneous CIs, something easier to understand (though in many ways not as good) as our CR. Because the CR at (G1) is surprisingly large, the real picture being suggested by the bottom picture in Figure G(i), I checked my calculation by simulation as follows: /*triangle.c To compile: \u0000 cc triangle.c RNG.o -o triangle -lm */ #include \"RNG.h\" int main()-( long int i, kount=0, N=100000; double y1,y2,y3,t1,t2,t3,c,fract; setseeds(); ) X1 X2 x n 8.2. The Orthonormality Principle and the F-test \u0000 295 for (i=1; i<=N; i++){ yl = aGauss(); y2 = aGauss(); y3 = aGauss(); c = (yl + y2 + y3)/3.0; t1 = y1 - c; t2 = y2 - c, t3 = y3 - c; if (t1*t1 + t2*t2 + t3*t3 < 1197*c*c) kount++; } fract = kount/((float) N); printf(\"\\nfraction was %8.6f\\n\\n\", fract); return 0; } Ga. Individual CIs. The RV (3/4(T1 — 61)/(Y1 + Y2 + Y3 — 71-) has the standard Cauchy distribution. Prove this later, when you have more technology. 8.2 The Orthonormality Principle and the F-test Much of the linear algebra in this section will be familiar to you, so I only give a brief resume of it. Where things may well be new (for example, perpendicular projections, and in a later section, tensor products), I move carefully through the theory. A. R' and its subspaces; span, spanning set. We work with the standard representation of points in Rn, a point of Rn being represented by a column vector X = (x1, x2, • • • , Xn7 = the superscript T standing for 'transposed'. Previously, we have written row vectors for vectors of data for typographical reasons. Henceforth, data vectors are always column vectors so Y = (Y1, Y2, ... , Yn ) T . illn is, of course, a vector space over R. We can add vectors, multiply a vector x on the left by a scalar c to form cx, and we have rules x+y=y+x, x+(y+z)=(x+y)+z, (ci + c2)x = cix + c2x, c(x + y) = cx + cy, etc. A subspace U of lEr is a subset which is also a vector space over R in its own right. It is enough that if x and y are in U and c E R, then cx and x + y are in U. As a picture, a line through the origin is a `1-dimensional' subspace of R3, a plane through the origin is a 2- dimensional subspace. The single point 0 is a 0-dimensional subspace. What 'dimension' is, will now be recalled. 296 \u0000 8: Linear Models, ANOVA, etc The span of a set of vectors is the smallest subspace of Rn containing each vector in the given set. If z(1), z(2), z(c) are vectors in IV, we write [z(1), z(2), \u0000 z(k)] for (2) the space spanned by {z(1), z (2) \u0000}, that is the space of all vectors of the form c1z(1) + c2z(2) + • • + ckz(k). , ) 2 (I We say that {z(1), z(2) , \u0000 is a spanning set for a subspace U if [z(i), z(2) , \u0000 z(k)] = U. The space [U, V] spanned by two subspaces U and V, the smallest subspace with U U V as a subset, is U + V, the set of all vectors of the form u + v where u E U and V E V. If U and V are subspaces of Rn, then U fl V is a subspace. B. Linear Dependence; basis; dimension; coordinates. \u0000 Vectors x(1), x(2), .. , x( k) are called Linearly Dependent (LD) if there exist constants (elements of R) c1 , c2, \u0000 , ck, not all zero, such that c1x (1) + • • + ckx(k) -= 0; otherwise, x(1), x(2), ... , x(k) are called Linearly Independent (LI). A basis for a subspace U is an LI spanning set for U: it is important however that we regard a basis as an ordered set. Every subspace U of Rn has a basis; and (non-trivial Fact) any two bases for U have the same number of elements, this number being the dimension of U. If {u(1), u(2), u(m)} is a basis for U (so that dim(U) = m), and u E U, then the ordered set of numbers c1 , c2, \u0000 , cm such that \u0000 u = c1u (1) + c2u(2) + • • • + cmu (m) \u0000 (131) is uniquely specified: we say that c1, c2, . . . , cm, are the coordinates of u relative to the basis {u(1), u(2), \u0000 , u(m)} of U. Let e(i ) := (0, --• , 0,1, 0, • • • 0)T, the 1 being in the jth place. The vectors {e(1), e(2), \u0000 e(n)} form the standard basis for W. C. Fact: the 'Extension to Basis' Principle Suppose that U and W are subspaces of ]Ere with U C W. Let m := dim(U) and s := dim(W). Then given any basis {u(1), u(2), \u0000 u(m) } for U, we can find vectors u(m+1) , \u0000 , 11(n) such that , {u(1) , u(2), \u0000 u(2) u(s)1 is a basis for W and fu(1) , \u0000 u(n) f is a basis for Ir. D. Inner (or scalar) products; orthonormal bases; etc. The space Rn is equipped with the standard inner (or scalar) product (x, y) defined by \u0000 (x, y) := xTy = \u0000 xi \u0000 (D1) i=1 8.2. The Orthonormality Principle and the F-test \u0000 297 Note that (y, x) = (x, y) and that (x, y z) = (x, y) (x, z). We define the length or norm Ilx11 of x via (the fact that it is non-negative and) 1142 := (x, x) = XTX = \u0000 (D2) i=1 We say that x and y are perpendicular (or orthogonal) and write x 1 y if (x, y) = 0. Pythagoras's Theorem states that (as you can easily prove) if x 1 y then llx ± y112 =114 2 + liY112• A basis { u(1), u(2), \u0000 , u07-01 for a subspace U of Ir is called an orthonormal basis for U if Ilu(i)11= 1, \u0000 (u(i), u(j)) = 0 (i The vectors in an orthonormal basis are therefore normalized in that each has unit length, , \u0000 , and they are perpendicular (orthogonal) to one another. Of course, f e(i) e(2) \u0000 e(' ) } is an orthonormal basis for Rn. Fact: Every subspace U q \u0000 possesses an orthonounal basis; and the obvious `orthonormal' analogue of the Extension-to-Basis Principle holds: replace 'basis' in Fact 296C by `orthonormal basis'. The techniques for constructing an orthonormal basis for U and extending it to an orthonormal basis for Rn (`Gram-Schmidt orthogonalization') will develop naturally when needed. Just accept the Fact for now. If {u(1), u(2), \u0000 , u(m)} is an orthonormal basis for a subspace U of lir , then, in the coordinate expression at 296(B1), we have U (D3) Exercise: Prove this. E. Orthonormality Principle; SN(U). We can move quickly to a form of the Orthonormality Principle if we assume the natural extension of what we did about characteristic functions in Section 5.5. Recall that if X is an Ek-valued RV, then the distribution of X is uniquely determined by the characteristic function R 9 a E e'x and that in particular, X has the standard normal N(0, 1) distribution if (and only if) E eica- = e 2 a2 for a E It is likewise true that if C is an Rm-valued RV, then the distribution of C is determined by the 'multivariate' characteristic function Irn 9 a H E ei(c''c) , 298 \u0000 8: Linear Models, ANOVA, etc (•, •) here denoting the inner product in Rni of course. In particular, Cl , C2, \u0000 , Cm are IID each N(0,1) if and only if \u0000 E ei(a 'c) = e- 2114a112, \u0000 a E Rm . The 'only if' result follows from the calculation \u0000 E ei(c\" ) = E ei(a1c1± —+a—cm) — E (eic1\"1 \u0000 eiamCm) = (E eia' c') - • • (E eiamcm) (by independence) = \u0000 = E eAlla112; and the 'if' part now follows from the uniqueness result described above. We say that an RV X has the statidard nor a1 distribution an a subspace U of \u0000 and write X SN(U) if X \u0000 in U and for every u E U, E et(u,x) \u0000 E el [111112. \u0000 (El) We need to prove that there is such a distribution and that it is unique. Let {u(1>, u(2>, \u0000 , u(m)} be any orthonormal basis for U. Let a l , ce2, • , a, be the coordinates of a vector u in U relative to this basis, and let C1, C2, \u0000 , Cm be the coordinates of X relative to this basis. Then (Exercise. \u0000 Use the orthonormality of the basis to check this) (u, X) u :-= (u, X)R. = (a, \u0000 , \u0000 11u112 = 11(1112, where subscripts indicate the spaces in which inner products are taken. Hence, property (El) amounts to E ei(a 'c) = e-211a1127 in other words, to the statement that C1, C2, \u0000 , Cm, are IID each N(0,1). The existence and uniqueness of the SN(U) distribution follows. ►►► Ea. Ortbonormality Principle, 1: 1 emphasize that a IT-valued RV X has the SN(U) distribution if and only if the coordinates of X relative to SOME orthonormal basis for U are HD each N(0,1), and then the coordinates of X relative to ANY orthonormal basis for U are 11D each N(0, 1), Note that if X — SN(U), then 11X112 Xm2• 8.2. The Orthonormality Principle and the F-test \u0000 299 Eb. Note on the non-central x2 distribution. Recall the non-central x2 distribution from 153Ja. Suppose that X — SN(Rn) and that a E Ill, a 0. Then we can choose u (2), \u0000 u(n)} of Rn with \u0000 _ an orthonormal basis /I = \u0000 Ilall —1a. Let Y be the vector of coordinates of X relative to U. Then the coordinates of X + a relative to U form the vector Z, where Zi = Y1 + Hall and Zk = Yk for k > 2. Since Y1, Y2, Yn are IID each N(0, 1), the distribution of 11X + all 2 = IIZII2 depends on a only via Hall. F. Perpendicular projections. I use the term 'perpendicular' projection rather than 'orthogonal' projection because a non-trivial 'orthogonal' projection is not an `orthogonal transformation'. Let U be a subspace of Rn . For x in Rn, we write Pux for the foot of the perpendicular from x to U, so that Pux E U and x — Pu x 1 u for every u E U. How do we know that there is such a vector, and that it is unique? Fa. Exercise. Check that if {u(1), u(2), \u0000 , u(m)} is any orthonormal basis for U, extended to an orthonormal basis {u(1), u(2) \u0000 u(n)., } of Rn , then In Pux := E(u(k),x)u(k) j=1 does the trick. Check also that Pu x is the closest point of U to x. Figure F(i) illustrates Exercise Fa where n = 3 and U = [11(1),u(2)], and Pk is projection onto [u(k)]. \u0000 ❑ x 0 \u0000 P1x u(1) Figure F(i): To illustrate perpendicular projections The space U-L. spanned by u(m+i) , \u0000 , u(n) in the notation of the exercise, is the space of all vectors perpendicular to U. We have PU := PUL = I — Pu, where I is the identity transformation on Rn . 300 \u0000 8: Linear Models, ANOVA, etc ►►► G. Theorem: Orthonormality Principle for Projections. Suppose that U and W are subspaces of R\" with U C W. Set m dim(U), s dim(W). We have Pw Ptr =P,, w here Z \u0000 f1 W. If G has the SN(RU) distribution, then PuG, (Pw — Pu)G and (I Pi,v)G are independent variables, and PETG SN(U), (Pty — Pu)G = PSG N SN(Z), (1 — Pw)G P G SN(W1). We have G = PSG + (Pw-Pu) + (I- Pw)G PLTG + \u0000 PSG \u0000 + \u0000 Pti-,G, FIG [T2 = 1iPuG1 2 + \u0000 flPzG112 \u0000 + 114GP r- \u0000 Xm \u0000 +X,,n \u0000 + \u0000 x4--8 2 Suppose that W ----- U + V, where U n V = {0}. Then Z is 'V corrected for U'. In the 'orthogonal factor' situation in which V is a subspace orthogonal to U (in that u 1 v whenever u E U and v E V), we have Z = V. Figure 314D(ii) will illustrate a special case. Ga. Important Exercise. Prove the above theorem by letting {u(1) , u(2), \u0000 u(n)} be an orthonormal basis for Rn such that {u(1), u(2) , . , u(\") } is a basis for U and fu('), u(2), . , u(8)} is a basis for W. Since the variables (u(z), G) (1 < i < n) are lID each N(0, 1) . Gb. Exercise. Prove that if U and V are subspaces of IV , then the statements (a) Pu±v = Pu + Pv, (b)U J_ V, (c) PuPv = 0 = PvPu, are equivalent. Show that if Pu Pv = 0, then PvPu = 0. \u0000 8.2. The Orthonormality Principle and the F-test \u0000 301 H. The Classical F - test. Here's the result which dominated Statistics for a long period. We shall see later how it adapts to yield CIs. Note that another way of saying that 171, 172, • ,Yn are IID, with Y., \u0000 N(pi, o-2), is to say that Y = p, o-G, where p = . • • , tin )T and G SN(Rn). 4/1, it2, In the following theorem, 1/0 actually implies HA. But this does not really matter. You have to remember that we know that Ho is not exactly true: we are only testing whether or not there is strong evidence that Ho is not an acceptable approximation to the truth. Of course, CRs will provide the correct language for simple situations. ►►► Ha. Theore the F - test for Linear Models (Fisher, ...). Suppose that Y = µ + crG, where p E and G SN( ). Let U and W be subspaces of with U C W, dim(U) m, dim(W) = s. Let Z \u0000 U -L fl W, as before. Then the size 77 Likelihood-Ratio Test of E U against it E W, that is, of Ho : E \u0000 0- > 0 against IIA:ttE W, a > 0, takes the form Reject Ho if R := \u0000 Y112 1 ( 8 — in) 114Y 112 I (n .5) > c, where P(F > c) = n if F F,„,,,„_ s. For every /to E U, the probability of rejecting HO if the true value of p, is po is exactly n. If n is large, then, if Ho is true, the deviance Dev(Y) \u0000 2 \u0000 R(Y) has approximately the XI_ distribution. Proof. With .e := lnlhd as usual, we have ilY —A112 \u0000 gi-L7 a; Y) = \u0000 n ln o- — \u0000 ln(27). 2o-2 The maximum likelihood when p E U is achieved when p = Pu y and \u0000 ( Ily PuYI12 \u0000 n ln o-) = 0, so, o-2 = ao- \u0000 2a2 ri-111Y - PuY112. 302 \u0000 8: Linear Models, ANOVA, etc Thus, lnmlhd(Ho;Y) = - in - in In (n- '11Y - PuY112) - in ln(27r). Similarly, In mlhd(HA; Y) = -in - In In (n-i llY -PwY112) - in ln(27r). Thus, dev(y) = 21n1r(y) = n In 11 2 ) 3, - PuYII2 = nln (1 + 11PzY11 IIY - PwY112 \u0000 HP' 112, ' whence the form of the LR Test is obvious. Before reading the next sentences, revise the definition of the F-distribution from Subsection 252F. If Ho is true, then a E U and Pz IL = Pii-,p = 0, whence 11PzG112/(s - m) PzY = aPz G, P -14LTY = o-Pfk,G, R = iiPili GII 2 I (n - s) . The 'F' result now follows since IIPzGII2 and 114G112 are independent and distributed as xs2- ni and xn2 _., respectively. If n is large, as we now assume that it is, and HA is true, then no-2/1I P1+717 112 \"' nIx7,2, is reasonably close to 1 with high probability, so, since ln(1 + A) ',-.-_, A for A small, if HA is true, then Dev(Y) = 21n LR(Y) '-'-' a-211PzY112• \u0000 (H1) Now, if Ho is true, then Pz it = 0, so PzY = aPzG and hence if Ho is true, then Dev(Y) r---, IIPzGII2 - x.9- m; \u0000 (H2) and this allows us to assign approximate p-values if we so wish. It will be explained later that this x2 result is part of a very general principle. Note that s - m is the dimension of HA minus the dimension of Ho in that HA corresponds to an (s + 1)-dimensional region of parameter space and Ho to an (m + 1)-dimensional one, the '1' arising from the a. ❑ I. The 'sharp-hypothesis' nature of the F-test. It is important to realize that the Null Hypothesis 1/0 in the F-test is a sharp hypothesis stating that it lies in a lower-dimensional subspace. All the usual problems with sharp hypotheses reappear: 1/0 is false, and a large sample would inevitably lead to its rejection at the 1% level; we must consider the natural scale at which we examine things, etc. As usual, Confidence Intervals (and simultaneous CIs when appropriate) provide a much better language for analyzing things. However, the geometry of the F-test is very relevant to the obtaining of CIs. 8.2. The Orthonormality Principle and the F-test \u0000 303 J. The F- test and choice of model. As explained in Subsection 236J, in Statistics, we seek acceptably good models which are parsimonious in that they involve few unknown parameters. We might wish to protect the Null Hypothesis, rejecting it only if there is rather strong evidence against it as measured by the deviance dev(y°1'). This is the point of view of Hypothesis Testing, where, when n is large, as we assume that it is, we use (H2) to assign a p-value and to determine the size of the test, both being calculated on the assumption that 1/0 is true. Ja. AIC for this situation. \u0000 Even though you will have gathered from the discussion in Subsection 236J that I am somewhat uneasy about AIC because of its 'sharp-hypothesis' problems (amongst other things), the fairly wide use of AIC in practice persuades me to give the relevant geometry in this context. We saw before 302(H1) that, since n is large, we essentially know u 2, and so we shall assume that u2 is a known constant, not an unknown parameter. Suppose that X is independent of Y with the same law as Y. Exactly as in Subsection 236J, the Decision-Theory and Kullback—Leibler approaches lead us to say that MA (corresponding to HA) is better than Mo (corresponding to H0) on average by an amount a -2E IIIX — PuYII2 — IIX — PwY1121 , the CF -2 appearing automatically in the Kullback—Leibler approach and being a natural scaling in the Decision-Theory approach. But our geometry tells us that IIX — PuYII 2 — IIX — POLTII2 = 2(X, PzY) — II-Pill- I12, and, since X is independent of Y, we have E(X,PzY) = (EX,EPzY) = (it,Pit) = liPzbaii 2. Also, EIIPzYII 2 = E IIPzil, + PzGii2 = 11PzI/112 + 0-2(s — m), whence a-2E {PC — PuYii 2 — iiX —PwIr ii2} = a-21iPzki 2 — (s —m). However, from 302(H1), EDev(Y) = a-2 11Pz 1-1112 + (s — m). Thus, to get 'on average' agreement with the Decision—Theory and Kullback- Leibler ideas, we need to subtract 2(s — m) from the deviance when comparing models MA and Mo, exactly in agreement with AIC. 304 \u0000 8: Linear Models, ANOVA, etc If we wish to compare a model Mi. : 1.1 E U1 with M2 : p E U2, where U1 and U2 are not nested, we can compare each with M : p E W where W = [U1, U2] is the space spanned by U1, U2. In this way, provided we believe the AIC philosophy, we can use AIC to compare Mi. and M2 directly. 8.3 Five basic models: the Mathematics The models at which we look are 'Sampling from a normal distribution', Linear Regression, two forms of ANOVA, and (at the end of the section) the General Linear Model. First, we look only at the Mathematics of the Frequentist theory, the most elegant Maths in Stats, which deserves not to be interrupted. Then we look at the Bayesian approach and at how WinBUGS allows us to study more realistic models. In the important next section, we give some consideration to goodness-of-fit and robustness questions, and how to extend to more realistic models. But first, the Mathematics. Advice: Please reread Subsection 28C before continuing. ■■ A. Important notation. Quadratic forms. For vectors x, y E IV and Rn-valued RVs X and Y, we again write Tb q(x, \u0000 7) \u0000 #) \u0000 (2(x, Y) the 'upper-case' notation being used for Pre-Statistics as usual. Check that q(x,y) = ( E ziyi) — nx v. A special vector. Recall that we define I := (1, 1, \u0000 i)T Re distributions. If we write R o -2x,2 , we mean that o--2R \u0000 v . and we use obvious extensions of this idea. We define P (x2, E [c_, c+]) := P(Z E [c_, c±]) where Z x /2, and use obvious extensions of this idea too. Upper percentage points. Let FT (77) and Itl,;(77) be the numbers such that > F;,3(77)} = y, \u0000 P{ItvI > itii*,(n)} = Th 8.3. Five basic models: the Mathematics \u0000 305 that is, such that PINT > F:.`,8(77)} =- 77, \u0000 P{ITI > itr,;(70 = 77, where W N F,,8 and T N t,. Because (see Subsection 253G) \u0000 F 1,1/ \u0000 t 2v) we have FI,„(77) = Itl:(77)2. Intervals. For intervals, we use shorthands: \u0000 c+[a,b] := [c+ a,c+b], \u0000 c±45 := [c— 8,c+ 5], c[a,b] := [ca, cb] (c > 0). (Al) ►►► B. The ‘Normal Sampling' Theorem. Let Y1, Y2, ... , Y be 11D each NUL, a where it and cy2 are unknown parameters. Then ICI RSS Q(Y,Y) \u0000 (Yk — Y and 2 2 Xn. 1, Y and Q(Y ,Y) are independent. The residuals Yk Y are the components of a vector with the SN(W L) distribution, W-1- being an (rt — 1)-dimensional subspace of R described below. Note. The independence of Y and Q(Y, Y) actually characterizes normal distributions. Note. RSS always stands for the Residual Sum of Squares appropriate to the model currently being used. \u0000 ❑ We saw motivation and pictures relevant to the theorem in Subsection 286D. The theorem is one of the mainstays of classical Statistics. Before proving it, we look at some famous consequences. Define RSS an-1(Y) \u0000 n — 1' 306 \u0000 8: Linear Models, ANOVA, etc the Estimator of Q2 based on the Residuals Yk — Y, with, of course, 8-92_1(Y) the non-negative square root of 'cr,n2_1(Y). This Estimator takes the standard form ANOVA Estimator of 0-2 Residual Sum of Squares Residual degrees of freedom • Then (see the definition of the t distributions at Subsection 253G) n \u0000 - pc) ern-1(Y) so that, if P(tn_i E [b_, b+]) = C%, then (y obs) Yobs + \u0000 \\ \u0000 [ b+,—b_] is a C% CI for p, as (because of the symmetry of t) is a n_i (y obs) Yobs + \u0000 [b_,b+]. If P (xn2_1 E [c_, c+]) = C%, then (yobs y obs) [c+-1 3 ci l] is a C% CI for o-2. The 'simultaneous CI' problem is here best dealt with by saying that if [p_, /1+] is a 97.5% CI for p and [Q_, a+] is a 97.5% CI for a, then we can be at least 95% confident that (A) cr) \u0000 114] X \u0000 ad-1- Do note that the exact degree of confidence is not 0.9752 because the left-hand side of (B1) is not independent of Q(Y ,Y). Proof of Theorem 305B. We have, in the notation of Theorems 301Ha (for testing p = 0 against p 0) and 300G, Y = p1 + o-G, U = {0}, W = [1], Z = [1]. The likelihood function is —11 114270 \u0000 IIY — 111112 2 \u0000 2a2 \u0000 nines, and for a given y, this is maximized when pi = Prny, a2 -= n-111Y Pody112. (Don't worry about n rather than n — 1.) Now, an orthonormal basis for Z is provided by the single vector z = n- 11, and PzY = (z,Y)z = Yl. tn-11 \u0000 (B1) 8.3. Five basic models: the Mathematics \u0000 307 But, since Y = \u0000 o-G and Pz l =1, PzY \u0000 o-PzG, so nlo--1 — z = Pz G, whence, as we already know, n2a-1 (Y — \u0000 — N(0,1). The point here is that Pz G has N(0, 1) component relative to the unit vector z. We have o-P-14;G = \u0000 = 132LY = (I — Pz )Y = Y — Yl, so that, since P-viLi G SN(W ±), X2n—i — 11-4G112 = cr-2114Y112 = a-211Y — 171112 = cr-2 Q(Y,Y)- The desired independence property follows because Pz G and Pcil-,G are independent. Do note that Pv4Y, which has the SN(W± ) distribution, has ith component Y. — Y, the ith Residual. \u0000 ❑ The Parallel-Axis consequence ofilY112 = IIPwY112 + HP1+717112, namely Y) (E y 22) ny2 is reflected in the ANOVA table, Table B(i), for testing p = 0 against p 0 in this case Source of variation Sum of Squares Degrees of freedom Variance Estimator Mean IIP[111711 2 \u0000 nY2 1 nY2 1 Residual IIPI YII 2 = Q(Y, ril n — 1 Q(Y, Y)/(n — 1) Total \u0000 n Table B(i): ANOVA for 'normal sampling' Ba. Exercise. Check that q(x, y) = (x — x1, y — y1) = (x — x1, y) = (x, y) — y. Bb. Exercise. \u0000 Check that Minitab found the correct symmetric 95% CI (-0.456, 0.942) for p from the data at 270(D1). You will find t-tables on page 516. ► Bc. Exercise. After values es , yps , \u0000 y.bs of IID RVs V1, - -Y 2, \u0000 , Y, have been observed, a new observation Ynew is to be made. Describe the natural two-sided 95% CI for Ynew • 308 \u0000 8: Linear Models, ANOVA, etc In Hypothesis Testing of Ho : µ = 0 against HA : 11 0, we would reject Ho at size n if the variance estimate arising from the mean is at least FL„_1(71) times the variance estimate based on the residuals, that is, if 2 \u0000 q (y obs, y obs) nYobs > FI,n_1(71) \u0000 — 1) Because of 305(A1), this tallies with the fact that we reject 1/0 if the symmetric 100(1 — 77)% CI forµ does not contain 0. Bd. A Zeus—Tyche double act. \u0000 The pair (Y, Q(Y, Y)) is sufficient for the pair (it, a2). Zeus chooses Y and Q(Y, Y) independently, with Y \u0000 N(p,o-2 In) and Q(Y,Y) 0-2xn2 _1. He tells Tyche the values of Y and Q(Y, Y). Tyche then knows that Y must lie on the (n-1)-dimensional sphere Sn_1 centre Y1 and with radius squared equal to Q(Y, Y). She knows that Y also lies on the hyperplane specified by the fact that Y — Y 1 is perpendicular to 1. Thus, she knows that Y lies on the (n — 2)-dimensional sphere 8n-2 where the hyperplane cuts Sn_1, and she chooses Y uniformly on Sn-2. We saw parts of this earlier. ► Be. Exercise. In a real experiment to compare the amount of wear occurring in shoes with two different types of sole, each of 10 boys was given a pair of shoes, one with Material A, the other with Material B, which of 'left' and 'right' being of which material having been chosen randomly. The results were as in Table B(ii): boy \u0000 1 \u0000 2 \u0000 3 \u0000 4 \u0000 5 \u0000 6 \u0000 7 \u0000 8 \u0000 9 \u0000 10 A 13.2 8.2 10.9 14.3 10.7 6.6 9.5 10.8 8.8 13.3 B 14.0 8.8 11.2 14.2 11.8 6.4 9.8 11.3 9.3 13.6 Table B(ii): Amount of wear for shoes trial What is your model? (Note: Logarithms would feature in mine.) Is there significant evidence that one type of material is better than the other? And if so, use a CI to quantify the difference. ► C. Behrens—Fisher problem: a key Bayesian-Frequentist conflict. The Behrens—Fisher problem, which has received much discussion in the literature, presents a simple and important context which emphasizes differences between Bayesian and Frequentist philosophies. Suppose that Xi, X2, ... X m, are IID, each N(it, Y2, • .. , Yr are HD, each N(AYI ay2), the Variables X1, X2, \u0000 Xm, Yi, Y2, • • • , YT being independent. The two 8.3. Five basic models: the Mathematics \u0000 309 samples could be thought of as obtained via different, independent, experiments. We are interested in Confidence Intervals for itx — my. Write /3„,_1(X) := 5-ni_ i (X)/.1Fn and Br _i(Y) := O-T—i(Y)/-‘g, so that Bm_i (X) is the usual Estimator of the SD of X. We know that in Frequentist theory, X — Tni_i (X) := Bm_1(x) \u0000 tm-1, Y — fly Tr _i(Y) := Br-1(Y) For Bayesian theory with the Frequentist prior 7r(p x , \u0000 o-y) oc a;l o-y-1 which is the reference prior (see Subsection 379N), we have corresponding results obs (µx x \\ + bra (x ob s)t m_i, \u0000 (iiv I y obs Xobs ) \u0000 Yobs \u0000 r_1(yobs)tr_i• (C1) Recall that in Bayesian theory, x°1's is the rn-vector of numbers obtained in the one and only experiment actually performed in the real world, and (xobs) is a number calculated from xths. Recall too that our Bayesian will regard px and py after the two independent experiments as independent Variables with distributions as at (C1). It therefore makes sense to a Bayesian to calculate the function am_1,r-1(•, •) on (0, oo)2 (2) such that if T„,(1) 1 — tni_ i and Tr _ i tr _1 are independent RVs, then vTr 2)11 < am-1,r-1(u, v)) = 95% \u0000 (C2) and to assert that post {1(//x — xobs) — (I-Ly — Yobs )1 < a m _ 1,r-1 (brrt-1 (x obs) 3 br (yobs )) \u0000 95%. Hence, (Robs Yobs) ± am-1,r-1 (bm_1(X), br-1 (yobs)) gives a natural 95% Bayesian Confidence Interval for az — However, this will be a 95% Frequentist Confidence Interval if and only if we have, for all \u0000 4y, o_y2 P{ (X /IX) - \u0000 - 1101 am-1,r-1(Brn-1(X)) \u0000 (Y))} = 95%, (??) (C3) a statement which, if true as Mathematics, a Frequentist would interpret in LTRF terms. The problem is that (C3) is false, and the fundamental reason is that Bm_i (X) is not independent of Tm_i (X). The left-hand side of (C3) is IF°{1/3,,_ (X)T,,_ I (X) — 13,_1(Y)Tr_1(Y)1 < arn_i,r_i (Bm,_i (X), Br_i (Y)) 1. If the Variables Bm, _1(X) ,Tm—i(X) Br —1 (Y), \u0000 (Y) were independent, then we could deduce (C3) from (C2) by conditioning on the values u, v of An_i (X), Br-1(Y). But the listed variables are not independent. This in itself does not actually disprove (C3), but it makes that result very doubtful. And it would be absolutely amazing if (C3) were to hold for every value of the ratio az I o-y. 310 \u0000 8: Linear Models, ANOVA, etc Let us restrict attention to the case where, as we now assume, m = 2, r = 2, so that we can easily work out everything in detail. The t1 distribution is the standard Cauchy distribution (Subsection 253G), so, by 167B and the 'Independence means Multiply' property for CFs, we have, for u, v > 0, E exp{ia(uTP ) — vT12))} = e—(u+v)Ial so that uTP) — \u0000 (u + v)t i . Hence, for u, v > 0, a l (u, v) = (u + v)K, where IP(Iti I < K) = 95%, so that K = (2/7r) tan-1(0.95). Now, X, Y, Bi (X), Bl (Y) are independent. We have (X — px) — (Y — py) — N(0, 1(o- + Next, B1 (X) — IN(0, lo-D I, B1(Y) \u0000 IN(0, uy2)I, so that the left-hand side of 309(C3) has the form P(INI < (ILI + IMDK), where L, M, N are independent with N ^ N(0,1 (<7 + ay2)), L N(0, \u0000 M N(0, o-y). But (check!) N/(L + M) t1, so that ]P(INI < IL + MIK) = 95%. Thus, since IL + MI < ILI + IMI and L and M can have different signs, we see that for this 'm = r = 2' case, the probability on the left-hand side of 309(C3) is always strictly greater than 95% whatever the values of the unknown parameters. That in this 'm = r = 2' case the Frequentist always has higher degree of confidence in the Bayesian Confidence Interval than the Bayesian has degree of belief may not seem to 'balance out'. However, the whole point is that this example is emphasizing that Bayesian degree of belief and Frequentist (or Mathematical) probability are not the same thing, even for situations where there is a universally agreed 'vague prior density'. Well, the prior we have discussed would be universally agreed if we assumed that the two variances 'had nothing to do with each other', reasonable in some models, not at all reasonable in others. Let us assume for now that the prior is reasonable. Bayesian: \"I have based my Credible Interval on the one and only actual performance of the experiment. I do not wish to consider long-term relative frequencies over hypothetical experiments which will never be performed.\" 8.3. Five basic models: the Mathematics \u0000 311 Frequentist: \"But you cannot escape the fact that if those experiments were to be performed and you always used that same strategy for getting your Credible Interval, then the long-term proportion of times when your interval would contain the true value of px — my would not be 95%.\" To be sure, there was a long period when furious arguments between Bayesians and Frequentists did Statistics no favours. These days, the politically correct thing is to ignore the controversy. But there have inevitably been some situations in this book (of which this is the most serious) when the philosophies disagree and even the numbers disagree. The Behrens—Fisher example will suggest to you many other important analogous situations. I have generally left you to form your own opinions. In writing most sections of this book (especially the Frequentist ones!), I have felt strongly Bayesian. However, in other sections, especially those where there is conflict, my sympathies have tended to lie with the Frequentist school. A 'schizophrenic' attitude is quite common. Ca. The Fiducial school. I should add that there is a third school of Statistics, the Fiducial school, based on ideas introduced by Fisher and studied deeply over many years by Barnard. I am regarding Fiducial as `Bayesian with Frequentist prior', thereby undoubtedly again offending everyone. Cb. The Welch Statistic. Welch discovered a method of getting approximate CIs for the Behrens—Fisher problem which works well for most situations met in practice. This is used by Minitab and other packages. ► Cc. Exercise: the pooled-variance model. The theory is easy and uncontroversial if we assume a model \u0000 , X2 \u0000 Xm are HD, each N(p x , o-2), \u0000 Y1, Y2 \u0000 Yr are IID, each N(//y, a2), the Variables X1, X2, \u0000 X m Y1, Y2, . , Yr being independent. The point now is that the two samples are assumed to have the same variance. Check that a Confidence Interval for µl — iL2 will have the form Yobs — gobs Itvl * (0.05)&4 1 + m where v = (m — 1) (r — 1) and x0b S xObS \u0000 (yObS y obs) := \u0000 . zi Crosscheck with the F-test. Explain why use of this model would be so disastrously wrong if applied to the 'shoes' data at Exercise 308Be. 312 \u0000 8: Linear Models, ANOVA, etc D. Linear Regression. Motivation and pictures were provided in Subsection 288E. As explained there, the model is that we observe the response Yk (1 < k < n) to a deterministic, experimentally-set, `covariate' xk, and use the model Yk = a ± /3 xk ± Ek = kt + P(xk — Y) ± Elc) where a, 0, p are unknown parameters related of course by the fact that it = a + 0 Y, and where the ek are IID each N(0, 0-2). We return to the general case. Some computer programs use the Yk = a + fiXk ± Ek formulation. This is often rather silly for commonsense reasons. The value a is the mean value of Y when x = 0. But the x-value 0 may be far away from the values featuring in the experiment. Linearity may well have completely broken down. Moreover, there will obviously be greater uncertainty about the value of a than of the mean it of Y when x = Y. Hence, for commonsense reasons, we use the model Yk = ii, + )3(xk — Y) + ek. \u0000 (DI) We shall see that this is the right formulation in the mathematics too. There are situations where the value of a matters, and we study how to deal with the pair (a, 0) for such cases. You are reminded that in the next section, we shall consider whether the model tallies reasonably well with the data. ►► Da. Theorem. Take the model (DI), where the ek are HD each N(0, 0-2). Then a 2\\ Y ti N (//, 71 ' Q(x, 2 B := \u0000 ,-, N (0, \u0000 q(x,x) Y) \u0000 q(x,x)) ' RSS := n (Yk — Y — B(xk — 0 2 k=1 2 2 ^' a Xn-2- Moreover, Y, B and RSS are independent. The value Yk — Y — B(xk — Y) is now the kth Residual. We have an ANOVA table, Table D(i). 8.3. Five basic models: the Mathematics \u0000 313 Source of variation Sum of Squares Degrees of freedom Variance Estimator Mean /2 n \u0000 2 1 n 2 Slope ,3 q(x, x)B2 1 q(x, x)B2 Residual RSS n — 2 RSS/(n — 2) Total 11Y112 n Table D(i): ANOVA for Linear Regression ► Db. Exercise. Assuming the theorem, check that if ^ 2 \u0000 RSS a n-2 ( X ) Y ) •.= — 2' — to-2' an-2 (x, Y) tn-2 • an-2 (X, IT ) then q(x, x)1B (These variables are not independent of course.) Check that the symmetric 100(1 — 77)% CI for /3 fails to contain 0 if and only if we would reject the hypothesis /3 = 0 against 0 at size n in the sense that q(x, x)B2 \u0000 (n) nRSS2 Proof of Theorem 312Da. Figure D(ii) shows the 3-dimensional subspace of Rn spanned by Y, 1, x, or equally, and it is much better to think this way, by Y, 1, z, where z = Pj x = x — Y1. Then we have a situation: U = [1], Z = [z], U _L Z, W := U + Z = [1, z] = [1, x]. In vector notation, we have Y = µl + /3(x — Y1) + o-G, where G \u0000 SN(Rn). An orthonormal basis for Z is {e}, where e = q(x, \u0000 z. As before, an orthonormal basis for U is {u}, where u = n 21. Using PuY = (u, Y)u, Pul = 1, Puz = 0, etc, we have 71-1 PuY = Yu = \u0000 o-PuG, q(x,x)-1 PzY = Be = 13e ± PresY = \u0000 = 0-14G, IIP YII 2 = RSS. The theorem now follows immediately from the Orthonormality Principle 300G for Projections. \u0000 ❑ 314 \u0000 8: Linear Models, ANOVA, etc Figure D(ii): Rn geometry of linear regression Note that PresY = Pfl = (I — Pu — Pz)Y = Y — Y1 — B(x — so that (PresY)k = Yk \u0000 - B(xk the kth Residual. Recall that 1PresY = PresG = a —l PfkrY\" SN(W') and that WI is an (n — 2)-dimensional subspace of Rn . ► Dc. Exercise: Predicting new observations. \u0000 Suppose that we have made our n observations Y1, Y2, ... Yn corresponding to x-values x1, x2, .. . , xn. As a Pre-Statistic, our best Estimator of the true mean pc + 0(x —Y) of an observation corresponding to x is Y + B(x - -X). Show that \u0000 Y + B(x — — {p, + 0(x — Y)} \u0000 tn-2- &n - 2 (x, Y) 71, + (;;),; 8.3. Five basic models: the Mathematics \u0000 315 Suppose now that, having made our n observations, we are going to make a new observation Ynew corresponding to a new value anew. Show that Ynew - Or + B(xne, - Y)} e\"...1 tn_2. ern _2(x, IT) {1 + + (xnqe(;, (1)2 1 This gives confidence bounds for our prediction of Ynew. Dd. Example. Minitab would do an example like this. In this case, the one depicted in Figure 289E(i), 71, = 20. MTB > set c1 \u0000 Column 1 to contain x values DATA> (0:19) \u0000 xk = k - 1 for k = 1,2, ... , 20 DATA> end MTB > set c2 \u0000 Column 2 to contain y values DATA> 1.30 2.65 0.89 1.09 1.60 1.96 1.89 1.50 2.99 2.77 DATA> 3.30 3.22 3.61 3.09 3.32 3.28 3.61 4.04 4.10 4.74 DATA> end MTB > regr c2 1 c1 \u0000 Perform the regression The regression equation is C2 = 1.17 + 0.166 C1 \u0000 Oops! Not a good formulation Predictor \u0000 Coef \u0000 Stdev \u0000 t-ratio \u0000 P Constant \u0000 1.1696 \u0000 0.2145 \u0000 5.45 \u0000 0.000 C1 \u0000 0.16610 \u0000 0.01930 \u0000 8.61 \u0000 0.000 The t - rat io and p are not very meaningful. s = 0.4977 \u0000 estimate ern-2 (x, yobs) of a Unusual Observations Obs. \u0000 C1 \u0000 C2 \u0000 Fit Stdev.Fit Residual St.Resid 2 \u0000 1.0 \u0000 2.650 \u0000 1.336 \u0000 0.198 \u0000 1.314 \u0000 2.88R R denotes an obs. with a large st. resid. My own regression program in 'C' gives: ybar its SD \u0000 b its SD 2.7475 0.1113 0.1661 0.0193 Sig \u0000 b- \u0000 b+ ybar- ybar+ 0.4977 0.1256 0.2066 2.5137 2.9813 Possible outlier(s): i = 2, x = 1.0000, y = 2.6500, crude SDs = 2.6408 316 \u0000 8: Linear Models, ANOVA, etc For my program, ybar = y and 'its SD' is &-2(x, yobs )Rrn, b = B(coact ) and its SD' is an-2(x, yobs)/Vq(x, x), Sig = n _ 2 (x7 yobs), (b , b+) is a 95% CI for /3, (ybar — , ybar+) is a 95% CI for a. Both programs draw attention to the unusual 2nd observation. More on this later. The bottom part of Figure 289E(i) is just a case of programming Exercise 314Dc. The outer curves in this case are nearly linear along the range shown, but would become clearly curved if the range of x-values were extended. All of the curves are hyperbolas, as is obvious from Exercise 314Dc. The second observation is an outlier, a point discussed in the next section. Think about how Figures 314D(ii) and 289E(i) are connected. The fact that the best regression line y m b(x -1-) with m = \u0000 B (u, act ) Yobs and b bobs minimizes the sum of squares of the residuals tallies with the fact that PwY is the nearest point of W to Y. I am playing down least-squares ideas because it is likelihood ideas which are fundamental in our approach. Here they tally with least-squares ideas because of the link between the normal distribution and Pythagoras's theorem. ► De. Leverage. The leverage of the observation (X k, yk) measures how much a change in yk would affect the fit m b(xk - at xk. We see that a change Ayk in yk would cause a change ( 1 + (xk - Y) 2) Wykq(x, x) in the fit. Here, the sum of all the leverage coefficients is the number, 2, of parameters. Note how leverage has in effect already featured in Exercise 314Dc. ► Df. Exercise on non-orthogonality. Consider a situation in which our model is Yk = + i3xk +e, Y = a1 + )3x o-G, where our concern is with the intercept a of the regression line on the y-axis. Of course, in our previous notation a = pc — 13Y, so that it is natural to use A := Y — YB as an Estimator for a. Prove that \u0000 (1 \u0000 2 EA = a, \u0000 Var(A) = o-2 \u0000 + n q(x, x) ) ' ( 1 \u0000 x 2 \u0000 2 A— a • n + q(x, x) ) \u0000 8-71_2 (X, Y) \u0000 tn-2 8.3. Five basic models: the Mathematics \u0000 317 This leads us to think that in a test of Ho : a = 0, )3 E R against HA : a E R, E R, we would reject Ho at size 77 if ( 1 \u0000 2 \u0000 1111 0)* n \u0000 q(x, x) \u0000 'an-2(X) IT) > I t :1-2 Check this out from the F-test in Theorem 301Ha by reversing the roles of 1 and x in our previous work. Show that if H = [1, x] fl [x]+, then PHY = (h, \u0000 1)h \u0000 (x, 1)x where h = 1 11h112 \u0000 (x, x) and q(x, x) 11h112 = n—x 2 n ± q(x, x) \u0000 PHY = Ah, 11PHY11 = 1A111h11- Of course, since [x] 1 H and [1] 1 Z, iiPwY112 = iiP[.]1( 112 + iiPHY112 = 11-13011( 112 + 11PzY112- We say that 11PHY112 is the Sum of Squares for intercept corrected for (or adjusted for) slope, that 11P[x] Y112 is the unadjusted SS for slope, that 11P[1] Y112 is the unadjusted SS for intercept. As we know, IIP[11Y112 is the Sum of Squares appropriate for testing a = 0 in the model Yk = a + Ek. ► E. Non-parametric Statistics; distribution-free tests. We consider just one example. ► Ea. Exercise: Spearman's 'Rank-Correlation Coefficient'. Let the components of Z = (Z1, Z2, \u0000 , Zn )T be a random permutation of {1, 2, ... , n}, all permutations being equally likely. Let xk = k (1 < k < n). Show that if we do a least-squares fit with a line Z = µ + 0(x — Y), then the Estimator B of will satisfy B = \u0000 Q(x, Z) = Rs := (+, E \u0000 - 4 (n ± 1)2 q(x,x) \u0000 -h (n2 — 1) The Statistic Rs is called Spearman's 'Rank-Correlation Coefficient'. Show that 1 IE (Rs ) = 0; Var(Rs) = n —1' Suppose that one wishes to test whether there is a drift up or down in a sequence of Observations W1i W2, • • • , Wn when one has no real knowledge of the types of distribution involved. One could make a Null Hypothesis Ho that the variables W1, W2, ... Wn are exchangeable, a hypothesis which makes no other 0 318 \u0000 8: Linear Models, ANOVA, etc assertion about the distribution of W = (W1, W2, \u0000 , Wn). For our convenience here only, we assume that with probability 1, no two Wk-values are equal. The Alternative Hypothesis is merely a vague one that there is some sort of 'drift'. Let Zk be the rank of Wk in that W(z.,) where the Wo are the usual Order Statistics. Intuitively, a positive value of Rs suggests a drift up, a negative one a drift down. The distribution of Rs under the Null Hypothesis is the same as that in Exercise 317Ea, and so a `p-value for Ho' based on the observed value rs(31's of Rs, namely PaRsi > rsobs ) may be calculated knowing only n and without any further assumption on the distribution of W. (You can find p-values for Rs for given n in most sets of statistical tables, all computer packages, and on the Web.) You can see why we are in a 'non-parametric' or 'distribution-free' situation. Assigning a p-value in this way amounts to a 'pure test of significance'. It harks back to Fisher's idea that if the p-value is very small, then one can say, 'Either the Null Hypothesis is false or the Null Hypothesis is true and an event of very small probability has occurred' and count this as evidence against the Null Hypothesis. We know that this reasoning is incomplete: one usually wants to know, 'Does the Alternative Hypothesis provide a significantly better explanation?'; but here, one has no well-defined Alternative Hypothesis (and no sufficiently precisely defined Null Hypothesis, come to that) for using Likelihood- Ratio ideas. Still, the use of such 'non-parametric tests' as that based on Spearman's coefficient can be at least a useful first step (especially if the p-value is not small!). They are widely used in psychology, Spearman's own field. Another use of Spearman's coefficient is when the Null Hypothesis states that (X1, Y1), (X2, Y2), . , (Xn,Yn) are 71 exchangeable RVs in R2, and when Zk is the rank of the Y-value corresponding to the X-value of rank k. You can see the point of this when doing a kind of 'regression' without any of the assumptions of the linear-regression model. Note that Bayesian theory is not equipped to do such non-parametric things. ►► F. Polynomial Regression. \u0000 As well as the Mathematics of Polynomial Regression (which is much easier than it looks), this subsection contains several fundamental Statistical points. (x1, es) , (x2, y ips) , \u0000 ( xn, yotbs Sometimes the data points (x \u0000 ) associated with regression clearly lie close to a curve rather than a straight line. We might then choose to use a model Yk = g(xk) Ek, Ek'S IID each N(0, o-2), 8.3. Five basic models: the Mathematics \u0000 319 where g is a polynomial of degree r: g(t) = ao ait \u0000 • + are, where r is less than the number of distinct xk values. For simplicity, we shall assume that all the xk-values are distinct. Note that since this model is linear in the parameters cep, al, , a r, it is not considered a 'non-linear' model in Statistics. Note that if we shift the origin of the x-values, the parameters a ir), al, , a r _i will change in a rather complicated way. It is therefore difficult to assign a clear meaning to these parameters individually. Fa. Nelder's dictum. This asks you to bear in mind an important related point. A model such as Yk -= a + \u0000 + Ek rarely makes sense (though it sometimes does) because an `affine' change of covariate x H x = co + cix would lead to a model Yk = + [34 ± '7)14 +Ek, with non-zero 3 coefficient. If, to use a Nelder example, each xk is a temperature, then we do not wish to change the nature of our model according to whether we measure in Centigrade or Fahrenheit. Fb. Some Mathematics. For 0 < q < r, let x(q) be the vector in Rn with kth component 4, the qth power of xk. Thus our model reads Y = a ox(°) + aix(1) + • • • + arx(r) + aG. \u0000 (Fl) Except in freak situations, the vectors x(0), x(1), \u0000 , x(r) will not be orthogonal in Rn . We therefore seek orthogonal vectors z(°), z(1), \u0000 , z(r) such that for every q, [z(°), z(1), • • • z(q) = [x(°), x(1) • • • )(M b \u0000 (F2) and then we can write our model as Y = 'Yoz(°) + 71z(1) + \u0000 + '-yrz(r) + o-G. This idea will also guarantee that there exist polynomials g(o), g(i) \u0000 g(T), with g(q) of degree q such that z/(: ) = 9(q) (xk) and -yog(3)(t) •-y1g(1) (t) + • • + -yrg(r) (t) = a() + a lt + • + a re = g(t). To achieve this we use the inductive Gram—Schmidt procedure, the first steps of which we have used previously. Use the notation (Z (S) x (q) ) Pq := P [z(9)17 K \u0000 ( z (S) \u0000 Z(S)) \u0000 < 320 \u0000 8: Linear Models, ANOVA, etc We take z(0) \u0000 x (0) \u0000 1, z(1) :- x (1) — Pox(1) = x(1) — K \u0000 (°) 1,0 Z z(2) x (2) \u0000 p0x(2) \u0000 p1x(2) \u0000 x (2) \u0000 K 2,0 z(0) \u0000 K 2,1 z(1) , etc, and, correspondingly, \u0000 9(°)(t) \u0000 1, (t) := t _ K1,0 g(o)(t), (t) := t — K2,0 9(°) (t) K2,1 9(1) (t ) etc. For every q, x(q) clearly belongs to [z(°), z(1), \u0000 , z(g)] and z(q) clearly belongs to [x(0 , x(1), ... , x(q)]. Equation 319(F2) follows. Now, everything is just as in the case of Linear Regression. We use (Z(q), Y) \u0000 a2 \u0000 Cq := \u0000 N (z(q),z(v)) \u0000 q' Ilz(q)112) as our (Unbiased) Estimator of -yq. We set RSS := 11PresY112, Pres :=I — Po — P1 — • • • — Pr in forming the Estimator RSS n — r — 1 of a2. Our Estimator of g(xnew) will be normally distributed with the correct mean and with variance \\ 0.2 Er (g( 1Z )(X (q) new)) 2 q=1 \u0000 II (F3) Fc. Model choice. So far, we have assumed that we have been given the correct model. We now consider a more relevant question for Statistics: Given some data, how should one decide on the degree r of the polynomial to be used in modelling? Obviously, one has to respect any underlying Physics, etc. (Perhaps, as in the next subsection, we should be using (say) exponential models rather than polynomial ones; but ignore that for now.) Generally, we try to comply with the Principle of Parsimony in that we seek to use the simplest model consistent with any scientific laws which apply to the case being considered. In the case of polynomial modelling, we try to use a model with r as small as possible. One reason, related to the 'more parameters means greater variance 8.3. Five basic models: the Mathematics \u0000 321 for predictions' phenomenon we have studied before, is that the variance at (F3) grows with r, particularly when anew is close to the edge of the range of data x-values (or even more so when xne, is outside that range). Notice the way that the use of orthogonality means that the models are nested in that the Co, C1, C2, . . . do not depend on the degree of the model. The reduction in the deviance resulting from adding another degree is easily determined. People sometimes look at the first value of s where an LR Test would not reject -y, = 0 at the 5% level, and then take r = s — 1. But there are no 'hard and fast' rules. You clearly could have a case where there is no particular advantage in replacing a linear by a quadratic model, but where the data plot has the characteristic 'S' shape of a cubic. (At this point, do remember Nelder's dictum.) You simply have to use common sense. Of course, the question of deciding on the degree of the polynomials comes under the heading of Model Choice; and the comments in 236J and, more especially, 303J, are relevant here. However, criteria have been designed specifically with Regression in mind. See, for example, the Mallows criterion in Mallows [155], Christensen [42], Draper and Smith [68] Gilmour [96], Montgomery and Peck [167], Myers [170]. See also Schwarz [209]. ► Fd. More on the Principle of Parsimony: sensitivity to data. There is always a danger that computer packages will entice people into using over-complex models. You must bear in mind that (in addition to the 'larger variance for predictions' effect, conclusions made using models with lots of parameters may well be very sensitive to small changes in data. Here's an extreme example to illustrate the point. Suppose that we are given 2n + 1 data pairs (X-n1Y-n), (X-n+1, Y-n+1), • • • , (X0, YO), - • , ( Xn-1, Yn-1), (Xn,Yn)• Let us also suppose that xk = k for —n < k < n. If we use linear regression to make a single best predictor of the y-value corresponding to x-value n+ 1, then changing yo by Ay° will cause a change Ay0/(2n+ 1) in our prediction. If we are so stupid as to find the lowest-degree polynomial f such that f (xk) = Yk for —n < k < n, then (Lagrange's interpolation formula) f (t) = 322 \u0000 8: Linear Models, ANOVA, etc From one point of view, this is a perfect fit to the data. However, a change Ayo in yo will now cause a change n (2n + 1)! \u0000 pri22n+1 ( \\Apr) Ayo ( 1) \u0000 AY° — \u0000 ) (n!)2 in our prediction f (n + 1). We used Stirling's formula via Exercise 11Na. Fe. Models necessitating the use of many parameters. Some models, by their very nature, demand a lot of parameters. (See, for example, those at 358Ka and 280Jd.) In regard to them, one important thing is not to get lulled by beautiful printouts into confusing individual CIs with simultaneous ones. The latter give the fuller picture, especially if the experiment is on new phenomena; and in models with wide-tailed distributions, simultaneous CIs could be markedly different from individual CIs. G. Remark on Multilinear Regression An important area of regression is where we may have several different regressors or covariates x, \u0000 ...s, t, u, . . so that the model is Yk = a + Oxk + 7sk + (54 + Auk + • • + Ek• In Polynomial Regression, we would have relations rk = xk, sk = xz, etc. But we are now considering the case where x, s, t, u are covariates (or regressors) with no mathematical relations connecting them. In our hurdler example, x might be windspeed, s the time during the season, t the height above sea-level at which the race was run, u the temperature, ... . Obviously the present model may be written in the form at 319(F1). but there is now no natural order in which to place the regressors, and hence no canonical way to play a Gram—Schmidt game. You can, if you wish, use AIC or the Mallows statistic to decide on how many regressors to keep in your model. See the books and papers listed at the end of the discussion in 320Fc. H. Non- linear Regression; exponential- decay models. \u0000 If Yk is my measurement of the temperature at time xk of my coffee, which I always forget to drink, then Newton tells me to use a model (non-linear in the parameters, and therefore outside the scope of what we have done) Yk = a + /e -7xk Ek) \u0000 (H1) where a, Q, •-y are unknown parameters. The fact that ry is unknown puts this outside the scope of what we have done. This type of 'exponential decay' model is important in a number of contexts. WinBUGS doesn't handle it willingly, but can be persuaded to, as the 'dugongs' example which comes with the package indicates. 8.3. Five basic models: the Mathematics \u0000 323 Ha. Exercise. Contrast the (H1) model with ln(Yk — a) = \u0000 — ryxk + Elc • \u0000 (H2) For which of (H1) and (H2) is the assumption of HD normal errors more reasonable? I. More motivation. I (hope and) trust that these ideas are giving a much clearer idea of the underlying geometry than the 'Y = Xf3 c' treatment usually presented, where, in the case of ANOVA, the matrix X is too hideous to contemplate. (Yes, I know why it is done that way for complete generality. And see Subsection 336Q!) I wish to show how Mathematics can handle the geometry of some important cases of ANOVA in a way which properly reflects the structure of those cases. We need to extend our Linear Algebra to cover this topic. J. Matrices and linear transformations; adjoints. An n x n matrix A = ( A11 Al2 Ant Ant A21 A22 Ann Al n A2n induces a linear transformation coA : Rn \u0000 Rn defined by coA (x) = Ax. By a linear transformation of Rn, we mean a map cp : Rn -4 R n such that for A, p, E R and x, y E Rn , co(Ax + py) = Aco(x) + pco(y)• If co is a linear transformation of Rn, then (p = (pit for some matrix A, namely the matrix with Aii -= cp (e(3)), the ith component of (p(e(3)). We say that A represents cp with respect to the standard basis f e(i), e(2), ,e (n)} of Rn . Here's the general story — which we shall continue in our study of the MVN distributions. If Lf = f u (1) u(2) \u0000 u (n) -1 is ANY basis of Rn, and (p is a linear transformation of Ir , then for some matrix B, co(ci ui + • + cnun) = diui + • • + dnun, where, if c = (ci , c2, . . , cm) and d = (d1, d2, \u0000 , dm), we have d = Bc. We say that B represents (p relative to the basis U. The range Rep) of a linear transformation co is the set of all vectors of the form cpw where w E Rn . Check that 1Z(cp) is a subspace of Rn. The dimension of R(co) is called the rank of (p. By the range or rank of a matrix A, we mean the corresponding entity for (PA- Now, Rn carries the inner product (•, -). The adjoint cp* of a linear transformation (p is defined via (x, <Pr) = (w*x, Y)- \u0000 (J1) 324 \u0000 8: Linear Models, ANOVA, etc Ja. Exercise. Show that if co is represented by B relative to an orthonormal basis U, then co* is represented by BT relative to U. Of course, BT denotes the transpose of B: (BT),i \u0000 (B) j,. A transformation yo is called self-adjoint if co = co*. Jb. Lemma. Let U be a subspace of R . Then the perpendicular projection P = Pu is a linear transformation of 1R such that p2 \u0000 p \u0000 plc \u0000 (J2) Conversely, if P is a linear transformation of Er such that equation (J2) holds then P is the perpendicular projection onto 7Z(P). Proof I'll prove the first part; you prove the `converse'. Let U be a subspace of Rn . If A, p, E R and x, y E Rn, then APux p.Puy E U, and, for u E U, (Ax \u0000 — APux — pPuy , u) = A(x — Pux, u) + p(y — Puy , u) = 0. Hence, Pu (Ax + AY) = APu (x) + PPu (Y) • Next, (x — Pox, Puy) = 0, so (x, Puy) = (Pux, Puy) = (Pux, by symmetry. Hence P* = P. Manifestly, PUx := Pu(Pux) = Pux, since Pux E U. ► K. Tensor products. \u0000 Here is the natural Linear Algebra to describe the Mathematics of the ANOVA model of Subsection 289F. The good thing is that 'nothing goes wrong' for tensor products: every result you could wish for is true. The tensor product 1RJ R K is the JK-dimensional space of vectors O of the form 6 = fejk 1 < < 1 < k < KI. In other words, 0 looks just like a matrix, but we must think of it as a vector in which the components are laid out in rows and columns instead of all being put in a single column. Of course, for such vectors 0 and (0 + 4)) jk \u0000 0 jk + 4 jk, (AO) jk :=- ACIik • 8.3. Five basic models: the Mathematics \u0000 325 For x E Re and y E RK , we define x y to be the vector in IR-1 R K with \u0000 (x y)jk \u0000 x jyk• We have, for x E RJ and y in RK , x (Ay + p,z) = A(x y) ti(x z), etc. Not every vector in 11V ® IRK is of the form x 0 y (can you see why?), a fact which is the source of the entanglement phenomenon in Quantum Mechanics, as we shall see in \u0000 Chapter 10. However, vectors x y span \u0000 0 RK : if U = {u(1), u(2), \u0000 , u(J)1 is a basis for IRJ and V = f v (i) ,v (2) \u0000 v (101 j- is a basis for RK , then the set U 0 V of vectors of the form u(7) v( k) is a basis for R1 0 RK . (Moreover, if x has coordinates c1, c2, . . . , c j relative to U and y has coordinates d1, d2, . . . , dK relative to V, then x0 y has coordinates c j dk relative to U 0 V. This makes the tensor product intrinsic, basis- independent, so it's good Mathematics.) For aJxJ matrix A and a K x K matrix B, we define the tensor product JK x JK matrix, the ((ji , k1), (j2, k2))th component of which is (A 0 B) (j2,k2) \u0000 A-3132Bk1k2• Then (intrinsic characterization) (A 0 B)(x y) = (Ax) 0 (By), \u0000 (K1) since the (ji , kl )th component of the left-hand side is E E (A 0 B)(ii ,k. \u0000 ) - 1,, (4 2 - A. 2, .3( ® 3)32 k2 j2 k2 - E E Ai122Bk1k2X hyk2 j2 k2 .i2 kz ki k2 Yk2) = (AX)ji ( B Y)ki • Because vectors of the form (x 0 y) span IR-1 0 IRK , property (K1) characterizes A 0 B. The standard inner product on RJ R K is, of course, (0,4.) := EE030>pc• k We have (intrinsic characterization) (x y, w z) = (x, vv) (y, z), because E E xjykwizk E \u0000 (E ykzk) • j k 326 \u0000 8: Linear Models, ANOVA, etc Finally suppose that U is a subspace of 1R -1 and V is a subspace of RK . Then U ® V is defined to be the subspace of 1R ® Ric spanned by the vectors u 0 v, where u E U and v E V. We have Pt V Pu PV \u0000 (K2) \u0000 where, of course, Pu®v acts on 11: \u0000 RK , Pu on le and Pv on RK. Proof of equation (K2). Let x E R 1, y E RK , u E U, v E V. Firstly, we have (Pux) (PvY) E U ®V, so that, since Pu 0 Pv is linear, we have \u0000 (Pu Pv) \u0000 0R K —> U0 V. Secondly, (x Y (Pu Pv)(x Y), u v) = (x, u) (Y, v) — (Pux,u)(Pvy,v) = 0, because — Pux, = 0 = (3r — Pvy, v). Hence, x y — (Pu Pv)(x y) is orthogonal to U 0 V; and the proof is complete. \u0000 111 Ka. Exercise. Prove that dim(U V) = (dim U)(dim V). ►► L. Two-factor ANOVA without interaction: theory. Recall the situation in Subsection 289F. We have J levels for A, K for B and the model \u0000 Yjk = \u0000 aj Oic Ejk, where /..t is the true overall mean, ce3 the true jth A-effect, Ok the true kth l3-effect, and Eai = 0 = \u0000 13k, \u0000 ei k are IID each N(0, a-2). Define \u0000 Q := P[i] on \u0000 R P[i] on Ile, and, on R-1 0 IRK , define projections as in the `Defn' part of the following table: Projection Pmean PA e ff P13 eff Pres Ptotal Defn Q 0 R Q L OR Q R I Q-L \u0000 RI / ®I SS JK M 2 K T E /3! E E E.4 E E y-32k num 1500 138 40 48 1726 dof 1 J — 1 K — 1 (J — 1)(K — 1) JK 8.3. Five basic models: the Mathematics \u0000 327 The 'num' column shows the 'observed values' for our numerical example in Subsection 289F. Other parts of the table are explained below. Let Y.. := 1JK EE Yjk 3 k 1 \\-■ := k 2-27j1\" 1 x—N Y*k := LYjk- Then, But (PmeanY) jk (PA effY) jk (PB effY) jk (PresY) jk = m := Y.. = Aj := Yj. - Y.. = Bk := *k Y** = Eik \u0000 Yjk Y j* Y*k Y** (Estd Overall Mean), (Estd jth A Effect), (Estd kth B Effect), (Estd jkth Residual). Pmean PA eff PB eff Pres are four projections onto the four mutually orthogonal subspaces [1] ® [1], \u0000 [1] ± \u0000 [1], \u0000 [1] 0[1]1, \u0000 [1]± 0 [1]1 , of dimensions 1 x I, (J — 1) x 1, 1 x (K — 1), (J — 1) x (K — 1), which together span Re 0 Ric . We set v = (J — 1)(K — 1). ► La. Exercise. Identifying clearly the spaces U, Z, W, etc, for the application of Theorem 301Ha, prove that the F-test for H0 : 'all A effects zero' rejects H0 at size i if K RSS/v AD (J 1) \u0000 > F *J-1,,(77) • For the example in Subsection 289F, estimate of o-2 based on estimated A means \u0000 34.5 5.75. estimate of a2 based on estimated residuals \u0000 6 Because 5.75 > P4,8 (0.05) = 4.46, we reject the Null Hypothesis that all aj are equal to 0 at the 5% significance level. It is not enough to be told that we would reject the Null Hypothesis: we want to quantify the differences between the aj's, the true A effects. 328 \u0000 8: Linear Models, ANOVA, etc Confidence Intervals. We continue to set u = (J - 1)(K 1). Let us now concentrate on obtaining CIs for true A means. We write 0; := for the true mean corresponding to the jth level of A. Then, with PA mean := Pmean PA eff 7 we have three mutually orthogonal projections (the product of any two is zero) PA mean, PI3 eff, Pres summing to I on R ® IRK. ► Lb. Exercise: Individual Confidence Intervals. Prove that, for an individual j, (rse bs —o \u0000 * Y 3bs Itlu(n) \u0000 Kv ) is a 100(1 — 71)% CI for 03, and check Minitab out for the example in Subsection 289F. Show that, for a fixed pair (ji , j2), (2 rssobs ,obs .Y.71* \u0000 .72* \u0000 I lv i/ \u0000 Kv ) is a 100(1 — n)% CI for Oji — 032 = ai l — ai2. ► Lc. 'Individual versus Simultaneous' for Confidence Intervals. Which of individual and simultaneous CIs we use depends on the context. Of course, one can always give both. If the experiment relates to a well-understood situation, we may identify before the experiment a particular 'contrast' ail — ah of interest, where (ji iz) is a fixed pair of distinct A levels. The individual CI for ail — ai2 is then appropriate for study. If, however, the experiment is one on a new situation and we wish to describe the overall picture, then there is a good case for using simultaneous CIs. There are complex issues here: for example picking out after the experiment a pair (jr, j2) corresponding to extreme features of the data should bring back memories of the later parts of Subsection 278J. As stated in that subsection, a study of such problems is outside the scope of this book. Giving both individual and simultaneous CIs (or a CR) is of course a safe strategy. then \u0000 Q - 1 (PA meanY T) \u0000 SN 018-1 0 [1]) , \u0000 a 1PresY \u0000 SN ([1]± ® [1]±) (dim J), (dim (J — 1)(K — 1)), 8.3. Five basic models: the Mathematics \u0000 329 Please note that my reference to STRANGE and MAD below is in the spirit of physicists' terminology, and certainly implies no disrespect for the methods. Note too that the technical definition of a contrast involving the ad's is an expression E cicej where E ci = 0. ► Ld. An 'F' Confidence Region. The usual arguments now show that if we write Tjk := Oj = \u0000 a these variables being independent. We shall have RSS = IlPresYll 2 \u0000 a2X,2, \u0000 v := (J — 1)(K — 1). If {e(1), e(2), \u0000 e(-1)} is the standard basis for RJ , and {v}, where v = K- 11, is an orthonormal basis for [1] in RK , then the e() 0v form an orthonormal basis for RJ 0 [1]. But 0- 1 (PA meanY — T, e(i ) 0 V) = (7-11 Yi. — Oi 1K whence, as it is easy to prove directly, the variables — 0j1.0 \u0000 (1 < j < \u0000 (L1) are IID each N(0, 1). The Orthonormality Principle implies that these variables at (L1) are independent of RSS. Hence, KE 3 (Yi* — j) 2 IJ \u0000 F RSS/v \u0000 j, v; and this gives us as 100(1 — n)% Confidence Region for the vector 0 a ball in RJ , J JFA,(77)rsebs ll Because it is not that easy to think of this CR, we tend to prefer simultaneous CIs. ► Le. Studentized-Range (STRANGE) SR \u0000 The Studentized-Range distribution SRS,,, is defined to be the distribution of max{X3 :1< j <J} - min{X3 :1<j< J} VR/v where X1, X2, . , X j, R are independent RVs, each X 3 being N(0, 1) and R having the x 2, distribution. The STRANGE distributions are not easy to calculate. Many computer packages these days have them available, but be careful to check the conventions used. centre {yobsobs 1 < < J } , \u0000 radius Kv 330 \u0000 8: Linear Models, ANOVA, etc \u0000 If SI( );,(7)) is such that P (SR \u0000 > SR*J0,(77)) = n, then there is 100(1 — n)% probability that \u0000 — X~2 < SR*J,,(71) \u0000 simultaneously for all (ji , j2). In our ANOVA situation, there is 100(1 — j)% probability that Y 31* - 32* - ( 0 i1 \u0000 32) < SW; 0/( 11) simultaneously for all pairs (ji, j2). Hence, we can be 100(1 — 7)% confident that, simultaneously for all pairs ( ji , j2), eii °32 E (V3°,13: — gf2b*s) \u0000 SRJ,v(Y) This is a good way to compare differences in A effects for small J. Note that in the example in Subsection 289F, SK 8 (0.05) \u0000 4.89 21 -ti8(0.05) \u0000 22 x 2.31 = 1.50, so that the 95% simultaneous CIs are here 1.50 times wider than the 95% individual CIs. \u0000 Lf. Exercise. Explain why SR;,, (n) = 211t1:(rl). \u0000 ❑ There can be 'conflicts'. One could find that one rejects at the 5% significance level the Null Hypothesis that all A effects are zero, while finding that all intervals in the 95% simultaneous CIs for differences 9,1 — 9i obtained via STRANGE contain 0. And it can work 'the other way round'. The intuitive reason is surely obvious: a ball is not a cube! ► Lg. Maximum Absolute Deviation (MAD). If J is large (6 or more), there are I J(J - 1) differences 0,, — 0,2, too many to contemplate. So the Maximum Absolute Deviation method focuses on the distribution of maxi \u0000 — XI VRIv under the same conditions as those in the definition of STRANGE. You can translate this into a method of getting simultaneous CIs for the 93 - p = a, in ANOVA. Again the MAD distributions are not easy to calculate. \u0000 ❑ Note. Tukey's paper [230] suggests many lines of investigation which we do not have space to pursue here. 8.3. Five basic models: the Mathematics \u0000 331 p.■ M. Two- factor ANOVA with interaction and replication. As explained in Section 28C, we use a model Yjk,1 = A + cej /3k (a0)jk + c jk,l to investigate the interaction between A and B factors. The notation (a0)3k just signifies a constant: there is no multiplication involved in this standard notation. We have EU, =0= /3k, E (cE13)ik = 0 for each (ce,8)ik = 0 for each k, and the E Jo are IID each N(0, cr2). Tensor products handle things nicely. We have Projection Defn SS dof Pmean Q®R®S JK LM 2 1 PA eff Q-L oRoS KLE/1.1 J —1 P13 eff Q0.1V- 0S JLE Bi2c K —1 P(AB) eff Q-L \u0000 RI \u0000 S L E E(AB),„ (J —1)(K —1) Pres I0I0S ± JK(L — 1) Ptotal I®I®I JKL Here, etc, etc. M \u0000 Ai \u0000 M, Really, you can derive for yourself any information you wish from this table, and you can see the relevance of the F-test for complex models. Ma. Exercise. Show that if, with v1 = (J — 1)(K — 1) and v2 = JK(L — 1), we have 11P(AB) eff \u0000 112 \u0000 11PresY112 > \u0000 ,v2(0 .05) \u0000 v2 \u0000 , then we reject at the 5% level the hypothesis that all ,AB effects are zero. We might therefore choose to investigate these interaction effects further. If there is not evidence at the 5% level against the hypothesis that all AB effects are zero, we might well stick with the additive (no-interaction) model. vl Mb. Exercise. Why are we not interested in the projection Q0R® Si ? What 'effect' does it relate to? 332 \u0000 8: Linear Models, ANOVA, etc ► N. The Bayesian view of linear regression. Return to the situation of Subsection 312D, so that Y = p1 ± /3z o-G, z = x — 1, G SN(R' ). With p = 1/o-2 as usual, the log-likelihood satisfies E(i.t, [3, p; y) = —P-tln(27) \u0000 — 2PIIY — pl — /34 2. However, if v = y — pl — Oz, then, we can decompose v 'orthogonally' as v = Ppdv P[zjv Pri ,zjv , whence (you check!) 1)11Y \u0000 Pz112 = \u0000 pn (p, — g)2 — 1pq(x, x)(/3 — b)2 — p(rss), where b = q(x, y)/ q(x, x) and rss is as usual. If we assume a prior for (,a, /3, p) where, with obvious misuse of notation, p, N (mu, prec \u0000 , \u0000 N (mo, prec rp), p N Gamma(K, rate y), these being 'independent', then we would have ln 71-(11, \u0000 p) = constant — 17.1,(/ — mp)2 — -7-0(13 — mo)2 + (K — 1) In p — -yp. Na. Exercise. Show that for the posterior density 7r(µ, 0, p y), we have Ca 1 P, p) \u0000 N(?,?), \u0000 (i3 I tt, p) \u0000 N(?,?), (p p, 0) ^ Gamma (K + zn, rate -y + 1piiy obs — pl — /3z112) , filling in the gaps where the \"?' signs appear. Table 211 J(i) of 'conjugates' should help greatly. \u0000 ❑ The above exercise tells one how to apply the Gibbs sampler to this situation. Here's the Example 315Dd done in WinBUGS . model const N = 20; var mu [N] , Y EN] , alpha, beta, rho ; 8.3. Five basic models: the Mathematics \u0000 333 x.bar <- mean(x[]) for(i in 1:N){ mu[i] <- alpha + beta * (x[i] - x.bar) Y[i] \u0000 dnorm(mu[i], rho) } alpha - dnorm(0, 1.0E-6) beta - dnorm(0, 1.0E-6) rho - dgamma(1.0E-3, 1.0E-3) } #data list( N = 20, x = c( \u0000 0, 10, 1, 11, 2, 12, 3, 13, 4, 14, 5, 15, 6, 16, 7, 17, 8, 18, 9, 19), Y = c(1.30,2.65,0.89,1.09,1.60,1.96,1.89,1.50,2.99,2.77, 3.30,3.22,3.61,3.09,3.32,3.28,3.61,4.04,4.10,4.74) #inits list(alpha = 0, beta = 0, rho = 1) 95% CI for beta after BUGS run of 100000 WinBUGS \u0000 Frequentist (0.1249, 0.2065) (0.1256, 0.2066) There are so few parameters that we can obtain conservative simultaneous Cis in the usual way. ► 0. The Bayesian view of ANOVA. You can see that the Bayesian theory is somewhat less elegant than the Frequentist for classical linear models. (However, the Bayesian approach has the advantage of allowing much more general models — as well as that of building in prior information). I am not going to bore you with all the details of the Bayesian version of ANOVA. I'm sure that you can see in principle how it would all go. There is a difficulty however. Consider the model Yjk = ej Ok Ejk) where the ei k are IID each N(0, o-2). We have to be rather careful about possible instabilities caused by the `aliasing' problem that we can add an arbitrary constant C to every O provided that we subtract the same constant C from every 13k. If we restrict the 13k by imposing the condition that E Ok = 0, then we are restricting the vector to lie on a submanifold (which in this case is a hyperplane) of RK, so will not have pdf on IRK. Still, WinBUGS seems to handle things well. The numerical example from Subsection 289F is now studied via WinBUGS via two variants of a program. Here's the first. model ANfreq; var Y[5,3], Ymean[5,3], tau[5], gamma[3], theta[5], mu, rho, gbar; { mu - dnorm(0, 1.0E-6); for(j in 1:5){ \u0000 # Setting up taus tau[j] \u0000 dnorm(mu, 1.0E-6); # Vague tau[] } for(k in 1:3){ \u0000 # Setting up gammas gamma[k] \u0000 dnorm(0, 1.0E-6); # Vague gamma[] } rho - dgamma(1.0E-3, 1.0E-3); # Vague rho for(j in 1:5){ \u0000 # Setting up Y's for(k in 1:3){ Ymean[j,k] <- tau[j] + gamma[k]; # Aliasing problem here Y[j,k] \u0000 dnorm(Ymean[j,k], rho); } } gbar <- mean(gamma[]); \u0000 # For for(j in 1:5){ \u0000 # finding thetas theta[j] <- tau[j] + gbar; } } #data list( Y = structure( .Data = c(5,12,7,13,12,17,13,12,14,5,9,13,4,5,9), .Dim = c(5,3) #inits list(mu = 0, tau = c(0,0,0,0,0), rho = 1, gamma = c(0,0,0)) Individual 95% CIs after WinBUGS run of length 10000 node \u0000 WinBUGS \u0000 Frequentist theta[1] ( 4.74, 11.29) \u0000 ( 4.73, 11.27) theta[2] (10.69, 17.19) \u0000 (10.73, 17.27) theta[3] ( 9.80, 16.29) \u0000 ( 9.73, 16.27) theta[4] ( 5.71, 12.19) \u0000 ( 5.73, 12.27) theta[5] ( 2.60, 9.39) \u0000 ( 2.73, 9.27) However, gbar and mu oscillate very wildly because of 8.3. Five basic models: the Mathematics \u0000 335 the aliasing problem. And here's the second. model ANalt; var Y[5,3], Ymean[5,3], gamma [3], theta[5], mu, rho, gbar; { mu - dnorm(0, 1.0E-6); for(j in 1:5){ \u0000 # Setting up thetas \u0000 theta[j] \u0000 dnorm(mu, 1.0E-6); } for(k in 1:3){ \u0000 # Setting up gammas \u0000 gamma[k] \u0000 dnorm(0, 1.0E-6); } gbar <- mean(gamma[]); for(k in 1:3){ \u0000 # Setting up betas beta[k] <- gamma[k] - gbar # to have zero sum } rho - dgamma(1.0E-3, 1.0E-3); for(j in 1:5){ \u0000 # Setting up Y's for(k in 1:3){ Ymean[j,k] <- theta[j] + beta[k]; \u0000 Y[j,k] \u0000 dnorm(Ymean[j,k], rho); } } } #data list( Y = structure( .Data = c(5,12,7,13,12,17,13,12,14,5,9,13,4,5,9), .Dim = c(5,3) #inits list(mu = 0, theta = c(0,0,0,0,0), gamma = c(0,0,0), rho = 1) 95% individual CIs for theta[j] values as good as before. 95% CI for mu found as (-814.0, 822.4) -- not surprising. So, WinBUGS can obtain the Frequentist answers. However, WinBUGS can do more, and we have to consider for each situation whether the Frequentist answers are appropriate. We discuss this in Subsection 358L. 336 \u0000 8: Linear Models, ANOVA, etc For important discussion of these and related matters, see Nobile and Green [175] and Besag, Green, Higdon and Mengerson [19]. ► P. An ANCOVA example. Suppose that Ykm \u0000 (1 < k < \u0000 < m <12) are RVs representing the number of unemployed people in some country in month m of year k, or else the amount of gas consumed in that country in that month. Consider two models, each of which builds in seasonal variation: Model 1: Ykm = µ + o(k - + Ym + Ekm, \u0000 = 0; Model 2: Ykm = + 13 — k + -Mm \u0000 + + ekrn, \u0000 7m, = \u0000 = O. In each case, the Ek m are (for now) assumed to be IID each N(0, u2). ► Pa. Exercise. Show that each model is easily analyzed via our geometry. Comment on the difference between the models. Criticize the independence assumptions on the Ek m . Note. ANCOVA stands for ANalysis of COVAriance. The examples just considered were ones in which the m was regarded as a factor (and any one-one function of m would have done as well) but there was linear regression on k, so the actual value of k mattered. This can hint at what more general ANCOVA models look like. ►► Q. The General Linear Model. Suppose that Y = X 13 ± a G \u0000 (Y : n x 1; X : n x s; : s x 1; G : n x 1) , where X is a deterministic 'design' matrix, Q is a vector of parameters, G SN(RTh ), and a > 0. Make the lull-rank' assumption that XTX is invertible, and note that vT X T Xv = (Xv) T Xv = IlXvI12, so that C := X T X (known to be invertible, and obviously symmetric) is positive- definite. The MLE Estimator (B, \"6-) of (3, a) will minimize IIY XBII 2 = 11Y112 — 2YTXB BT X T XB. Let W = XTY. We want to minimize BT CB — 2WTB. 8.3. Five basic models: the Mathematics \u0000 337 Differentiating with respect to each B, suggests that B = C-1W. Indeed, if B = C-1W + D, then, using the symmetry of C, BT CB — 2WTB WTC- 1CC-1W DTW WTD DTCD 2WTC -1W 2WTD _wTc-lw DTCD. Hence, by this algebra rather than by calculus, we see that IIY - XBII 2 is minimized when B = C-1 W = (X T X)-1X TY, with minimum value RSS = 11Y112 W TC-1W = 0( 112 — ITT X(X T X)-1X TY. Of course, there's meaningful geometry behind all this (rather meaningless) algebra; and we examine it below. First, though, let's look at an example. Qa. The case of Linear Regression. For the familiar Linear Regression model from Subsection 312D, we have ( 1 x1 — x 1 x2 — x 1 x n, — X = x T x ( n 0 0 q(x, x) ) ' = \u0000 ) XTY — nY Q(x,Y) B = \u0000 B = Q(x,Y) (;) ' \u0000 q(x, RSS = 11Y112 \u0000 nY Q(x,Y) ) = 11Y112 nY2 — q(x, x)B2, n-1 0 \u0000 n 0 q(x, x) ) Q(xyy) which is all as it should be. ► Qb. The geometry of the general case. Let P = X (X T X)- I X T Then it is clear that PT = P and P2 = P. Hence, by Lemma 324.113, P is perpendicular projection onto the range 1Z(P) of P. Since Pv = X {(X T X)-1X Tv} , we have R(P) C R(X) = X(Rs). We want to prove that R(P) R(X), so that P is projection onto the range of X. (Q1) ( Y11 Y12 Y21 Y22 ( 1 1 0 1 0 \u0000 / ti a1 1 1 0 0 1 a2 + a -G = \u0000 + a-G. 1 0 1 1 0 I \u0000 01 1 0 1 0 1/ \\ 02 338 \u0000 8: Linear Models, ANOVA, etc If1Z(P) is a proper subspace of '1?,(X), then we can find a E Rs such that Xa 0 and (Xa, Ph) = 0 for all h in IfIn . But then, 0 = aT X T Ph = aT XT X (xT x)-1 xT h aT xT (X a, h) = 0 for all h E 1r, whence Xa = 0. This contradiction establishes 337(Q1). We have, if W 7Z(P) and B = (X T X)-t X TY , then XB PY = X + SN(W), RSS \u0000 XII[12 = 114-Y[12 \u0000 A-s and XB is independent of RSS. Note that XB = PY is the best fit of Y allowed by our model, both in ML and least-squares senses. Obviously, IIXB — X/3112/s RSS/(n — s) giving a Confidence Region for the best fit. Qc. The case of ANOVA. Suppose we have a 2 x 2 two-factor ANOVA model without interaction. Then However, for Linear-Algebra reasons, X T X cannot be invertible. One way to see this is that, really because of aliasing, Xa = 0 where a = (1, —1, —1, 0, 0)T. Of course, there are ways round the aliasing difficulty. For example, we can use a1 + a 2 = 0 and 01 + /32 = 0 to write instead ( Yll Y12 Y21 Y22 1 \u0000 1 \u0000 1 1 \u0000 1 —1 1 —1 \u0000 1 1 —1 —1 ) + crG = X /3 ± a-G. Nl Now X T X = 4/, and B is what it should be. Just think what X looks like for the J x K x L two-factor ANOVA with interaction and replication at Subsection 331M: it's something of a mess. Moral: if your model has structure, then make sure that your Mathematics reflects it. 8.4. Goodness of fit; robustness; hierarchical models \u0000 339 8.4 Goodness of fit; robustness; hierarchical models A. Introduction. \u0000 The previous section studied the first stages of the Mathematics of Linear-Model theory. As its title suggests, this section considers the first stages of associated Statistics. Goodness of fit is concerned with the question: do the data suggest that our model is unacceptable? Suppose that we can assume that Y1, Y2, ... , Yn are IID with common DF F. Our model may assume that F is some particular DF G or that F belongs to some parametrized family of DFs. Is there strong evidence in the data to reject such an assumption? This is a 'distributional' test of fit. Recall that with Y as a typical Yk, F(x) := lP(Y < x). and that the Sample Y = (Y1, Y2, ... ,112) determines the so-called Empirical Distribution Function (EDF) Fr„,(•; Y), where Fr,(x;Y) := 1 1tific : Yk < XI, \u0000 (Al) the proportion of k such that Yk < X. One way to decide whether to abandon the assumption that F = G is to consider the Kolmogorov Statistic: DK(n,Y ,G) := stlp IFn(x, Y) — G(x)I \u0000 (A2) a distance between the functions Fr,,(•; Y) and G. We study this in Subsection 342C. Before that, we shall however consider a visual way, using quantile- quantile (qq) plots, of assessing whether there is strong evidence against the hypothesis that F = G. Our study of the famous Chi-Squared method of assessing goodness of fit begins in Subsection 345Da. It seems fair to say that assessing goodness of distributional fit has more of the flavour of a cottage-craft industry than of a science, unless well-specified Alternative Hypotheses are available. I spend more time on qq plots than I believe the method merits, because the qq-plot method is likely to be widely used with its availability on so many packages. Each of the methods has advantages and glaring faults; and don't always expect compatibility of answers! But wait for hierarchical modelling, though that has its own difficulties. We have talked about goodness of fit in regard to hypotheses on DFs. Brief indication of goodness of fit in regard to hypotheses of independence is given in chi-squared tests for contingency tables in Subsection 3630, in a study of 340 \u0000 8: Linear Models, ANOVA, etc correlation in Subsection 372H, and implicitly in the mention of Spearman's rank correlation coefficient at 317Ea. But there are other important aspects of testing independence which we have to skip. Robustness features in Statistics in many ways. I have already mentioned it in connection with sensitivity to small changes in data, and in regard to testing the effect of slightly different priors and different initial values in MCMC methods. Here, however, we are thinking of the robustness of methods to changes in (the `likelihood' part of) our model. Let's take an example. Student's 't' method of getting CIs for a mean assumes that the underlying distribution is normal. However, in many situations, the normal distribution tails off too quickly to be realistic. If the underlying distribution is different from normal, by how much are results obtained by Student's T method in error? Simulation often allows us to answer such questions. See Subsection 3541. Hierarchical models, the study of which is often possible only by MCMC methods, can be more realistic than classical ones. They also do much to avoid goodness-of-fit and robustness difficulties. In particular, they can allow the machine to decide for itself amongst classical models or amongst 'mixtures' of such models. An often-used example is where we choose the underlying standardized distribution of errors to be StandT,„ where v itself is 'random' taking values in some such set as {3, 4, 6, 8, 12, 16, 24, 32}. However, we shall see that the Gibbs sampler may run into difficulties with this. Outliers — now there's a problem! An outlier is an observation for which the associated residual is significantly larger than might be anticipated from the vast majority of other observations. One must check each outlier to see whether it is genuine and not the result of an error in measurement or in recording data. Non- genuine outliers can seriously distort analysis. If outliers are genuine, then one must use a larger-tailed' model (or a hierarchical one). See Barnett and Lewis [10] and Rousseeuw and Leroy [200]. B. Quantile- quantile plots. \u0000 Suppose that Y1, Y2, \u0000 , Yri, are IID each with strictly increasing continuous DF F on R. For 0 < p < 1, the unique value x such that F(x) = p is called the quantile of F corresponding to p. Recall from 50B and Fact 99G that if Uk \u0000 F(Yk), then U1, U2, • • • 7 Un are IID, each U[0, 1]. If Um, Up), . , U(n) are the Order Statistics for U1, U2, \u0000 ,U n (that is, U2, . ,Un arranged in increasing order), then (see Exercise 108La) E U(k) — \u0000 n + 1' so that, very roughly speaking, U(k) should be close to k An+ 1), and Y(k) should be close to F-1(kAn+ 1)), the quantile of F corresponding to kAn+ 1). To test 8.4. Goodness of fit; robustness; hierarchical models \u0000 341 N(0, 1) against N(0, 1) \u0000 StandT4 against N(0, 1) Figure B(i): Quantile-quantile plots Figure B(ii): Understanding a qq plot: p = 0.17 = (I)(x) 342 \u0000 8: Linear Models, ANOVA, etc t N (a) \u0000 (b) \u0000 (c) Figure B(iii): Three pictures of an example whether it is reasonable to assume that F = G, we can therefore plot the values (G-1(k I (n + 1)), Y(k)) , which should lie moderately close to a straight line if F is indeed close to G. On the left-hand side of Figure 341B(i), I have simulated the situation where n = 200 and F = G = 43, the DF of N(0, 1). On the right-hand side, I have taken for F the DF of StandT4. The grey curve is the theoretical curve through the points (G-1(x), F-1(x)). To help you think about qq plots, the lower half (b) of Figure 341B(ii) again illustrates the theoretical qq plot for the DF of StandT4 against N(0, 1). In the top half (a) of the figure, the DF of N(0, 1) is plotted in grey, and the DF of StandT4 in black, with horizontal and vertical scales as indicated. Figure (b) has the same horizontal scale as (a), and is 'true' in that its vertical and horizontal scales are the same. Quantiles corresponding to p = 0.17 are shown in both (a) and (b) in Figure 341B(ii). The vertical line through 4.-1(p) features in (b) and as the leftmost of the two close vertical lines in (a). The horizontal distance between the two close vertical lines in (a) is the same as the vertical distance between the two horizontal lines in (b). Apologies for the 'overkill' in this description. A qq plot is a useful visual device. But 'how far from the line is significant'? (We look at quantifying a qq plot at Cb.) Let me be honest and say that my first simulation of a qq plot with F = G = 4 was that at (a) in Figure B(iii). Clearly, the simulated points lie below the line. But do they lie sufficiently far below for me to suspect the random-number generator? ► C. Distribution of the Kolmogorov Statistic. Suppose that Y1, Y2 • • , Yn are IID each with strictly increasing continuous DF G on R. Note that if in is the median of G, then, with 2 replacing 1.96 as usual, Fn (m; Y) is approximately i , 4n-1) for large n, the usual n- warning not to expect too great accuracy. 8.4. Goodness of fit; robustness; hierarchical models \u0000 343 Kolmogorov proved the fine result, best understood in terms of the theory of Brownian motion (see Billingsley [22]) that, as n \u0000 oo, P (DK (n, Y, G) > \u0000 C Nrn 00 r=1 (-1) r-1 exp{-2r2c2}, \u0000 (C1) where DK is at 339(A2). The right-hand side is 5% when c = 1.36. Ca. Exercise. Use the F inverse' Principle to check that for each fixed n, the expression on the left-hand side of (C1) does not depend on the common DF G (assumed strictly increasing continuous) of the Yk. Note that if P (DK (n, Y, G) > \u0000 = 77, then P (G(x) E [F,i(x,y) — cn--1 ,Fri(x,y) cn- 1]) = 1 — so we obtain a 100(1 — 77)% Confidence Band for the whole function G. For the data from the qq plot at (a) in Figure 342B(iii), we would get the centre 'wavy' line in (b) as the observed EDF Fn (•, y obs) and the region between the top and bottom wavy lines as Confidence Band for the true DF G. The smooth curve represents the graph of 4) which lies within this band. Indeed the 'p-value' based on the Kolmogorov statistic, the probability that DK would be at least as large as the observed value if the common DF of the Yk is indeed G, is about 11%. From this point of view, there is not strong evidence to reject the hypothesis that F = G. Of course, we are in a very difficult situation if we try to formulate an Alternative Hypothesis ... . Wait for the Chi-Squared method. I have already remarked that the 72- 1 difficulty, very clear from (C1), means that we cannot expect too accurate results. Just look at the top half (a) of Figure 341B(ii) to see that the DFs of StandT4 and N(0, 1) never differ by more than 0.053. You can see that you need a large sample to separate these DFs on the basis of the Kolmogorov Statistic. Cb. `Kolmogorov Statistic' quantifying of a qq plot. In the following discussion, we assume n so large that we can treat Fmk Y) as if it were continuous (rather than a right- continuous function with jumps). Suppose that Y1, Y2, ... Yn are IID each with strictly increasing continuous DF G. One possible way of quantifying a qq plot is by 'inverting' the Kolmogorov result. That result states that for large n, we can with 95% confidence state that for all x, Fn(x;Y) > G(x) — 1.36n-1 , \u0000 Fn(x;Y) < G(x) + 1.36n-1 . (C2) For x = G -1 (p), so that G(x) = p, we have from (C2) Fn(x) := Fn(x;Y) > p — 1.36n- 344 \u0000 8: Linear Models, ANOVA, etc Plot 342B(iii)(a) with bounds StandT4 vs N(0,1) Figure C(i): Quantifying qq plots for n = 100 so that the quantile for Fr, associated with p — 1.36n — 2 is less than or equal to x. Writing c for p — 1.36n — z , we have Fn (c) < G-1(c + 1.36n-1 ). This leads us to say that a 95% Confidence Band for the qq plot, if the true DF is G, is the region between an upper curve { (G-1 (c), G-1(c + 1.36n- 1. )) : c < 1 — 1.36nA and a lower curve { (G-1 (c), G-1 (c — 1.36n- 1)) : c > 1.36n- 1 }. These curves for G = 4 and n = 100 are shown in Figure C(i). The qq plot corresponding to Figure 342B(iii)(a) is also shown in the left-hand Figure. The theoretical StandT4 against N(0, 1) qq plot is shown in black on the right-hand figure. It helps emphasize that one needs large sample sizes to differentiate between certain DFs. Cc. Exercise. Think about possible disadvantages of the method, but in a 'before the experiment' way. ►► D. The Chi-Squared Principle (ChiSqP) in Hypothesis Testing. Neither the qq method nor the Kolmogorov method of assessing goodness of fit is based on likelihood; and it has been our contention all along that it is likelihood that matters. 8.4. Goodness of fit; robustness; hierarchical models \u0000 345 We are now going to move towards the Chi-Squared method of assessing goodness of fit which is based on likelihood considerations and which to that extent is philosophically satisfying, but which has a lot of arbitrariness and which can fail to see obvious things. You have been warned not always to expect consistency with the Kolmogorov method. Here is an important general principle. We saw it in the case when s = 1 and m = 0 in Subsection 230D and for the case of Linear Models in Theorem 301Ha. Revising these cases will help you understand the following theorem. Don't worry about the `submanifold' terminology: examples will give the idea. ►►► Da. Wilks\"Theorein (Chi-Squared Principle (ChiSqP)). Again suppose that 171,11-2, are 1ID each with pdf/pmf f (y 10). Let Ha take the form '0 E Bo' and HA the form '0 E BA', where BA is a submanifold of some I'd of dimension s and Bo is a submanifold of B4 of dimension m. Then if Ho is true and n is large, Dcv (Y) = Dev(I/A, Ho; Y) = 21n LR(Y) \u0000 Xs 2 - For a sketched proof see Subsection 377L. ► E. ChiSqP for the discrete case. Suppose that Y1, Y2, • • • Yn are HD RVs each with values in {1, 2, ... , b}, such that = k) = pk (1 < k < b,1 < m < n), 1.d pi = 1. Let Nk be the number of Yn, equal to k. Then lhd(p; Y) = pNi 1pi2v2 p bNb, and this is maximized by f• where „ Nk P —n (It is understood that 0° = 1.) Let Ho be the Null Hypothesis that pk = pok where the pok are completely specified positive numbers summing to 1. Let HA be the hypothesis that the pk are arbitrary non-negative numbers summing to 1. Then (with the anticipated `entropy' in the first equation) In mlhd(HA; Y) = In mllid(Ho; \u0000 = Nk ENK , —, \u0000 (0 In 0 = 0), Ti E ATic lnpok. 346 \u0000 8: Linear Models, ANOVA, etc Then 21n LR(Y)— By ChiSqP, if n is large and Ho is true, then 21nLR(Y) since here, s = b — 1 and m = 0. (The condition Epk = 1 defines a (b — 1)-dimensional hyperplane in Rb. The single point (poi,Poz, • • -,Pob) is 0-dimensional.) There is a famous (but not always that good) approximation to 21n LR(Y) due to Pearson, which in fact predates LR tests. Let Nk npfrok Ak := nPok `Observed - fitted' `fitted' Nk — nPok `fitted' being calculated on the basis of the Null Hypothesis, and note that > npokAk = 0 and Nk = npok(1 + Ak). Then 21nLR(Y) = 2 E npok(1In(1 + = 2 E npok (1 + \u0000 (Ak — 1,6 2,) ± error = 2 Enpok (Ak + \u0000 + error, so that 21nLR(Y) \u0000 npok AZ =: PearsonStat. We usually write PearsonStat as PearsonStat (Observed — fitted 2 fitted Later, we shall see a nice direct explanation (with mysterious connections with quantum theory) of the fact that, if 1/0 is true and n is large, PearsonStat is approximately Ea. Detecting `over-enthusiasm'. An important application of this technique is to test when data fit the Null Hypothesis better than is believable. Fisher (see [217]) showed that data as closely in agreement with Mendel's theory of inheritance as Mendel's own 'data' would occur by Chance only about 4 times in 100000 (if Mendel's theory is true — which it is, of course). So there is evidence that Mendel (or someone working with him) showed a certain over-enthusiasm for Mendel's theory. I won't accuse a monk of deliberately cheating! See Wright's article in [217]. 8.4. Goodness of fit; robustness; hierarchical models \u0000 347 . F. ChiSqP for goodness of fit. An obvious idea for assessing whether it is unreasonable to assume that the common DF of IID Yi, Y2, ... Yn is a given DF G is to take a 'partition' — 00 = Xo <x1 < • • • < Xb = 00, and apply the method of the previous section where Nk is the number of Y, falling in (xk_i , xid, with POk = G(xk) G(Xk-1)- (Of course, if each Yk must be positive, we can take xo = 0.) Suppose for a moment that we have decided on the value of b. Then it is best to take x k = G-1(klb), the quantile of G corresponding to k/b. There is little point in looking at cases where n < 80. There are some reasons for choosing b close to 15 (172 2/5 00) See, for example, Kendall, Stuart, Ord and Arnold [133]. Figure 342B(iii)(c) takes b = 16 and shows the observed values nibs, \u0000 ,gn bs of N1, N2, . \u0000 Nb, the black horizontal line indicating the fitted values. The horizontal lines in Figure (a) there give `equiprobable' spacing. One finds the following results: We have for observed values N_1,N_2,...,N_b, using 16 equal-probability intervals: 10 10 8 2 9 7 6 9 6 5 6 4 8 5 1 4 Each fitted value is 6.25. Pearson = 17.4400 with corresponding p-value 0.2932. twolnLR = 20.1175 with corresponding p-value 0.1675. Here then is a case when PearsonStat does not approximate 21n LR well. Recall that the p-value from the Kolmogorov statistic was 0.11. Fa. Important Exercise. Again consider a case where a Chi-Squared test is to be performed using equi-probable intervals. (a) Describe a situation where the Chi-Squared method would tell strongly against the F = G hypothesis but the Kolmogorov statistic would not. (b) Describe a situation where the Kolmogorov statistic would tell strongly against the F = G hypothesis but the Chi-Squared method would not. (c) Now discuss the discrepancy between the Kolmogorov and Chi-Squared results for our example. 348 \u0000 8: Linear Models, ANOVA, etc ► G. Goodness of fit with parameter estimation. Let Y1, Y2, , Yn yet again be IID with common DF F. The model (discussed in the last few subsections) that F is some completely specified DF G is, of course, hardly ever used. Much more common is the assumption that F belongs to some family such as the family {MA, a2); a E IR, a > 0} of normal distributions. Let us stick to this normal case for illustration. We have to estimateµ and a2 from the observed sample and consider whether the `best' N(p, a2) should be rejected on the basis of the data. If the sample size n is large (and hence much bigger that the number, 2, of parameters being estimated), we can use the qq, Kolmogorov, or Chi-Squared method just as before without worrying too much. If, however, the sample size is small, our method must take into account that we have estimated p and a2; and the qq and Kolmogorov methods cannot do that. The only sensible question is: how is the Chi-Squared method affected by estimation of parameters? Well, we have to use a partition —oo = x o < x i < • • • < xb = CXD, as before, and compare how the Pok (A, 0-2) = G(xk; 0-2) — G(xlc—i;12, a 2) (where G(.; p, a2) is the DF of N(p, a2)) match up to the MLE under HA. Now, we have dim(Bo) = 2 and dim(BA) = b — 1 as before, so ChiSqP relates to b — 3 degrees of freedom. But this use of ChiSqP is valid only if we choose (µ, a2) so as to maximize H Pok 0.2 ) Ark and this is never done in practice. In practice, one normally uses Y and Q(Y, Y)/(n —1) as Estimators for p and a2; and the limiting distribution of the deviance 21n LR is then not x6_ 3. If, with these usual estimates of p and a2, the xg_ i method says that the result is significant, we go along with that; and if the x6_3 result is not significant, we agree with that. The 'in-between' cases mean that you will have to consult the literature. H. 'Hierarchical t' alternative to normal; and serious problems with ► the Gibbs sampler. For all its great theoretical importance, the fast tail-off of the normal density means that it may not provide that good a model for the pdf of a single observation in practice. Of course, when n is large, the CLT guarantees that Y will be approximately normal. It is quite common these days to assume that an 'Error Variable' in Linear- Model theory has a tv distribution where v itself is 'random'. This alternative to the 'normality' assumption is not without its practical difficulties however. In addition, it continues to assume a symmetric distribution of errors. 8.4. Goodness of fit; robustness; hierarchical models \u0000 349 Let us concentrate for now on a simple example in which we believe that each Yk has mean 0 and variance 1. Recall from Exercise 255Ge that the 'Standardized ti,' distribution StandT„ is the distribution of {(v — 2)/v}1 Tv, where T,, t,,. We assume that the common DF of the Yk is StandTv, where v is a 'random' element v E \u0000 {3, 4, 6, 12, 16, 24, 32}. Of course, the StandT32 distribution is very close to N(0, 1). Indeed, the fact that all the described distributions are close needs thinking about. (Figure 341B(ii)(a) shows how close the StandT4 and N(0, 1) DFs are.) Suppose that Y1, Y2, \u0000 , Y, are IID each with the StandT, distribution. Even though we are not really in the Hypothesis Testing situation suggested by the following discussion, the HT language is useful here. If Ho is 'I/ = 6' and HA `1) = 4', then lhd(HA; \u0000 Y) ln LR := In \u0000 = S := lhd(Ho; Y) n k=1 f4(Yk) Zk := In where \u0000 is the pdf of the StandTv distribution. Now a := 1E0(Zk) ^s —0.007, and 0 2 := Var(Zk) 0.0135. (I obtained these approximate values by the cheap method of taking a million simulated values (more than once).) Suppose now that we wish n to be large enough that we have 95% probability under Ho of the extremely mild requirement that Ihd(HA; Y) < lhd(Ho; Y), that is that P(Sii < 0) > 95%. Now Sn is approximately N(na, n02), so that P(Sr, < 0) ti 4.(-71,1a10) 95% = 4)(1.64) so that, very roughly, we need n 1.6420 2/a 2 ,----- 740. Ha. Exercise. Show that a rough CLT estimate of the sample size required to have 95% probability under Ho that LR < 1/10 is 1320. \u0000 ❑ The moral is that the different v are not easily distinguished. That's something you ought to bear in mind, but I am not claiming that it invalidates the 'hierarchical' idea here. Hb. Numerical example. \u0000 I simulated a sample of size 600 from the StandT6 distribution. Assuming a uniform prior pmf (7(v) = 1/8) on the set I worked out the exact posterior pmf for n = 5, 50, 200, 600. The results are shown in the rows labelled `Exact' in Table H(i). Note the considerable change in posterior pmf between n = 200 and n = 600. Now, for a sample of size 200, a likelihood ratio for HA : V = 4 against Ho : v = 6 at least as large as 0.7204/0.2017 will occur about 5% of the time, and since we have chosen a worst case after seeing the data, the real p-value is surely significantly more than 5%. The exact results for n = 600 are more in favour of the (true!) Ho than one normally gets at this sample size. The Gibbs results are from a Gibbs sampler program described later in this subsection. In each case, 100000 sweeps of the sampler were used, no `Burn-in' time being allowed for in such a huge number of sweeps. The results are good for n = 5 and 350 Sample size = 5 8: Linear Models, ANOVA, etc 3 \u0000 4 \u0000 6 8 12 16 24 \u0000 32 nu 0.2319 0.1653 0.1257 0.1112 0.0989 0.0933 0.0881 0.0855 Exact 0.2361 0.1652 0.1244 0.1090 0.0973 0.0937 0.0881 0.0863 Gibbs Sample size = 50 0.0133 0.1502 0.2676 0.2291 0.1434 0.0975 0.0575 0.0412 Exact 0.0139 0.1557 0.2690 0.2252 0.1381 0.0975 0.0580 0.0426 Gibbs Sample size = 200 0.0321 0.7204 0.2017 0.0408 0.0041 0.0009 0.0001 0.0000 Exact 0.0748 0.7908 0.1094 0.0224 0.0024 0.0002 0.0000 0.0000 Gibbs but sampler has gone rather crazy Sample size = 600 0.0000 0.1500 0.7514 0.0964 0.0021 0.0001 0.0000 0.0000 Exact Gibbs sampler gives meaningless results Table H(i): Results for a hierarchical t model n = 50 - which persuades me that my programs (in their final versions) are correct. For n = 200, the Gibbs results are far worse, and in fact, as we shall see, the sampler has already gone crazy. For n = 600, the results are meaningless. A WinBUGS program (also described later in this subsection) coped no better. For what is going wrong, see the Discussion 353Hf later in this subsection. Hc. Programming the exact posterior pmf. We take a prior pmf 7r(v) (v E uniform on B. We have n 71-(, I y) oc 7(, ) fl myk) k=1 where f,,, the pdf of the StandT, distribution, is given by 1 \u0000 P(2 (v + 1)) ( \u0000y 2 y 0)+1) MY) = \u0000 1 + V(v 2)7r F \u0000 4v) \u0000 v - 2 Here are the key parts of the program: int nu[9] = {0,3,4,6,8,12,16,24,32}; int main(){ for(w=1; w<=8; w++){ nuw = nu[w]; sum = 0.0; for(k=1; k<=n; k++) { sum = sum + log(1.0 + y[k]*y[k]/(nuw - 2.0)); } lnKscaled = - 0.5*log(nuw - 2.0) + loggam(0.5*nuw + 0.5) - loggam(0.5*nuw); 8.4. Goodness of fit; robustness; hierarchical models \u0000 351 lnpost = n*lnKscaled - 0.5*(nuw + 1.0) * sum; a[w] = exp(lnpost); asum = asum + a[w]; } for(w=1; w<=8; w++) printf(\"%7.4f\", a[w]/asum); printf(\"\\n\"); } Hd. Programming a Gibbs sampler for this case. We use the idea from Exercise 255Ge that we can assume that Pk Gamma (1 /), rate Zv - 1) , \u0000 (Irk \u0000 NO,precpk). We take a uniform prior on ID for v. The joint pdf/pmf for (I), {(Pk, yk) : 1 < k < m}) is proportional to n au \u0000 w(v) \u0000 2 r(zv) 1] k=1 7r(v)4v - W vne-(1v-1) \" a-PO(1v e-EphyZ ra vyn We see that (Pk I v, P\" ) ,Y) \u0000 Gamma(1-v + 1, rate Iv - 1 + 7r(viP,Y) c ay. where p(-k ) signifies the vector of all pi-values with i k. The pmf 701/3, y) may be calculated from the sufficient statistics rhosum = sumlnrho = 2 pk , sumrhoysq = \u0000 PkYk• Here are key parts of the program: int dof[9] = {0,3,4,6,8,12,16,24,32}; int code, nu, double rhosum, sumlnrho, sumrhoysq, y[601], prob[9]; void ChooseRhos(){ /* Choose rhos given nu */ int k; double rho; rhosum = 0.0; sumlnrho = 0.0; sumrhoysq = 0.0; for(k=1; k<= n; k++){ rho = rnewGrate(0.5 * (nu + 1.0), 0.5 * (nu - 2.0 + y[k] * y[k])); rhosum += rho; sumlnrho += log(rho); sumrhoysq += rho * y[k] * y[k]; } 352 \u0000 8: Linear Models, ANOVA, etc } void nuProbsOf /* for nu = dd = dof[d] given rhos */ int d, dd; double lna, asum = 0.0; for(d=1;d<=8;d++){ dd = dof[d]; lna = 0.5 * dd * n * log(0.5*(dd-2.0)) - n * loggam(0.5*dd); lna = lna - 0.5*(dd - 2.0)*rhosum + 0.5*(dd - 1.0)*sumlnrho - 0.5*sumrhoysq; a[d] = exp(lna); asum += a[d]; } for(d=1;d<=8;d++) prob[d] = a[d]/asum; } void ChooseNu(){ /* Choose nu for the next sweep */ double U, psum = 0.0; code = 0; U = Unif(); do{code++; psum += prob[code]; }while (psum < U); nu = dof[code]; } int main(){ int c, sweep, Kount[9], sumKount; double estp[9]; for(c=1; c<=8; c++){prob[c] = 1.0/8; Kount[c] = 0;} ChooseNu(); for(sweep = 1; sweep <= Nsweeps; sweep++){ ChooseRhos(); nuProbs(); ChooseNu(); Kount[code]++; } for(c=1; c<=8; c++) sumKount += Kount[c]; for(c=1; c<=8; c++) estp[c] = ((double) Kount[c])/sumKount; for(c=1; c<=8; c++) printf(\"/.7.4f\", \u0000 estp[c]); } I can spell `count'; but since it is a reserved word in some languages, I have got used to spelling it with a 'IC' in programs. He. Programming WinBUGS for this example. Here's the program. model t_hier; { for(a in 1:A) { nuProb[a] <- 1/A; tau[a] <- nu[a.]/(nu[a] - 2); } w \u0000 dcat(nuProb[1:A]); 8.4. Goodness of fit; robustness; hierarchical models \u0000 353 v <- nu [w] ; for (k in 1:n){ y[k]-dt(0, tau[w], nu[w]); # variance = 1 } } #data list(n = 50, A = 8, nu = c(3,4,6,8,12,16,24,32), y=c( 0.1519, -0.2051, 0.3190, -0.1753, -2.3757, 0.8941, -0.0373, 1.9039, 0.7230, -1.3515, )) #inits list(w = 3) As stated earlier, it too runs into serious difficulties. Hf. Discussion. The problem with the Gibbs sampler is that v tends to get stuck at the same value for long periods. The fact that it is difficult to separate v- values from the data would incline one to think the opposite: that v-values would fluctuate rather rapidly; but this is not the case. n=50 nu 3 4 6 8 12 16 24 32 3 0.8477 0.1523 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 4 0.0135 0.9131 0.0733 0.0001 0.0000 0.0000 0.0000 0.0000 6 0.0000 0.0410 0.7628 0.1940 0.0022 0.0000 0.0000 0.0000 8 0.0000 0.0003 0.2214 0.6712 0.1035 0.0036 0.0000 0.0000 12 0.0000 0.0000 0.0047 0.1606 0.6211 0.2066 0.0069 0.0001 16 0.0000 0.0000 0.0001 0.0077 0.3122 0.5523 0.1202 0.0075 24 0.0000 0.0000 0.0000 0.0000 0.0167 0.2042 0.5430 0.2361 32 0.0000 0.0000 0.0000 0.0000 0.0006 0.0206 0.3293 0.6495 n=200 3 0.9997 0.0003 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 4 0.0000 0.9999 0.0001 0.0000 0.0000 0.0000 0.0000 0.0000 Table H(ii): Illustrating the 'sticking' phenomenon The alarming extent of the 'sticking' phenomenon is illustrated in Table H(ii). The matrix at the top shows for sample size 50 the estimated probability that the v-value indicated by the row is followed by that in the column. When the sample size is 200, things go completely crazy. The second row of the table for n = 200 indicates that for 10000 occasions when v was 4 before a sweep, it stuck at 4 except for the one time when it switched to 6. 354 \u0000 8: Linear Models, ANOVA, etc [[ For those of you who know about Markov chains: The transition matrix for the `n = 50' case in Table H(ii) is close to being tri-diagonal: it is mostly concentrated on {(i, j) : Pi < 1}. It is trivial to calculate the invariant measure for a tri-diagonal transition matrix by using `symmetrizability' or 'time-reversibility' or whatever you want to call it. See Norris [176]. Thus, on comparing Tables 350 H(i) and H(ii) for n = 50, we should have 0.0133 x 0.1523 0.1502 x 0.0135, 0.1502 x 0.0733 0.2676 x 0.0410, 0.2676 x 0.1940 ti 0.2291 x 0.2214. The values of the ratio RHS/LHS (RHS being right-hand side) are respectively (to 3 places) 1.001, 0.997, 0.977. Thus, our worrying picture is coherent. 1] I am not going to delve into the sticking phenomenon here. Equation 351H1 gives some pointers. I warned you earlier (at 275Eb) that adding as many new nodes to the Gibbs sampler as there are observations is a potential invitation to trouble if the sample size is even moderately large. Sure, the Ergodic Theorem does apply, but you may need millions of sweeps of the Gibbs sampler to derive benefit from it. Hg. Extending the Gibbs sampler to include (p,, a 2). Suppose that our model had been that , Y, are HD, each with the same distribution as + aZ, where Z ti StandL, and ,u, and a are assigned priors. The exact method would then break down, but the Gibbs sampler is easily modified. You should do the modification as an exercise. The Gibbs sampler is also easily modified to allow a model where Yk = By 'YvTv) Tv ''''' tv) where (say) v E {1, 2, 3, 4, 6, 8, 12,16, 24,32, cob and where each (0,,,-y,,) is assigned a prior. But the 'sticking' phenomenon persists. ► I. Robustness of methods: an example. Suppose that Y1, Y2, ... , Y6 is actually an IID sample from the t4 distribution, but that we produce a `95%' CI I for the mean of the Yk using the 'Student t' method which is exact only for samples from normal distributions. What is the true level of confidence for I? The answer is that it is just under 96%. Of course the same is true if each Yk has the same distribution as /./ + 7/4 where T4 N t4, where it and -y are constants. 8.4. Goodness of fit; robustness; hierarchical models \u0000 355 We see from this that the 't' method is robust to certain changes in the model. However, we are certainly aware by now that the t4 and N(0, 1) distributions are rather similar; and moreover, the t4 distribution is symmetric. How would the T method perform if 'errors' were markedly skewed? Well, we can try the case when each Yk has the standardized E(1) distribution, the distribution of — ln(U) — 1, where U ti U[0, 1]. We find that the true level of confidence for I is then about 89%, so that we have now lost robustness. Of course, we can easily estimate the true levels of robustness by simulation. Here are the main parts of a program which utilizes functions described earlier in the book: double Skew()freturn -log(Unif()) - 1.0;} void Do(double F()){ long c, OK = 0; int k; double sum,ssq, y, mean, estV, SE; for(c=0; c<L; c++){ sum = 0.0; ssq = 0.0; for(k=0; k<n; k++){y = F(); sum += y; ssq += mean = sum/n; estV = (ssq - mean*sum)/(n-1); SE = sqrt(estV)/rtn; if (fabs(mean) < t*SE) OK++; y*y ;} } printf(\"\\n P(mu in CI) is approx '47.5f\", ((double) OK)/L); } int main(){ setseeds(); sethtol(1.0E-5); setnu(n-1); t = invT(0.025); rtn = sqrt((double) n); Do(aGauss); prepKM_T(4); Do(KMStudT); Do(Skew); return 0; } Other simple methods from Classical Statistics may be tested for robustness in similar fashion. It is obvious how to test MCMC methods for robustness by comparing CIs obtained when one modifies the model. I have earlier emphasized testing robustness to changes in priors. ► J. Examining regression models. For many subsections now, we have considered problems associated with the simplest of situations: that when Y., Y2, • • • , Yn are IID. Inevitably, when it comes to regression, etc, one has all the analogous problems and a set of new ones. Recall from Subsection 312D that in Frequentist theory with model Yk = kt4- 0(xk \u0000 Ek'siDeachlq(0,a 2) we use Estimators Y, B, efn _2 (X, Y) for P, /3, a, and that Ri := Yk — Y — B(x k — 356 \u0000 8: Linear Models, ANOVA, etc is the ith Residual. The column vector R of Residuals is just R = PresY = uPres G• and E .1=1,Ri is the (i, j)th component of E RRT. We have, with u and e as in Subsection 312D, E {RRT} = a 2E {PresG (PresG)T} = a 2PresE{GG T}PrTes 2 Pres-PrTes = U2PresPres = U 2 = u \u0000 Pres = a 2 (/- — UUT — eeT) , since E GGT = I. We have worked componentwise with expectations of matrices, and used the linearity of expectations and property 324(J2) of projections. (We are, of course, relating everything to the standard orthonormal basis {e(' ), e(2), , e (n)} of Rn.) We see that, with — { 1 if i j, 0 if i j, as usual, E RiRj = a 2 {at 1 \u0000 (xi — Y)(xi — ) q(x, x) If n is large and each (x, — Y) is small compared with q(x, x) 2 , that is if all the leverage coefficients are small, then the Residuals R, behave rather like IID N(0, a2) variables. (Of course, E Rk = 0 and >2(xk — Y)Flk = 0, but we can to an extent forget that. The Multivariate-Normal nature of R will be obvious from the next Section.) When assessing whether a regression model is appropriate, one thing we always do is to look at a plot of the standardized residuals, the kth standardized residual being r ocbs / n_ 2 (x, y obs \u0000 obs \u0000 obs \u0000 obs ), \u0000 — rk — Yk — Yobs b i \u0000 6, \u0000 (xk Under the conditions mentioned above (since ern _2 (x, y obs ) \u0000 a) the standardized residuals should look rather like an IID sample from a normal distribution; and we can assess this via qq, Kolmogorov or x2 methods. Again, outliers should be checked for accuracy, and if genuine, we would have to consider a model 'with larger tails'. And see Rousseeuw and Leroy [200]. Look at Figure J(i). For each part n = 50. The top of each figure shows the regression line and the bottom the standardized residuals on a magnified scale. Figure (a) looks reasonable: we would expect on average 50 x 0.05 = 2.5 observations with standardized residual of size 2 or more. Figure (b) shows a situation where non-linear regression is appropriate: the method from 318F should be tried first. In (c), it is clear that the residuals have a positively skewed distribution. In (d) we have a (rather extreme) heteroscedastic case where the variance of the observation changes (here decreases) with the x-value. 8.4. Goodness of fit; robustness; hierarchical models \u0000 357 (a) Reasonable \u0000 (b) Non-linear \u0000 (c) Skewed resids (d) Heteroscedastic 2 -- :'• 0 \u0000 • \u0000 ' \u0000 `7* -2 (Magnified) Standardized residuals Figure J(i): Regression diagrams Ja. Confounding. This can again be a problem. Compare 179Gb. Sketch a situation where the sample really divides into two samples each with a positive 0-value, but where the combined sample suggests a negative 0-value. Note. 'Confounding' (which has several meanings) does not always signify something bad: deliberate confounding plays an important useful role in experimental designs. ► K. Logistic regression; and other 'Bernoulli' models. Suppose that Y1 , Y2, • • • , Yr are independent, Yk being Bernoulli (pk). We suppose that 1 logit(pk) := 111 \u0000 = -y Nxk — Pk pk so that Pk = Pk (7, 0) = 1 ± e7+0(xic —x) We think of Yk as the indicator function of Fk, where Fk occurs with probability pk which depends on the level xk of some stimulus. For example, x k might represent age of a component and Fk the event that it will fail in the next month. Or xk might represent an electric/chemical stimulus and Fk the event that a neuron subject to that stimulus will `fire'. You should check that lhd(y,0; Y) = fi{1 ± e7-F0(xk—y)} , e-r+O(xk —x) onY+0.Q(x,Y) 358 \u0000 8: Linear Models, ANOVA, etc so that the pair (Y, Q(x, Y)) of Statistics is sufficient for the pair (7,0) of parameters. Logistic regression is an example of a Generalized Linear Model. We know how to test H0 : = 7 E R against HA : E R, E R by the LR test. One could only obtain the mle \u0000 4) of ('y, 8) on the computer, of course, by solving E (yes _ pk) = \u0000 E - (yzbs - filo = 0, \u0000 Pik pk or, ,j). All standard packages will do logistic regression. There is, of course, no difficulty in analyzing the model on WinBUGS for any sample size. The program would contain xbar <- mean(x [] ) ; for (k in 1: n){ logit (p [k] ) <- gamma + beta*(x [k] - xbar) ; Y [k] \u0000 dbern(p [lc] ) ; } Ka. Other Bernoulli models. \u0000 With the WinBUGS package comes an example (`Surgical') in which for hospital j (1 < j < J), r 3 babies died out of 71,3 who had heart surgery. One way to study this is to assume that the true failure rates p, (1 < j < are independent (a fixed-effects model). Another way is to use a linked (or random-effects) model in which it is assumed that failure rates across hospitals are in some way similar. Thus, for example, one can suppose that logit(pi ) = \u0000 ,,, N(p,prec p). In the prior for this hierarchical model, the parameters bi are assumed independent N(p, prec p), with the hyper-parameters pc and p having 'vague' priors. You can check, if you wish, that WinBUGS only has to sample from log-concave densities. CIs for the different p3 will be 'pulled towards one another' to some extent. We shall look at the effects of using 'linked' models on an ANOVA case in the next subsection. ►► L. Modified (`random-effects') ANOVA models. How might we modify the classical model? Let's write the general additive model in the form Yjk \u0000 j + 13k + crEjk, \u0000 Ok = 0, where our primary concern is with the 03 's. One thing to consider — and such things must be considered separately for every situation — is whether to adopt the 8.4. Goodness of fit; robustness; hierarchical models \u0000 359 idea at Ka of using a 'linked' or 'random effects' model. For example, we might use a Normal-linked Normal' model in which 01i 02, \u0000 , Oj are BD each N(//, prec r), where we take vague priors for pt and r. (Note. This model can be analyzed by classical Frequentist methods, as was done long ago by Daniels.) If one does do this on the example which we studied in Subsection 289F, then, as we shall see, one changes CIs to a fairly significant extent. For the moment, assume that an additive model (one without interaction) is appropriate. Of course, using normal errors in any way might be less sensible than using 't' errors; and one can do this with linked or unlinked models. Consider four possible models for the numerical case at Subsection 289F. Classical (unlinked Normal) model: /3k vague, c's Normal, a vague. Normal-linked Normal model: 03 \u0000 N(/ 7 prec r), /3k vague, /2, r vague, c's Normal, a vague. Unlinked t4 model: 03, i3k vague, c's t4, a vague. t4-linked t4 model: Oi = \u0000 t4, /3k vague, tt, r, a vague, c's t4. Here are results for our numerical case, the last 3 columns being produced after runs of 10000 on WinBUGS Classical N-linked N unlinked t4 t4-linked t4 theta[1] (4.73,11.27) (5.80,11.93) (3.85,11.74) (4.92,11.93) theta[2] (10.73,17.27) (8.34,15.86) (11.14,16.83) (8.61,15.91) theta[3] (9.73,16.27) (8.23,14.99) (9.89,15.87) (8.60,14.97) theta[4] (5.73,12.27) (6.64,12.26) (6.00,11.93) (6.21,12.43) theta[5] (2.73, \u0000 9.27) (4.11,11.55) (3.20, \u0000 8.88) (3.95,11.11) Because of comments in Subsection 276H, I tried several different priors, but all with similar results. That the 'N-linked N' model pulls things together compared with classical model is sensible. The results for the `t4-linked t4' model are not that different from those for the 'N-linked N'. The `Unlinked t4' results puzzled me a little, so I ran them on WinBUGS for different length runs with different seeds, all with similar results. It is interesting that this method is reflecting better than the others the fact that the first row 360 \u0000 8: Linear Models, ANOVA, etc is out of line with the other rows (see the next section), because of the slow tail-off of the t4 density. The degree of flexibility you have is enormous. Of course, you could use a `hierarchical t' model, allowing different degrees of freedom, etc. In an important paper on agricultural field experiments, Besag and Higdon [20] use 'hierarchical t' models and show how to build in (amongst other things) the fact that neighbouring plots of soil will have similar fertility. For an important recent paper on these topics, see Nobile and Green [175]. La. Some comments on flexibility. \u0000 I repeat that it remains important to remember the Principle of Parsimony: use the simplest reasonable model. By all means investigate related models, but use them to convince yourself that when you talk of a 95% CI, the 95% must be taken with several pinches of salt: what you really mean is your confidence lies somewhere between 93% and 97%, or something similar. If you have rejected various models before deciding on your final one, you should say so. And do be careful about data snooping: most pieces of data will have some strange aspects; and focusing on them after seeing the data can be very dangerous. ► M. Some thoughts on goodness of fit for ANOVA. Figure M(i) is, I hope, self-explanatory. One should always draw such a figure. What it shows for our numerical example is that the first row (A level 1) is `out of line' with the other rows. It suggests the possibility either of interaction (with non-zero (a0) 1k terms) or increased error variance for row 1. Of course, the fact that row 1 seems 'out of line' is reflected by the fact that its residuals are larger than those for the other rows. In fact row 1 contributes a fraction 26/48 of the residual sum of squares. We can solve the problem: How likely is it that some row will contribute as least as high a proportion of rss if our model is correct? It is important to realize that if our model is correct, the described probability will be the same whether Ho holds or not. In this case, we find that the probability is close to 10%, so we would be cautious about claiming interaction if there is no a priori reason to suspect that it might occur. ► Ma. Exercise. Estimate by simulation the probability that for our additive ANOVA model with 5 rows and 3 columns, some row will contribute a fraction at least 26/48 of the total rss. Of course, interaction should be investigated via a model with replication as in Subsection 331M. If you are presented only with a J x K table, then you 8.4. Goodness of fit; robustness; hierarchical models \u0000 361 15 0 2 \u0000 3 B levels Figure M(i): Diagram showing possible interaction must provide a diagram such as that at M(i), commenting on what possibilities of interaction it suggests. You must also do the analogue of Exercise Ma. If that exercise forces you to agree that there is interaction, your first reaction would be to beg for results with replication; but if they prove impossible to obtain, then read the discussions of interaction in Cox [48]. N. A 'paradox'. See Exercise 10.4 in Cox and Hinkley [49], based on work of Stone, Springer and Dawid. I present it mainly as an example to show how grown up you now are in that you can cope with quite complicated things and understand their geometry. Consider the random-effects model Y3k = µ + a j E l k , cei N(0, prec pb), Elk N(0, prec p,„), 362 \u0000 8: Linear Models, ANOVA, etc with all variables ozi(1 < j < J) and E3k (1 < j < J, 1 < k < K) independent. The suffices b and w refer to 'between sample' and 'within sample' terminology. Na. Exercise. Find (using 'bare hands', rather than our geometry which I do below!) Pb, Pw; Y7 a )• By 'completing squares', integrate out the ad's to show that 2t(p, Pb, pw; y) = const Jln pb + JK ln pw — Jln p PbPw SSw(y) JK pbpw _ — pwSSw(y) \u0000 (Y. — where P = K Pw Pb, SSw (y) = \u0000 (Yjk — ) 2 , SSb(y) = KE \u0000 - y,,„)2 Note that the variables Y.., SSw(Y), SSb(Y) are independent, and that the triple they form is sufficient for the triple (a, pb, pw) of parameters. Remark. The terms involving pbpwlp would make for very difficult Gibbs sampling if one did not use the ad's as nodes! Nb. Discussion. (This is largely to see how the geometry works.) Working in RJ 0RK, with aw = 11pw and 01 = 1/pb, we have Y = ul ®1 ± o-bH ®1 ± o-wG, H SN(R J ), G SN(R J R K ). We have with U := [1] in R-1 and V := [1] in RK , and with Grow the vector of row means of G (so that (Grow)3 -= \u0000 N(0, o-w2 /K)), PuovY = Y..1 ®1 = Fi1 ®1 + o-b(PuH) ®1 + o-wPuovG, Pu LevY = (Pui(abil+ awG row)) ®1, \u0000 (`between') PaJew-LY = awPm oviG. \u0000 (`within') We have the component formulae: (Pu-LovY)ik \u0000 Y** = ab (H 3* H**) + aw (G3* G**), (PRJov-LY)ik = Yjk Y We find that, with o-2 = Ko-g + o-w2 , so that pbpw I p = 1/0-2, we have rs, N(0,0-2/JK), SSb(Y) = 11Puiovill 2 sst,(Y) = 11Pill'ovill 2 rs' 2 2 awXJ(K-1)) 8.4. Goodness of fit; robustness; hierarchical models \u0000 363 these variables being independent. (So, we have found useful consequences of the geometry!) We could find a Confidence Interval for 11 via the fact that (JO (Y.* \u0000 (Ni) ISSb(Y)/(J 1-)P This would be exactly as if we were told only the results Y 3,, (1 < j < J). However, since in particular, a 2 > v,2y , we must use all the information. The 'paradox': If we use the prior density 1 (A, aw, \u0000 cx cra (a>a,,,>0)(al m - then the CIs for p given all the information will be wider(!) than those obtained from (N1). Like so much else from the past, it doesn't seem terribly relevant nowadays. Surely, in any case it is (a-b, aw) which should have prior (abo-„)-1. Nc. Exercise. Write a WinBUGS program to get a CI for p, using appropriate vague priors. Use the a's as nodes, of course. ► 0. Contingency tables. Since we have studied the Chi-Squared Principle, we ought to look at one of its most famous uses: that in contingency tables. Suppose that each individual in a large population falls into one of J categories relative to 'factor A', and into one of K categories relative to factor B. For example, some diseases are more likely to be caught by rich people than poor, some less. We might divide income into J ranges to give the A factor, and classify whether or not individuals contract a particular disease as B factor (K = 2). Let Rik be the probability that an individual chosen at random belongs to category j relative to A and category k relative to B. Let aj := E k Rik be the `marginal' probability that an individual chosen at random belongs to category j relative to A, and define /3k analogously. Primary interest has attached to deciding whether factors A and B are independent in that the Null Hypothesis HO Pjk = ctjOk holds. We use the 'full' Alternative Hypothesis : EEpik = 1. This seems a situation where Hypothesis Testing makes good sense. 364 \u0000 8: Linear Models, ANOVA, etc Suppose that a random sample of size n is taken from the population. Let Nik be the number in the sample falling into category j relative to A and category k relative to B. Let Ai := n-1 E k Njk and Bk := n-1 E N 3 -k. The Maximum Likelihood associated with Ho will occur when (for all (j, k)) cad = Ai and i3k = Bk. Exactly as in Subsection 345E, we find that for large n, if Ho is true, then (N3k — nAiBk)2 PearsonStat := EE \u0000 rtAiBk has approximately the X2i, distribution where v = (J — 1)(K — 1), the difference in dimension between HA (for which dim = JK — 1) and 1/0 (for which dim = J — 1+ K — 1). ► Oa. Exercise. The Star-Ship Enterprise is approaching Planet ChiCrazy. Each individual on the planet is known to be either Green-eyed or Red-eyed. Based on certain information, Dr McCoy has set up a Null Hypothesis that for an individual chosen at random, eye-colour and sex are independent. Mr Spock, typically, has made the precise Null Hypothesis that PGM - PRM - PGF - PRF, where PGM is the proportion of Green-eyed Males. The Enterprise receives a message that 400 randomly chosen individuals from the huge population will greet them. When they land, they find that these individuals form the 2 x 2 Table Oa (Actually, these are not quite the numbers, but I've made the calculation Green Red Male \u0000 87 113 Female \u0000 113 \u0000 87 Table 0(i): ChiCrazy table easy for you without spoiling the message.) Check that for Dr McCoy and Mr Spock, the Pearson statistic takes the value 6.76. Check that Dr McCoy has to reject his Null Hypothesis at the 1% level, but that Mr Spock does not have to reject his even at the 5% level. \"This is simply not fair;\" says McCoy, \"if your Null Hypothesis is true, then mine must be true. Yet I have to reject my Null Hypothesis, but you do not have to reject yours.\" \"Yes,\" says Spock, \"but you see, there is logic in the situation ... \". What does Mr Spock go on to say? Ob. Exercise. Suppose that (X1, Y1), (X2, Y2), \u0000 , ( X„,17,) are n IID Ek2 variables each assumed to have the same distribution as (X, Y). Suggest a test of independence of X and Y. 8.5. Multivariate Normal (MVN) Distributions \u0000 365 8.5 Multivariate Normal (MVN) Distributions This section brings lots of nice Mathematics to the service of Statistics. I hope that you will like it. Let's first think about why we need some more algebra. Suppose that X1, X2, X, are RVs, all with zero mean. Then the variance-covariance matrix V with Vii = Coy (Xi, Xi ), so that V = E XXT , is symmetric (V = V T ) and nonnegative-definite in that wTVw> 0, w E Rn This is because 0 < E ((w, x)2) = IEwTXXTw = wT(Exx T)w = wTvw. So, it is not surprising that we need to study nonnegative-definite symmetric matrices. I am sure that you know that if B is an n x n matrix, then we can solve Bw = 0 with w 0 if and only if det(B) = 0, that is, if and only if, B is singular. ►► A. Fact: Spectral Theorem for symmetric matrices. If A is a symmetric matrix, then there exist (non-trivial) orthogonal (that is, perpendicular) subspaces , W2, . , W,. of IC' with sum Cn such that A = \u0000 72P2 \u0000 • ' 'Yr Pr \u0000 (Al) I = + P2 ± • \u0000 Pr, where Pk denotes perpendicular projection onto Wk and 71,72, • , -Yr are distinct real numbers. Then, g (A) = g(N)Pi g(72)P2 + • • + g(`ir)Pr \u0000 (A2) for any polynomial function g. Here the -yk are the distinct eigenvalues of A, the distinct roots of det (7/ — A) = 0, and Wk is the space of eigenvectors of A corresponding to -yk: Wk = {W E Cn : Aw = -ykw} . s11 0 0 8 21 S22 0 S31 832 S33 811 821 831 O \u0000 822 S32 0 \u0000 0 833 • V12 V13 V22 V23 V32 V33 366 \u0000 8: Linear Models, ANOVA, etc If dk = dim(Wk), then det(A/ — A) = II(A — y k ) dk, \u0000 det(A) = 11 -ykdk Proving that the roots of the equation are real is easy — see Fact 449Aa. The more difficult thing with Fact A is to prove that A has enough eigenvectors to span IV. Because n = Pk and PkPi =- 0, it follows by induction from (Al) that Am = \u0000 P1 + 'Y2 P2 + • • • + ' Yrrn -Pr, so that 365(A2) follows for polynomials g. B. Positive-definite symmetric matrices. As we have seen, we say that a symmetric matrix A is nonnegative-definite if wTAw > 0 for every w E RV. We say that A is positive-definite if wTAw > 0 for every non-zero w in lir . Let A have spectral decomposition as in Fact 365A. Then • A is nonnegative-definite if and only if 'Yk > 0 for every k, • A is positive-definite if and only if Yk > 0 for every k. This is because w TAw = (w TP1 + • • • + wT/37.)(-yi Piw + • • • + = \u0000 + • + -YriiPrw112, while 11w112 = IIP/w112 + • • • +1Prw112. Note that if A is a nonnegative-definite symmetric matrix and we put S := 77 \u0000 +...+ ITT•Pr, then (S is nonnegative-definite symmetric and) S2 = A = SST. Ba. Exercise. \u0000 Prove that if A is positive [respectively nonnegative] definite, then the matrix formed by taking just the first k rows and first k columns of A is positive [respectively nonnegative] definite. Prove that if A is positive [respectively nonnegative] definite, then det(A) > 0 [respectively, det(A) > 0]. Bb. Choleski decomposition. \u0000 This decomposition is very useful for simulation. Suppose that V is positive-definite symmetric. Then we can find a lower triangular matrix S such that SST = V. Note that the matrix SST is symmetric, and (Why?) is positive- definite if S is non-singular. Writing out the equation SST = V, we have 8.5. Multivariate Normal (MVN) Distributions \u0000 367 and we must have 2 = \u0000 sr1 = V171811 (r > 1), 42 — V22 41 — (V11V22 V12V21) /41• etc, etc. Note that 2 2 \u0000 ,2 S11 522 • • ' 'kk \u0000 ( V11 \u0000 Vlk = det \u0000 vki \u0000 Vkk and since the determinant is positive, it is easy to believe that the Choleski decomposition will work. C. The Multivariate Normal Distribution MVN,(p,, V). Let p E Rn and let V be a nonnegative-definite symmetric n x n matrix. Then a Random Variable Y with values in R is said to have the MVINI,,(p,, V) distribution if, for every vector a in Rri, E exp WY) exp (i A \u0000 Va) . \u0000 (Cl) Uniqueness of the distribution is guaranteed by Characteristic-Function (Fourier) results. If S is such that SST = V and Y = p + SG where G \u0000 MVNn (0, /) (so that G1, G2, \u0000 Gr, are IID each N(0, 1)), then iaTY _ iaTiz + ia T SG = iaT p + i(ST cx)T G, so that 1E exp (iotTY) = exp (icxT p — a aT S I ST a) = exp oa r it (IT Va) so that Y has the MVN n (A, V) distribution. Note that this and the Choleski decomposition give a way of simulating Y if V is positive-definite. Note that since E (G) = 0, we have E (Y) = p. Also, E (Y — p)(Y — p)T = ES(GG T )ST = (GG T )ST = SST = V, so thatµ and V have the correct significance as mean vector and variance-covariance matrix for Y. ► D. Pdf for MVN„,(il, V) when det (V) 0. Suppose that p E 11111 and that V is a positive-definite symmetric n x n matrix. Let Y MVNn (p, V). We use the representation Y = p + SG, SS T = V, so that G = S-1(Y — p). Now, G has pdf on Il given by f G(g) = (27r) — exp(-1gTg)• (2 ) Vdet(V) exp \u0000 Y tO TV-1(Y A)} • \u0000 (D1) 71- 2 fy(y) 1 368 \u0000 8: Linear Models, ANOVA, etc If g = S -1 (y — p), then J(y) = det ((S-1)T) with our conventions. Thus IJI -=- 1/ Aidet (V) and .fY(Y) = fG(S-1(y — p))1.11, whence, since gTg (y itL)T (s-1)T s-1(y µ) = (y p)T v-1 (y \u0000 ) we have Of course, the fact that fRn fy(Y)dY = f ec • • • \u0000 fy(yi, • • • Yn)dYi • • dYn = 1 \u0000 (D2) —00 \u0000 —00 follows from our argument. E. Some key properties of MVN distributions. ► Ea. Linear maps preserve the MVN property. Suppose that Y MVN, (µ, V), that B is an m x n matrix, and that Z = BY E Rm. Then, for a c \u0000 a TB = (BT cor so that E exp (ia TZ) = E exp (i(BT a)TY) = exp (iaT Bp — z c;tT BVBTa) , so that Z MVN m (Bp, BV BT ). Thus, as claimed, linear transformations preserve the MVN property. Eb. Partitioned matrices. The (m + n) x (r + s) matrix / an ami Cll \\ \u0000 en ]. al,, air an, b11 bmi d11 dm]. bls \u0000 N bms d13 • dm, / ( A is written C B D ) where A is m x r, etc. Partitioned matrices 'of compatible dimensions' multiply in the usual way: (C B)(E AE BG AF BH 1 H ) CE-F.DG CF+DH )• ► Ec. 'Orthogonality implies independence within MVN'. Suppose that W MVN,±s(p, V), that, with Y E R\", Z E Rs, we have Yz , E(W) = = Apt: , V = E(W — p)(W — p)T = 1 Vr v.° z . 8.5. Multivariate Normal (MVN) Distributions \u0000 369 Do note the supposition about zero entries in V. Then, with a = (gy), we have E exp (ia yTY + ia zTZ) — exp Za T Va } = exp {lay µ y — lot'Vyucxy } exp ia Tz \u0000 aT z Vzzaz } = E exp (ia yTY) E exp (iaTZ) ; and that this is true for all ay E RI' and all a z E Rs guarantees that Y and Z are independent. Thus, if Y and Z have a joint MVN distribution, and if Y and Z are orthogonal (yet another sense of the term!) in that E YZT = 0, then Y and Z are independent. Ed. Exercise. Check out result 295Ga. F. Conditional distributions within an MVN scenario. Suppose that Yxx yX \u0000 MVN ( 0 0 ) , vyx Let Z = X — AY, where A := VzvV y'. Then, yZ \u0000 .ro — I) yX is MVN, and EZYT = EXYT — AEYYT = Vxy — AVyy = 0, so that Z is independent of Y. We know that Z MVN(0, B) for some B. Indeed, since E ZYT = 0, we have B = EZZ T = EZXT = Vex – VxyVyyl Vyx• In particular, X 1 Y U MVN (AY , B), A := VxyVyTil B V;„ AVM,,. \u0000 (Fl) the general theoretical regression formula for multivariate normal variables. ►► G. The Bivariate Normal (BVN) distribution. Suppose that X and Y are real-valued RVs such that Vx y Vy y VN(Ax , Ay; 0.! , o_y2 ; ) p. B \u0000 := MVN \u0000 lux , \u0000 2 ar 1-ty \u0000 Payax PCIsCly 0_2 ) ) Note that the symbol p is now restored to its traditional 'correlation coefficient' use. By the result of the last subsection, we have Y X MVN (µy p-0_°-Y (X — Px), (4(1 — P2)) • 370 \u0000 8: Linear Models, ANOVA, etc Thus, as we know from Chapter 3, ay y = /Ly p— ( x — Az ) crx is the theoretical linear regression line of Y on X. It is used to predict Y-values from X- values. Note that the variance of a prediction of a Y-value from an X-value is ay (1— p2), so is very small if p is close to 1. We know from Chapter 3 that if p = 1, then we can predict Y from X exactly (with probability 1). There are many situations in which BVN distributions have been used. One might for example have X for man's height, Y for height of his son (both at maturity). It was this example which led Francis Galton to introduce Regression Analysis. See Freedman, Pisani and Purves [83] for illuminating discussions on this type of example and especially for the reason for Galton's term 'regression'. Axes of ellipses of constant likelihood in grey Regression lines in black. 1: Y on X; 2: X on Y Figure G(i): BVN with (us, uy, p) = (2, \u0000 1/f2) 8.5. Multivariate Normal (MVN) Distributions \u0000 371 \u0000 ► Ga. The correlation coefficient p as a measure of association. \u0000 If there is good evidence that p 0, we say that there is evidence of an association between the 'X' and `Y\"factors'. This does not imply that there is any causal relation. The situation may be confounded in that there is a common cause for the two 'factors'. It is of course possible that there is a causal relation with (say) an increase in X causing (on average) an increase in Y; but remember that p is symmetric in X and Y, and (if an increase in X does on average cause an increase in Y) it is often extremely unlikely that we could also claim that an increase in Y causes an increase in X, even though a larger Y-value would lead us to predict a larger X-value as having caused it. Gb. Contours. The logarithm of the joint pdf of X and Y is f.X,Y(X) Y l aux, I-ty; ax, ay; P) \u0000 = — In(271-) — 1n(o-zo-y) — 2 ln(1 — p2) \u0000 (G1) 1 \u0000 f (X \u0000 _ eux )2 + (y µy)2 \u0000 21)(X - \u0000 (y p y) 2(1 \u0000 p2) \u0000 0-2 asay The contours (level curves) on which the pdf of X and Y is constant are ellipses centred on (Ax \u0000 )2' and with major axis parallel to the vector ( \u0000 T \u0000 2po.xay 7 { (ax2 cry2 ) + 4p2 crx2 cry2 } If \u0000 (crx2 ay) \u0000 . Coordinate geometry (though, in effect, the Spectral Theorem) tells us that the major- axis vector is an eigenvector of V corresponding to the larger eigenvalue of V (or, more properly, an eigenvector of V-1 corresponding to the smaller eigenvalue of V-1). After shifting the origin to (pr , py)T, a contour ellipse has the form z Tv—l z = r 2 = (s—l z )Ts—l z, so that z = Sw, where w lies on the circle of radius r. We can therefore easily draw the ellipse which contains the 'high pdf' region (the smallest region of the plane) of given probability p. We simply choose r such that p = P(GT G < r) = 1 — eAr. The theoretical regression line of Y on X bisects any vertical chord of any ellipse 'of constant likelihood', so must go through the rightmost point of any such ellipse. The theoretical regression line of X on Y bisects any horizontal chord of any ellipse 'of constant likelihood', so must go through the topmost point of any such ellipse. Gc. Exercise. Prove the italicized result above by using the formula for the sum of the roots of a quadratic equation. Gd. Remark. Freedman, Pisani and Purves [83] contains very much wisdom about correlation, as about so much else. Be careful though that you are not misled into thinking that their 'SD line' is the major axis of an ellipse of constant likelihood. If, for example, ax = 1.6, ay = 1, p = 0.2, then the SD line is at 32° while the major axis is at 11°. The SD line has no particular theoretical significance. 372 \u0000 8: Linear Models, ANOVA, etc ►► H. Statistics associated with correlation coefficients. \u0000 Suppose that x2 ay2; p) (Xl, Y1), (X 2, 172), • • • , (Xn, \u0000 are IID, each pair having the HVN(ttx, \u0000 0. distribution. Then f;(,y (x, Y Px Py; ax, ay; P) = —n, ln(27r) — n ln(o-xo-y) — \u0000 ln(1 — p2) 1 \u0000 f Q(x, + n(Y — px)2 Q(y, y) n(y — py)2 2(1 — p2) 1 \u0000 Q2 \u0000 0-2 2p Q(x,Y) +n(Y - 1-lx)(V 0-xa y For the mle (Az lby; 6-x, ay ; /3) of (px, py; ax, ay; p), we have the hardly-surprising results: fi x = \u0000 = y, ax = rt-1Q(x, x), n-1Q(x, y) ax ay 21 = \u0000 Q(Y,Y), Ha. Exercise. Check out these formulae. Have faith if it starts to look a little messy! Verify that (Px, Py; ax, ay; /3) is a sufficient statistic for (p x, p,y; ax,av; p). ► Hb. Test of independence or of 'no association'. One of the most celebrated Hypothesis Tests in Statistics is the 'test of independence', that is, of Ho: p = 0 (and µ x, ett y E R and ax, ay E (0, CC))) against HA : p 0 (and px, py e R and ax, a y E (0, co)). Hc. Exercise. Check that we have the very simple formula 21nLR(X,Y) = —nln(1 — R2), where R is the MLE of p, the Sample Correlation Coefficient: R = \u0000 Q(X, Y) {Q(X, X)Q(Y, Y)} • Hence, we reject 1/0 at size a if robsI > ra , where Pp=0 (IRI > r,) = a. You can see from 'shifting and scaling' that the distribution of R under 1/0 does not involve any of /Ix, Ay, Qx, ay • It was conjectured by 'Student' and proved by Fisher, that if p = 0, then T``„_2 := \u0000 R 2 v \u0000 n-2- \u0000 (HO 8.5. Multivariate Normal (MVN) Distributions \u0000 373 It would be argued by some that in these days of computers, it is not necessary to know how to prove this result. However, we can prove it in a way that helps us understand our geometric picture more fully. Proof of result (HI). Suppose that p = 0, so that we have independence of the X and Y components. Then, a-;1(X — X1) — SN([1]1), 0;1(Y — Y1) — SN([1]1 ). Hence, Tn_2 has the same distribution as (13 ,V) T \u0000 — 2, { (U, U) (V, V) — (U, V)2} where U, V are independent each SN(Rn-1 ). We can even prove that conditionally on U, T has the t71,_2 distribution. Suppose that U 0 is fixed. Then, we can write V = aU + W, where all Ull — N(0, 1) and W SN([ULL). We then find that T = \u0000 — 2, 1WII and since 11WII2 \u0000 x2n-2, the result follows from the definition of the t distribution. 111 Hd. Fisher's z-transform. Fisher discovered the remarkable fact that, whatever the value of p, for large n, Z \u0000 ln \u0000 \u0000 1 + R R-J N ln 1 + p \u0000 1 \u0000 2 1 — R \u0000 2 1 —p' n-3 Hotelling later discovered refinements of this result. The proofs of these results are complicated, and I skip them. These days, you can even find a Confidence Interval for p directly on the Internet (under several sites found via 'correlation coefficient') by typing in your values of ebs and n. Note that if p = 0, then, remembering that tn_ 2 N(0, 1) when IL is large, we have \u0000 2 \u0000 N(0, 1)2 \u0000 y ti R2 - \u0000 Tn-2 = x n-2+T,~_ 2 Ti showing that 21n LR x?, as predicted by the Chi-Squared Principle. Of course, for large n, we could use the Chi-Squared Principle to provide a rough estimate of the significance level of the data for testing H0 against HA. For the value ra for such a test when n = 32 and a = 5%, both Fisher's exact 'T' method and Fisher's approximate 'Z' method give (to 3 places) 0.349 whereas the 7C2 method gives 0.336. And, of course, you can get individual CIs for all the parameters for this BVN situation from WinBUGS . Use a variety of sensible priors. Vki := COv(Xk, Xt) = { ■ \u0000 Pk — pz if , k = E, E Xk = Pk, E Xk Xe = 8ktPk, — PkPQ \u0000 if k .e. 374 \u0000 8: Linear Models, ANOVA, etc ► I. Multivariate Central Limit Theorem. Suppose that X is a Random Variable with values in Rb , with each X 3 in £2. Let p := E X, V := E (X — p)(X — 11)7 . Let X(1) ,X(2) , ... be independent Rb -valued RVs each with the same distribution as X. Then, for large n, S(n) = X (1) + + X (n) \u0000 MVN b(n , nV) in the usual sense that for any 'rectangular region' A in lRb , IE (S(n) — nµ E A) —> P(Z E A), where Z MVNb(0, V). The proof is just a vector form of the proof of the CLT. J. Application to the Pearson Statistic. Return to the situation of Subsection 345E. So, suppose that Y1, - - Y 2, • ..,Yn are IID RVs each with values in {1, 2, ... , b}, such that P(17,, = k) = p k (1 < k < b,1 < m < n), \u0000 = 1. Let Nk be the number of Ym equal to k. Let X (,,,,) \u0000 {1 if Ym = k, k 0 if Ym k. Then, for fixed m, X(m) E le has the same distribution as X, where in particular, By the Multivariate CLT, for large ri, S (n) = \u0000 N2 \u0000 Nb ) T \u0000 MVNb(np, nV). Let Nk — 71,19k Wk = • Vnpk Then, for large n, where (you check) C = PI [u ] W MVN b(0 , C) u := (VPI 7 • • 7 N/F71) T - 8.5. Multivariate Normal (MVN) Distributions \u0000 375 Thus, C has one eigenvalue 0 (with associated eigenvector u) while all other eigenvalues are 1. Thus, for a e ]R, I + 2aC has one eigenvalue 1 and all others 1 + 2a, whence, det (/ + 2aC) = (1 + 2a)v, where v = b — 1. Now, if W MVN(0, C) where C is symmetric and strictly positive-definite, then with Q = E Wk, and using 368(D1) and 368(D2), Ee —aQ = (270 -16 (det C)-1 \u0000 e- w T (C -1-2anw dw {det(C-1 + 2a/)} (det C)1 It is easy to argue by continuity that the equation Ee'Q = {det(/ + 2aC)}-1 holds if C is just nonnegative-definite, as our C is. (Just replace C by C + El where E > 0, and then let E ,I, 0.) Since PearsonStat= E WI and W MVN(0, C), we now see that Ee—a(PearsonStat ) (1 + 2CE) 2L , so that PearsonStat \u0000 Gamma(1 v, \u0000 = x2, as required. Ja. Remarks. I describe a — loose — analogy with Quantum Theory which it seems hard to tighten into anything of real significance. As we shall see in Chapter 10, if A is the diagonal b x b matrix with (k, k)th element k and v is a quantum state (unit vector in le), then a measurement of the 'observable' associated with A when the system is in state v will produce the result k with probability vz, and the system will then be in state e(k), the kth element of the standard basis. In our x 2 context, the Null Hypothesis is — loosely — related to the statement that the quantum system is in state u. The matrix C corresponds — loosely — to testing whether or not the Null Hypothesis fails. If the system is in state v and the quantum measurement associated with the observable C is made (for our particular C), then we shall obtain result 0 with probability (u, v) 2 = cos (p, where (p is the angle between u and v, and result 1 with probability sine (p. If the measurement associated with C produces result 0, then the system will be in state u and the Null Hypothesis will be true; if 1, then the system will be in a state perpendicular to u which corresponds to the assertion that the Null Hypothesis is false. It is as if measuring C commits Nature to making a definite decision: is the Null Hypothesis true or not? = {det(/ + 2aC)}-1 . ►► K. Asymptotic multivariate normality for MLEs. It will not surprise you that most results from Chapter 6 have multi-parameter extensions. 376 \u0000 8: Linear Models, ANOVA, etc Ka. Exercise. (We use r, s, t to look different from i, n.) Let Y be a real-valued RV with pdf f (y 0), where 0 = (01, 02, • • • , 08) c Rs. Define an Rs-valued RV K(0, Y) and a symmetric s x s matrix If* (0) via Of \u0000 2f* {K(0, Y)}, — ae (Y 10), \u0000 {I f* (0)}''t \u0000 E aoraet (Y 0). Show that, with E denoting E (• 10), E K (0 ,Y) = 0, \u0000 E K (0 ,Y)K (0,Y)T = f * (0). Show that for any vector w E wT/ f . (0)w = E {(w, K(0,Y)) 2 } > 0, so that I. (0) is nonnegative-definite. Now let Y1, Y2, \u0000 , Yn be IID real-valued RVs each with pdf f (y10) which is as in the Exercise. Let (3 be the MLE of 0. That the MLE is consistent in the sense of Subsection 193C may be established by the methods sketched there. So, for large n, we may assume that 0 is close to 0. From the above exercise and the Multivariate CLT, we have, for large 72, with the obvious .4(0) = nI f* (0), at { u(o)}, := \u0000 (Y I 0) = aer n i=1 {K(0,Y,)}, , SO \u0000 U(0) \u0000 MV1\\1,(0, h(0)). Let us assume that /i(0) is positive- definite. By Taylor's Theorem, \u0000 0 = U(6) \u0000 U(0) + It(0) \u0000 — 0) , so o - 0 -h(e)--Iu(e), a Newton—Raphson result. By property 368Ea, o \u0000 MVN, (0, /t(0) ) and that's the Frequentist story. Kb. Exercise. Mimic Subsection 204F to give the Bayesian version. The fact that, for large n, Frequentists and Bayesians can behave as if 0 — (3 MVIV, (0, Mor i) was the mainstay of important computer programs such as GLIM and GENSTAT. You can see how one could do (for example) the large-sample analysis for logistic regression at 357K. But really, what is the point when WinBUGS can treat any sample size? 8.5. Multivariate Normal (MVN) Distributions \u0000 377 ∎ L. Heuristic explanation of Wilks' Chi-Squared Principle. We now set out to give a heuristic explanation of Wilks' Chi-Squared Principle at Subsection 345Da. We need a preliminary Lemma. La. Lemma. If Y MVN,(0, V) where V is non-singular, then yTv-ly XT. Proof. With V = SST, we have Y TV-1Y = G TSTV-1SG = G TGT XT. 111 Differential-geometry considerations (see the Subsection JP3) mean that in seeking to understand the Chi-Squared Principle, we can reduce to the case when B = (f) E Rs, v being a nuisance parameter in Rm and yo having values in W, where r -= s m in the notation of Subsection 345Da, and where we have Ho : (1:7 = Sao, v E R\"'\" , \u0000 HA : (i) E Rr , v E \u0000 , (po being a fixed vector in W. We suppose that the Maximum-Likelihood Estimator associated with Ho is e0 = — ((Pc') and the MLE associated with HA is 8A = ((!(;;, A) • A Define R to be the s x s matrix with (i, j)th component a2f Rii := aoi ae; evaluated at e A and partition R as R -= ( R`Pc° Rvc, Rvv) • With at/a,. denoting the column vector with kth component afla,k, we have at \u0000 at „ o—o = \u0000 (y1s00,1,0) (YlwA,vA) ay. \u0000 ay. •-,-• Rvc, (50A — coo) + Rv, Cl ) A — Po) • Thus, 1)A — 1)0 \u0000 —11,-,1Rvw C A — (Po) • \u0000 (L1) By Taylor's Theorem around eA (remember the minus sign in the definition of R), —21nLR = 22 (Y coo, i)o) — 22 (Y I c; A> 11 A) _ (P0 — \u0000 ) T R (‘Po — SPA 1 v0 —LA \u0000 1/0 — VA 378 \u0000 8: Linear Models, ANOVA, etc and so, substituting from (L1), we have 21nLR \"°(OA — (Pc)T (I, \u0000 Rvw Rvv ) 1 —Rj9=1,,p ) (OA — Coo) \u0000 = (O A — wo)T (1=,,,„ — RpvR 1Rvp) (coA \u0000 0) • Now suppose that Ho is true, so that the true value of 0 is (Cavo) for some v e We have seen on several occasions that, by consistency of the MLE, the value of R at 0A is essentially equal to its value at the true 0. Moreover, we have seen that the Strong Law (or better, the CLT) effectively allows us to replace R by MO), the two differing by a factor 1 + 0(n- 1) in a rough sense. By the asymptotic-normality results of the previous subsection, 1 0 \u0000 11,\" T (c° A 'Po \u0000 MVN,, (( 0 ) — v R now being identified with It (0). All that remains is to prove that Ryy Rev Ruco R„ has 'top-left entry' (./79, 9 —119,,,R,,,1R,,,) 1, because the desired result then follows from Lemma 377La. The following exercise settles the algebra. ❑ Lb. Exercise. Prove that if A and D are invertible square matrices, then ( A B 1 (A— BD 1C) \u0000 —A-1B(D — CA-1B)-1 C D ) \u0000 —D-1C(A— BD-1C)-1 \u0000 (D — CA-1B)-1 provided the left-hand side exists. ►► M. Model choice, Frequentist and Bayesian. What we have been seeing in the previous two subsections is that for large sample size, results from Linear Model theory transfer to the general situation. I am not going to chase it through, but we would find that the Akaike Information Criterion may be motivated for the general context much as it was in Subsections 236J and 303J. Again see Burnham and Anderson [35]. In Bayesian theory as in Frequentist, one wants to balance parsimony against `accuracy' and so to minimize a suitable expression of the form 2 ln(maximum likelihood under model) — function(sample size, number of parameters in model) 8.5. Multivariate Normal (MVN) Distributions \u0000 379 The recent paper by Spiegelhalter, Best and Carlin [215] presents a very interesting development of this idea which covers hierarchical models, etc. The `number of parameters' itself needs definition for hierarchical models. Some hints which might help you understand the paper are scattered throughout this book. See also Exercise Ma below. Since this book is already getting too much like an encyclopaedia, I leave discussion to the experts. But do note that the S—B—C paper employs many topics originating in Frequentist theory. To be sure, these topics could all be presented within a Bayesian approach, but I think that the situation helps exemplify the need for a broad background in Statistics culture. Ma. Exercise. The trace, tr(M), of a square matrix M is the sum of its diagonal elements. Let A be an m x n matrix, and B an n x in matrix. Prove that tr(AB) = tr(BA). Deduce that if Y MVN(0, V), then E(YTBY) = tr(BV). Prove that the trace of a perpendicular projection is equal to the dimension of its range. Hint. Show that if u is a unit vector, then the trace of P[u] (= uuT) is 1. (Alternatively, use the fact that the trace of a square matrix M is the sum of the eigenvalues of M.) N. Reference priors and Differential Geometry. \u0000 We now consider the multiparameter analogue of Subsection 204G. Suppose that 0 is a multidimensional parameter: 0 E Rs. Under a one-one change of variables 0 = v(co), we would like our assignment of priors to satisfy 7r(v) = ir(v(0)) Let h(y I co) = f(y I v(co)), so that Oh* \u0000 Of* 00i acoj \u0000 L.-, 060i Ocoi' Dh = (Df)M, where Dh is the row vector with jth component 0h* lacoi, D f is the obvious analogue, and M is the matrix with (i, j)th element 00, /099i, so that det(M) = J. Verify that izi(w) \u0000 fy aco, aco ah. ah. i h(Y I Co) dy = >2, E mkiik,(0)mti, \u0000 (Ni) k so that /(cp) = M T /(0)M, \u0000 det(i(cp)) = J2 det(/(0)). Hence, the Jeffreys prior det (/(0))1 is invariant. 380 \u0000 8: Linear Models, ANOVA, etc Na. Differential Geometry and Statistics. The results just described can be seen as part of a 'differential geometry of Statistics', something in which there has been, and is, much interest. See, for example, Amari [4], Barndorff-Nielsen and Cox [9], Kass and Vos [126], and Murray and Rice [168]. Marriott and Salmon, in the introductory chapter of [158], provide a concise and readable introduction to some of the ideas. It is clear that Differential Geometry provides a useful language for some statistical considerations, and that it can focus attention on new issues of importance. It is however true that the marriage of the two subjects is not as perfect as one would hope: as we shall see below, 'mathematical correctness' from the point of view of geometry need not correspond to statistical good sense. Yet, and even though the fusion of the subjects seems unlikely to produce results which approach in depth those in other areas of Differential Geometry, it remains a very useful venture. In this discussion, we shall take 11: s (rather than a general manifold) as our parameter space P. The idea is to make this parameter space into a Riemannian manifold by using the Fisher-information metric to assign appropriate lengths to paths in P, lengths which will generally not be the Euclidean lengths of the paths. We shall assume that II (0) is everywhere positive-definite. (Recall that it has to be nonnegative-definite.) Suppose that we have a map t H 0(t) from [0, oo) into parameter space R8. Think of 0(t) as the position of a particle at time t. We wish to speak of the (Fisher metric) distance S(t) travelled by the particle during time-interval [0, t]. The idea is that the speed S'(t) > 0 is given by a 'modified Pythagorean' formula 2 Si(t)2 = 01(t)Ti(e(t))01(t) = E ( a f*(Y OM)) \u0000 . \u0000 (N2) The arc-length S(t) is of course f[0 t] S'(u)du. The formula 379(N1), describing the `covariant-tensor' property of I shows that arc length is unaffected by a change in parameterization. Indeed, with obvious notation, 0V) = M (p' (t), whence cid(t)Ti(cp))(p'(t) = ce(t)T r(e(t))19V). The appropriate definition of the volume element for the Riemannian manifold is then (det(I(0)) 2 c10, exactly in agreement with the Jeffreys prescription. We know that this definition is coordinate-free, and it can be motivated via the use of `normal coordinates' at 0. Some of the above ideas are necessary for the 'differential geometry' reduction utilized at the start of the proof of Wilks' Theorem in Subsection 377L. I skip the details. 8.5. Multivariate Normal (MVN) Distributions \u0000 381 Of course, the Fisher metric and Jeffreys prior belong only at the very beginning of Differential Geometry in Statistics. The next stages inevitably involve connections and curvature. Read the literature mentioned above. On all mathematical grounds, the Jeffreys prior, the Riemannian volume element, is 'correct'. But as Statistics, the Jeffreys prior does not necessarily make much sense, as the following example (due to Neyman and Scott) illustrates. Nb. Exercise. Suppose that (Yl k : 1 < j < J, 1 < k < K) are independent RVs with Yjk N(A3 Prec P). Prove that the Jeffreys reference prior is 7 (111412, \u0000 P) a p2 You may well agree with me that the inappropriateness of the Jeffreys prior as Statistics is already evident: why should the prior depend on J? Suppose that we do use this prior. Prove that then (P \u0000 - Gamma(1JK, rate z W(Y)), where W(Y) := \u0000 y j42, and that W(Y) ^— P XJ(K- 2 1.)- Consider letting J oo while keeping K fixed. Then, by the Strong Law, W(Y) 1 K - 1 JK p K so that the distribution of (p I Y) will concentrate around Kp/(K - 1), not around p. If K is small, the difference really matters. Jeffreys himself modified the idea of a reference prior to cope with this type of example. See Bernardo and Berger [15] (with the amusing comments from McCullogh). I think McCullogh is right in that a definitive theory of reference priors seems impossible. People will rightly continue to use proper 'vague' priors in their computer work, and as long as they check robustness to change in prior, change in model, change in initial values for Gibbs sampler, things should generally be OK. Remark. I do apologize that I often tell you that something is a good idea only to have to say later that it does not work in many important cases. That is the way that Statistics is: full of surprises and never dull (to take the charitable view!) 382 \u0000 8: Linear Models, ANOVA, etc 0. A final thought on Statistics — for now! These few chapters (and parts of Chapter 9) have been the only things I have ever written on Statistics, though I have always had a keen interest in the subject. I have greatly enjoyed writing this account. (Please note that there are further Statistical things in Chapter 9: explanation of de Finetti's Theorem, discussion of Sequential Sampling, some discussion of reference priors, etc.) Mathematicians approaching Statistics might begin by feeling very frustrated about the lack of any definitive solution to various problems; and it has to be agreed that there is nothing of the depth of the Riemann Hypothesis around. But the more one thinks about Statistics, the more one finds its sheer untameability attractive: it is, after all, trying to cope with the real world — and with very important things in that world. Amongst the main motivations for writing this book are my own sadness that I have not been more actively involved in Statistics throughout my career and my wish to persuade others not to follow my example in that regard. 9 SOME FURTHER PROBABILITY In this chapter, we look briefly at three further topics from Probability: Conditional Expectations (CEs); Martingales; Poisson Processes. The first of these leads directly on to the second. The third is somewhat different. First, we look at the conditional expectation E (X I A) of a Random Variable X given that event A occurs, and at the decomposition E (X) = P(A)E (X I A) + P(Ac)E (X I Ac). We used these ideas in solving the 'average wait for patterns' problems in Chapter 1. We then examine one of the central breakthroughs in Probability, the definition (due in its most general form largely to Kolmogorov) of E (X I Y), the Random Variable which is the conditional expectation of X given the Random Variable Y. This is a rather subtle concept, for which we shall later see several motivating examples. To get the idea: if X and Y are the scores on two successive throws of a fair die, then E (X + Y I Y) = 31 + Y. Here, E (X I Y) = E (X) = 31 because, since X is independent of Y, knowing the value of Y does not change our view of the expectation of X. Of course, E (Y I Y) = Y because 'if we are given Y, then we know what Y is', something which needs considerable clarification! Oa. Exercise. What is E (Y I X + Y) for this example? (We return to this shortly, but common sense should allow you to write down the answer even without knowing how things are defined.) It is to be understood whenever the definition of conditional expectation is the more fundamental one, a corresponding conditional probability is defined as 384 \u0000 9: Some further Probability illustrated by the examples P(F I 17) \u0000 E (IF I 11), P(F I g) := IE (IF I g)- One of the most useful results in Probability is the Conditional Mean Formula (one of the 'Golden Rules' of Conditional Expectation) E(X) = EE (X 1Y). The 'Two-Envelope Paradox' makes a very instructive example, so study it carefully. Doob's Martingale Theory is one of Probability's great success stories. As we shall see, a martingale is a mathematical abstraction of the evolution in time of a gambler's fortune in a fair game. Amongst the main results is the Stopping-Time Principle that 'one cannot cheat the system'. It is really amazing that so apparently simple a concept as that of 'martingale' should have had such wide application. When I mentioned much earlier that Probability has been able to solve — in a number of key cases, for the first time — problems from Complex Analysis, Partial Differential Equation Theory, Potential Theory, etc, it is really Martingale Theory which has allowed this success. It is no accident that the definitive account of Martingale Theory is in the book Probability and Potential by Dellacherie and Meyer. (That account is very much more advanced than this book.) I shall sometimes refer to my book, [W] for proofs. In our brief look at martingales, we shall see how they allow us to solve the 'average wait for patterns' problem, to solve the hitting-time problems for Random Walks, to prove the Ballot Theorem, etc. Likelihood-ratio martingales are important, and the application of the Stopping-Time Principle to them is associated with (Wald's) method of Sequential Sampling, an important technique in Medical Statistics and for Quality Control. (Sequential Sampling, however, goes beyond the stage where Martingale Theory helps. See Jennison and Turnbull [119].) I promised a martingale explanation of de Finetti's Theorem and of the Strong Law, and I stick to that, though I cannot provide all details of the proofs here. We take a brief look at the very fashionable (in my opinion, too fashionable!) topic of martingales in finance, proving the simplest case of the Black—Scholes Theorem. The Poisson process models (under conditions which you should later think about in relation to the real world) the arrival of customers at a queue, or of calls to a telephone exchange, or of 'radioactive' particles at a Geiger counter. There are 9.1. Conditional Expectation \u0000 385 `spatial' versions, too. Poisson processes form a good area in which to develop your intuition. 9.1 Conditional Expectation ■ A. Conditional Expectation E (X I A) of a Random Variable X given that event A occurs. Let A be an event for which P(A) > 0, and let X either be in .C1 or be non-negative (or both). We have P(A n B) \u0000 E (-Nis) P(B I A) := \u0000 P(A) — ]P(A) If we interpret E (/B I A) as P(B I A), this suggests defining E (1\" E \u0000 AX) (X I A) := \u0000 P-average of X over A. P(A) (Al) To understand the last equality, note that if C2 is discrete with all its subsets as events, then E(X I A) = EA X(W)P(w) P(A) x(w)P({w} I A). (A2) For a discrete RV X (in L1), E (X) = \u0000 xP(X = x), \u0000 E (X I A) = \u0000 xP(X = x I A), x which you can easily check, and which is reassuring. It is very convenient to write \u0000 E (X; A) E (1AX), \u0000 (A3) the (Lebesgue) integral of X over A. Then E (X; A) is read as 'the expectation of X and A'; and then the expectation E (X I A), the expectation of X given A, satisfies E (X I A) = E (X; A)/P(A). 386 \u0000 9: Some further Probability B. Rules for E (X I A). ► Ba. If X is independent of A (that is, X and IA are independent RVs), then E (X I A) = EX. Intuition: 'If we are told that A has occurred, then that does not change our estimate of the mean of X'. The result is obvious because independence guarantees that E (IAX)= E (IA )E (X) = P(A)E (X). ► Bb. We have (X) = P(A)A (X I A) + IP(Ae)IE (X Ac) Proof This is obvious because X = (I A + JO X . When X = IB, equation (B1) reduces to the familiar P(B) = P(A)IP(B I A) + P(Ac)P(B Ac). We use (B1) in 'commonsense' (rather than formal mathematical) fashion. ► Bc. Linearity. We have, for X1, X2 E G1 and al, az E R, E (al + a2X2 I A) = a i E (Xi I A) + a2E (X2 I A). This is because by linearity of the 'absolute expectation', E Rai + a2)(2)/A1 = aiE [Xi /A] + a2E [X2 .[A] • ► Bd. Important Note. If X, X1, X2 are non-negative and al, a2 are non-negative, then the previous three rules apply even if some or all of E (X), E (X1), E (X2) are infinite. C. Duration of gambling game. Consider the gambling game at Subsection 117B. The gambler starts with a fortune of a where 0 < a < b. Her fortune thereafter behaves as SRW(p) up until the first time T > 0 when it reaches either 0 or b. We wish to calculate ya := E a (7'). Let A be the event that the gambler wins the first game. Then it is intuitively obvious that E a (T I A) = 1 + E a+1 (T) for 1 < a < b — 1: the '1+' counts the first game, and then the process 'starts afresh from a + 1' . It is a very common mistake to forget the `1+'. See Discussion below for how this intuition can be made rigorous. We have, from (B1), Ya = P(1 + Ya+1) + q(1 + Ya—i) 9.1. Conditional Expectation \u0000 387 or, as we more usually write it, Ya = 1 + PYa+1 + qYa, \u0000 (1 < a < b — 1). All we have to do is to solve this difference equation with boundary conditions yo = yb = 0. See the next subsection for how to do this. Ca. Discussion. WHY can we say that E a (T I A) = 1 + E a±i(T), if A is the event that the gambler wins the first game? This is one of those intuitively obvious things which are tricky to pin down rigorously. The following argument is perhaps best skipped on a first reading of the book! But DO the next Exercise. ® If we think of the walk SRWa(p) (where 0 < a < b) as Wn a + + X2 + • + then T(w) is some (measurable) function T(w) = h(a, X i(w), X2(ai), of a, Xi (w), X2(w), .. On the set A = {w : X i (w) = +11, we have the obvious relation T(w) = 1 + h(a + 1, X 2(w), X3(w), .). The variable h(a + 1, X2 ( w ), X 3(W) , .) is independent of X1 (see Fact 99G which relied on the 7r-system Lemma 45M) and (the 7r-system Lemma is needed again here) has the same DF as h(a + 1, Xi(c..)), X2(w), .) , whence E a (T I A) = 1 ±E (h(a + 1,X2(w),X3(w),...) A) = 1 ± h(a ± 1, X2(w), X3(w), .) 1 +E h(a + 1, Xi(w), X 2 (w), • -) = 1 + E a+i (T). ► Cb. Important Exercise. Let W be SRW(p) started at 1. Let T := inf{n : \u0000 = 0}, and let yk := E k (T) for k = 1, 2, .... Prove that Y2 = 2Y1, \u0000 Yi = 1 + PY2 + O. (The first of these really appeals to the Strong Markov Theorem. Compare Subsection 130D. ) Solve for yi when p = z and when p > 1. Write down the obviously correct answer when p < 2. Show that if yk (n) := E k (T A n), where T A n := min(T, n) as usual, then y1(n) = 1 + py2(n — 1), y2(n) < 2y1 (n). Compare the discussion at 119Da. D. Solving difference equations, 2. This continues from 118C. The General Solution of the differential equation ax\" (t) + bx' (t) + cx(t) =- K el'', \u0000 (a 0) 388 \u0000 9: Some further Probability is (with a, /3 the roots of amt + bm + c = 0) K \u0000 + Aeat e t a(7-a)(7-0) \u0000 + Be/3' if a, )3, -y are distinct, x (t) = \u0000 Ktet b) + Aeat + BeOt \u0000 if -y = a ,a, a(a— Kt2e't + (At + B)eat \u0000 if -y = a = [3. 2a The pattern can be appreciated via 49,,e't = teat and aa2 eat = get, where aa=alaa. You will have done the 'Particular Integral plus Complementary Function' idea. These ideas transfer immediately to difference equations. The General Solution of the difference equation ax(n + 2) + bx(n + 1) + bx(n) =- -yn is given by 1 \u0000 Kryu \u0000 + Aan + .13,13n if a, 0 , 7 are distinct, a(7 - a)(7 - 0) x (n) _ Ka(ricva;;- + Aan + B On \u0000 if -y = a )3, Kn2ai - 2 + (An + B)an \u0000 if -y = a = O. You would see the pattern better if n2an-2 were replaced by n(n — 1)a' and nan by na' in the last expression. E. The conditional expectation E (X Y), Y discrete. Suppose that X and Y are RVs, with Y discrete and X either non-negative or in Li or both. We then define the Random Variable Z = E (X IY) via Z := c(Y), that is, Z(w) = c(Y (w)) where c(y) := E (X I Y = y). [Of course, there is no RV ' (X IY)' of which E (X IY) is the 'expectation' any more than there is an event 'A B' of which IP(A B) is the `probability'.] When you consider Z(w) = E (X I Y) (w), all that you know about w is the value Y(w). Let's consider an example. Ea. Example. Suppose that a tetrahedral die with score (1, 2, 3 or 4 on each throw) is thrown twice. Suppose that IP(Score is i on any throw) = pi, (i = 1, 2, 3, 4). Let X = sum of scores, \u0000 Y = product of scores. 9.1. Conditional Expectation \u0000 389 : (1, 1) (2,1) (1,2) (2, 2) (1, 3) (2, 3) (1, 4) (2, 4) (3,1) (3, 2) (3, 3) (3, 4) (4,1) (4, 2) (4, 3) (4, 4) 2 3 4 5 1 2 3 4 X: 3 4 5 6 Y: 2 4 6 8 4 5 6 7 3 6 9 12 5 6 7 8 4 8 12 16 2 3 4 z4 Z: 3 z4 5 6 4 5 6 7 Z4 6 7 8 P(w) = pipi if w = (i, j) Table E(i): X, Y and Z = E (X I Y) for an example In Table E(i), values of X, Y and Z = E (X 1Y) are listed. If the actual outcome is (1, 4), then Y = 4. But if you are told only that Y = 4, then the outcome could be any of (1, 4), (2, 2), (4, 1), with corresponding X-values 5, 4, 5 and absolute probabilities pip4, P2P2, POi• Hence, we have 5P1P4 4P2P2 5P4P1 Z4 - /31/34 P2P2 + Check out all the other (easy) entries. Eb. Exercise. The following experiment (considered at Exercise 52Da) is performed. A fair coin is tossed twice and the number N of Heads is recorded. The coin is then tossed N more times. Let X be the total number of Heads obtained, Y the total number of tails obtained, and let Z = E (X 1Y). Continue the table from the earlier exercise to the form beginning IP(w) X (w) Y (w) Z (w) 11111-111 \u0000 16 \u0000 4 \u0000 0 \u0000 4 ► Ec. Essential Exercise. Prove that if Y is discrete and if h is a function on the range of Y such that h(Y) is either non-negative or in rl, then E(h(Y)1Y) = h(Y). Prove more generally that if either X and h(Y) are both non-negative or h(Y)X is in ,C1, then we have the 'taking out what is known' Golden Rule IE(h(Y)X (Y) = h(Y)E (X (Y). \u0000 (El) 390 \u0000 9: Some further Probability ►► F. A fundamental property. We continue with the assumptions of the previous paragraph. Things have been fixed so that the P-average of X over a set { w : Y(w) = y} is equal to the P-average of Z over this set. (Indeed, over this set, Z is a constant equal to that P-average of X over the set.) More significantly, over any set of the form Y E A, Z has the same P-average as does X. Let's spell this out. If Z— E (X Y), then, for any set A, * (X;Y E A). E (Z;Y E A), \u0000 E(X I Y E A) = E (Z 1Y E A). (F1) indeed, (for our discrete Y) Z E (X I Y) is characterized by the fact that Z is a function of Y and E (X; Y E A) = E (Z; Y E A) for every set A. Of course, E(X; Y E A) means E (X; B) where B is the event {w : Y(w) E B}. Subsection 399M on the Two-Envelopes Problem, which you can read now if you wish, explains clearly the benefits of this way of thinking. Proof of (F1). With Measure Theory (`Dominated and Monotone Convergence Theorems') providing justification if A is infinite, we have the intuitively obvious argument: E(X;Y E A) = \u0000 E (X; Y = y) = \u0000 P(Y = y)E (XIY = yEA \u0000 yEA = E lP(Y = y)c(y) = E (c(Y); Y E A) = E (Z; Y E A). yEA Why is the 'characterization' part obvious? \u0000 ❑ Note that the Conditional Mean Formula which states that E (X) = EE (X I Y), \u0000 (F2) is the case A = ► G. Linearity. Suppose that X1 and X2 are in .C1, that Y is discrete, and that Al and A2 are real numbers. Then, E(Ai Xi A 2X2 I Y) = AiE (Xi I Y) A 2E (X2 I Y). The result is also true if X1, X2, A1, A2 are non-negative and Y is discrete. Ga. Exercise. Prove this. Gb. Exercise. The purpose of this exercise is to give a clearer explanation of the result at Exercise 71Kb. Again,µ is the mean, and a2 the variance, of a person chosen at random from Bath. We write t for the total height of all people in Bath. 9.1. Conditional Expectation \u0000 391 Suppose that two different people are chosen at random from the n Bath residents. Let X be the height of the first and Y the height of the second. Express E (X IY) in terms of t, Y and n. What is E (XY I Y)? Use your answer to find E (XY) in terms of p, a2 and n; and finally deduce the result from Exercise 71Kb: Cov(X, Y) = a z n — 1 • ► Gc. Exercise. Let X and Y be the scores on two consecutive throws of a tetrahedral die. I've changed to tetrahedral so that you can look at Table 389 E(i). Consider the map (i, j) 1- 4 (j, i) from St to itself. Does this map change probabilities? In the light of the map, how are E (X I + Y) and E (Y I + Y) related? What is the sum of these two variables? What is E (X I X + Y)? ► Gd. Important Exercise. This exercise is an essential step in the martingale proofs of the Strong Law and of de Finetti's Theorem. Let X1, X2, . X n be discrete RVs with the property that for every permutation r of {1, 2, ..., n}, P(X,-(i) = xl, . , X,-(n) = xn) = IF(X1 = \u0000 • Xn, = xn) for all choices of xl, x2, . . . , xn. It is easily seen that X1, X2, . . . , Xn are identically distributed (Why?), but they need not be independent. Prove that if Sn = X1 + X2 + • • • + Xn (whence Sn = X T(i) + + X,-(n)) then E (X1 I Sn) = Sn/n. After the previous exercise, you should not need any hints. ►► H. Conditional Variance. Recall the Parallel-Axis result that if X E £ 2 and E (X) = p, then Var(X) := E [(X — p)2] = E (X 2) — p2. If X E L2, Y is discrete, and Z := E (X I Y), then 0 < Var(X I Y) := E [(X — Z)2 I Y] = E (X2 I Y) — Z2. I am not going to fuss over the fact that Z E £ 2. The algebra of the result is clear because, since Z is a function c(Y) of Y, E(ZX I Y) = E (c(Y)X I Y) = c(Y)E (X I Y) = Z2. Note that (assuming that Z E L2) we have E(Z 2) < EE (X 2 I Y) = E(X2). 392 \u0000 9: Some further Probability That Z is in .C2 is proved by a 'truncation' (or 'staircase') argument. For X E £2 and Y discrete, we have the Conditional Variance Formula Var(X) \u0000 EN (X I Y)] + Var[E (X I Y)], \u0000 (H1) another of the 'Golden Rules'. ► Ha. Exercise. Prove property (H1). ► Hb. Exercise. Let X E .C2 and let Y be discrete. Let c(Y) = E (XIY). Let f be any function on the range of Y. Show that E { [X — f (Y)]2} = E (X 2) —E {c(Y)2} + E {[f (Y) — c(Y)12} Hence, E (X I Y) is the least-squares best predictor of X given Y amongst 'linear' and 'non-linear' predictors. ► I. Variance of a 'Random Sum'. Suppose that N is an RV taking values in {0, 1, 2, ...}, and that Xi, X2, ... are identically distributed RVs each with the same distribution as an RV X. Suppose that N, X 1, X2, . . . are independent. In particular, Xi, X2, ... are IID. We suppose that N and X are in L2 and write itx := E (X), crl := Var(X), \u0000 := E (N), QN := Var(N). Let SN be the 'Random Sum' (the number of terms is random): SN := \u0000 + X2 +•• • + XN \u0000 (:= 0 if N = 0). We want to find the mean and variance of SN. We write ST, := \u0000 + X2 + • + Xn as usual. You should note the steps in the following argument carefully because it is an argument which is used very frequently. For each fixed n, c(n) := E (SN I N = n) = H (Sn I N = \u0000 (logic applied to the definition!) = E(Sn) \u0000 (X1 + X2 + • • • + Xn, is independent of N) = ni,tx \u0000 (standard). 9.1. Conditional Expectation \u0000 393 Here we have used the fact that on the set {w : N(w) = n}, we have (SN)(w) := S N (w)(w) = S n(w), and the fact that for a fixed integer n, Ste, is independent of N. Hence, E (SN I N) = c(N) = N ptx and, a result which is hardly a surprise, E (SN) = EE (SN I N) = I-tx • pp. Ia. Exercise. Prove that Var(SN) = a 2N/t2x + \u0000 - Explain the good intuitive sense of this. Assume that the number of coaches which visit Bath on a winter's day is a Poisson RV with parameter 20 (and hence mean 20 and variance 20). Assume that each coach has 32 seats and that, 'independently of everything else', the probability that any particular seat is taken is 3/4. Let T be the total number of people who visit Bath by coach on some winter's day. Find E (T) and Var(T). ► J. Generating Functions for Random Sums. Again suppose that N is an RV taking values in {0,1, 2, ...}, and that X1, X2, . .. are identically distributed RVs each with the same distribution as an RV X. Ja. Important Exercise. This exercise is crucial for Branching-Process Theory. Suppose that each Xk takes values in {0, 1, 2, .. .}, and let gx (a) denote the probability generating function of X. Let Sn be as in the previous subsection. Prove that gsm( ) = giv(Yx(a)) - Hint. Your argument should mimic that in the previous subsection: c(n) := E (as N N = n) = • • • . Jb. Exercise: A CLT for a random sum. Suppose that N is Poisson (n), and that each X is exponential E(1). Prove that SN has mean n and variance 2n. Show that (for < 1) na 1 — ) Ms N (a) = E exp(aSN) = gN(Mx (a)) =- exp Prove that the CGF C z„(a) of the standardized form Zri := (SN — n)/ 2n of SN satisfies (for a < Cz,(a) =- 1 — (2n)- 1a . Deduce that, as n \u0000 oo, P(Z, < x) \u0000 (1,(x) for every x E IR. l a g 394 \u0000 9: Some further Probability ■ ■ K. The Galton—Watson Branching Process. The original problem might now be seen as sexist. A certain Reverend Watson, the last of the male line descended from some Watson in the past, posed the question: what is the probability that the male line descended from a newborn boy will die out? Francis Galton, whom we have already met in connection with regression, discussed the problem (and got it wrong!). Let's make it non-sexist by having asexual reproduction for some type of animal, so we have 'animal and children' rather than 'man and sons'. Note however that many interpretations are possible. In constructing the atomic bomb or nuclear power station, the 'Watson' question had to be asked where 'animal' equals atom and where a 'child of that atom' is another atom split by a neutron emitted from the first. For another important application of the idea, see the next subsection. Generation 0 1 Population Zo = 1 z, = x,(1) 7/ n + 1 A ,(n+1) A 1 Zn Z n+1 4 +1) (re+1) Recursion: Xi (n+1) + • • • + X zn \u0000 = Zn+1 Figure K(i): A Galton—Watson branching process Figure K(i) illustrates the type of family tree (starting with one individual) in ( which we are interested. We write X,n+1) for the number of children (who will be in the (n + 1)th generation) of the rth animal in the nth generation. Let Zn denote the number of animals in the nth generation. I hope that the diagram explains why the fundamental recurrence relation is Zn+1 \u0000 x ln+1) + x2n+1) + ... + x(zn+1). \u0000 (K1) 9.1. Conditional Expectation \u0000 395 The key assumption is that xli) , \u0000 X21), X31>, X12), \u0000 4 ) , xf 3) , \u0000 4 ) , X32), x33>, are IID RVs with values in {0, 1, 2, .. .}, each with the same distribution as an RV X. We write pi for px, and we assume that it is finite. We want to calculate 7r := 1P(extinction) = P(Zn = 0 for some n). We assume throughout that IP(X = 0) > 0. From Subsection 3921, we know that E (Zn+1) = E (Zn ),a, so that E (Zn) = µn, as intuition suggests. It is therefore plausible that 7r= 1 if it < 1, and 7r < 1 if it > 1. We now define 7rn := IP(extinction by time n) = P(Zn = 0). Measure Theory in the form of 43D(a) guarantees the plausible result that 7rn t 7r as n 1' co, and we shall take this for granted. Now, 7rn-F1 = P(Zn±i = 0) CO = >f, P(Zi = k; Zn+i = 0) k=0 CXD = >2, P(Z1 = k)P(Zn±i = 0 I Zi = k). k=0 But P(Zn±i = 0 I Zi = k) is the probability that k independent family trees each starting from one individual will die out by time n, that is 7rnk. Thus, CO lVn+1 = EP(X = k)71-nic = g(irn), k=0 g being the pgf of X. We have 71-n+1 = 9(7n), \u0000 7rn t 7r, \u0000 71- = 9(v), 396 \u0000 9: Some further Probability 1 7-3 7ri \u0000 72 7-3 \u0000 7 = 1 Graph of g when ii = g' (1) < 1 i 7r3 7-2 0 \u0000 7ri 7 2 71-3 71- = p \u0000 1 Graph of g when p = g' (1) > 1 Figure K(ii): The famous Branching-process pictures 9.1. Conditional Expectation \u0000 397 the last by continuity. Since g\" (t) > 0 for t > 0, g is convex: its slope is never decreasing. Recall that g'(1) =1E (X) = p. Figure K(ii) shows the two situations which can arise. In the top figure, gi(1) = p < 1, so the slope of the g-curve is always less than 1 within [0, 1]. In this case, there is only one root, namely 1, of g(ir) = 71 in [0, 1]. The case when p = 1 is exactly similar, except that the 45° line touches the curve at (1, 1). Thus, 7i = 1 when p < 1. The case when p = g'(1) > 1 is illustrated in the lower figure. The curve is steeper than the line at (1, 1), and so will cut the line at precisely one point in (0, 1). From the picture, 71n, t 7r,„, where 7r„.0 is the unique root of g(7r) = 7r in (0, 1); so, 7r = 7roo. We often argue as follows. First, 711 = P(Z1 = 0) = g(0) 5 7r,„, and, since g is a non-decreasing function, 7rn < 71-00 implies that 7rn+1 = g(7rn) < g(7r00) = 71-co , whence, by induction, 7rn < 7r,, for all ri,, and so 7r = 7roo. Summary. If ti < 1 then 7r = 1, and if ii > 1, then 7I- is the unique root of g(ir) = 71 in (0, 1). Ka. Exercise. Suppose that P(X = k) = pqC for k = 0, 1, 2, ..., where 0 < p = 1 — q <1. Show that g(a) = p1(1 — qa), ea = g' (1) = qlp, and 7r = plq if q > p and 1 if q < p. ► Kb. Use of pgfs. The fundamental recurrence relation 394(K1) combined with the `Random Sums' result from Exercise 393Ja shows that gn±i(a) := E (azn+i) = gri (g x (a)) = gn(g(a))• Hence, g2 =gog, g3 =gogog, gn =gogo•••og (Th-fOld iteration). Only in a few simple cases is it possible to give a closed-form expression for the nth iterate. Note that gm+1 = g o gn, so that 7rn--1-1 =P(Zn+1 = 0) = g21+1(0) = 9(921(0)) = 9(7n)• 398 \u0000 9: Some further Probability ► Kc. Exercise. Show that if G \u0000 gll 912 921 922 is a non-singular 2 x 2 matrix and we define the associated fractional linear transformation via G(a) = glice + g12 g2i + 922 then, if H is another such matrix, we have G(H(a)) = (GH)(a), GH being the matrix product. Composition of linear fractional transformations therefore corresponds to multiplication of matrices, and since there are standard `eigenvalue' methods of computing powers of matrices, we can handle the case described in Exercise 397Ka. Indeed, for that case, as you can verify by induction, for p q, 9n (a) = pun (1 — a) ± qa — p qp,n (1 — a) + qa — p Check that, for p # q, 7rn q n+1 p n+1 For more on this, see, for example, [W]. ► L. A 'typed' branching process. Suppose that our population consists of animals of two types, A and B. Suppose that an A type either, and with probability has 1 A-type child, or, and with probability 1, has 1 B-type child, or, and with probability 1, has no children. Suppose that a B-type either, and with probability 2, has 2 B-type children, or, and with probability 1, has 1 A-type child. Let a [respectively, b] be the probability of extinction starting from a population of just 1 A-type animal [respectively, just 1 B-type animal]. ► La. Exercise. Calculate a and b, proving that both are less than 1. ► Lb. Challenging Exercise*. Continue with the preceding exercise, but suppose in addition that each animal at birth has a chance p of being transferred right out of the population under consideration. Find the critical value Po such that the animal population started with 1 animal has a positive probability of surviving for ever if, and only if, p < po. Answer: po = (210) — 1. Consider the population consisting of all people with AIDS. Divide it into types: heterosexual men, bisexual men, homosexual men, heterosexual women, bisexual women, lesbians. Let a 'child' of an individual be a healthy person who catches AIDS from that individual (could be a newborn baby). Transferring out p( q n p n) 9.1. Conditional Expectation \u0000 399 of population could be (we hope for the future) by cure, or else by death or moral decision. I hope that this indicates that this type of question is not just for fun, even if it hardly provides a sophisticated model in the case of AIDS. For key further reading on branching processes, see Kendall [134, 135], Harris [109], Athreya and Ney [6]. ► ► M. 'Paradoxes' of 'Two-Envelopes' type. Please re-read Subsection 16Q. You may remember that I said that I would try to present this discussion in a way much of which is comprehensible if you have only read up to page 16. But the problems are subtle, and it is necessary to go into the matter in the detail given here. Accept for the moment that we can find a constant c > 1 and pair (X, Y) of positive- integer-valued Random Variables such that P(X = x; Y = y) = P(Y = x; X = y) for all positive integers x, y, \u0000 (M1) and E (Y I X = x) > cx for every value x which X can take. \u0000 (M2) (In case you have jumped on to this subsection, I explain that E (Y I X = x) is the (correct probabilistic) average value of Y over those outcomes for which X = x.) Then, of course, E (X I Y = y) > cy for every value y which Y can take. \u0000 (M3) In a certain game, you are told the probabilities at (M1). An independent sequence (X1,171), (X2, Y2), (X3, Y3), ... of pairs of integer-valued Random Variables is then chosen, each pair having the same probability distribution as (X, Y). For each n, you will be given an envelope containing Xn dollars, while your 'opponent' will be given an envelope containing Yn dollars. Suppose first that at each game, you are given the choice of whether you wish to swap envelopes with your opponent. (In some sense, you are favoured over your opponent at every game.) Based on (M2), you decide to swap at every game. (a) Let's be very clear what equation (M2) has to say about this situation. Let A be any non-empty finite subset of the integers such that X has positive probability of being in A. Let TA be the set of those values m for which Xm, is in the set A, and let TA(n) be the set of values m < n for which m is in TA (that is, for which Xm is in A). Then the content of (M2) is that (with probability 1) for all large values of n, the sum of Ym values for which m is in TA (n) is at least c times the sum of Xm values for which m is in TA(n). In this sense, your swapping strategy works for those games for which the value of Xm is in A. (b) Let us plod laboriously through the analogous situation where it is your opponent who is given the chance to swap, and does swap at every game. Let B be any non-empty finite subset of the integers such that Y has positive probability of being in B. Let WB be the set of those values m for which Ym is in the set B, and let WB (n) be the set of values m < 71 for which m is in WB (that is, for which Ym is in B). Then the content of (M3) is 400 \u0000 9: Some further Probability that (with probability 1) for all large values of n, the sum of Xm values for which m is in WB (n) is at least c times the sum of YM values for which m is in WB (n). In this sense, your opponent's swapping strategy works for those games for which the value of Ym is in B. (c) There is no contradiction here. You cannot find A, B such that TA = WB. Thus, (a) and (b) refer to different subsets of the set of all games. This is why there is no contradiction. The difference may seem 'small', but we can only have 399(M1) and 399(M2) holding simultaneously if both X and Y have infinite expectation: thus, what happens for outcomes of very small probability can have a huge impact on averages. (d) It is indeed the case that we can look at the set of all games. However (as already remarked and as is proved in the discussion at Mb below) it is a consequence of our assumptions about X and Y that E (X) = E (Y) = co; and, of course, 'c times infinity is still infinity'. However, for the final resolution of the paradox, we would really need the fact that, with probability 1, E x \u0000 Xm lim inf v .,m — <n yin < c-1 - r Em <n up sup \u0000 — \u0000 > c. \u0000 (M4) L ■m<n \u0000 m It is moderately easy to prove this if we further assume that Y/X is bounded: that, with probability 1, there is a finite deterministic constant K such that Y < K X . This is the case in all examples which concern us. I give a proof of a stronger result for this case at Appendix A9, p501. (e) Playing the swapping strategy when X < k will gain for those games when X < k, but what happens on those games will be completely dominated by what happens on the other games. (f) This will all be very clearly visible in the following example. I repeat that all versions of the 'Two-Envelopes Paradox' will have the features of this example, even though these features are harder to see in other versions. Ma. Example. Suppose that X and Y have joint probability mass function as shown in Table M(i). Thus, for k =- 0,1, 2, ..., P(X = 4 k; Y = 4k+1) — P(Y = 4 k; X = 4k+1) = 2—(k+2). Check that 4 x \u0000 if x = 1, E (Y I X = x) = 1.5x if x = 4k (k > 1). If we sum down to row D (inclusive), then the contributions to the means E (X) of X and E (Y) of Y are equal. However, if we consider the swapping policy 'Swap if X-value is less than or equal to 4k', then this takes us down to row E, by which time the contribution to E (Y) is more than 1.5 times the contribution to E (X). If you sum down to row B you have the sum corresponding to X < 4k-1. To get the sum corresponding to Y < 4k-1, you have to replace row B by row D, and just look at the impact of that change. 9.1. Conditional Expectation \u0000 401 Contribs to Prob X Y S E (X) E (Y) C E (Y I X) 2-2 1 4 i- 1 1 x 4 2-2 4 1 1 1 4 x 1.5 2-3 4 42 1 2 4 x 1.5 2-3 42 4 2 1 42 x 1.5 2-4 42 43 1 4 42 x 1.5 - . • - B 2 —(k+1) 4k-1 4k 2k-3 2k-1 4k-1 x 1.5 D 2 —(k+1) 4k 4k-1 2k-1 2k-3 4k x 1.5 E 2 —(k+2) 4k 4k+1 2k-2 2k 4k x 1.5 `S' refers to symmetric grouping, `C' to 'conditional on X' grouping Table M(i): Joint pmf and other information about (X, Y) The sheer 'lack of robustness' is well displayed by this example. Rows far further down the table have an even greater impact on averages than does row E, even though they have still smaller probabilities. Notice that the fact that X and Y are symmetric does not prevent the fact that it is obviously better for you to swap if X = 1 because you then know that Y = 4. ► Mb. The role of infinite expectations. It is impossible to find Random Variables U and V both in G1 such that P(both E(U IV) > V and E (V 1 U) > U) = 1. for if this were so, then the Conditional Mean Formula would lead to the contradiction E (V) = EE (V I U) > JEW) = EE(U I V) > E(V). People have correctly identified that the presence of infinite expectations is necessary for 'Two-Envelope Paradoxes'. However, I do not regard the assertion that certain expectations are infinite as any kind of explanation of such 'paradoxes'. (It is however true that, as David Chalmers has pointed out, the naive intuitive idea that 'on average I gain by swapping' in the Two-Envelopes problem makes little sense if the expected amount I receive is infinite whether or not I swap.) Be extremely careful if using conditional expectations of RVs which are not in ,C1. In particular, in contexts with improper Bayes priors, always try to avoid using any conditional-probability statements; otherwise apparent paradoxes such as the Two-Envelopes Paradox' and that in Subsection 281K will always be lurking in the background. As we have seen, this can happen with proper priors if variables have infinite expectation. 402 \u0000 9: Some further Probability The Two-Envelopes Problem is taken up again at 4040b and Appendix A9,p501. ►► N. Further important properties of CEs. \u0000 Results such as those in Subsection 390F form a general pattern summarized by the Tower Property: E (X I information) \u0000 more info at information) \u0000 (N1) of which the rigorous form is given at equation 405(P1) below. Examples are E (X) = EE (X Y, U), E (X I Y) = E (E (X I Y, U) I Y). \u0000 (N2) Discussion of equation (N2). If Y and U are discrete, then T := E (X 1Y, U) = c(y, u) := E (X 1Y = y; U = u) on {w : Y(w) = y; U(w) = u}. But then, for a subset H of the range of (Y, U), and with double summations over {(y,u) E H}, E (X; (Y, U) E H) = EE E (X; Y = y;U = u) = EE P(Y = y; U = u)c(y, u) = E (T; (Y, U) E H). Especially, taking H = A x IR, we have E(X; Y E A) = E(T; Y E A), whence E (X 1Y) = E (T Y), which is result (N2). ► Na. An independence result for CEs. If X is non-negative or in Li, and Y, U, . are finitely many discrete RVs, and if X is independent of (Y, U, . .), then E(X IY,U,• • •) = E(X). You prove this. 110. • Nb. A more subtle independence result for CEs. If X is non-negative or in Gl, U and Y are discrete and U is independent of the pair (X, Y), then E (X I Y, U) = E(X Y). \u0000 (N3) 9.1. Conditional Expectation \u0000 403 Proof. Let A be a subset of the range of Y, and B be a subset of the range of U. Let Z = E (X I Y), a function c(Y) of Y. Since Y is independent of U, whence ZI A(Y) = c(Y)IA(Y) is independent of IB(U), E (Z; Y E A;U E B) = E ({Z/A (Y)}/B(U)) = E (Z/A (Y))E (/B (U)) = E (Z; Y c A)P(U E B) = E (X; Y E A)IP(U E B), using the Golden Rule 390(F1) at the last step. But, since (X, Y) is independent of U, whence XI A(Y) is independent of IB(U), we have E (X; Y E A;U E B) = E ({X/A (Y)}/B(U)) = E (X/A (Y))E (/B (U)) = E (X; Y E A)IP(U E B) = E (Z; Y E A; U E B), using the previous equation at the last step. Thus, Z = E (X I Y, U), as required. That for any subset H of the range of (Y, U), E [Z; (Y, U) E H] = E [X; (Y, U) E H] is clear (why?). \u0000 I=1 Nc. Exercise. Give an example to show that if we only know that X and U are independent, then 402(N3) need not hold. What is the intuition behind this? ► 0. Conditional expectation for the 'continuous' case. When Y is not discrete, E (X IY) is defined only modulo subsets of 12 of probability 0. Recall that 'almost surely' or `a.s.' for short means 'with probability 1'. A Random Variable Z is called a version of E (X I Y), and we write Z = E (X I Y), a.s., if Z is a nice (Borel) function c(Y) of Y and for every nice (Borel) subset A of R, E (Z; Y E A) = E (X; Y E A). \u0000 (01) Two versions Z and Z of E (X IY) will agree almost surely: P(Z = Z) = 1. The 7r- system Lemma implies that if (01) holds for every set A of the form (—co, a], then it will hold for all Borel sets A. You will guess what happens: if X and Y have a joint pdf fx,y(x, y), then Z \u0000 (02) where e(y) E(X Y E dy) = f x xiy (x y) dr. 404 \u0000 9: Some further Probability ► Oa. Exercise. Check that (02) works when (X, Y) is 'continuous', ignoring nonsenses about sets of measure zero. Check that 389(E1), 390(F2), and Result 390G transfer to the situation of arbitrary Y. ❑ If (X, Y) BVN(fix, py, Qom, av2, p), then, as we have essentially seen already, \u0000 E (X I Y) = + \u0000 (Y — Ay). o-y (This property is shared by any elliptically-contoured distribution centred on (eux, gy) — Yes?!) We know (for the BVN case) that EVar (X I \u0000 = Eo- (1 — p2) \u0000 ax2 (1 p 2) . Moreover, 2 2 \u0000 Var (E (X I Y)) = P \u0000 o- Y 2 = n2fr2 -2 \u0000 x and we see that this tallies with the Conditional Variance Formula 392(H1). Ob. The Two-Envelopes Problem — again. You puzzle out the following case. Give your intuition free rein. Suppose that someone chooses a real number Z at random, puts Z units in one envelope and 2Z units in another. You choose one of the envelopes at random, and are then asked whether you wish to swap it for the other. Take the situation where Z (> 0) has proper pdf 1 fz(z) = ( 1 + \u0000 > o). (03) Let X be the amount in the first envelope you pick, Y the amount in the other. Then (you check!) 1 (dx)-11P{X E (x,x+ dx);Y = 2X} = 2 f z(x) = 2(1 + x) 2 \u0000 7.1(4 (dx)-1IP{X E (x, x dx); Y = 1X} = (dx)-1 P{Z E \u0000 ix + Wx)} 1 *.fz(2x) = 4(1 + lx) 2 \u0000 r2(x). So, with r(x) := ri (x) r 2(x), then conditionally on X E dx, Y = 2x with probability pi (x) := ri(x)17-(x), Y = Zx with probability p2 (x) := r2 (x)/r(x). Conditionally on X E dx, the expectation of Y is 2xpi (x) \u0000 xp2(x), 9.1. Conditional Expectation \u0000 405 and because 8(1 + x)2 > 2(1 + x)2 for x > 0, this is greater than x for x > 0. We therefore have both E (Y I X) > X and E (X I Y) > Y. Convince yourself that there is no contradiction. (The condition that A is a non-empty finite set in our previous arguments should be replaced by the assumption that E (X; A) is finite and positive.) Of course, in this problem, (X, Y) is not a 'continuous' variable in 1182: X and Y do not have a joint pdf. That's why I said \"Give your intuition free rein\". ►► P. Remarks on the general theory of CEs. \u0000 Perhaps the greatest of Kolmogorov's many great contributions to Probability was the realization that the right general concept is that of (a version of) the conditional expectation E (X I g) of an RV X given a sub-a--algebra g of T. The crucial sets of the form Y-1(A) := {c4.) : Y(w) E A}, A a Borel subset of 111, over which we integrated within the definition of E (X I Y), form a a-algebra of subsets of C2, the a-algebra a(Y) generated by Y. The conditional expectation E (X Y) is really E (X I a(Y)). Analogously, E (X I Y, U) is really E (X I a(Y, U)) where a(Y, U) is the a-algebra generated by (Y, U), that is, the collection of subsets of C2 of the form {a; : (Y(w), U(0.))) E A}, where A is a Borel subset of R2. In general, if Z is a version of E (X J g), then E (Z; G) = E (X; G) for every G in G. The idea that Z must be a function of Y is replaced by the idea that Z must be G-measurable in that cr(Z) c G. A version is then determined almost surely. The benefits of such a generalization are considerable. Amongst other things, it equips Probability with a quite different level of subtlety, something of which we shall get a hint in our discussion of de Finetti's Theorem. The general Tower Property says that if G and 7-1 are sub-a-algebras of the a-algebra F of all events, and g C 71, then E \u0000 E0E(x \u0000 a.s.; \u0000 (P1) and this is just definition chasing. Every property which we have seen for conditioning relative to a discrete Random Variable Y holds in general, provided we add `a.s.' here and there. With regret, I have decided that conditional expectations relative to a-algebras are too technical for this book. See, for example, [W] and Rogers and Williams [199] for detailed accounts, both of which base the theory on the least-squares 406 \u0000 9: Some further Probability best predictor idea at 392Hb. See Bingham and Kiesel [23] for a well-motivated account. 9.2 Martingales This is only a brief introduction to a great topic. It relies on the theory of conditional expectation, and we shall use results for that theory which we have proved only when the conditioning RV is discrete. In non-mathematical English, a martingale is a kind of gambling strategy or else part of a horse's harness which stops the horse from throwing its head up. Doob's Martingale Theory does relate to gambling systems, and it puts constraints (`Doob's Uperossing Lemma', not studied here) on oscillatory behaviour. Many common structures support enough martingales to allow us to derive any desired properties by using Doob's theory. (And many more-subtle ones don't.) ►► A. Definition of martingale. \u0000 A (discrete-time) stochastic process (or, simply, process) is a finite or infinite sequence X = (X1, X2, . . .) of random variables. We study martingales associated with an underlying process X = (X1, X2, ...). The idea is that the values Xi (wact), x2 pact ) x n pact) are known to the observer at time n. A martingale M relative to X is a process M = (Mo, Mt, Ma, .) such that Ma is a deterministic constant and the following three properties hold: (Mt) \u0000 Mn = fn(X1, X2, - - X it) 7 \u0000 fn \u0000 7 (fn being a deterministic (and Borel) function), so that the value /11,,(wact) is known to the observer at time n; (M2) Mr, E \u0000 every n, (M3) TE(Mn f-1 I X17 X2, • - • Xn) =Mn (a.s.) for very n > 0. When n -= 0, (M3) states that E (Ml) = E (M0). I usually write EMI) rather than Mo because the theory extends to cases where Mo is not a deterministic constant. 9.2. Martingales \u0000 407 We can regard Mn as the hypothetical fortune at time n of a hypothetical gambler in a hypothetical fair game: if the present time is n, given the past and present history X1, X2, . . . , X n, the gambler's fortune Ma+1 one time step into the future is on average what it is now, Mn. (Even if we are studying a 'real' gambling game, we may wish to build from it a hypothetical game in which (for example) the hypothetical fortune at time n of a hypothetical gambler is some function of our real gambler's fortune and the value n. We shall exploit this idea in later examples.) Aa. Exercise. Prove that if M is a martingale relative to X, then E (Mn ) = E (M0 ) for every n. Prove that if m, < rt, then E (Mn I Xi , X2, . . . , Xin) -= Mn , \u0000 a.s.. It is convenient to write .Fn for the history up to time n, so that the sequence (.Fn) represents the 'evolution'. Compare the situation in Section 127. Strictly speaking (see that section), .Fn, is the o--algebra of those events of the form {cid : (Xi (w), X 2(w), . . . Xn(w)) E U1 \u0000 (Al) where U is a nice (Borel) subset of Rn. Then property (M1) states that Mn is .Fn- measurable. The definition of .F0 as the trivial o--algebra F0 = {0, Q} carrying no information, tallies with our assumption that M0 is a deterministic constant. Then the expectation on the left of (M3) is fundamentally E (Mn+1 Fn), and the key martingale property may be written in the neater form \u0000 I - E (Mn+1 I Fn) = Mn; a.s.. The precise significance of (M3) is that if Fn is any set of the form (Al), then E (Mn+1; Fn) = E (Mn; Fn). \u0000 (A2) In practice, the martingale property is often easy to verify, as examples will show. ► Ab. Creating martingales by 'projection'. Here is one important way in which martingales arise. Suppose that we have an evolution (.Fn), and that E L l. Then, if we define Mn := E ( I Fn), then M is a martingale. This is because, by the Tower property, Mn = E W Yn) = E ( E ( I -Fn+i ) I -Fn) = E (mn+J. I -Tn )• 408 \u0000 9: Some further Probability B. 'Sum', 'product' and `Likelihood- Ratio' martingales. ► Ba. 'Sum' martingales. Suppose that X1, X2, \u0000 , X7, are independent RVs, each in ri and each of mean 0. Let a E R, and define Mn := a + + X2 + • ••+ Xn, \u0000 Mo := a. Then (M1) and (M2) automatically hold, and, as regards (M3), we have E (Mn+1 I Xi, X2, • • • 7 Xn) = E (Mn Xn+1I Xl, X2, • • • , Xn) E (Mn I Xi, X2, . , Xn) \u0000 (Xn+1 I X1 X2, \u0000 Xn) Mn EXn+1 = Mn, \u0000 a.s.. Here, we used the facts that Mn is known when X1, X2, ... X n are known, that Xn+1 is independent of (X1, X2, . • Xn), and that EXn+i = 0. Hence, M is a martingale relative to X. Of course, we could have used the shorthand E (Mn±i I ..rn) = E (Mn + Xn+i I Yn) = E (Mn I Fri) \u0000 (Xn+1 I Yn) = Mn EXn+1 = Mn, \u0000 a.s.. ► Bb. Exercise: 'Product' martingales. Suppose that the Random Variables Z1, Z2, . . . , Zn are independent, each nonnegative and in G1 and of mean 1. Let Mn := ZiZ2 Zn, \u0000 Mo := 1. Show that M is a martingale relative to Z. Note that what we really need is that E (Zn±i .7.n) = 1, which does not require independence. ► Bc. Exercise on Random Walks. Let X1, X2 . . . be IID Random Variables with P(Xn = +1) = p, \u0000 P(Xn = —1) = q := 1— p, where 0 < p < 1. Let a E Z and define Wn := a + + X2 + • • • + Xn so that W is SRWa(p). Prove that if vv„ Mn := Wn — (13 — On, 1772 = P then M and V are martingales relative to X. Hint for the 'V' part: Use the previous exercise. 9.2. Martingales \u0000 409 ► Bd. 'Likelihood-Ratio' martingales. \u0000 Suppose that f and g are two pdfs, positive on the whole of R. Let Y1, Y2, ... be a finite or infinite sequence of IlD Random Variables each with pdf f . Define the Likelihood-Ratio for time it: g(171)g(Y2)... g(Yn) f (Yi) f (Y2) • • • MT.) with Ro := 1. By using Exercise 408Bb, prove that R is a martingale relative to Y. I,. C. `Markovian' martingales. These form an important class. Ca. The proportion martingale for Polya's urn. Let Br, denote the number of Black balls in Polya's urn of Exercise 76D at time n, and let Mn := n + 2' the proportion of Black balls in the urn at time n. We prove that M is a martingale relative to B. There is an obvious `Markovian' (`lack-of-memory') property E (Bn±i I Bi B2, . • Bn) = E (Bn+1 I BO (the system does not remember how it arrived at the Bn situation), and it is also clear that c(b) \u0000 E (/3„,+1 I Bn = b) = n ± b \u0000 2 (b + 1) + n + 2 — b (b) = b(n ± 3) n + 2 \u0000 n ± 2 Hence, Bn (n + 3) E (Rn+i I Bi, B2, ... Bn) = O n) = n + 2 and the desired result is now obvious. ► Cb. Exercise: A 'game of cards' martingale. A pack consisting of b Black and r Red cards is shuffled and placed face down on the table. The cards are then turned over one by one. Show that if Mn is the proportion of Black cards left before the nth card is revealed, then M is a martingale relative to B-process, where Bn is the number of Black cards left just before the nth card is revealed. Of course, the time-parameter set for M is {1,2,...,b +7.}. ► Cc. Exercise: A 'quadratic' martingale. Suppose that X1, X2, ... are BD RVs in £ 2 with E (Xk ) = 0 and Var(X k) = a2. Let M denote the martingale with Mn = a + + X2 - \u0000 X, Show that if \u0000 Vn := \u0000 — no-2, then V is a martingale relative to X. Bi, 410 \u0000 9: Some further Probability Cd. Exercise: Other martingales associated with Polya's urn. We change the notation for Polya's urn by letting the number of Black balls in the urn at time n be By, +1, so that By now denotes the number of 'new' Black balls in the urn at time n (that is, not counting the original Black ball). Prove that for 8 E (0, 1) (and with Rn the number of new Red balls, of course), (n + 1)! Un := By!(n — Bn)! (1 0)B\"ORn defines a martingale U relative to B. Hint. Consider the two values which Zn+1 :, Un±i /Un can take for a given value b of By, and calculate E (Zn+1 I Bn). Discussion. How could we anticipate that U is a martingale? The answer is provided by the isomorphism in Subsection 79H. Then, in the notation of Subsection 78G, we have from 79(G4), (Mr-I-P(B-a I Fn) = Un, where .Ty is the history up to time n for the urn process (or, under the isomorphism, for the coin-tossing process). The answer now follows heuristically from result 407Ab. Ce. Kingman's 'OK Corral' martingales. In a paper The OK Corral and the power of the law, Paul Mcllroy and I studied the following problem. Two lines of gunmen face each other, there being initially m on one side, n on the other. Each person involved is a hopeless shot, but keeps firing at the enemy until either he himself is killed or there is no-one left on the other side. Let pc(m, n) be the expected number of survivors. Clearly, we have boundary conditions: p(m, 0) = m, µ(0, n) = n. \u0000 (C1) We also have the equation ti(m, n) = m m + n ii(m, n \u0000 n 1) + m + n p(m — 1,n) (m, n > 1). \u0000 (C2) This is because the probability that the first successful shot is made by the side with m gunmen is m/(m + n). m POI) M) 3 2Km71 2048 319.556354 319.857107 4096 537.627362 537.933399 8192 904.382093 904.692518 Table C(i): Three-quarters power law for the 'OK Corral' problem Mcllroy and I considered the case when m = n, for which the computer produces Table C(i), in which K := 3— 4 7rAr (i) = 0.52532558. 9.2. Martingales \u0000 411 \u0000 We had arrived at the fact that p(rn, m) \u0000 2Km 4 heuristically by using a diffusion approximation. Kingman, in a fine paper [137] with a well-chosen title, found lots of martingales for this problem, and used them to prove our conjecture and much more. ►► D. Stopping times. Please re-read the beginning of Section 127 up to the end of Subsection 128A. We allow our stopping times now to take values in {0, 1,2, . ; oo}, making the obvious modification from those with values in {1,2,3,...; oo}. ► E. The 'Constant- Risk' Principle (CRP). It is essential to have good ways of showing (when it is true) that a stopping time T is finite with probability 1, equivalently, that P(T = oo) = 0. Here is a simple, very useful, criterion. (CRP): Suppose that T is a stopping time relative to the evolution {.fin}. Suppose that for some k > I. and some e> 0, we have for every n, P(T < n k I Fn) > c fox\" very w (oh, OK, for almost every w). Then IP(T = oo) = 0. Ea. Application to SRW. Let's see an application of this before proving it. So consider Simple Random Walk SRWa (p), W, started at a, where 0 < a < b and 0 < p < 1. Let T := inf{n : Wn = 0 or Wn = b}. \u0000 (El) How do we prove the (hardly surprising) fact that T is almost surely finite? Consider P(T < n + b Fn). (Note that we have taken 'lc = b', as it were.) We have P(T < n + b = 1 on the set T < n which is in .Fn. So suppose that w is such that T(w) > n; then Wn lies between 0 and b, and b 'Heads' (+1 X-values) in a row would take W above b, forcing T to have occurred before time n + b. In other words, for every w, P(T <n-i-biTn) > pb for every n, so that the conditions of (CRP) hold. Proof of (CRP). Using the obvious fact that {T > n + k} = {T > n} fl {T > n + we have IP(T > n + k) = ET{T>n+k} = Ei{T>n} I{T>n±k} \u0000 = EE (/{T>n}i{T>n+k} \u0000 = E [- IT>n jE (I{T>n+k} I Yn )] < E [/{T>m} (1 — c)] = (1 — c)P(T > n). 412 \u0000 9: Some further Probability Thus, P(T > n + k) < (1 — c)P(T > n), and so, as you can easily see, P(T > mk) < (1 — c)\"1-1, and P(T = oo) = O. \u0000 ❑ ►► F. Doob's Stopping-Time Principle (STP). This is one of the most interesting subsections in the book, describing one of the most important results in the subject and a few first applications. The result says that under suitable conditions, we can use the following idea. A martingale is a fair game and a stopping time is a time after which our hypothetical gambler can decide to stop playing. His hypothetical fortune would then be MT, and since he cannot cheat the system, we should have EMT = E Mo. Now, we clearly need conditions. For let W be SRW(2) started at 0, and let T := inf{n : Wn = 1}. We know from Subsection 118D that ]P(T = oo) = 0. However, in this case, E WT = 1 # 0 = E Wo. Here, you can cheat the system, and we shall examine why at 414Fk below. • • • Fa. Theorem: Doob's Stopping- lime Principle (STP). Let X be a process, M a martingale relative to X, and T a stopping time with P(T no) = 0. Assume that we can write either ± In for al l n < T or Mn = Bn In for all n < T where for some deterministic constant K, and T IBn(ca)1 < K for al w and all n < 7'(cv), 0 < in _j (w) < im(w) for all cid and all 71 < T(w). = E Mo. The idea of the notation is that up to time T, the process B is Bounded and the process I is Increasing (or at least non-decreasing). Before seeing how to prove this result, we look at a number of applications. 9.2. Martingales \u0000 413 ► Fb. Probability of gambler's success for p = 2. Consider SRWa (0, where 0 < a < b. Define T as at 411(E1). Then we already know that P(T = oo) = 0. Here, we can take B = W, with I the process always equal to 0, because B is then bounded between 0 and b up to time T. Hence E (WT) = E (W2) = a. But E (WT ) = bP(WT = b) + OP(WT = 0) = bP(WT = b), so that Pa (WT = b)= alb. ► Fc. Exercise: Probability of gambler's success for p 1. Consider the same problem where 0 < p < 1 and p 1. Find Pa (WT = b) by using the martingale V at Exercise 408Bc. ► Fd. Average duration of gambler's game when p = 2. Suppose that p = 1, and that T continues to be defined by 411(E1). We know from Exercise 409Cc that An := \u0000 — n defines a martingale. But 1/1T, is bounded by b2 for n < T and the process I with (w) := n is increasing. Hence we can apply STP to conclude that E \u0000 — T) = E(W,? — 0) = a 2. But E \u0000 = b21P(WT = b) = b2 x (alb) = ab, and hence E (T) = a(b — a). ► Fe. Exercise: Average duration of gambler's game when p \u0000 1. Solve the same problem as in the previous exercise for p \u0000 You have already met the right martingales. ► Ff. Exercise: Distribution of duration of gambler's game. Let 0 < p < 1. For the game where the initial fortune is a, we have = a + + X2 + • + X., as usual. Prove that for 0 < 9 < 1, then for some real number a we have E (ax ) ) = 1/0, and then a bE oT WT \u0000 aoE \u0000 WT = 0} = a.. Find E (0T ), which, in principle gives the distribution of T. This helps illustrate the point that there are often enough martingales to solve problems in which we are interested. ► Fg. Waiting times for patterns — an example. This very nice way of solving the `Waiting for Patterns' problem is due to S.Y.R. Li. Let me describe the intuitive idea. \"How long on average do you have to wait for HH if you keep tossing a fair coin? Consider the situation where a fair coin tossed in a fair casino falls THTHH. Suppose that just before each toss, a new gambler with a total fortune of £1 arrives, and bets his £1 that the first toss he sees will produce Heads; if he wins, he bets all he then has, £2, that the next toss will also produce Heads. When two Heads in succession occur, the casino closes down. 414 \u0000 9: Some further Probability Our 1st gambler leaves with nothing. So do the 2nd and 3rd gamblers. However, our 4th gambler leaves with £4, and the 5th with £2. Whatever way we get to HH, the total fortune of the gamblers at the end is £6. Because everything is fair, the average fortune brought in by the gamblers is £6, but this is exactly the average number of tosses to get HH. By the same argument, the average number of tosses to get a Head followed by a Tail is 4.\" Fh. Exercise on waiting times for patterns. Give some thought as to how the argument just given may be justified. (Consider the total amount won by the gamblers up to time n minus the total amount brought in by the gamblers up to time n.) Then solve the following problem by the same method. Every second a monkey types a capital letter chosen from the 26 letters of the alphabet. How long on average is it before he types the pattern ABRACADABRA? ► Fi. Exercise: A card game. A pack consisting of b Black and r Red cards is shuffled and placed face down on the table. The cards are then turned over one by one. Just before some card is turned over, you must say 'The next card is Black'. If you are right, you win; otherwise, you lose. Show that whatever strategy you adopt, your probability of winning is bl(b+ r). ► Fj. Exercise: a martingale proof of the Ballot Theorem. Prove the Ballot Theorem 126L by looking at the previous exercise — perhaps in a suitable 'mirror'. It. Fk. Why can you sometimes cheat the system? Again, let W be SRW(1) started at 0, and let T := inf.{ n : 147, = 1}. We know that P(T = oo) = 0 and that, in this case, E WT = 1 0 = E Wo . However, let D be the gambler's minimum fortune before time T: D := inf{Wn, : n < T}. We know from 413Th that for k E N, we have P(D < —k) = 1 k +1' and the trouble derives in part from the fact that E (IDI) = oo. If Tk := inf { n : Wn = 1 or Wn, = —k}, then Tk \u0000 T (a.s.) as k --> oo and WTk \u0000 WT. However, 0 = EWTk 4 E WT = 1. Here, WTk is equal to —k with probability 11(k+1) resulting in a contribution of —kl(k+1)R,, —1 to E WTk from a set of very small probability. What is wrong here is what is called in Measure Theory 'lack of uniform integrability'. The conditions imposed within the STP preclude this kind of bad behaviour. ► Fl. Proof of the Stopping-Time Principle. Suppose first that for some positive integer 9.2. Martingales \u0000 415 r, we have, for all w, T(w) < r. Then E (MT ) = \u0000 E(MT ;T = n) \u0000 (Linearity) 7z: 0 E (Mr, ; T -= n) \u0000 (Logic) n=0 = EE(M r ;T = n) \u0000 (by 407(A2), since IT = n} E .Fn) E \u0000 (Linearity) = E (MO ) \u0000 (by Exercise 407Aa). Now, let us suppose that T is any stopping time with P(T = oo) = 0. Then, for each r in N, T A r := min(T, r) is a stopping time (you check this) which is at most r, so, by what we have just proved, E MT A, = IE Mo • To finish the proof, we have to use Measure Theory. Let's stick to the `+' case of the `either/or' in the theorem. Then, MT Ar = BT Ar ± IT Ar \u0000 E MTA r = E BTAr \u0000 ITAr • As r \u0000 oo, BT Ar \u0000 BT (a.s.) and, since IBTAT I < K, the Bounded-Convergence Theorem 65L shows that we have EBTAr EBT . Next by the Monotone-Convergence Theorem 60D, E /T A, E/T. Finally, EM0 = EA/I \u0000 BT E IT = EMT, and the proof is complete. (There is no possibility that E/T can be infinite: it must be E Mo — EBT .) The —' case is exactly similar of course; or we can use the fact that —M is a martingale!. \u0000 ❑ G. Optional Sampling. Here is an important extension of the Stopping-Time Principle. ►►► Ga. Theorem:boob's Optional-Sampling Theorem (OST). Make the assumptions of Doob's Stopping-Time Principle, and let S be a stopping time with S < T. Let Fs be the a-algebra describing the evolution of X up to time S, as explained in Subsection I298. Then (Ms is Fs-measurable and) E(AITI.Fs) = Ms, (a.s.). This is a nice extension of the martingale property to stopping times, and it plays a huge part in more advanced theory. The proof really mirrors exactly that of the STP, the first step being to prove that if T satisfies T < r for some positive integer r, then 416 \u0000 9: Some further Probability E (Mr .TT) = MT, a.s.. The desired result when T < r is then a consequence of the Tower Property; and one uses the Bounded-Convergence and Monotone-Convergence Theorems as before. ► ► H. Wald's Sequential Hypothesis Test. This is a key topic for Quality Control and for Clinical Trials. ► Ha. Lemma. If M is a nonnegative martingale and T is a stopping time such that IP(T = oo) = 0, then EMT <E In the light of the proof of the Stopping-Time Principle, this follows immediately because `Fatou's Lemma' in Measure Theory implies that EMT = E \u0000 M - -TAr) < lim inf EMTA, < EM o. r->00 Hb. The testing problem. Suppose that we are able to make observations of IID RVs Yl, Y2, ... and have two hypotheses: Ho : the common pdf/pmf of Y1, Y2, • • is f HA : the common pdf/pmf of 1/1, Y2, \u0000 is g, with f and g being strictly positive with the same domain. Suppose that we have to choose whether to accept Ho or to accept HA. (I know that we do not usually talk of accepting hypotheses, but stick with it.) We want Type I and Type II errors controlled by IP(reject Ho I Ho) < a, \u0000 IP>(reject HAI HA) < /3. We wish to make our decision utilizing on average the smallest sample size. ► Hc. Quality- Control Example. We might have the situation where on a production line Yk = ie \u0000 i \u0000 d \u0000 tem pro \u0000 is OK, where \u0000 Y2, . are IID with IP(Yk = 1) = p = 1 — P(Yk = 0)) {1 if the kth item produced is defective, 0f th kth where 0 < p < 1. We wish to test 1/0 : p = po against HA : p = p1 (or, to be more sensible, Ho p < po against HA p > Pi, Ho corresponding to an acceptable situation, HA to a situation where something has gone wrong). ▪ Hd. Wald's Strategy. Actually, I present a slightly different (and, I think, neater) strategy than the usual one. P(accept HA) — a \u0000 where a = ln(1/a), b = ln(1/(3), b 9.2. Martingales \u0000 417 Let R be the Likelihood-Ratio martingale under 1/0 of Exercise 409Bd. Consider the strategy: n = 0; Repeat n = n + 1; Observe Yn; Calculate Rn; Until Rn > a-1 or Rn < 0; If Rrt > a-1 accept HA; Otherwise accept Ho. Of course, T := in* : Rn > a -1 or Rn < 01 is a stopping time relative to Y. With Po relating to Ho, we have Po(T = 00) = 0. We know this from the consistency result 193(C1), though inevitably this result has a martingale proof due to Doob (for which see [W]). By 416Ha, we have 1 -=- E > E oRT > a-1P0 (RT > a -1) = a -1P(reject Ho I Ho), N so that P(reject Ho I Ho) < a, as required. By interchanging the roles of Ho and HA, we find that P(reject HA HA) < 13. The whole point is that on average, fewer observations are needed for this sequential test than for a fixed-sample-size test with the same error bounds. He. Example. \u0000 Consider the case when Yk \u0000 N(O, 1) with HO : 0 = 0 and HA : 0 = p = 0.4. We want a < 0.05 and Q < 0.10. Now, under Ho, 1nRn = \u0000 P(Yk — \u0000 N( —iit2n, ou2n)- Very crudely, the expected time to get Rn < Q is therefore (10)/(-1 p2) 29, so E 0T should be at most a value near this. By contrast, the fixed-sample-size test at Subsection 229Ce required a sample size of 54. You can see that, as common sense suggests, the Average Sample Number for the sequential test will be greatest when the true mean 0 is 2 p. Then there is no drift in In R, and we have to rely entirely on random fluctuations to bring the test to a conclusion. Suppose now that the true value of 0 is 2 p. Then it is easy to see from the CRP that P(T = 00) = 0 — you can take k = 1. Now, 1nRn is a martingale, so that roughly (ignoring 'overshoot') 0 = E In R0 = ln(l/a)P(accept HA) + ln(0)P(accept Ho), whence 418 \u0000 9: Some further Probability in analogy with the discussion at 413Th. But, by Exercise 409Cc, (In R7,) 2 — np2 is a martingale, whence we might expect that µ 2]E (T) (T) \u0000 a \u0000 b a2 + a a ± b b 2 = ab , in analogy with the discussion at 413Fd. This suggests that in the worst case, the ASN is about abp,-2 = 43. \u0000 ❑ That's all I'm going to say about sequential testing. For a serious study of its most important application, see Jennison and Turnbull [119]. ► I. Doob's Martingale—Convergence Theorem. This theorem is one of the most important results in Mathematics, not just in Probability. For Doob's marvellous `uperossing' proof, see [W]. ►►► Ia. \u0000 Martingale-Convergence Theorem (MCT). Let M be a martingale relative to X.. Suppose either that M is bounded in .C1 (for some finite K, we have Ea \u0000 < K for all n) or that each Mn is nonnegative. Then M \u0000 lira MTh exists almost surely, It is important that the convergence will not necessarily be in L : it will be in El if and only if Mn E (Moo I (a.s.) for every n. However, if M is bounded in L2 (for some finite K, we have E(114„12) < K for all n), then Mn M,„3 in L2 (whence also in C'). Ib. Application to Polya's urn. We see that the proportion of Black balls in Polya's urn will almost surely converge to a limit 0, something we also know from the isomorphism explained in Subsection 79H. ► Ic. Application to branching processes. If Zn is the population size at time n for a Galton—Watson branching process for which := E (X) < co, then E (Zn+1 I Z17 Z27 • \u0000 Zn) = E (Zn+1 I Zn) — 11•Zn7 so that Mn := Zn/µn defines a nonnegative Z-martingale. We know that if p < 1, then (assuming as always that P(X = 1) 1), the population will eventually die out, so that Mcc = 0 and we do not get L1 convergence. Kesten proved that if p > 1, then we get Ll convergence if and only if E [X ln(1 + X)] < 00. See [W] for a little more on this branching-process martingale and lots more on the MCT. 9.2. Martingales \u0000 419 ► J. Levy's Theorem for 'Reverse' Martingales. At first sight, this seems a rather crazy idea. However, it leads to a very nice proof both of de Finetti's Theorem (which, you may remember, many Bayesians see as one of the main justifications of their philosophy) and of the cornerstone of the whole subject, the Strong Law. Let S = (S1, S2, ...) be a process. A reverse martingale relative to S is a process R such that R1 E r l and Rm = E (Ri I Sm, Sm+i, ...) a.s.. [Clarification. If we consider for n > m, the RV Rm,n := E (Ri I Sm, Sm+i, • • • Srn+n) a.s., then by result described at 407Ab, Rm,n will be a standard martingale as n varies, and in fact R„., = lim R„,,n (a.s.) and in .C1. 71 Accept this.] ►► Ja. Fact (Levy's Theorem). If R is a reverse martingale relative to S, then A„, := lirn Km, exists almost surely and in G1 and E (R1 I Roo) = Roo, almost surely. For proof, see [W]. ► J13. Martingale proof of de Finetti's Theorem. Please re- read Subsection 220P. Proof of Theorem 221Pa. Let ST, := Yi +Y2 + - - - + Ya• Define R1 := Y1 and (for n > m) Rm := E (Ri I Sm, Sm+1, • • -), \u0000 Rm,n := E (Ri I Sm ) Sm-1-1) • • • Sm-l-n)- Now, by logic, Rm,n = E (RI I Sm, Xm+i , • • • Xm+n) • But the n-tuple (Xm+i, ... Xm+n ) is independent of the pair (R1, Sm), so that by an obvious extension of Result 402Nb, R m,n = E(Ri I Sm ) = 1E('11 Sm). Let r be a permutation of N which leaves each number greater than m fixed. This permutation will preserve the probabilistic structure of the whole process, and will leave Sm exactly the same (for each w). Hence, we see that E (ifi I Sm) = E (Y2 I Sm) = • • • = E (Ym I Sm ) = their average = E ( S S m) = S m \u0000 m 420 \u0000 9: Some further Probability We now see that Rm = SmIm (a.s.), and it follows from Levy's Theorem that 0 Ern Sm/m exists almost surely. We now use the fact that Eyj1Yi2 Y„ = Sm(Sm — 1) ... (Sm — r + 1), \u0000 (J1) the sum being over all m(m — 1) ... (m— r +1) r-tuples i1i i2, • • . , it of distinct numbers chosen within {1, 2, ... , m}. Assume this for the moment. By the obvious extension of the 'invariance under permutation' argument, we see that for distinct i1 -2, • • • ,it within {1, 2, ... , m}, E (17,1 17,2 • Yt, I Sin) SM+1) Sm(Sm — 1) ... (Sm r + 1) m(m — 1) ... (m — r + 1) But, as m —> co, this tends (a.s.) to er, just because Sm/m tends to O. Hence for distinct it,i2, \u0000 • • ir, E (Y„Y2 Yi, I Or) = Or a.s., that is, IP(Yii = 1; Yi2 = 1; \u0000 ; Y = 11 9) = Or (a.s.), and that is the desired result: given 0, the variables Yi are IID each Bernoulli(6). \u0000 ❑ Jc. Exercise. Prove the identity (J1) using the fact that Y2 = Y. This is very simple; don't let the complicated appearance of the result put you off. Try it for r = 2 first, using the fact that with i, j restricted to {1, 2, ... ,74 E = s m _ jai Jd. Martingale proof of the Strong Law. Let X1, X2 . . be IID Random Variables in G1 with p = E (Xk ). You can see that the argument used for de Finetti's Theorem shows that L := limSriln exists almost surely, where Sr, := X1 + X2 + \u0000 Xn. What we need to do is to prove that L = p almost surely. The rl convergence guarantees that EL = p. The crucial point is that for each m, L (a.s.) depends only on the values of Xm, Xm+i , ...; indeed, m. X +1+ \" ' Xrn±n L = hm n (a.s.), just by Analysis. Hence L is independent of m'Sm, so that, for a, E R, E exp(iaL + \u0000 = E exp(iaL) E exp(i0m-1Sm). 9.2. Martingales \u0000 421 Let m —>- oo and use the Bounded-Convergence Theorem to see that the Characteristic Function (pi, := E 0\" of L satisfies co(a ± 13) = So(a)C0(0), so that co(a) = eK\" for some K. However, (p(—a) = Ee —'«L must be the complex conjugate of c,o(a), so that K is pure imaginary: K = i7 for some real - y. But then E eia L = eic\", and so L has the same distribution as the deterministic variable equal to -y on the whole of Cl. Clearly, - y = 1.1, and the proof of the Strong Law is complete. ❑ ►► K. The Black—Scholes option-pricing formula. Option pricing, the Black- Scholes formula, etc, are all based on martingales and stochastic calculus; and at one stage, even being able to say the word 'martingale' tended to get you a highly paid job in finance. Nowadays, everyone in finance knows something about martingales, so you would need to, too. Because you might be interested, I try hard here to improve on the account of the Mathematics in [W], though I also express my unease about the topic more explicitly than I did there. I do think that the Black-Scholes formula is nice if one accepts the 'hedging strategy' philosophy, and am well aware that finance has posed some problems on martingales of real intrinsic interest. But I must also confess that I cannot summon up too great an enthusiasm for anything which has to do with money, and that in many ways I regret the prodigious talent that has gone into mathematical finance (when people could have been trying much harder problems in Statistical Mechanics, for example). The site (by Uwe Wystup) http : //www.mathfinance.de is informative about the literature on finance. (Of course, Bingham and Kiesel [23], 'already' in the bibliography, is very relevant. I also recommend Hunt and Kennedy [114] and Karatzas and Shreve [124].) Idealized situation. We assume that there is only one type of stock you can own, and one type of bond. The value Br, of one unit of bond at time n is deterministic: it is Br, = Bo(1 + On, with fixed known interest rate r. The value of one unit of stock jumps at time n to a new value Sn according to 'random' instantaneous interest Rn. We have Sn = (1 + Rri)Sn-i, Bn = (1 + r)Bn-i• We assume for our idealized world that it is known that Rn can only ever take one of the two known values a and b where a < r < b: Rn E fa, bl, - 1 < a < r < b. 422 \u0000 9: Some further Probability The value So is a known constant. European(K, N) option. In this option, you pay an amount x at time 0 to have the option to buy at time N one unit of stock for an amount (strike price) K. If your SN > K, you will exercise that right at time N, gaining value SN — K ; otherwise, you will not exercise that option. Thus, your gain at time N will be (SN — K)+, the positive part of SN — K. For now, we regard N and K as given constants. The problem: What is the fair price you should pay at time 0 for that option to buy one unit of stock at cost K at time N? Perhaps it might seem that the answer should be E {(SN — K)+}, the mean value of your gain at time N. However, the Black—Scholes formulation does not require any probabilistic structure to be imposed on the sequence R1, R2, - - - Previsible portfolio strategy. Let me help by saying straight away that time 0 is rather anomalous in the formulation I now give: there is no real distinction between Al and Ao. Or to put it another way, Ao and Vo play no role in what follows except 'to fill out a pattern'. Suppose that at time 0 you own Ao units of stock and Vo units of bond, so your fortune is Xo = AoSo + VoBo. Before time 1, you rearrange your portfolio reinvesting the amount Xo as Ai units of stock and V1 units of bond. So, Xo = A l So + ViBo. Thus (Al, Vi) is your 'stake' on the 'game' at time 1 after which your fortune jumps to Xi = A151 + V1B1. Generally, for n > 1, • your fortune just before time n is Xn_i = AnSn-1 + VnBn-1, • your fortune just after time n is Xn =- AnSn + VnBn, which you then reinvest as X n = A n±iSn + Vn+1Bn (— AnSn ± VnBn)• Note that we must be able to calculate An from the values Ri, R2, ... , Rn_i (equivalently from Si, S2, . . . , Sn_i): we do not know the value of R,T, at the time when we have to decide on An. The process A is called previsible relative to the process R: 'it is known one time unit before it looks as if it should be'. This is the right way to formulate things: previsible processes are the things we integrate in stochastic calculus. Hedging strategy for our European option. By a hedging strategy for our European(K, N) option, with initial value x, we mean a previsible portfolio 9.2. Martingales \u0000 423 strategy such that X0 = x and that, for every possible outcome, every sequence of values that R1, R2, . . . can take, we have X N = (SN — K)± . Thus the strategy exactly matches the option price at time N. The Black—Scholes concept of a Fair Price. We say that x is a Black—Scholes fair price at time 0 for the option if there exists a hedging strategy for the option, with initial value x. p p THEOREM (Black—Scholes). There exists a unique Black—Scholes fair price x for our European(K, N) option, and a unique associated hedging strategy. Moreover, x = (I ± r) NE Bs { (SN — K)±} \u0000 (10) where EBs is expectation associated with the probability measure PBS which makes R1, R2, . . . independently identically distributed, each with the unique distribution on {a, b} with mean r, so that r — a \u0000 b — r \"'Bs (Rk = b) = p : — b a' PBS (Rk = a) = q := b — a I emphasize that nothing is being assumed in the theorem about the true 'real- world' probabilistic structure (in which the values of R1, R2, . .. might well be highly correlated). The law PBS is a contrived law, created merely to allow neat expression of some elementary algebra. Remark. \u0000 I have heard several statisticians express some unease about the formula because it does not involve real probabilities, unease which I share. Of course, we are aware that the people who offer the option used their own beliefs about the real probabilities in deciding on the appropriate value for K, and we would use our beliefs about those probabilities in deciding whether or not actually to buy the option at the Black—Scholes fair price. But still ... . The case when N = 1. Let's consider the case when N = 1 without utilizing any probabilistic structure. First assume that a fair price and associated hedging strategy exist. Let us prove that they are unique. We must have, for every possible outcome, AiSo + VIA) = x, Ai Si + ViBi = (S1 — K)±. Since Si = (1 + R1)S0 and B1 = (1 + 4B°, we have (1 + r)x + Ai S 0 (Ri — r) = (So + Ri So — K)+. 424 \u0000 9: Some further Probability Since R1 = b or R1 = a, (1 ± r)x + AiSo(b — r) = (So + bS0 — K)+, (1+ r)x + ApSo(a — r) = (So + aS0 — K)+. whence, (b — a)A.1.50 = (S0 + bS0 — K)± — (S0 + aS0 — K)+ . If (1 + a)So > K, then Al = 1, (1 + r)x = S0(1 + r) — K, V1 = K (1+ r)Bo . If (1 ± b)S0 > K > (1+ a)S0, then (b — a).A1S0 = S0(1 + b) — K, (1+ r)x = r \u0000 a {S0(1 + b) — K} , b — a a+1 S 0(1+ b)— K Vi If K > (1+ b)S0, then Ai =V1 = x = O. Thus in each case there is at most one Black—Scholes fair price x at time 0. Conversely, since we now know what the hedging strategy must be, we can confirm that x is a Black—Scholes fair price. Ka. Exercise. Prove that the values of x just found agree with 423(K1) with N = 1. That V1 is generally negative is another disturbing feature of the Black—Scholes philosophy. It means that in the hedging strategy, we have to borrow an amount V1B0 of money against the bond rate of interest in order to buy our stock. (Would we be able to do this? I doubt it very much.) But, on with the Mathematics. Proof of uniqueness of x for general N. \u0000 Suppose that a hedging strategy (A1, V1, A2, V2, ...) exists, with initial value x, and that we adopt it. Recall the situation X, = A nSn ± Vn Bn, X n-1 = AnSn —i+ VnBn— 1 7 Sn = Sri - 1 + Sn—iBn, Bn = Bn-1 + rBn —i• Hence, X, — Xn —i = AnSn—l-Bn + Vn B n _ir = rX n_i ± AnSn_i (Rn — r). If we set r +1 \u0000 b — a Yn = (1 + r) — n Xn, 9.2. Martingales \u0000 425 the discounted value of our fortune at time n, then Yn — — = ( 1 + r) —n AnSn—i(Rn — r). Now, as we have observed previously, An is a function of R1, R2, • • • , Rn_1, as is Sn—i.• Hence, for our contrived measure PBS, EBs(Yn — Yrt-1 -8 1, R2, • Rn-1) = (1 + r) —nAnSn_iEBs(Rn — r I Rt, R2, • Rn-1) = (1 + r) — A n Sn _ iE Bs (Rn — (by independence) = 0 (since E Bs Rn = r). Thus, Y is a martingale relative to R. But, by the definition of a hedging strategy, X N = (S N — K)+, so, by the martingale property, x = X 0 = EBs(YN) — (1 + r) NEss {(SN — K)± } Thus, there is at most one Black—Scholes fair price, and if a fair price does exist, it is given by formula 423(K1). \u0000 ❑ Proof of existence of a hedging strategy with initial value x as given by 423(K1). The obvious thing is to start afresh and to define Yn := EBS (1 ± r) —N (SN — 10+ I R1) R2, • • • Rn} • \u0000 (K2) Then, by the result at 407Ab, Y is a martingale (on time-parameter set {0, 1, 2, ... , N}) relative to R. For some function fn on {a, b}n, we have Yn = fn(Ri, R2, • • • , Rn)• But now, the martingale property E' BS (Yri l Ri, R2, \u0000 , Rn - 1) = Yn - 1 yields = P fn(Ri, R2, • • • , Rn-1,b) qfn(Ri, R2, . . , \u0000 a) = Yn - 1 = fn - 1( -8 1, R2, • • 1 R n -1 )' \u0000 (K3) Hence, fn ( R1, R2, • • • , Rn-1, b) — fn-1(R1, R2, ••• , Rn-1) fn-1( R i., R2, • • • , Rn-1) — fn (R1 , R 2, \u0000 , \u0000 , = lin (say). 426 \u0000 9: Some further Probability Note that Hn does not involve 17„, so H is clearly previsible relative to R. We have Yn - Yn - 1 = fn (171 7 R2, • • • , Rn-1, Rn) - fn-1 (R1, R2, . . . , Rn-1) = Ib(Rn) ffn(R1, R2, • • • , Rn-1, b) - fn-1(R1, R2, ...,R_1)} + Ia(Rn) Un(R1, R2, • • • , Rn-1, a) — fn—i(Ri., R2, ...,R_1)} = feb(Rn) - Pia (Rn)} Hn • Now, Rn - r = Ib(Rn)(b - r) + l a (Rn )(a - r) = (b - a){q/b(Hn) - Pia(Rn)}, and so, finally, Yn - Yn_1 = (b - a)-1 Hn(Rn - r), where H is previsible. So, we can define a previsible process A to satisfy (1. + T) —nA.,57t-1 = (b — a) —i lin. Then, of course, we define Xn := (1 + r)nYn, V n := (Xn — AnSn—i)/Bn-1, and we have all the elements of the hedging strategy, with initial value Y0 given as x in the Black—Scholes formula 423(K1). The Black—Scholes Theorem is proved. \u0000 ❑ That each Hn is positive, whence each An is positive, is clear from 425(K3) and the obvious fact that fn(Ri, R2, ... , Rn—1, b) > fn(Hi., Hz, ... , Hn_i, a)• The fact that each An is positive means that there is no 'short-selling' of stock. Important Remarks. In the discrete-time case which we have been considering, the introduction of the probability measure IPBS was merely a device to allow neat expression of some elementary algebra. However, in the continuous-time version, where the algebra is no longer meaningful, martingale theory provides the essential language for doing the appropriate analysis. 9.3 Poisson Processes (PPs) The Poisson Process is used to model (under conditions on which you should decide later) arrivals 9.3. Poisson Processes (PPs) \u0000 427 • of customers at a queue, • of calls to a telephone exchange, • of particles at a Geiger counter, etc, etc. For now, we work with the time-parameter set [0, oo), our arrivals falling in this half-line. We let N(A) denote the number of arrivals in a time interval A, and write, for example, N (t, t u] for N(A) when A = (t, t u]. p.o. A. Two descriptions of a PP(A). Let me begin by giving the two most N(0, t] t Figure A(i): Simulation of Poisson N(0, t] when A = 1 direct ways of describing a Poisson Process, the first as a case of the more general concept of a Poisson Point Process (PPP). 428 \u0000 9: Some further Probability Description 1, A Poisson Process of rate A, denoted by PP(A), is one whose arrival times form a PPP of constant intensity A on [0, oo); which means that • the numbers of arrivals MAO, N(A.2), • • • , N(AT ) in a collection AI, A2, .. . An of disjoint time-intervals are independent RVs; • the number arriving in any interval of length t has the Poisson(At) distribution, Description 2. A PP(A) can also be fully described by the fact that the arrival times are T2 := 6 + 2, T3 := 6 + 6 + 6, .. where the interarrival times 6, 6, 3, ... are IID RVs each with the exponential distribution of rate parameter A. Description 2 allows us to construct a triple (12, .F, P) for our Poisson Process because we only need the appropriate triple for the IID sequence (en). There are difficulties (surmounted later in this section) in constructing an appropriate triple from Description 1. Description 2 also allows us to simulate a Poisson Process. Figure 427A(i) shows a simulation of N(0, t] against t for the case when A = 1. The line at 45° is also shown. Note that the interarrival times are often very small. We know that there is consistency in Description 1 in that \u0000 N(0,t] + N(t,t+h] = N(0,t+ h] \u0000 (Al) tallies with the fact that if X and Y are independent RVs, X — Poisson(p) and Y Poisson(v), then X + Y Poisson(p, v). We might expect Description 2 to tally with the lack-of-memory property of the exponential distribution. But do things have to be this way: can we use distributions other than Poisson and exponential? To answer this, it helps to develop the Poisson Process in a different way. ►► B. The 'Poisson' property from more basic assumptions. Let us assume of our process of arrivals of rate A that • the numbers of arrivals N(24.1), N(A2), \u0000 , N(An) in a collection A1, A2, . . . , An, of disjoint time-intervals are independent RVs; • the distribution of the number of arrivals in an interval depends only on the length of that interval; 9.3. Poisson Processes (PPs) \u0000 429 • for the number N(0, h] of arrivals during time-interval (0, h], we assume for small h that P{N(0, h] = 1} = Ah + o(h), P{N(0, h] > 1} = o(h). Here g(h) = o(h) means that g(h)/h —> 0 as h .I, 0. See Appendix A3, p496. We clearly have ]P{N(0, h] = 0} = 1 — Ah + o(h). Ba. Theorem. Under the above assumptions, N(0, t] ,--, Poisson(Xt). Proof Fix a with 0 < a < 1. From our study of probability generating functions, we know that it is enough to prove that gt(a) := Ea N(o,t] = eAt(a--1). Let t, h > 0. Since the two RVs N(0, t] and N(t, t + h] are independent and N(t, t + h] has the same distribution as N(0, h], we find that gt+h(a) = 9t(a)9h(a), so the exponential form of g is not surprising. For small h, we have, heuristically, 9h(a) = Ea N(\" 11 = aVIN(0, h] = 0} + a l-EV(0, h] =1} + • • • = a°{1 — Ah + o(h)} + al{Ah + o(h)} + o(h) = 1+ A(a — 1)h + o(h). Hence, gt±h(a) = gt (a)[1 + A(a — 1)h + o(h)] or gt+h(a) — gt(a) = A(a h \u0000 (ce 1)9t (a ) + 9t(a)o(h) Let h 1,0 to get a Otgt (a) = A(a — 1)gt (a), \u0000 go(a) =- Ea° = 1. (B1) 430 \u0000 9: Some further Probability But if y'(t) = by(t) and y(0) = 1, then y(t) = ebt. Hence, gt (cE) — eA(0,—* = eAt(a—i), as required. (Note. If are keen on rigour, check that (B1) holds where the derivative is two-sided, not just a derivative to the right.) I hope that this helps you understand why Description 1 is as it is. ► C. Tying in Descriptions 1 and 2. Make the assumptions of the previous subsection, equivalently, those of Description 1. Let Tn be the time at which the nth customer arrives. We have > t) = FIN(0, = \u0000 =- e —at. (Of course, the probability that a customer arrives at time 0 is 0.) By combining this idea with independence of numbers of arrivals in disjoint time intervals, one can prove that Description 2 is valid. I skip doing this — though see remarks in Subsection 433E below. Let us work in the other direction to see that Description 2 implies Description 1. So, assume that := 6, T2 := \u0000 6; T3 S1 + 6 + 6, - • 7 where the interarrival times 6, 6, 6, ... are IID RVs each with the exponential distribution of rate parameter A. Postulate that the nth customer arrives at time Tn. In this formulation, N(0, t] := max{n : Tn < t}. Let us show that N(0, t] Poisson(At). We know that Tn has the Gamma(n, rate A) distribution with pdf .fT„ (t) = (n — 1)! • It is intuitively clear that we can use the independence of -72+1 and Tn in a calculation PIN(0, = n} = P{T7, < t < Tn+1 } = f P{T7, E ds; end 1 > t s} = f PITn E ds}P{n+i > t - s} .5=o t \u0000 f t An t n_l e -At = f ./T,s (S)e —A(t—s)dS \u0000 e — 0 \u0000 (n — 1)! \u0000 mt—s)ds \u0000 Ane — At Jo s t \u0000 _ ' i ds = one at to (n — 1)! 0 \u0000 (n — 1)! n (At)ne —at n! as required. We see how closely interrelated Poisson and exponential distributions are. The martingale explanation is mentioned at Subsection 433E. Antra—le —at 9.3. Poisson Processes (PPs) \u0000 431 Ca. Exercise. Calculate P(N(0, 1] = 0, N(1, 2] = 1) , first (easily!) using Description 1, and then using Description 2. Do the same for P(N(0, 1] = 1, N(1, 2] = 1) . Cb. Exercise. This re-does Exercise 22Gb in a way which is now more meaningful and more informative. Let S be the first time when there has been a gap of length c during which no customers have arrived. Let Ls be the Laplace transform (see Subsection 155Q) Ls (a) := E (e as), \u0000 (a > 0). Let denote the arrival time of the first customer. Give a clear explanation of how the IID property of the interarrival times leads one to the conclusion that E (e-as ) = { e -0,c if > c, e-' 6Ls(a) if < c. Deduce from the Conditional Mean Formula that c Ls (a) = e'ce-Ac + f Ae-Ase-\"Ls(cx)ds, 0 and that Ls(a) = A + ae(A±a)c . Recall that A is fixed. By evaluating —dLs(da a) and setting a = 0, show that eAc - 1 E (S) = \u0000 A . Cc. Exercise. Let W denote the first time when the gap between the arrival of two consecutive customers is less than c. Find Lw (a) and E (W). Cd. Exercise. Deduce from the equivalence of Descriptions 1 and 2 that, for n > 1, f t Ansn-i e-A. Jo (n - 1)! \u0000 ds = Hint. Interpret in terms of Tn and Nt. 00 e-At (At)k k! k=n A ± a 432 \u0000 9: Some further Probability ► D. The `Boys and Girls' Principle. Recall the result of Subsection 100I. It said that if the Number of births is Poisson(y) and each birth (Independently of everything else') produces a boy with probability p, a girl with probability q, then Number of boys — Poisson(w), Number of girls ti Poisson(-yq), the Number of boys is independent of the Number of girls. Here is an important extension of that pnnciple. ►►► Da. Fact: 'Boys and Girls' Principle for Poisson processes. The following two Models have exactly the same probabilistic structure. Model 1: Men arrive at a queue in a PP(A) process, Women arrive at the queue in a PP(z) process, and these two Poisson Processes are independent. Model 2: People arrive at the queue in a PP(A ,u) process, Each person (independently of everything else) is a man with probability p, a woman with probability q, where A A + \u0000 q A + Assume this. It should not be too surprising now. ► Db. Exercise. Men arrive at a shop in a Poisson Process of rate a, and Women arrive at the shop in an independent Poisson Process of rate /3. Let M(t, u], W(t, N(t, u] be the numbers of Men, Women, People arriving during time-interval (t, \u0000 If the time-interval is (0, t], write M(t), W(t), N(t) for short. Calculate (a) the probability that the first person to arrive is a Woman; (b) the probability that the first two to arrive are both Women; (c) JP[M(t) = m I N(t) -= n], where m < n, (d) P[N (t) = n I M(t) = m], where n > m. ► Dc. 'Coupon-collector problem': a Poisson version. \u0000 Of course I eat three BarleyabixTM per day when I've got some. Every time I buy a packet, I get one of the collection of 71. dinosaurs. I buy packets at random, at the times of a Poisson Point Process of rate 1 per week — from a 24-hour, 7-days-a-week shop. Explain why the set of times when I get a T Rex is 9.3. Poisson Processes (PPs) \u0000 433 a PPP of rate 1/n independent of the PPP of times when I get a Velociraptor. Let S be the time at which I get my first T Rex, and let T be the time in weeks that I have to wait to complete my collection. Prove that # \u0000 n P(S < t) = 1 — exp (— —t , \u0000 P(T t) = {1 — exp (- 7711 , On average, how long do I have to wait for my first dinosaur? When I have k different dinosaurs (k < n), how much longer on average do I have to wait for my next new dinosaur? Explain (don't try by integration!) why E (T) = nl 1 \u0000 1 + \u0000 + + 1 2 + I) , n n — 1 so that, by well-known mathematics, E (T) \u0000 n In n in that the ratio of the two sides tends to 1 as n —> oo. How does Var(T) behave as n —> co? Prove that, for any e > 0, P(ITnAnlnn) — 11 > \u0000 0 as n oo. ■ E. Martingale characterizations of Poisson Processes. Let N(t) := N(0, t], where we use Description 1. Then (a) N(0) = 0; (b) t \u0000 Nt is right-continuous and 'constant except for jumps by 1', (c) A := Nt — At defines a martingale relative to N. In regard to (c) we mean that E{Nt±h — A(t + h) INs : s < t} = Nt — At. This rearranges as E{Nt±h — Ntl Ns : s < t} = Ah; and this is intuitively obvious since Nt+h — Nt = N(t, t + h] is independent of {N, : s < t} and has mean Ah. We saw similar things in our study of additive martingales. The amazing thing, which shows the power of the martingale concept, is the martingale characterization result that properties (a), (b) and (c) imply that N is a PP(A): in particular that Nt+h — Nt is independent of {N, : s < t} with the Poisson ( A h ) distribution. To prove this, one first uses stochastic calculus (see, for example, Volume 2 of Rogers and Williams [1991) to show that, for 0 < a < 1, ceisite —At(a-1) • is a martingale relative to N. 434 \u0000 9: Some further Probability In particular, therefore, EaNt = eat(a-1) so that Nt Poisson(At). Next, one uses the fact that if T is a stopping time (for example, the time of the first arrival, the first jump of N), then by the appropriate Optional Sampling Theorem, E (NT+t+h — NT-Ft I TT \u0000 = h, FT-Ft being the a-algebra describing the history up to the stopping time T + t. Putting these ideas together carefully, one finds that NT+t - NT is a PP(A) independent of TT . You can start to see why martingale theory settles elegantly the fact that Description 1 implies Description 2. Let T1 be the time of first jump of N. For 0 > 0, (A +a)Nt e-at defines a martingale relative to N which is bounded on [0, T]. Thus, E A±a e —aT' — 1 \u0000 Ee'T1 — \u0000 A 1 A \u0000 A + a whence T1 is exponential rate A. Analogously, TT, is Gamma(n, rate A). And so on. ►► F. Poisson Point Processes (PPPs). We have already met a PPP of constant intensity A on [0, oo). Generalizations are immediate. For example, a PPP of constant intensity A, PPP(A), on R2 is a random set P of 'points' in R2 such that • the numbers of points N(A i ), N(A2), \u0000 , N(An) in a collection A1, A2, . \u0000 An of disjoint (Borel) subsets of R2 are independent RVs; • the number of points in any (Borel) region of area a has the Poisson(Aa) distribution. We might use this to model the positions of faults randomly produced in some surface. Fa. Exercise. Suppose that we have a PPP(A) P on R2 . Fix a point in 1W2 . What is the pdf of the distance from the nearest point of P to that fixed point? ► G. The 'Waiting- time Paradox'. Let 7\" be a PPP(A) on R, and let a be any point of R. Let Tk (k = 1, 2, 3, ...) be the kth point of '1:3' to the right of a, and let T_k (k = 1, 2, 3, ...) be the kth point of P to the left of a. Then , T_2 - T_3, T_1 - \u0000 a — \u0000 — a, T2 T1, T3 T2, • • • , A 9.3. Poisson Processes (PPs) \u0000 435 are IID each exponential rate A with pdf Ae —At on (0, oo). People do not like the fact that the gap around a is the sum of two HD exponentials and has pdf A2te —At. In a sense, 'a is more likely to fall in a bigger gap, and IP{(gap around a) E dt} OC IP{(typical gap) E dt} x t, where we have boosted the probability by the length of the gap'. You arrived at the bus stop at a bad time! Ga. Exercise. \u0000 Find ways of making the 'boosting by length' idea rigorous. You might for example wish to consider a whole sequence of observers arriving at times , —26, — 6,0,6, 26., H. Conditional Uniformity. This is another version of the 'Boys and Girls' Principle. Suppose that we have a PPP(A) P on EV. Given that a (Borel) region A contains n points of P, these points are distributed in A exactly as if we made n independent choices uniformly within A. Suppose, for example, that B is a (Borel) subset of A. Then, P{N (B) = k N (A) = n} \u0000 (nk)pkqn—k \u0000 (H1) where p= IBI/IAI, All denoting the (hyper-)volume of A. Ha. Exercise. Prove (H1). How would you prove the general result? ► Hb. Exercise. Continue with Exercise 432Db. Calculate (e) TP[M(s) = k M(t) = ml, where s < t and k < m. (f) P(M(1, 3] = 2 I M(0, 2] = 2). Part (f) is a bit tricky. Hc. Exercise. Consider a Poisson process of rate A and suppose that we use the improper Trequentise prior 7r(A) = A' for A. Let A denote the event `K arrivals during [0, a]' and B the event 'IC n arrivals in [0, a + 1]'. Show that (formally) 1 \u0000 1 IP, (A) = \u0000 P„ (13) = Kn , P, (A I /3) = (n K ) ( a \u0000 a+ ) and that IP(B A) therefore agrees with the answer to Exercise 213Ke. That P,(A) = K -1 irrespective of a shows just how daft improper priors are! ► Hd. Conditional uniformity and sufficiency. Suppose that we know that an observed set of points within a region A of R2 is a realization of a PPP(A) on A, and that we wish to estimate A. Then, from the point of view of sufficiency, all that is relevant to inference about A is the total number of points n'ths (A) in A. This is 436 \u0000 9: Some further Probability because after Zeus chose nobs (A) and reported its value to Tyche, Tyche (without knowing the value of A) then chose the n°1's (A) points in A independently, each according to the uniform distribution on A. ► He. Rigorous construction of a PPP. Suppose that for each (Borel) subset A of Rn there is defined a RV N(A) with the Poisson(AIAI) distribution, that for disjoint subsets B1, B2, .. /37, with union B the RVs N(B1), N(B2), \u0000 , N(Bn) are independent and PIN(B) = N(B1) + N(B2) + • • • + N(Bn)} = 1. Can we say that P{N(C U D) = N(C) + N(D) whenever C and D are disjoint} = 1? \u0000 (H2) Here we meet a serious difficulty. There are uncountably many Borel subsets C of R2, and there is no reason why the set F (say) within {} in (H2) should be measurable; and if it is not measurable, it is impossible to assign a probability to it. However, the conditional-uniformity result does allow us to construct a model for a PPP(A) P where F is measurable with measure 1. We do this by concentrating more on the points in P than on the NO values. Divide up lIkm into a disjoint union of sets A1, A2, ... each of volume 1. Choose IID Random Variables N(A1), N(A2), ... each with the Poisson(A) distribution. Within each Ai, scatter randomly N(Ai) points; and there you have your PPP. For any Borel subset B of Rn, N(B) is now defined to be the number of points of P within B. ► I. Reference priors: the controversial case of rare accidents. All of the nuclear-powered electricity-generating stations on Planet Altair-3 were installed by Altair Nuclear Industries (ANI) exactly 1 of their years ago; and no more are to be built. It is known that nuclear accidents in the power stations happen at the points of a PP(A) process with an Altair-3 year as unit of time. The first accident has just occurred. The resulting repair will mean that the PP(A) property will be preserved. A public inquiry wants to answer the question: how many accidents will occur on average in the next Altair-3 year? Witness 1, a Frequentist, says that the number of accidents in the next year will be Poisson(A) and that the best estimate of A from just about any point of view is 1, so he answers '1'. ANI object that it is not fair that the situation is being considered just after a nuclear accident. Witness 1 quotes the conditional- uniformity result and says that sufficiency means that only the total number of events up to now matters, not when they occurred. He backs this up by saying that if T is the Time of the first accident, then fT(1 I A) = and that if N is the number of accidents in the first year, then PN(1 I A) = Ae—A, 9.3. Poisson Processes (PPs) \u0000 437 so that the likelihood as a function of A is exactly the same whether or not you are told that the first accident happened at time 1 or that 1 accident occurred during the first year. Witness 2 is a Bayesian who believes that your prior represents your prior degree of belief about A and cannot depend on the way that data is collected. She takes a prior A-1 for both the Poisson and exponential descriptions, and basically agrees with the Frequentist. ANI are not happy; but then they hear that Bayesians who believe in reference priors will assign a lower mean to the exponential situation. Witness 3 is called. He is a Bayesian who believes strongly in reference priors. He explains to the inquiry team about invariance's being essential for consistent behaviour. He says that he always considers the whole picture: prior knowledge together with how the data are collected. He says that one should use prior A-1 for the approach based on T leading to a mean number of accidents for the next year as 1, but prior A — z for the approach based on N leading to an estimated mean of 11. So though he reflects the difference between the models in the way that ANI want, the values of his estimates seem only to make things worse for them. Witness 4 is You. What do you say in the light of the above? ■ J. Poisson Line Processes (PLPs). \u0000 How should we model the random i Figure J(i): The line x cos 0 + y sin 0 = r scattering of lines on the plane 1R2? Here's the answer. Let P be a PPP(A) on the rectangle (0, co) x [0, 27r). Each point of P is a pair (r, 0), and that determines the line x cos 0 + y sin 0 = r 438 \u0000 9: Some further Probability Figure J(ii): A window on a simulation of a PLP shown in Figure J(ii). These lines make up your PLP. The key thing is that Adrd0 is preserved under translations, rotations and reflections. Check this. If you walk along the x-axis, you will find that the points of intersection of the lines of the PLP with the x-axis form a PP (of what intensity?) on R, but that the angles co E [0,7) which the lines make with the x-axis behave as if having pdf sin (p. This is because (for the case indicated in Figure 437J(i)) r = 2 sin co, 0 = co — 27r, Adrc10 = (sin co)dechp. Bear this in mind when you take your first drive in a straight line over the surface of Planet Poisson and find that more of the mysterious lines on its surface are approximately perpendicular to your direction of motion than close to it. For the fascinating subject of Stochastic Geometry, see Stoyan, Kendall and Mecke [222]. ► ► K. Just when things were about to get really interesting ... I stop my account of classical Probability and move on to Quantum Probability. I've said many times that it disturbs me that Quantum Probability is based on quite different principles and that I feel I ought therefore to include an introduction to it. Even so, Quantum Probability is no more interesting than Classical Probability when the latter explodes into new directions which we have not met 9.3. Poisson Processes (PPs) \u0000 439 in this book. [W] and Rogers and Williams [199] are natural follow-ups to this book, but see Appendix D for other books on stochastic calculus. To get an idea of the wider scope, do look at Diaconis [62], Davis [54], Pemantle [182], for fascinating extensions of the Polya-urn idea, at Hall [106], at Aldous [3] for enough problems to keep you going, etc, etc. If you are keen on seeing standard Group Theory applied to Probability and Statistics, see Diaconis [63]. In regard to more advanced Group Theory, see below. Statistical Mechanics is full of problems which I consider to be at least as interesting as the Riemann Hypothesis (and some of which may well be related to it), and contains some of the most profound Probability ever done. The papers on dimer coverings by Kastelyn [127] and by Temperley and (M.E.) Fisher [224] make a great lead-in. See also Baxter [12] and Thompson [226]. In connection with the Ising model (now popular with statisticians for image processing), look at Onsager's wonderful (Nobel-Prize-winning) paper [180], at Kaufman's also- wonderful simpler proof ([128]), and at Baxter and Enting [13] for a glorious piece of ingenuity. Now that I think of it, the papers by Onsager and by Kaufman use some of the Pauli-matrix theory in the next chapter. See Itzykson and Drouffe [116]. For the remarkable polaron problem, see Varadhan [231] and Deuschel and Stroock [61]. Do at all costs just peep at the paper [145] on Cardy's brilliant work for which Grimmett's book [102] provides excellent background. When I said in the Preface that at very advanced level, Group Theory and Probability are linked in an amazing way, I was thinking about how Virasoro algebras and Conformal Field Theory link work such as Cardy's to the work of Borcherds and a galaxy of other stars on the Monster Group and 'moonshine'. See Borcherds [27], Goddard [98], di Francesco, Mathieu and Senechal [64] for some of the best Maths there is. On some of the amazing story surrounding the fact, quoted by Borcherds, that exp (7-V163) = 262537412640768743.99999999999925 . - - , see Cox [47], a book rightly described by American Mathematical Monthly as `unique and sensational'. So it's not entirely a case of including a chapter on Quantum Probability to help provide desired education for a well-rounded probabilist: Onsager, Kaufman, Cardy and others have shown that Quantum Probability is apparently necessary for some of the most challenging problems in Classical Probability. (And understanding phase transitions is relevant to some MCMC studies ... .) Sure, most of the books and papers just mentioned are miles beyond the level of this book; but that's the great thing. 10 QUANTUM PROBABILITY and QUANTUM COMPUTING The true logic of this world lies in the calculus of probabilities James Clerk Maxwell Absolutely right, James Clerk, as always. But which calculus of probabilities? At present, we have two, totally different, calculi: Classical and Quantum. For discussion of the relation between the two calculi, see Subsection 461M. Please see my thanks to Feynman, Gruska, Isham, Nielsen and Chuang, Penrose, Preskill, ..., in Appendix D. In computer jargon, this chapter is a 'several-pass' account: we begin with vague ideas, and tighten these up in stages. No previous knowledge of Quantum Theory is assumed. Except for a brief discussion of the Uncertainty Principle, we deal exclusively with comparatively easy finite-dimensional situations. We do behave with somewhat greater freedom in our use of Linear Algebra. I ought to stress the following: Quantum Computing is by no means the most wonderful part of Quantum Theory. If Nature had not chosen to exploit in the design of spin-z particles the fact that (as we shall see later) `SU(2) is the double cover of SO(3)', then there would be no Chemistry, no Biology, no you!. To a large extent, we are using Quantum Computing as the simplest context in which to learn some Quantum Probability. My hope is that by the time you finish this chapter, you will be better equipped to read critically the huge amount of 'popular Science' (some very good, a lot not so good) 10.1. Quantum Computing: a first look \u0000 441 on the Uncertainty Principle, entanglement, etc, and that, more importantly, you will be persuaded to learn more from the real experts. Note. At the time of writing this, the distinguished experimenter Humphrey Maris claims that at very low temperatures, it is possible to split an electron into `electrinos'. This, if true, would have serious impact on some aspects of Quantum Theory, but the great success of the subject in so many areas remains of course. 10.1 Quantum Computing: a first look I assume units chosen so that h = 1, where h = h/ (270, h being the so-called Planck's constant. I use c for 2- 1, not for the velocity of light! (In Section 10.5, h and c are restored to their conventional usage in Physics.) A. Introduction. Interference. The quantum world is extremely strange. We know that if we have a small light source, a screen, and a sheet in which there is a small hole between the light and the screen, then the light will illuminate a small region of the screen. If we now make another hole very near the first, then some of the region previously illuminated will now become dark: there is interference. [The Mach-Zender interferometer illustrates the effect much more spectacularly.] We understood this classically in terms of the trough of one wave being superimposed on a crest of another. For brief discussion of Quantum Theory and `realism', see Subsection 491A. But when the light is let through one photon at a time (as can be done in the laboratory), the same phenomenon occurs. The probability that we will find a photon in some region is the square of the modulus of the value of a wave function, and, since the sum of two wave functions can be zero at a point where neither is itself zero, probabilities can 'cancel each other out' in a way totally unexpected from classical Probability. The photon cannot be considered to pass through one hole rather than another: it is somehow aware of the existence of both holes; it somehow investigates the whole system. Utilizing interference. In quantum computers, we can utilize interference to cook things so that even though all answers are possible, the correct answer is much more likely than others. We rely on the fact that in some sense a quantum computer can simultaneously investigate many possibilities, something often referred to as quantum parallelism. Utilizing entanglement. Quantum parallelism also refers to the weird phenomenon of entanglement. Entanglement, within quantum computers and in its manifestation in the Aspect experiment, is discussed in later sections. States. Quantum states are vectors, which in this book we usually restrict to lie in a finite- dimensional space. States evolve in a fully-understood way described by Schrodinger's equation. As a linear equation, this is in some ways simpler than equations in classical mechanics. In a quantum computer, we must operate on states by cooking up the right Schrodinger equation and running it for precisely the right amount of time to make the changes we want. 442 \u0000 10: Quantum Probability and Quantum Computing U-mode quantum mechanics. The evolution of quantum states determined by Schrodinger's equation is what is called unitary, which is linked to the fact that probabilities have to sum (or integrate) to 1. No controversy surrounds the way in which a quantum system behaves when we are not observing it. R-mode quantum mechanics. If we make a measurement of some 'observable' (position, momentum, spin, ...) when the quantum system is in state v, then, according to the 'orthodox' (or 'mathematical' or 'operator') theory, the measurement obtained is randomly chosen in a way depending upon v. All predictions made on the basis of the Bohr interpretation have been confirmed in the most spectacular way by experiment. However, many serious thinkers doubt that this randomness (R-mode quantum mechanics, but the R is for 'reduction') will be the last word on the topic. (See for example Penrose [183], where the 'U' and `R' terminology is used. Penrose makes a convincing case that taking account of gravity will provide the eventual explanation of the R-mode. It may well be the case that when Witten has finished off everything else, he will apply M-theory to this too. See [100].) All that matters for us is that the mathematical formalism gives the right final answers for predicting the results of experiments. So, in a quantum computer, every 'logic gate' will correspond to a unitary evolution; and when we make a measurement at the end, the answer will be random. What can/could quantum computers do? The first demonstration of what a quantum computer could do was that of David Deutsch who showed that it was possible to tell if a function from the two-point set {0,1} to itself satisfies f (0) = f (1) with only one call to the (quantum-encoded) function. A classical computer would need two calls. But then a quantum computer can in some sense do lots of things simultaneously. Grover's algorithm showed that a quantum computer could find the correct name corresponding to a telephone number in a (quantum-encoded) telephone directory with N numbers in about N/Kr(ln N) 3 steps, whereas any conventional program would take on average (N + 1)/2 steps. This algorithm has actually been implemented on a quantum computer for N = 8, and probably by the time you read this, for larger N. Grover, and then others, also described how certain statistics (mean, variance, etc) of a sample could be computed more quickly. This leads on to faster numerical computation of multivariate integrals (for given average error). These things also lead to a reduction `from N to -Nig' in computing time. Schor's famous factorization algorithm showed that a quantum computer could factorize a number into prime factors exponentially faster than a conventional computer can. For an L-bit number, Schor's algorithm needs on average about L2 In L In In L operations, whereas the best existing algorithm for a conventional computer needs about exp(K/J (ln L)) operations, where K is a known constant. (Note. Much security — on the Internet, in financial transactions, in secret-service doings — is based on the apparent impossibility of factorizing numbers which are the product of two large primes. This idea is the basis for the RSA algorithm which made a fortune for Rivest, Shamir and Adleman, but which had been independently invented earlier by British cryptographers, Ellis, Cocks and Williamson. Results such as Shor's put such security under threat. But 10.1. Quantum Computing: a first look \u0000 443 now, seemingly uncrackable quantum codes exist. See Preskill [187], Nielsen and Chuang [174].) Quantum computers could do certain types of simulation (that of spin-glass systems, and presumably of other interacting systems of Ising' type widely used by statisticians in image analysis). They could probably speed up (other) MCMC methods for complicated situations. That quantum computers will out-perform parallel classical ones at some of the most common computations is far from clear. It is 'bad news' that a quantum computer cannot beat a classical one at one of the most important operations in numerical mathematics: iteration of functions. See (quant -ph 9712051). Technologies. Several completely different technologies are being investigated. It seems fair to say that none of these stands much chance of leading to a quantum computer of useful size. Here's where new ideas are most desperately needed. At the moment of writing this, the leading technology in the sense that it can deal with the most qubits, doing some calculations with 7 qubits, is based on NMR techniques. However, it will surely soon be overtaken by quantum-dot or ion-trap technology, and possibly (see Knill, Laflamme and Milburn [138]) by optical-computer technology. NMR computing requires unbelievably sophisticated technology, essentially used for many years by physicists and chemists. Very roughly, the idea is to use spins of nuclei, etc, within molecules to store 0 (spin up) and 1 (spin down); but we must remember that the quantum state will be a superposition of these! The natural frequencies of these spins, and the way in which 'spins can precess around each other', are very accurately known; and this allows manipulation of spins by electromagnetic pulses of very precisely controlled frequency and duration. The precession provides the coupling necessary for things such as the CNOT gates described later. The manipulation of a single spin relies on resonance with its natural frequency. For a little more on NMR technology, see Subsection 470F. The Nielsen-Chuang book gives an excellent account of the state of the art. Doubts. A number of physicists doubt that quantum computers of useful size will ever be made. In many of the technologies, it is immensely difficult to stop the computer's being influenced by the environment. NMR technology has the advantages that there is a high degree of isolation from the environment, and that one has about 1018 separate computers, one for each molecule. However, only a minute excess of those 'computers' are doing what you really want. Even its most enthusiastic advocates doubt that an NMR computer will be able to handle a 'practically useful' number of qubits. Some people seem to question whether NMR computers are truly `quantum', but it seems to me that since they can do Grover's algorithm, they utilize entangled states at a certain stage of the calculation. Localized computing. The geometry of real-world implementations makes it difficult ever to have full quantum parallelism, so what could be achieved with only local parallelism needs thorough investigation. This process has been begun by Seth Lloyd and others. 444 \u0000 10: Quantum Probability and Quantum Computing B. 'A second pass'. We now look at things a little more closely. ► Qubits. A 1-bit classical computer deals just with the numbers 0 and 1. A 1-qubit quantum computer deals with complex superpositions of 0 and 1. The (normalized) state of such a computer is a vector in C2, that is, a vector \u0000 Vo \u0000 ( 1 0 ) \u0000 ( 0 ) = VoZ ± 11 V = V \u0000 = Vo \u0000 , \u0000 1 \u0000 1 , where vo and v1 are complex numbers satisfying I vo12 + Iv' 12 = 1. Thus if vr = xr + yr the point (x0, yo, x1, Y1) lies on the 3-dimensional sphere S3 of radius 1 in 4-dimensional real space R4. (Recall that in Mathematics, 'sphere' equals spherical (hyper-)surface; that a sphere together with its interior constitute a ball.) • • Self-adjoint matrices; observables. For a square matrix A with complex entries, let At be the complex conjugate of the transpose of A. A matrix is said to be Hermitian, or self-adjoint if At = A. A mathematical observable is a self-adjoint matrix. Every real-world observable derives from a self-adjoint operator (perhaps on an infinite-dimensional space); but it may not be possible to make real measurements corresponding to some mathematical observables. ► Measuring our qubit. For our qubit, we have the special observable M represented by the matrix 0 0 M = ( 0 \u0000 I . If we observe M when the system is in normalized state v, then • with probability 1v012, we get the measurement 0, and the state jumps to z, • with probability Hi 12, we get the measurement 1, and the state jumps to u. This is quantum mechanics in R-mode. Thus state z (for 'zero') corresponds to the deterministic measurement 0, and state u (for `unity') corresponds to the deterministic measurement 1. In Dirac's 'bra and ket' notation, z would be written as 10) and u as 11). Thus 10) := z := ( 0 \u0000 11) := u := \u0000 131 I. \u0000 (B1) Please note that I now define 'deterministic' to mean 'with probability 1'. ► Superposition. In a generic state v, the system is in some mysterious superposition of 0 and 1. 'Superposition' is a rather dangerous term. You must never think of state v as signifying that 'with probability Ivo 12, the state is z and with probability IVi 12 the state is u'. The conflict between superposition and probabilistic mixture will be emphasized several times. We have seen that it is true that if a measurement of the special observable M is made when the system is in state v, then after the measurement of that particular observable, the system will be in state z with probability Ivoi2, and in state u with probability Iv]. 12. The states z and u are specially related to M: they are eigenvectors of M. (More on this shortly.) 10.1. Quantum Computing: a first look \u0000 445 ► Schrtidinger's equation for the unitary evolution of the state v(t) at time t is expressed in terms of a special self-adjoint operator 9-t, the energy or Hamiltonian operator 7-1. (I am using 9-t for the Hamiltonian so as to avoid confusion with the Hadamard gate H utilized below.) Schrodinger's equation reads dv(t) dt —i7iv(t) \u0000 (B2) with solution (clarified later, but not surprising to our intuition) v(t) = Utv(0), Ut = \u0000 := I + ( 1 \u0000 (—it71)2 + 2! ► Unitary matrices. Recall that a matrix U is called unitary if UUt = Ut U = I, that is, if U-1 = Ut. We shall see later that Ut = e-itlt is unitary for every t > 0 if and only if is self-adjoint; and that if U is unitary, then II Uv II 2 = 11v112, the correct law for mapping one state to another. ► Logic gates: a NOT gate. A NOT logic gate on a classical computer interchanges 0 and 1. On a quantum computer, it has to be represented by a unitary matrix N, and it must interchange z and u. Thus, N = ( 0 1 1 0 ) [[N.B. We are skipping for a moment the p\\\\\\roblemsassociated with 'phase'.]] To implement a NOT gate, we therefore need to find a self-adjoint operator 9-t such that N = e —'n for some t, make sure that 71 serves as the Hamiltonian operator for our physical implementation, run the system for time t so that N is the desired unitary evolution, and we have our NOT gate. We could take N = Ut where ( 1 —1 t = Or,7l = —1 \u0000 1 \u0000 ' You should check that (d/dt)Ut = —i7-tUt. ► Phase. If instead we take Ut = e—it cos t i sin t i sin t cos t = \u0000 . ) then Ut ( cos t — sin t := \u0000 = sin t \u0000 cos t and, if we choose t = 17, then Utz = u and Utu = —z. However, —z yields a deterministic M-measurement 0 just as z does, because of the 1.12 in the rules determining probabilities. But, of course, le'912 = 1 for any real 0. Thus, the normalized states v and eil9v lead to the same probabilities: they are usually considered as corresponding to the same normalized state. However, we need to watch phases carefully. Still, we could use ) 446 \u0000 10: Quantum Probability and Quantum Computing our new Ut, where t = 17, to implement a NOT gate. But this has all been Mathematics. To implement such things in the real world, 1-1 has to be restricted to be an observable which describes the energy of the implementation system. Solving Deutsch's problem. Here's a plan for a 1-qubit computer to solve Deutsch's problem. Encode the function f via the unitary map Uf where (Ufv)k := (-1)f (k)vk, \u0000 k = 0, 1. Clearly Uf corresponds to the diagonal matrix with kth diagonal entry (-1)1(k) = ±1. Let p be the unitary transformation 1 ( 1 —1 ) \u0000 0 —i \u0000 = e -'t ic \u0000 0 where 71 := \u0000 , \u0000 . P := —11 . 1 \u0000 1 \u0000 ) • Then, you can check that if you 'input' z, then apply p, then U1, then p-1, and finally use M to measure, you obtain deterministic results measured value of M = { 0 if f (0) = f(1), 1 if f (0) f (1) . The encoding Uf of the function f is called an oracle, a 'black box' which you can query or consult to find desired information about f . Think of f as the ordered pair [f (0), f (1)]. Then in 1 call to the f-oracle, the quantum computer can distinguish between the 2 sets {[0, 0], [1, 1] } and 1[0, 1], [1, 0]}. A classical computer needs 2 calls, but can then distinguish between the 4 possible functions. The point is that a good quantum algorithm can sometimes get some desired result faster than a classical algorithm. Compare 'good versus bad' for classical algorithms. ► C. Grover's algorithm. \u0000 This algorithm originates in Lov Grover [104]. It is usually presented as finding the name corresponding to a given number in a telephone directory of K numbers. To do this in any classical way would, on average, involve 1(K + 1) operations. [Actually, if we are so unlucky as to miss the correct number on the first K — 1 consultations of the directory, we know that the Kth number must be the one we are looking for. So the mean number of consultations can reduced by 1/K.] For a quantum computer, the phone book is encoded as an oracle, and you need only consult this oracle a constant multiple of N/7K times (on average). We now turn to the mathematical statement of the problem, and give the geometric interpretation which was discovered by several people after they read Grover's more algebraic account. Suppose that a function f on {1, 2, ... , K} is given such that there is precisely one value ko of k such that f (ko) = 1 and that f (k) = 0 for k k o. We want to start with some well-chosen quantum state and perform a sequence of steps which will take that vector nearer and nearer to the target vector t := iko) := (0, 0„ \u0000 , 0, 1, 0, \u0000 , 0), 10.1. Quantum Computing: a first look \u0000 447 the 1 being in the koth place. Let e := K 2 (1,1,...,1), with equal components. Then we know that, whatever the value of ko, the inner product (t, e) = and this knowledge makes e a good choice of initial vector. We work entirely in the plane [t, e] spanned by the real vectors t and e. Write R[e] for reflection in the line through 0 and e; and define R[t] analogously. Then (accept that for this discussion that) R[ti is the (not necessarily 'well-encoding') oracle which encodes the phone book. Define the Grover transformation G := —R[e]R[t]. \u0000 (Cl) It is geometrically obvious that this extends to a unitary map on CK . We return to this point later when we consider implementation. RiivR[t] e 0 • \u0000 4R[tie Gv (2co + a — 7r) \u0000 The case when N = 4 Figure C(i): The effect of Grover's G on states in [t, e] The left-hand picture at Figure C(i) (in which a is negative!) shows (within the plane spanned by t and e) the effect of the operation G on a vector v. All vectors from 0 will be of unit length. Consider the angles they make with t, calling co the angle from t to e with co in (0, Pr). We already know that cos co = K. We see that if v is at an angle a, then R[t}v is at an angle —a, then R[e]R[tiv is at an angle (2(p + a), and finally, Gv is at an angle (2(p + a — 7r). Hence, the effect of G is to rotate v through an angle 2co — 7i. Set 9 := 7r/2 — co, so that sin 0 = K. Then Gre will be at an angle cc — 2r0 = 2 — (2r + 1)0. We therefore chooser to make (2r + 1)0 as close as possible to 17r. Since sin 0 = KA, then for large K, so that r 47r.V.K. If K is large and we start with quantum state e and evolve with Gr for this r, then we have high probability sin2(2r + 1)0 that a measurement of the state with M = diag(1, 2, ... , K) 448 \u0000 10: Quantum Probability and Quantum Computing will yield the correct value /co. If it doesn't, try again. Boyer, Brassard, Hoyer and Tapp (quant-ph 9605034) show how Grover's algorithm may be speeded up somewhat on average by using a smaller r (and trying again if the scheme fails). The right-hand picture at Figure 447C(i) shows (at smaller scale) that if K = 4, then one obtains (deterministically) the right answer after just one Grover iteration! Ca. Exercise. Show that for Grover's algorithm with K = 4 and with t = (0, 0, 1, 0)T, we have R[e]R[t]e = (0, 0, —1, 0)T, e = (1,1, \u0000 R[t]e = (- 1, —1i, 1, \u0000 , confirming that Ge = t. D. Fascinating thought. Is it possible to exploit the striking property of K = 4 just described? Grover's algorithm has been proved 'essentially best possible' amongst certain natural classes of algorithms for finding the correct one out of K items: on average, an algorithm in any of these classes will take at least some multiple of , calls to the oracle. However, intuition raises the possibility that in the nice phrase of G. Chen and Diao (quant-ph 0011109), a 'divide and conquer algorithm based on the magic number 4' (and outside the above-mentioned 'natural classes') might allow one to find deterministically the correct one amongst 471 objects with just n calls to a more sophisticated oracle: to be more precise, one might be able to find the correct one amongst 4n items in 0 (nr) steps for some fixed r, a call to an oracle being counted as one step. This would of course mark an 'exponential' improvement on Grover's algorithm. Chen and Diao present an interesting idea for such an improvement. However, as they themselves point out, a detailed analysis of the number of quantum gates in the full quantum circuit for their algorithm needs to be made; and at the time of my writing this, Chen tells me that this analysis will take some further months. (Of course, the degree of complexity, memory requirements, etc, for implementation of an oracle, would be an important factor in practice, particularly for sophisticated oracles.) I look forward with interest to the Chen- Diao results which will surely be posted on the quant-ph site. 10.2 Foundations of Quantum Probability How would one start to think about implementing Grover's algorithm in the real world? Before we can answer that, we need a 'third pass' through the subject, this time finalizing several things, including the Fundamental Postulate of Quantum Probability 451C. A. Some basic algebra. Much of this is material covered in Chapter 8 with slight modification because we now work with Cn rather than W' and our 'field of scalars' is C, not IR. Thus we regard Cm as a vector space of dimension n over C, never as a vector space of dimension 2n over R. 10.2. Foundations of Quantum Probability \u0000 449 With superscript T for transpose as before, for a (column) vector (vi , v2, \u0000 , vn )7' in Cm, we define vt := \u0000 v2, \u0000 , vn), with 'bar' denoting complex conjugate. We define (for y, w E Cn), := vtw = Evkwk, \u0000 IIvII2:= (vt,v) = E Ivk I2. Note that (w, y) = (v, w). Two vectors v, w are called orthogonal if (v, w) = 0. I now revert to 'orthogonal' rather than perpendicular, because 'perpendicularity' is not that clear a concept in complex spaces. The 'Extension to Basis' Principle in its `general' and orthogonal forms carries over. The definition of basis remains the same. If {w(1), w(2), , WM} is an orthonormal basis for a subspace W of C'2 , then orthogonal projection Pw onto W is obtained via Pw v = E (w(k)7v)w(k), so that Pw has the matrix representation Pw E w (k) (w (k))t A matrix P corresponds to an orthogonal projection onto a subspace if and only if (compare Lemma 324Jb) pt _ p p2 . We know that a complex n x n matrix A is called self-adjoint if A = At, equivalently (as you can check) if (Av, w) = (v, Aw) for all y, w. Especially, if A is self-adjoint, then, for any v E Cn, vtAv = (v, Ay) = (Av, v) = (v,Av), so that vtAv is real. We shall see that, when v is a state, that is, when livil = 1, vtAv represents the mean or expectation E v (A) of A when the system is in state v. The following result is extremely important. Aa. Fact: Spectral Theorem for self-adjoint matrices. If A is a self-adjoint matrix, then there exist orthogonal subspaces V17 1/ \u0000 • 7V,- of Cn with sum Cn such that A = \u0000 + a2P2 -I- • • + ar Pr) \u0000 (Al) where Pk denotes orthogonal projection onto Vk and a l , a2, \u0000 , a,7 are distinct real numbers. Then, g(A) = g(ai)Pi + g(a2)P2 + + g(ar)Pr \u0000 (A2) for polynomial or exponential functions g. Each Pk is a polynomial in A with real coefficients, and therefore commutes with any operator which commutes with A. 450 \u0000 10: Quantum Probability and Quantum Computing Here the a k are the distinct eigenvalues of A, the distinct roots of det(aI — A) = 0, and Vk is the space of eigenvectors of A corresponding to ak: Vk = \u0000 E Cn : Av = kv} . Note that if v 0 and Av = av, then dilvil 2 = (av,v) = (Av,v) = (v, Av) = (v, av) = alIv II2, so that d = a and a is real. Of course, the tricky thing with Fact 449Aa is to prove that A has enough eigenvectors to span Cn. Because PZ = Pk and PkP,T, = 0 hen k m, it follows by induction from 449(A1) that An = a7/31 a 721P2 ± • • + a7 PT, so that 449(A2) follows for polynomials g. For any n x n matrix B, we define eB := exp(B) := / B + B2 + 2! the series converging componentwise. Fact: we have for any n x n matrix 71, dt e —iat \u0000 m e-WM = e—itn( interpreting the derivative componentwise. Compare Schrodinger's equation. Fact: we have eA+B = eAeB when AB = BA. B. Notes on unitary matrices. We already know that a matrix U is called unitary if UUt = UtU = I. If U is unitary then (you prove this) IlUv112 = I v112 for every vector v. We need evolutions to be unitary to preserve probabilities. Ba. Exercise. Show that if IlUvI12 = 11112 for every vector v in Cr', then U is unitary. Hint. Show that if A := UUt — I, then for all vectors v and w in C2, R(vtAw) = 0. Why does the desired result now follow immediately? Bb. Exercise. Show that for a square matrix A, (eA)t = exp(At). Deduce that if 1-1 is self-adjoint, then e-itn is unitary for all t > 0. Prove the converse result that if e-itn is unitary for all t > 0, then 1-1 is self-adjoint. 10.2. Foundations of Quantum Probability \u0000 451 C. The Fundamental Postulate of (finite-dial) Quantum Probability. Suppose that CI is the vector space in which the (normalized) quantum states are vectors of unit length, and that A is an associated observable with spectral decomposition as at 449(A 1). Then, if a measurement of A is made when the system is in the state v (*here jvjj 1), then Nature • chooses a number in {1,2, , , r}, choosing k with probability pk(v) = wkvir, • reveals ak as the measured value of A, and • makes the state jump to PkvAlPkv. We write Pv for the probability law associated with state v, and so will write Pv (A is measured as ak) = pk(v) = liPkv112- As I have already mentioned, several leading physicists suspect that the postulate may not be the last word. What matters to us is that all calculations made on the basis of the (general form of the) postulate have been verified experimentally. \u0000 ( 0 1 \u0000 i Let's take n = 2. We have already met the matrix \u0000 0 \u0000 in its unitary guise, as 1 a NOT gate. But the matrix is also self-adjoint, and as such it is an important observable, az, one of the Pauli spin matrices. Let us again write M for diag(0, 1) and c = \u0000 . If our system is in state v = c(z + u), then a measurement of M will produce the result 0 or 1 with probability 2 each, and the state will jump to z or u accordingly. Let A = o-x for the moment. If A is measured either in state z or in state u, we obtain +1 or —1, each with probability 1. But if A is measured in state v which is a `superposition' of z and u, then, since Av = v, we obtain the deterministic result 1. You can see why we need to be careful about superposition. Ca. Eigenvectors of the Hamiltonian. If 71 is the observable which is the Hamiltonian for Schrodinger's equation and if v0, the state of the system at time 0, is an eigenstate (eigenvector) of 7-1 corresponding to (real) eigenvalue A, then the state at time t will be vt = e —iAtvo, still an eigenvector of 7-1 with eigenvalue A. As mentioned before, 7-1 describes the energy of the quantum system, and the fact just described is a 'conservation of energy' law. (We have to be very careful though that, for example, we do not become trapped into thinking that an electron will stay for ever in an excited state: we have to remember that simple idealized models will be subject to all sorts of perturbations and to `second-order' forces. Even the vacuum in Quantum Theory is not a classical vacuum.) 452 \u0000 10: Quantum Probability and Quantum Computing ► D. Density matrices and expectations. For the mean of observable A in state v, we have E v (A) = EakliPkvil 2 = > ak(Pkv)t Piv = vtAv = trace(vtAv) = trace(vvtA) = trace(pvA), \u0000 (D1) where pv is the density matrix pv = vvt associated with v. Recall that the trace of a square matrix is the sum of its diagonal elements, that vtAv is a number (in fact a real one) and is therefore equal to its trace, and that if F is an r x s matrix and G is an s x r matrix, then trace(FG) = trace(GF). ► E. Superposition versus probabilistic mixture. Let v and w be orthogonal states, and let x be the superposition c(v + w) where c = \u0000 . The density matrix (pv + pw) corresponds to a probabilistic mixture of states: 'With probability 2 prepare state v; otherwise, prepare state w'. However, p. = 2 (v + w)(v + w)t = 2 (pv + Pw) + 2(vwt + wvt), showing a clear distinction between superposition and probabilistic mixtures. Probabilistic mixtures do play an important part in the theory, especially in the study of subsystems of a full quantum system. We shall see the density matrix l(pv +pw) arise in this way in Subsection 467C. ► ► F. Commuting observables. Suppose that A and B are commuting observables such that AB = BA. Then (a fact which you can probably see how to prove) there exists an observable C and polynomials g and h with real coefficients such that A = g(C), B = h(C). We can measure A and B simultaneously, because the experiment of measuring them simultaneously is equivalent to that of measuring C to give a result 7 (in which case A yields measurement g(7) and B measurement h(-y)). A much better way to think about this is that i f AB = BA, then the 'probability that A is measured as a and B is measured as [3' is the same if A is measured before B as if B is measured before A: commuting observables A and B have a proper joint distribution. Suppose that A and B are any two operators, with spectral decompositions A = cciPi + a2P2 + • • + cerPr7 B = N1Q1 + 132Q2 + • • • + (3sQs• Then, if A is measured before B, PV (A is measured as a k then B is measured as f3t) II k \u0000 2 11Q/Pkvil2 = IIPkvil 2 10.2. Foundations of Quantum Probability \u0000 453 As explained in the statement of the Spectral Theorem 449Aa, if A and B commute, then Q, commutes with Pk, and so the above probability does equal Pv (B is measured as i3e then A is measured as ak) when B is measured before A. Fa. Exercise on non-commuting observables. Let A = ( 10 —1 ' 0 ) \u0000 1 B \u0000 1 0 , v= 0 Show that if the state is v and A is measured before B, then Pv (A is measured as 1 then B is measured as 1) = whereas if B is measured before A, then Pv (B is measured as 1 then A is measured as 1) = G. The Heisenberg Uncertainty Principle as Mathematics. Suppose that A and B are observables. Let µv(A) := E v (A) = (v, Av) be the mean of A when the state is v, let Av := A — pv(A)/ (analogously to X = X — fix ), and define Varv(A) := E v (A2) = (v,A 2v) = (Av, Av) = IlAv112. Of course, Vary (A) = E v (A2) — [fiv (A)]2. We define SDv(A) := [Vary (A)]= Now, AE is generally not self-adjoint (it is if A and B commute). However, we can write AE = s+ + is_, where + \u0000 AB -BA AB — BA S+ = \u0000 2 \u0000 , S_ = \u0000 2i \u0000 2i and S± and S_ are self-adjoint. Thus, since (v, S+v) is real, 1(v,ABv)12 = 1(v,S+v)12 +I(v, S_v)I2. Combining this with the complex Cauchy—Schwarz inequality, we obtain 1(v,S_v)1 < 1(v,Abv)I = 1(Av,Bv)1 IlAvIl 11Bv11, 454 \u0000 10: Quantum Probability and Quantum Computing and we have the Heisenberg Uncertainty Principle: SD, (A)SD,(B) ? Rv, S_ \u0000 AB — BA = \u0000 2i \u0000 (GI) Note that for the spinor representation of spins, SDv (o-x)SDv (ay) > I (v, azv)I. • • H. The Heisenberg Uncertainty Principle as Physics. We have to be very careful about interpreting the Uncertainty Principle as Physics. I stick to the orthodox `operator' picture. The Uncertainty Principle gives a lower bound for the product of SDv (A) and SDv (B) both associated with the same state v. The quantity SDv (A)SDv (B) is therefore relevant only to situations in which state v is prepared on a large set of occasions, where A is measured on a large subset of those occasions and B is measured on a large disjoint subset of those occasions. (One of the ways in which we could know that the system is in state v is if [v] is the 1-dimensional eigenspace of some operator C corresponding to some eigenvalue A and if C has been measured with result A.) The quantity SDv (A) SDv (B) has no relevance to simultaneous measurement. If, for example, we measure B before A, then the measurement of B generally changes the state to a new state w, and then the value SDv (A) has no relevance. Ha. Example. Take the example of Exercise 453Fa. Here, SDv (A) is obviously 0 because A is deterministically 1 in state v. However, if B is measured before A, then that measurement changes v to a new state w, and (you check) whatever the result of the measurement on B, A will then be measured as +1 or —1 with probability 2 each, so that SDw (A) = 1. Note. In this example, E v (A) = 1, but for each of the two possible values b of the measurement of B, we have \"E (A B = b) = 0\". So there is no 'tower property of conditional expectations'. We wouldn't expect there to be because the measurement of B has changed the state. Of course, I have misused conditional expectations, which is why there are 'double quotes'. • • I. Classical Heisenberg Uncertainty Principle for position and momentum. I give a fairly detailed discussion because this is one of the most widely discussed topics in Science. It is part of infinite-dimensional Quantum Theory, and we can only treat it heuristically here because (amongst other things) self-adjointness now becomes a rather subtle concept needing precise specifications of domains of operators. From time to time, however, I make some remarks on the way to make things rigorous. (I want to persuade you to read Reed and Simon [191].) The state of a particle moving on the real line is a map v : R \u0000 C, x \u0000 v(x), 10.2. Foundations of Quantum Probability \u0000 455 such that v E £20) in that 11v112 := f Iv(x)12dx =1. There is no difference between v and v; it's just that I often use v to relate to the finite- dimensional situation we have studied. We define the inner product of two states v and w in G2 (I[8) as (v, w) := f v(x) w(x) dx. The position of our particle is represented by the observable A, where (Av)(x) := xv(x), whence (g(A)v)(x) = g(x)v(x). In particular, for a subinterval r of IR, IPc (position in F) = \u0000 / r (A) = (v, /F(A)v) = f v(x)/r(x)v(x) dx = f Iv(x)12dx, so that 1v(x)12 is the pdf for position when the state is v. Momentum is described by the operator (Bv)(x) := — iv' (x), B = \u0000 =- — iax• Note that B is (formally) self-adjoint because integration by parts 'with things at infinity vanishing' shows that (v,Bw) = f \u0000 = f iv w dx = f \u0000 dx = (Bv, w). Now we have (AB — BA)v = x{ — iv' (x)} + iox{xv(x)} = iv(x), so that we have, formally, the famous Canonical Commutation Relation (CCR) AB -- BA = and S_ = 41.. Thus in this case we have the classical Uncertainty Principle (in units of h) for position A and momentum B: SD, (A) S11,, (B) > \u0000 for atl states (I1) Some steps towards a rigorous formulation of the CCR may be found at 458Ie below. 456 \u0000 10: Quantum Probability and Quantum Computing Ia. Exercise. Show that if we have the 'Gaussian' state ( x2 v(x) = (27r0-2) —' exp 4a2 then the distribution of the position of the particle in state v is N(0, 0-2) and we have ]E„ 1 v (A) = 0, Vax,(A) 0-2, E v (B) = 0, Vary (B) = \u0000 4o-2 so that in this case, we have equality at (I1). Ib. Having fun. Let's have some (heuristic) fun by continuing with the context of the Exercise. We have {exp(i0B)v}(x) = {exp(/38x)v}(x) 02 = v(x) ± 011(X) yVn (X) + - = v(x + 13) (by Taylor's Theorem) = (271-a 20 exp { (x2 + \"x '82) } 40-2 Hence we can evaluate the Characteristic Function of momentum B in state v as 02 \u0000 1 \u0000 ( 0x \u0000 x2 ) exp \u0000 dx E v exp(i/3B) = exp (— 40_2 ) f (27ro-2).1 exp \u0000 2a2 \u0000 2o-2 and the integral is the value evaluated at 0/(20-2) of the MGF of N(0, 0-2), namely, exp ( 1 2 02 \\ 2 a 40-4 So, the Characteristic Function of B in state v is ( 1 2 1 \u0000 E v exp(i/3B) = exp \u0000 2 0 40-2 ' and hence, in state v, momentum B has the N(0, 1/(4o-2)) distribution. You may be uneasy because exp(i(3B) is not self-adjoint (it is unitary, not that this is relevant now) and so I should not be using E, exp(i/3B). However, I am sure that you will allow me to extend E v by linearity and continuity. I could alternatively work with moments of B, with the same results. Of course, it is meaningless in this quantum setting to talk of a joint pdf of (A, B) in state v. We know that the observable (AB — BA)/(2i) takes the value 2 , and this fact is rather difficult to square with the idea of a joint distribution of (A, B)!! 10.2. Foundations of Quantum Probability \u0000 457 p. Ic. Fourier duality. Let's extend the idea just explained to calculate the distribution of momentum for an arbitrary state v in .C2. For a nice function v : R C, define the Fourier transform vA of v via f VA \u0000 '— \u0000 r \u0000 'bet v(a) da, \\/ all integrals around here being over IR. Then we have the Fourier inversion formula 1 ir • E v A e mo (b) db. V27r Compare 167(A2). Those with a keen eye for rigour are advised that we can extend the Fourier transform and its inverse to continuous maps from L2 (R) to L2(R), where L2(R) is £2(R) with functions equal except on sets of Lebesgue measure zero being identified. Indeed, Parseval's formula holds for v, w e L2 (R): (v, w) = (v^, w^ ), so that Fourier transform is a unitary map. Now, formally, 7r \u0000 (Bv)(a) = —iv'(a) = \u0000 f eiabby^ (o' ) db. 2 so that we must have (Bv)^ (b) = \u0000 (b). Next, with one forgivable misuse of notation, E v (e;eft) NT\", (eiesvr) (v, eiOBv) = ( _ ( vA (eiebv)n) = f eiBb iv A (012db, so that IED~ (B E db) = Iv A(b)12db, a formula which makes sense for all v E L2(R). Note that i(vA)'(b) = \u0000 1 7r f \u0000 2 e-ibaav(a) da. We therefore have the nice duality between the 'position' and 'momentum' representations of the CCR: (Av)(a) \u0000 av(a), \u0000 (Bv)(a) = —iv'(a), (Av)A(b) \u0000 i(v A)' (b), (Bv)^ (b) = bv^ (b), v(a) so that Fourier transforms interchange the roles of position and momentum modulo the sign change necessary to preserve the CCR. 458 \u0000 10: Quantum Probability and Quantum Computing ► Id. Discussion: more quantum weirdness. You may wonder about such questions as the following. Suppose that I set up an experiment which returns 1 if the particle is observed as being in the interval (a, 0), 0 if not. The corresponding observable is C, where (Cv)(x) = /(,0 )(x)v(x). If the system is in state v, and C is measured as 1, then the new state w will satisfy w(x) = -y 11(,0 )(x)v(x), where -y := {f: Iv(Y)12c1Y} • What can we say about the particle's momentum when it is in state w? You can immediately see a strange feature. The value Bw(x) = —iw' (x) will generally not exist when x = a or 13, except as a multiple of a delta function. We therefore expect the value E w (B2) to be infinite. Indeed, if one knows anything about delta functions, one expects the pdf of B in state w to tail off essentially as 1 /b2, so that B will have neither variance nor proper mean. Let us work out the easy case when a = —8 and ,3 = 6, where 0 < b < 2 and where v is a nice infinitely differentiable function on R with f 1v(x)12 = 1 and such that v E- 1 on (-8, 8). Then the pdf of B in state w is (you check!) lwA \u0000 = sin2 \u0000 (613) 7r8b2 tallying exactly with our guess. From the point of view of classical Probability, the `pseudo-periodic' behaviour of the pdf of B is bizarre, but if you take Fourier transforms when there are delta functions lurking in the background, such things are bound to happen. Ie. Interesting mathematical digression. The CCR is not that easy to formulate rigorously because A and B are defined only on subspaces of L2 (R). Hermann Weyl's solution was to rewrite the CCR as \u0000 TtU., = eist UsTt, where U, = \u0000 Tt = e—itB \u0000 (I2) We have for the position representation, for every v E L2 (R), \u0000 (Usv)(x) = e —isxv(x), (Ttv)(x) = v(x + t), \u0000 (I3) the second being formally the Taylor-series expansion we have seen before. However, the correct theory (of 'strongly continuous one-parameter semigroups', or, in this context, of `strongly continuous one-parameter unitary groups') means that the Tt equation at (I3) is correct for all v in r2(R), even for nowhere-differentiable v and for those infinitely differentiable v for which the Taylor series sometimes converges to the wrong value. When A and B are (truly) self-adjoint operators, we can define Tt and U, as at (I2). If the commutation relation at (I2) holds, then the CCR follows formally on taking a2lasat and then setting s = t = 0. Conversely, if the CCR holds, then the commutation 10.2. Foundations of Quantum Probability \u0000 459 relation at (I2) follows formally from the Raker—Campbell—Hausdorff formula'. Let's take a heuristic look. Suppose that operators F and G satisfy FC = CF, GC = CG, where C FG — GF. You show by induction that FThG — Gn F = nFn-1C, whence F \u0000 = se sFcr esF _ es \u0000 — \u0000 (G + sC)es F . Hence, esF Gn = (G + sC)nesF , and e e tGstCsF \u0000 tG sF stC esFetG = e \u0000 = e e e . Formally, 458(12) is the case when F = —iA, G = — iB, C = J. The Uncertainty Principle and Fisher information. You may have thought while reading the above treatment of the classical Uncertainty Principle, \"Haven't we seen something like this before when we studied the Cramer—Rao MVB inequality?\". Return to the classical Heisenberg principle and suppose that v is such that E v (A) = 0 = E v (B). Let h(y) be the pdf Iv (y)I 2 = v (y)v (y) for position. Consider now the location-parameter situation of Subsection 198H. We have, with the notation there, /(Y) = f h(y) 2 dy. But IW(Y)1 = 2R{v(Y)v'(Y)} < 21v(Y)117/(Y)I, so that /(Y) < 4 J 11/(y)12dy = 411BvI12 = 4Vary (B). And now, since the Cramer—Rao inequality says that /(Y) > 1/Vary (A), the classical Heisenberg principle follows. I do not read too much into this, since both the Cramer—Rao Theorem and the Uncertainty Principle are almost-immediate consequences of the Cauchy—Schwarz inequality. Frieden's claim ([86]) to derive much Physics from Fisher information is, unsurprisingly, controversial. His interpretation of the Uncertainty Principle appears to be different from ours. (On this, as with much else in his interesting book, I would have welcomed much fuller discussion.) 460 \u0000 10: Quantum Probability and Quantum Computing K. Tensor products — again. I reiterate points made in Chapter 8, but with a slightly different (but strictly equivalent) way of thinking about tensor products. Recall that the (m n) x (r anti C11 / all s) matrix ai r \u0000 bii am, \u0000 bmi Clr \u0000 d11 \\ brns d15 ( A is written C B D J the (A \u0000 (E \u0000 (AE + BG AF + BH CD GH \u0000 CE+DG CF+DH) . The tensor product v 0 w of a vector v in Cn with w in C8 is the vector in Cr\" ViW v w vnw If we are given the tensor product v 0 w of two vectors of length 1, then (you check) the pair (v, w) is defined uniquely only modulo phase transformations v e i\"v, w H e -jaw. \u0000 (K1) Note that (v w , s t) = (v, s) (w, t), Hv Owl' =HvH 11w11- The tensor product A 0 B of an n x n matrix A and an s x s matrix B is the ns x ns matrix A 0 B = aliB \u0000 ainB ani.B \u0000 a nn B and then, for v E Cn and w E Cs, (A 0 B)(v 0 w) = aniB \u0000 annB a11B \u0000 a inB \u0000 vi w ) VnW = (Av) 0 (Bw). You proved in Chapter 8 that not every vector in Cn 0 C5 = Cis is of the form v 0 w (most are 'entangled'). You noticed that if 7/ > 1 and s > 1 and t Ths)T = vo w E ci es , t = (t ii , \u0000 , tis, \u0000 , then a number of algebraic relations follow: for example, tjjt kk = tjktkj, since ti k = v3wk, etc. However, a linear map on Cn 0 CS is determined by its action on v 0 w vectors. We have \\ Cni \u0000 Cm, 41 \u0000 dns/ where A is m x r, etc. Partitioned matrices 'of compatible dimensions' multiply in usual way: (A 0 B)t = At 0 Bt. 10.2. Foundations of Quantum Probability \u0000 461 p. L. Pure-product states; 'Multiply means Independence'. Suppose that A is as in Fundamental Postulate 451C and that v e Cri is a state of the quantum system (`first system') associated with A. Suppose that B is a mathematical observable on Cs, with spectral decomposition B = Ql Q1 + 02Q2 + • • • + OtQt, and that w E Cs is a state of the quantum system (`second system') associated with B. The pure-product state v w represents 'first system in state v and second in state w', but there is an important 'Multiply means Independence' element in all this. If we study the joint system, then 'the observable A for the first system' becomes A 0 I, and its spectral decomposition is A0/=-Ect i(Pi 0/). If a measurement of A 0 I is performed when the system is in pure-product state v 0 w, then the result is a, with probability II Pivr, and the new state of the total system is (11Piv11)-1 (Piv) \u0000 w. This is of the product form v 0 w, so the second system is still in state w. You can check that in the pure-product state v w, A and B behave as independent Random Variables in that A ® I and I B commute and Pvow (A ® I is measured as cri; then I B is measured as 0j) = 11Piv11211Q7w112 = Pvow (A ® I is measured as cti)Isvow (/ 0 B is measured as Ai) = lP (A is measured as ai )IIDw (B is measured as i3j) I think that the independence assumption which is built into the use of pure-product states should often be more emphasized in the literature. Sometimes, I get niggling doubts because of it. For entangled states which are not pure products, things become much more interesting. ►► M. Classical Probability and Quantum Probability. Michael Stein has, rightly, pressed me to discuss the question: Are Classical Probability and Quantum Probability in conflict? Here is an attempt at a brief answer. Classical Probability is a special case, the so-called 'commutative case', of Quantum Probability: it can calculate probabilities associated with a model if and only if that model is a 'commutative' one in the set of all Quantum Probability models. Where both theories can calculate, their answers are therefore in agreement, but Classical Probability cannot be used to calculate probabilities for 'non-commutative' situations. Of course, when probabilities associated with a 'non-commutative' model have been calculated by Quantum Probability, the long-term relative-frequency interpretation of 462 \u0000 10: Quantum Probability and Quantum Computing those probabilities relies on the classical Strong Law. The calculation of probabilities connected with the Aspect experiment could not be further from Classical Probability; but when the significance of the results of many experiments is assessed, it is via the classical use of standard deviations, Quantum Probability having told Classical Statistics what hypotheses to test. Classical Probability as commutative Quantum Probability. Let me explain for the simple experiment of tossing a fair coin twice how the Classical Probability model may be viewed as a commutative quantum model. Let Ek denote 'Heads on the kth toss', and let Xk denote the indicator function of Ek. We have the familiar picture w HH HT TH TT (w) 1 1 0 0 X2(w) 1 0 1 0 For the quantum setting, the space V of states is the 4-dimensional space (over C) of functions mapping 12 to C (equivalently of complex 4-vectors parametrized by SI). Let Xk be the operator (diagonal matrix) consisting of multiplication by Xk, so that (Xkv)(w) = X k(W)V(W). Let p be the (diagonal) quantum density matrix corresponding to multiplication by P. Then E p(Xk) = trace(pXk) = Eclass (XO• Note that all our operators are diagonal and therefore commute with one another. In the quantum setting, /Ek , multiplication by IE k , is orthogonal projection onto the subspace Ek := {z E V : z 0 off Ek }. This subspace stands for the event in Quantum Probability. Note that Ei corresponds to Ec. What about the intersection E n G of two events E and G? For the classical picture, /Enc = IEIG. For our quantum picture, which in this case has to agree with the classical, leng = lEig • When Quantum Probability differs. Regard an 'event' as a subspace U of the space of states, and its 'indicator' as the orthogonal projection Pu onto U. Then, except in special circumstances, Pu Pv, Pv Pu and Puny are all different. (Take U to be the x-axis, V the line at 45°, in R2, and note the effect of the three operators on (1, 2). Draw the picture.) The logic of subspaces (quantum events) is completely different from the logic of subsets (classical events). In particular, the correct analogue of 'disjoint events' is `orthogonal subspaces'. Thus the classical result IELJE = IE \u0000 < \u0000 EnF —0 < \u0000 > TEIF = 0, corresponds to the quantum result (see Exercise 300Gb) Pu+ v= Pu + Pv < \u0000 > U I V<> Pu Pv =- O. Quantum logic does, of course, have some similarities with the theory of Linear Models. 10.3. Quantum computing: a closer look \u0000 463 Ma. Exercise. Let U and V be subsets of Cn (or of Rn ). Show that if any two of Pu Pv Pv Pu and Puny are equal, then they are equal to the third; and that a necessary and sufficient condition for equality of the three is that (U fl Y-L) 1 (v n Y-L) where Y := U n V. [[ In Classical Probability, for events U, V, it is automatic that (U n \u0000 n (v n Yc) = 0 where Y Un V. ]] 10.3 Quantum computing: a closer look This is our final pass through quantum computing. A. Quantum gates and circuits. A quantum gate is just a unitary map U taking an input state in the space spanned by tensor products of qubits into an output state in the same space. Some simple cases, now to be explained, are shown in Figure A(i). For each picture, the input state (often a tensor product of qubits, but sometimes an entangled state) is on the left, the output state on the right. A quantum circuit is just a series of gates. ► Aa. The 1-qubit NOT gate. Figure A(i)(a) shows the NOT gate which we have already studied. It is usually represented as shown in the left-hand or top-middle picture. The top-middle figure indicates that an input z [respectively, It] will produce an output u [respectively, z]. ► Ab. The Hadamard gate. An important 1-qubit gate is the Hadamard gate (Figure A(i)(b)) associated with the unitary matrix H= c (1 1 —1 1 ) where c = Of course, this has no classical analogue. ► Ac. The CNOT gate. The most important 2-qubit gate is the CNOT (controlled-NOT) gate shown in Figure A(i)(c). In a classical computer, this changes (`NOTs') the second bit (the lower one in the picture) if the first (control) bit is 1, and leaves the second bit unchanged if the first bit is 0. The first bit is left unchanged. Each of the two diagrams in Figure A(i)(c) is really four diagrams, one for each (i, j)-component. The right one signifies that the unitary map must take z (8) z z z, z u z u, u z \u0000 u u, u u u z, the first of these corresponding to reading the first component in each 'matrix'. The unitary transformation associated with this CNOT gate is U = Po / + \u0000 N, \u0000 Po = P[z], \u0000 = P[u]. which conveys well the sense of what U does. You will have seen that we are taking the top qubit first in tensor products. •\u0000 • Z Z Z Z 11 Z Z 11 uu U=P00I+P1ON zu Quantum (c) CNOT gate •\u0000 U: 464 \u0000 10: Quantum Probability and Quantum Computing 0 \u0000 1 \u0000 z \u0000 u \u0000 N= ( 0 1 ) 1 \u0000 0 \u0000 u \u0000 z \u0000 1 0) or N Classical \u0000 Quantum (a) NOT gate z \u0000 ^ c(z + u) u H = c (1 1 \u0000 _ I ) where \u0000 = 2 2 \u0000 C(Z - u) \u0000 1 - 1 Quantum U1 \u0000 U2 \u0000 U3 (d) 'Interchange' circuit Figure A(i): Some simple quantum gates Ad. The 'Interchange' circuit. The 'circuit' in Figure A(i)(d) is a three-gate circuit which interchanges the two input qubits (modulo the phase ambiguity at 460(K1)): we have U(v 0 w) = w 0 v. (b) Hadamard gate 10.3. Quantum computing: a closer look \u0000 465 Let's check out the 'unitary' picture. We have U = U3 U2 Ui (note the order!), where \u0000 U1 = U3 = ® Po + N \u0000 , \u0000 U2 = PoOid- Pi®N. We have 1 0 N = (0 ) \u0000 0 0 = ( P°_ ( 0 0 ' \u0000 1 0 ) ' \u0000 0 1 ' whence \u0000 (0 1 0 ) = N \u0000 PN = (0 0 ) = N Po. PoN = \u0000 1A, \u0000 i 0 \u0000 1 0 We shall always work out products in the order typified by \u0000 (a + b)(c + d + • • •) = ac + ad + • • • + bc + bd + \u0000 , but will omit terms which are obviously zero because of such identities as 0 = PoP1 = Po /VP° = P1NPi . We have U2U1 = Po P o + PoN \u0000 + 01 NP0 + N NP i , \u0000 U3U2U1 = Po P o + PI N PON Pi ± A T PoN Pi N \u0000 Pi N = Po Po + N Po N + \u0000 + N N Po = (Po Po + \u0000 PO( I 0 I) (N N)(Po \u0000 + \u0000 Po). That U does the right thing to z z, z u, u 0 z, u 0 u is now clear. ► B. Entangling circuits. Consider the circuit at Figure B(i)(a), where the input state is z z. You check that the output state U is given by Lf = (Po I ± Pi N)(H I)(z z) = c(z0z+u0u), where c = 2 2 . It is not possible to express U = c(1,0,0, 1)T as a tensor product v w. For no pair (v, w) of qubits is it possible to regard Li as corresponding to 'top qubit is v and bottom qubit is w': the state U is entangled and cannot be separated out. In a sense now to be explained, 'a measurement on the top qubit will affect the bottom qubit' so we have a different situation from that in Subsection 461L. In Figure B(i)(b), we consider the effect of measurement. Measurement of the top qubit corresponds to using the observable M0/, and of the bottom qubit to the observable I ® M. Note that these two observables commute. Consider what happens when the top measurement is made first when the state is U. Check that we have the spectral decomposition M I = OPw o + 1/3w, Wo = [z] ® C2, W 1 = [u] 0 C2. 466 \u0000 10: Quantum Probability and Quantum Computing (a) Simplest entangling circuit (b) Measurements on state U (c) For Grover 2-qubit, ko = 3 (d) Measurements on state V Figure B(i): Entanglement in quantum circuits Note that Wo = [z z, z u] and Wo is the space of eigenvectors of M corresponding to eigenvalue 0. Now, cz 0 z E Wo and — cz z = cu u, so that (14 — cz z, z z) = 0 = (/4 — cz z, z u). Hence U — cz z 1 Wo, and so Pw0U = cz 0 z, \u0000 Cu 0 u. A measurement of M I when the system is in state U will produce a result 0 or 1 with probability 2 each. If the result is 0, then the new state is z z, so that (modulo the type of phase transformation at 460(K1)) each of the top and bottom qubits is in well- defined state z; and a measurement of the bottom qubit must give 0. We see that the two measurements must give the same results. The system is involved in a strange conspiracy. There is more on entanglement later. Of course, we can have entangled triplets, etc. Entangled triplets feature in the amazing phenomenon of quantum teleportation about which you can find far too much on the Net. But Bouwmeester, Ekert and Zeilinger [28] have the correct story. ► Ba. Exercise. Figure B(i)(c) shows part of the circuit for Grover's algorithm for the case when K = 4. (Compare the next subsection.) The controlled (I. gate is Po ® + ® \u0000 = I 1 ° 0 —1 ) 10.3. Quantum computing: a closer look \u0000 467 Show that the output state V is 1(1, 1, 1, —1)T and that it is entangled. Show that if V is inputted into the circuit in Figure B(i)(d), then the two measurements produce the same results, this independently of the order in which the M I measurement and I 0 H operation are performed. C. Probabilistic mixtures for subsystems. Consider the 2-qubit system in state U as in the previous subsection. Let A be an observable associated with the first qubit, so A is 'really' A 0 I. We have E u (A) = (LI, (A 0 /)/4) = 1(z z, (A 0 /)z z) 1(z z, (A 0 nu u) ± 1(11 u, (A ®/)z z) 1(u u, (A ®/)u u) = 2 (z, Az) ± 2 (u, Au) = trace(pA) where p = a (zzt + uut) = So, we can use the 1-qubit density matrix p to work out expectations within the 'first qubit' system. If A has distinct eigenvectors a, with corresponding unit eigenvectors x, y, then a measurement of A, that is, of A 0 I, is made when the 2-qubit system is in state U, then TP(A is measured as a) = IP(A is measured as 8) = If A is measured as a, then the new state of the 2-qubit system is the normalized version of x w, where w = (LI x x)x + (U , x y)y, so that each qubit is now in a definite state (modulo the usual phase ambiguity). D. Circuitry for Grover's algorithm. Before continuing, please check that tensor products are associative in that (t0v)0w=t0(v0w), \u0000 (A®B)®C=A®(B®C). We suppose that the number K of 'telephone entries' in Grover's algorithm is 8, the numbers being 0, 1, 2, ... , 7. We use C8 as C2 0 C2 0 C2 as our underlying space, and regard 10)=z0z0z, 11)=z0z0u, \u0000 17)=u®u®u, as the computational basis corresponding to the measurement M = diag(0, 1, 2, ... , 7). Suppose that the target vector t is 17), that is, u u 0 u. Then simple geometry shows that reflection R[ti in the line joining 0 to t is given by R[t] = 2P[t] — I. [[Note that if P2 = P = Pt, and R := 2P — I, then RRt = R2 = 4P2 — 4P + I = I.]] In particular, —R[t] flips the phase of t (taking it to —t) but preserves the phase of the other elements of the computational basis. So, how do we build a circuit for the gate —RN? 468 \u0000 10: Quantum Probability and Quantum Computing U1 \u0000 U2 \u0000 U3 \u0000 U4 \u0000 U5 4,=172= ( 1 0) 0 -1 V= 1 \u0000 o 0 i Figure D(i): A phase-flip Toffoli-type gate for Grover's algorithm In 'unitary' terms, we want our circuit to act as U where (I have written the sum in the ordei which your later calculation will yield) U = Po 0 Po 0 I + Po 0 Pi 0 I + Pi 0 Pi 0 (I) ± Pi 0 Po 0 , where ( 1 (I) = 0 —1 0 ) • On the right-hand side of Figure D(i) is an implementation in terms of 2-qubit gates, where 0 \u0000 V= ( 1 \u0000 ' \u0000 vvt = 1, v2 = 4). 0 i By considering the effect of the right-hand circuit on each element of the computational basis, check that it has the desired effect. In terms of unitary transformations, we have \u0000 U5 -= /300/0/±P1O/OV, \u0000 U4 = P00/0/±P1ONO/, etc. You can if you wish, for practice with tensor products, prove that U5 U4 U3 U2 Ul is the desired U. As a check on an earlier stage in your calculation, you should have \u0000 U3U2U1 = Po P o + \u0000 NPi V + Po P i + \u0000 NP0 ® Vt. [[Note that common sense and a look at Figure D(i) immediately suggest that \u0000 U3U2U1 = (I I I )(Po ® Po \u0000 + (I I I)(P0 P i ® I) + (I N Vt)(Pi P o I) + (I N V)(Pi 0 Pi 0 -1). ]] 10.3. Quantum computing: a closer look \u0000 469 So, we have our circuit for the oracle —R[t]• But if S is a unitary map such that Sv = w, where v and w are unit vectors, then Rx = 2wwt — I = S(2vvt — /)St = SR[v]St. Now, with t =uou® u, we have (N 0 N 0 N)t = z 0 z 0 z. Moreover, e = 0(1, 1, ... , 1)T = (HO H 0 H)(z 0 z (3) z). Of course, Grover's algorithm does not actually need the minus sign in formula 447(C1). It should now be clear that we can implement Grover's algorithm if we can construct • all 1-qubit gates of type N or H, • all 2-qubit gates of type CNOT, controlled V, or controlled Vt. The 'all' refers to the fact that we wish to be able to apply N or H to each individual qubit, and CNOT, controlled V and controlled Vt to each ordered pair of qubits. Of course, if we have all CNOT gates, then we have all Interchange gates available. You will realize that we can build the required Grover oracle, whatever the value of ko in {0, 1, 2, ... , 7}. ► E. A universality result. It is a fact, important for theory if not so much for practice, that we can achieve any n-qubit unitary transformation provided that we have • all unitary 1-qubit gates, • all 2-qubit gates of type CNOT. See, for example Chapter 4 of Nielsen and Chuang [174]. Crucial to this is the Linear-Algebra fact that, given any 2 x 2 unitary matrix U, we can find unitary 2 x 2 matrices A, B, C and a real number a such that ABC = I, U = e'ANBNC, N = ( 0 1 ) 1 0 ) Then we can implement a controlled-U gate as shown in Figure E(i). The point is that, as should now be obvious to you, U5U4U3U2U1 = Po 0 (ABC) ± Pi 0 (AN BNC) = Po 0 I + e —i' Pi 0 U. Since U6 = ( Po + e jaPi) 0 I, we have U6 U5 U4 U3 U2 U1 = Po ® I + P1 0 U, as required. Check that for U = (1 0 0 1 . , so U equals the V of Figure 468D(i), we can take ( e —im14 \u0000 0 \u0000 ei7r/8 \u0000 0 A = 0 \u0000 eh-pi ) 7 B - \u0000 C = ( 0 e —t7,-/8 ) , a= 7/4. 470 \u0000 10: Quantum Probability and Quantum Computing Ui \u0000 U2 \u0000 U3 \u0000 U4 \u0000 U5 \u0000 U6 U Q_ \u0000 e?ce Figure E(i): Implementation of a controlled-U gate F. An NMR implementation of Grover's algorithm. The ideas here are borrowed from Gershenfeld and Chuang [92]. My account is very much a 'first pass' at this topic. The lead possessed at the time of writing by NMR computing is already seriously under threat. In NMR computing, each molecule in a liquid is a separate quantum computer, so one has about 1018 separate quantum computers. However, one can only arrange that a small excess of molecules have close to the desired initial state; and all experimental measurements detect only the 'majority view' held by a very small percentage majority of the molecules. Gershenfeld and Chuang successfully carried out Grover's algorithm for K = 4, using chloroform CHCe3, but with the carbon-13 isotope of carbon rather than the standard carbon-12 so as to impart a spin to the carbon nucleus in the molecule. The spins of the C and H nuclei are the observables which give the 'bits'. As do Gershenfeld and Chuang in their paper, we now take a naive non- quantum view of things to get a very rough sense of the flavour. For this purpose, we think of the spin of a nucleus as an angular-momentum vector which describes a tiny bar magnet. So take a very classical deterministic view for the moment. A constant, vertically up, magnetic field is imposed on the liquid. This means that an excess of spins of the C and H nuclei will point upwards (giving us an excess of the desired z z state). (Think of z as 'spin up', u as 'spin down'.) The spins of H nuclei can be 'rotated' by electro-magnetic waves of the correct 'resonant' frequency; similarly for the carbon spins. This fact provides 1-qubit gates. Figure F(i) is my crude picture of how a CNOT gate may be implemented. We consider the spin of the carbon nucleus which, as stated earlier, can be rotated about the x or y axes by RFPs (radio-frequency pulses). The carbon spin may be allowed to precess about the vertical z-direction in a sense determined by whether the spin of the H nucleus is down or up. The 'spherical' picture in our figure shows possible successive 10.3. Quantum computing: a closer look \u0000 471 (y) pulse Precess (z) HT (z) Figure F(i): Intuitive picture of NMR operations on carbon spin positions 0,1, 2, 3 of the carbon spin after the '3-gate' operation: 90° RFP(x), precess(z) through { +90° if H spin is down, —90° if H spin is up, 90° RFP(y). Check out that for this operation, if the H spin is down, then the carbon spin is reversed between up and down, while if the H spin is up, the sense of the carbon spin is preserved, exactly what is needed for a CNOT gate. You can see that RFPs of the correct nature have to be applied for precisely the correct time, and that precession time has to be precisely controlled too. Of course, spin is a much more complex matter — see the next section. One's rotations are unitary evolutions of states which determine probabilities, etc. What one has to do is to use the known explicit Hamiltonians which describe the `1-qubie response to RFPs and the '2-qubit' interaction. G. The 'No-cloning theorem'. This result says that it is impossible to build a circuit which, for every qubit v, makes a copy of v onto a 'wire' which was previously occupied by z (say). In the following formulation, the t and s should be thought of as occupying 'work-space'. 472 \u0000 10: Quantum Probability and Quantum Computing There exists no n-qubit circuit with unitary map U with the cloning property that for every qubit v, there exist unit vectors s and t in 01-2 and a real number a such that U (z 0 v 0 s) = ei' v 0 v 0 t . Proof We shall write such an equation as U(z ® v ®?) = v ® v ® ?, a question-mark `?' standing for a vector in Cn-2 the value of which does not interest us, and can change from one use of '?' to the next. Suppose that a U with the cloning property exists. Then, with c = 2 — i , we have, for some real numbers 0, -y, 5, c(z ± u) 0 c(z ± u) ®? = eif3U(z 0 c(z + u) ®?) eii3cU(z 0 z 0 ?) ± ei)3cU(z 0 u 0 ?) eh'cz 0 z 0 ? ± ei5cu 0 u 0 ?. Apply Po 0 P1 0101- 0- -- 0 I to the first and last vectors in the above sequence to get a contradiction. \u0000 ❑ 10.4 Spin and Entanglement Entanglement is one of the most mind-bending things known to Science. Before discussing entanglement, we need to discuss spin. My purpose in this section is to provide background which will allow you to read the many other discussions of 'Bell-Aspect' material, that is, of Bell's inequality (though we concentrate on the Bell-Clauser-Horne-Shimony- Holt (Bell-CHSH) version) and the famous experiments (on the Bell-CHSH inequality) done by Alain Aspect and since by many others. I try to motivate the spinor representation quickly with what is very much a mathematical version of spin. The way that spin features as intrinsic angular momentum in Physics is described in the next section. A. Notation and introductory comments. We shall have Vectors V, A, etc, in ][83, Quaternions (explained below) 2, V, A, etc; Quantum states: vectors v, w, etc, in C2 and tensor products of these; SO(3), the group of familiar rotations of IV, the group of real 3 x 3 matrices S which are orthogonal (S-1 = ST ) and of determinant +1; 10.4. Spin and Entanglement \u0000 473 SU(2), the group of complex 2 x 2 matrices U which are unitary (U-1 = Ut) and have determinant +1. The first results of interest (explained fully below) are as follows: • SU(2) may be identified with S3, the unit sphere in R4; • SO(3) may be considered as S3 with opposite points identified: SU(2) is the `simply-connected double cover' of SO(3). The SU(2) description of rotations is used in computer graphics and in robotics, this because it gives the best way of doing calculations with SO(3). See, for example, Exercise 476Da below. Aa. Comments. Nature views the spins of spin- 2 particles (electrons, protons, neutrons, etc) in terms of the SU(2) picture, rather than the 'obvious' SO(3) picture. As mentioned earlier: but for this extraordinary 'piece of luck', we would not be here. It is a fact, discovered by Pauli, that spin-1 particles must obey the Exclusion Principle: no two spin- 2 particles can occupy the same quantum state. It is essential to realize that, here, quantum state of a particle signifies the full quantum state (which may involve energy, kinds of momentum, spin, ...) of that particle, not just the 'spin state' on which we shall be focusing in this section. The Exclusion Principle lies very deep, requiring Relativity Theory, Quantum Theory and other deep Mathematics. Not even Feynman (see 111-4-1 in [78]) could find any simple explanation of it. For a fine introduction, see Olive [179], and for a full study, see Streater and Wightman [220], but be warned that the latter is very much more advanced than this present book. It is the Pauli Exclusion Principle which leads to determination of electron shells in atoms, explains which elements are possible and how atoms combine to form molecules, and thus forms the basis of 'modern' Chemistry and Biology. See Feynman [78]. I mentioned that the Pauli Exclusion Principle relies on Relativity. However, as we shall see later, it is the case that Nature's utilization of SU(2) rather than SO(3) is itself very much tied up with Relativity. ► B. The Special Unitary Group SU(2). Suppose that a matrix U = a db is unitary and of determinant 1. Then, since UUt = ac db ( b d) = \u0000 ac+ bd Ic12 Id12 11'12 \u0000 aE± bct we must have lar Ibr = 1 = Ic12 ld12, ac bd = 0, ad — bc = det(U) = 1. Hence, a = a(ad — bc) = lard — bac = (1a12 + Ib12)d = d. 474 \u0000 10: Quantum Probability and Quantum Computing Similarly, c = —b. Thus u= ( : Lb ab ) ' where I al2 ±1b12 = 1. Thus, if a = ao + iai , b -= bo + ib1, then a6 + a? + b,?, +1,7 = 1, and we have the fact that SU(2) = S3. ► C. Quaternions. \u0000 The great Irish mathematician Sir William Rowan Hamilton introduced (in addition to Hamiltonians, parts of Hamilton—Jacobi theory, etc) the number system of quaternions. We can think of a quaternion as being of the form Q = qo + V, V = qii + q2,7 + Dic, where qo, qi, q2, q3 are real numbers, and where 12 _ 3-2 K2 _ 1, \u0000 J1C = I = -KJ, 1CI -= ,7 = -i1C, LT = /C = -,7i. \u0000 (C1) Thus, with V as the vector (qi , q2, q3)T corresponding to V, (qo + V)(ro + W) = [qoro — V.W] + V x W, where, on the right, the term in [ ] is real, V.W denotes the scalar product of the vectors V and W, and V x W is their vector product. The quaternions form a skew field: we have all the usual rules for sums, products, inverses, etc, except that multiplication is not commutative. We have, if Q 0 (= '0 + 0'), QQ-1 = 1 = Q-1Q, where Q -1 = (qo — V)/11 QII 2 , where IIQII2 = 4 + 4 + q4 + 4. The map from the set of quaternions to a subset of complex 2 x 2 matrices defined by ao ± al/ + ba ± bilC F--> ( a b ) b a is an isomorphism onto its range in that it preserves sums, products, etc. Note that the unit quaternions, those of norm 1, those on S3, map onto SU(2). Now, ( a \u0000 )—b a = ao + i(aia z + Nay + biax), where a ,, ay, o-x are the Pauli matrices: 1 \u0000 0 \u0000 —i \\ \u0000 0 az — ( 0 —1 ) ' ay— ( 0 i 0 ) ' ax = ( 1 01 ) This is Pauli's time-honoured notation. 10.4. Spin and Entanglement \u0000 475 Let A = a11 + a2,7 + a31C be an imaginary quaternion (ao = 0) of norm 1. Then, A2 = —(4 + 4 + 4) = —1. [[Nota bene. We will have in the product AA the expression al a2/..7 + az al J1 which will be zero because .1,7 ]] Hence, as in de Moivre's Theorem for complex numbers, 7Z(10, A) := el °A = [cos 2 9] + (sin ---(1)A. \u0000 (C2) This is a unit quaternion, and so essentially an element of SU(2). D. The Special Orthogonal group SO(3) of rotations. Consider right- handed rotation through an angle 0 about the axis specified by the unit vector A. It is Figure D(i): Rotation R(0, A) through angle 0 about axis A clear from Figure D(i) that, for any vector V, we have R(0, A)V = P[A] V + (cos 0)(V — P[A] V) + (sin 0)A x V = (1 — cos 0)(A.V)A + (cos 0)V + (sin 0)A x V. The key result we now prove is the quaternion representation R(0, A)V = R(P , A)V12,( - 1 0, A). \u0000 (D1) Here, A and V are A and V considered as imaginary quaternions, and R.(10, A) is at equation (C2). Proof. In the following calculation, we write c = cos 10, s = sin 2 0, and write V.A for V.A and V x A = V x A, identifying vectors with the associated imaginary 476 \u0000 10: Quantum Probability and Quantum Computing quaternions. We enclose real numbers in square brackets when this helps clarify things. We find that 1?.(0 , A)V'IZ(— i 0 , A) = ([c] + sA)V ([c] — sA) = ([c] + sA) {cV + [sV .A] — sV x A} = c2V + [csV.A] — csV x A — [scA.V] + scA x V + s2 (V .A)A + s2 A.(V x A) — s2 A x (V x A) = c2V + 2scA x V + s 2 (V .A)A — s2V + s2(A.V)A = 2s 2(A.V)A + (c2 — s2)V + 2scA x V = (1 — cos 0)(A.V)A + (cos 0)V + (sin O)A x V, as required. We used the well-known facts A.(V x A) = 0, A x (V x A) = IIAII 2V — (A.V)A, in the above calculation. 0 Da. Exercise. Prove that R(a, APO , B) = R(y, C) for some -y, where C is a scalar multiple of (cos la sin 1,3)B + (sin Z (2k cos 0)A + (sin px sin - )3)A x B. Now try to prove this without quaternions. Db. Exercise (optional). Discover another way of proving the results earlier in this subsection, based on the following ideas. We want (why?) — d R(0' A)V = A x R(0, A)V, dB whence, for V I A, d2 ( do— p \\,a , A)V = — R(0, A)V, an easily solved 'simple harmonic' equation. Next, suppose that we know that for any imaginary quaternion V, S(V) := R,(10, A)V'R.( — i0, A) is another imaginary quaternion. Then, using d aTz(1 0, A) = A'1?.(0, A), \u0000 cioR(-10, A) = —0-Z00, A)A, show that (d1d0)S(V) = AS(V). 10.4. Spin and Entanglement \u0000 477 Dc. The double-cover homomorphism. The map 7Z(10, A) \u0000 R(0, A) defined via 475(D1) is a group homomorphism (it preserves products) from SU(2) to SO(3). However, the mapping is two-to-one: R,(1(0 + 27r), A) = —7Z(2 0, A) leads to the same value of R(0, A) as does R.(0, A). (In Group-Theory language, 'the kernel of the homomorphism is {+1, Of course, we have 'non-uniqueness of representation of rotation' in that R(0 + 2n7r, A) = R(0, A), R(8, A) = R(-0, A). However the two-to-one property of the map R,(1 0, A) H R(0, A) does hold because 7?.(1(0 + 2n7r), A) = ( - 1)71740 , A), \u0000 1Z(10 , A) = \u0000 — A). Precisely two elements of SU(2) map onto each element of SO(3). p Dd. Nature's view of spin-1 particles. In effect, Nature sees what we think of as R(9, A) as 7Z(2 B, A): She sees things before we take the homomorphism. So Nature views the rotation as the element R(109, \u0000 = (cos 0)I + \u0000 10)1( Aiaz A 2Cry A30 -x) of SU(2). Again, R.(2t, A) = C it(l iA) and so infinitesimal rotation — which is spin — about axis A is considered to be the observable (the 'Hamiltonian for the 7Z(2 t, A) evolution') ziA = \u0000 + A2o y + A3o-z). But qo + qil + q2,7 q31C H qo — q31 — q2,7 — qi 1C is an isomorphism (`automorphism') of the skew field of quaternions, really because ( - 1C, — J , — 1) is a right-handed system like (1, K). This observation brings the Hamilton and Pauli notations into line. De. Topology of the double cover (optional). This topic is not strictly necessary for understanding the rest. The two-to-one mapping taking 1Z( -10 , A) to R(0, A) is continuous from SU(2) to SO(3). If we take a (continuous) path t S(t) (t E [0, to]) on SO(3) and choose a point 7Z(0) of SU(2) which projects onto 5(0), there is a unique continuous way of 'lifting the S-path' onto a continuous path R(t) (t E [0, to]) on SU(2) such that for each t, 7?.(t) projects onto S(t). We cannot have sudden jumps to antipodal points. Now fix a unit vector A, and consider the path R.( A) t E [0, 27r] in S3 = SU(2). This path goes from I to its antipodal point — I. Its projection on SO(3) is a closed path going from I to I. It is clear that this path on SO(3) cannot be deformed continuously into a path consisting of a single point; for at each stage of the deformation, the lifted path on S3 would have to connect a point to its antipodal point. By contrast, S3 is simply connected in that any path may be deformed continuously into a path consisting of a single point by pulling towards a suitable 'pole' of the sphere S3. Thus, SU(2) is much nicer topologically than SO(3). 478 \u0000 10: Quantum Probability and Quantum Computing DE Rotations as elements of SO(3) (optional). We have not proved that a real 3 x 3 matrix corresponds to a rotation if and only if ST = S-1 and det(S) = 1. \u0000 (D2) We do not actually need this fact, which takes quite a little time to prove in full. Sketched proof of 'if' part. Suppose that S satisfies (D2). First, S extends to a map S : cC3 — > C3. If SW = AW where A E C and W E C3 \\ {O}, then 1A1211W112 -= AWtAW = WtStSW = WtST SW = WtW =-11W112, so that IAI = 1. Now the characteristic equation det(AI — S) = 0 has real coefficients, and the product of its roots is det(S) = 1. Complex roots occur in a conjugate pair. You can see that one root must be +1. Hence we can find a unit vector A such that SA = A. (Of course, A or —A will determine the axis of rotation of S.) Next, we use S-1 = C(S)T / det(S), C(S) the matrix of cofactors of S, so C(S) = S, in proving that S(V x W) = (SV) x (SW). If V 1 A then (SV)T(SA) VTST SA = V TA = 0, so that SV I SA. Hence, for a unit vector V E [A]±, we can write SV = AvV + /Iv (A x V), where A--2v +µv = IISVII 2 = IIVII2 1. You prove, using the fact that S(A x V) = A x SV, that Apxv = Av and kt.Axv = itv• Next, if W E [A]-1, then W = aV + /3A x V for some a and 0. Prove that Aw = Av, f-tw = eav, and deduce the required result. \u0000 ❑ Sketched proof of the 'only if' part. Suppose that S is a rotation: S = R(O, A) for some (0, A). Then S preserves lengths, whence, for V, W E 1183, 2(SV, SW) = IIS(V W)I12 IISV112 = 2(V,W), whence V T (STS — /)W = 0 and STS = I. Hence det(S)2 = det(ST) det(S) = 1. The continuous function yo \u0000 det(4,a, A)) for yo E [0, 0], takes values in 1-1,11 and so must only take the value 1 (which it takes when co = 0). \u0000 ❑ ► E. The spinor representation of spin. Recall that ( 1 0 0 Henceforth, we shall regard (YY ( 0 (El) (A1 + A 26,„ + A30. ) 10.4. Spin and Entanglement \u0000 479 as Nature's observable for the spin of our spin- particle about A. This observable is a self-adjoint matrix acting on C2. This is the `spinor' representation for a spin-1 particle. We immediately derive from Hamilton's relations at 474(C1) the properties 2 \"y \u0000 tr a a = —azay, azax —axaz, axay = —ayax• ay az We write as y where {F, G} is the anticommutator {F,G} :=-- FG GF. Hence, whatever the unit vector A, we have 2 \u0000 I . A If -y and w are such that o-Aw = 7w, where w 0, then 2 W = UAW = -yoAw = 7 2W, so that 7 = ±1. Hence, whatever the value of A, a measurement of the spin O-A of our particle in the direction A will produce a value which is either 2 or —1. Note that the eigenvector w corresponding to spin measurement +1 for spin io-v about the y-axis is a multiple of (1, i)T, and this will represent the state of the system after the measurement. It is not wise to consider what the significance of the eigenvectors is. Remember this when the answers for real eigenvectors in the later calculations seem strange. ► Ea. The 'opposite-spins' state W in C2 ® C2. Please note that the symbol W will now stand for a (very special) vector in C2 0 C2, not for a quaternion. We shall not see quaternions again in this book. In coordinates, ( yii \u0000 ) 0 —1 = 2 -1 (z0u—u0z). \u0000 (E2) Note that if v and w are vectors in C2, so that v = az + /3u, w = -yz + (5u for some a, 7,8 E C, then (you check!) vAw := v0w—w0v = (cr8 — 13-y)VV. It is no coincidence that the determinant (a8—/3'y) appears in this Grassmann (or exterior- algebra) product. This is part of the correct algebra for Jacobians. Let A be any unit vector. Let wi and wA— be unit eigenvectors of o- corresponding to eigenvalues +1 and —1 respectively. Please note that superscripts + will appear, but 0 480 \u0000 10: Quantum Probability and Quantum Computing superscript t for adjoint will not appear again. (So you will not need to ask Macbeth's question.) We shall have w+A w A— — w—A A w+ = AW for some non-zero A, A being non-zero because wl and wA are linearly independent. In fact, IA1 = \\a Hence, A(o-A crA)YV = \u0000 (—wj) — \u0000 = —AA), and so, (QA 0 UA)W = —IV for every unit vector A. \u0000 (E3) The multiplier —1 of W is the determinant of QA. ► F. Weird real-world consequences of entanglement. \u0000 We look at entanglement here as if we had never discussed it in connection with entangling circuits. The mathematics of the last subsection relates to a situation in which a spin-0 particle splits into two spin-1 particles of opposite spins, the state W describing the 'spin-state' of the pair of particles after splitting. For unit vectors B and D, we regard o-Vt-) = crB 0I as (2 representing the state of the first particle in direction B and o-D) = I0 al) as representing the spin in direction D of the second particle. Of course, as already mentioned, the full quantum state of the system should include features other than spin. However, it is the case that we can here focus just on spins without committing error. Note that the state W cannot be written as a single tensor product s 0 t which would represent the situation 'first particle in state s, second in state t': the state W is entangled. Hence, when the quantum system is in state 1/V, neither particle has itself a well-defined quantum state. The two particles must be considered as an inseparable pair — as we have seen before and as we shall now examine again, a measurement made on one of the particles will affect the other. Imagine that the two particles drift apart, and that Observer 1 measures the spin of the first particle in direction A. Now, WA ®wA, w A ®wl, wA ®wA- form an orthonormal basis for C2 ® C2. We have W = A -1 (wI w A — wA®wAl , Hence, the vector W — A-1w1- \u0000 = \u0000 wHA- is orthogonal to wl 0 wl and to wl 0 w;,,. Hence A' \u0000 wA is the orthogonal projection of W onto the space [w-1-] 0 C2 of eigenvectors of a(fil;) corresponding to eigenvector 1. Especially then, if Observer 1 obtains a result +1 from her measurement of 10.4. Spin and Entanglement \u0000 481 o-A , then the state of the system immediately jumps to the unentangled state wl wAT, so that Particle 2 now has definite state wi (modulo phase); and measurement of o-(2) gives definite result —1. Performing the measurement of aA on Particle 1 fixes the state of Particle 2, irrespective of how far away it is. This relates to the EPR—Bohm paradox, Bohm's version of the Einstein—Podolsky- Rosen paradox. p. Fa. Non-locality. \u0000 This curious entanglement of particles is not even an effect that travels at the speed of light. Observers 1 and 2 using the same A will get opposite results even if their acts of measurement are 'space-like separated' so that no signal travelling at the speed of light can alert either particle, before its own spin is measured, of the result — or the direction — of the measurement on the other particle. This strange non- locality phenomenon does not, however, enable Observer 1 to send a meaningful signal to Observer 2 at a speed greater than that of light. Remember, for instance, that there is no way of detecting that the spin of a particle has become fixed in some direction. [[Note. According to Relativity Theory, it is meaningless to say of two space-like separated events that one occurs before the other.]] Various papers appear on the Internet and in journals claiming faster than light communication (get your search engine to look for 'FM' or 'faster than light'), for example, the F1L communication claimed by Nimtz of Mozart's G minor 40th Symphony, a great choice of signal whether the Physics is right or not. G. Reduction to 'two-dimensional' situation. To proceed further, we shall restrict attention to the case when measurements of spins are always in directions parallel to the (z, x) plane. We shall write (for a single particle) ( cos a o-a :-- (cos a)o-, + (sin a)o-z = sin a sin a — cos a (G1) Ga. Exercise. Define (cos /3 \u0000 _ \u0000 — sin /3 u u = \u0000 = sin0 ) ' \u0000 cos Show that o-„,u/3+ = +ua-t o, o-auo — so that the eigenvectors of as corresponding to +1 and —1 are _ \u0000 _ Wa = \u0000 Wa = u1 a. Recall that the convention regarding such equations as those below is that one first reads the top sign throughout, then, separately, with the bottom sign throughout; one does not switch between top and bottom signs in mid-equation. It is not like reading ±1 as `either +1 or —1'. Show that we have for same signs: Pw (0(1) is measured as ±1; o-(02) is measured as ±1) = (N, \u0000 wo+) =- z sine 2 (a — )3), 482 \u0000 10: Quantum Probability and Quantum Computing and for opposite signs: PIN (Q(1) measured as ±1; o-(2 0 ) is measured as +1) =(WINrc ®wi) = z cost 2 (a — p). Deduce that E w o1,1) o-(02) = — cos(ce — 0). Gb. Exercise. Consider the 1-particle situation. Suppose that the true initial state of our particle is w:),E. Show that if o-, is measured before o- o, then lP (ac, is measured first as 1; ao is then measured as 1) cos2 1 (ry — a) cost 1(a — 13) ■ H. Polarization of photons. Experimental tests associated with entanglement have been done with photons (which are spin-1 particles, though of a peculiar type), not with spin-i particles. The necessary modifications are easily made however, and we can utilize our existing notation for the mathematical symbols o-„,111,;, etc. We first consider a beam of light, moving parallel to the y-axis, which is plane- polarized (or linearly-polarized) in that the electric field (light is an electromagnetic wave) always points in a fixed direction perpendicular to the y-axis. This fixed direction corresponds to an angle a in the (z, x) plane. If we hold a piece of polaroid in the (z, x) plane with polarization direction at angle 0, then a fraction cos2(a — 0) of the intensity will pass through the polaroid, and a fraction sin2 (a — /3) will be absorbed. For a single photon of that light, the probability that it will pass through the polaroid is cos2(a — [3). Now for the Quantum version. The polarization of a single photon in direction /3 in the (z, z) plane is represented by the observable 4720 with eigenvectors \u0000 uf3+ corresponding to eigenvalues 1, —1 respectively. If the photon is linearly polarized at angle a in the sense that its state is u-,„E, and if a measurement of pQ is made on it, the result is +1 with probability cos2(a — /3), this because (ua, u+ ) -= cos(a — 0), \u0000 (ua , u13 ) = sin(a — 0). So a measurement +1 corresponds to 'going through the polarizer', a measurement —1 to 'being absorbed'. It is very important that PaPO Pt3Pcx unless a — 0 is a multiple of 27r. \u0000 (H1) 10.4. Spin and Entanglement \u0000 483 The entangled state Lf := .\\/ — 1 u+ u o+ lc uo--) = ( ( for every 8, 0) 1 represents two photons with the same linear polarization: (Pe po)U = +U. However, we shall continue to work with the opposite-polarization (that is, now, perpendicular-polarization) state W. From Exercise 481Ga, we have C(a, /3) E wptp p(02) \u0000 2(a - /3). \u0000 (H2) Note that this is correctly —1 if a = /3 and 1 if a — = 17. [[Remark. It is true that W looks a more fermionic state than bosonic in that interchanging the identities of the two particles changes W to —W. However, W is not the full quantum state of the system, which would involve positions, momenta, etc.]] One can create an opposite(perpendicular)-state photon pair by using a certain type of crystal to split an incoming photon into two perpendicularly-polarized photons of lower energy (lower frequency). Further note on polarization. It is possible to have elliptically-polarized light in which the electric field 'spirals' around the direction of the beam. This idea can be incorporated into our Quantum picture. For example, right-circular polarization is represented by the vector (1, OT, which, not too surprisingly, is the eigenvector of ay corresponding to eigenvalue 1. But we concentrate on the 'plane-polarized' situation. ► I. Hidden Variables: the difficulties. We work with the opposite-polarization state W. How do particles 1 and 2 contrive that if each is tested with a polarizer at the same angle [3, then precisely one will go through, this independently of the value /3? It might seem at first sight that from the moment of their creation, for every /3, Particle 1 had hidden within it the value r (01) of a Random Variable R(1) which would be the measured 0 value of pp. if the chosen direction of measurement is /3; and that Particle 2 has hidden within it the realization r(2) = —r(1) for the same purpose. This is a Very Naive Hidden-0 Variables (VNHV) picture of the quantum world as having a classical one hidden behind it. It is a very strange picture: the function 0 1-÷ r (01) from S I- to { —1, 1} is either constant or is discontinuous. [[Of course, similar remarks apply to the '3-dimensional' case of spin-2 particles.]] J•S. Bell's idea was to derive inequalities which would have to be satisfied if the VNHV picture is true, inequalities in conflict with identities which follow from (H2). Such inequalities (and the VNHV theory) could therefore be expected to be 'shot down' by experiment, and brilliant experiments of Aspect and others provide convincing demonstration that it is indeed Quantum Theory which wins. The particular inequality 484 \u0000 10: Quantum Probability and Quantum Computing which we consider, the Bell—CHSH inequality was produced independently by Bell and by Clauser, Home, Shimony and Holt. It provides the case tested in actual experiments. The essential insight is to note that if 4)1), 1 7r r (1) r (2 ), \u0000 and r (2! are numbers taking — 7r values in the two-point set 1-1, 11, then (7-o(1) + r(1) r(2) + (r(1) — r(14 1) r(2) o r \u0000 0 \u0000 7r _17r can only take the value +2 or —2. Now, the expectation of an RV taking values in {-2, 2}, must lie within [-2, 2]. Hence, if the VNHV theory is true, we must have the Bell—CHSH inequality [(R°1) ± Milr) ) R(127) + (R°1) R(111r) ) R(2 \u0000 < 2, Thus, VNHV leads to the result 1C(0, sr) + Cftir, \u0000 + C(0, —Pr) — Cr, \u0000 < 2. \u0000 (I2) However, it is immediate from 483(H2) that Quantum Theory yields the identity C(0, Pr) + C( 4T-, Or) + C(0, —kir) — C(17r, —Pr) = —2Nd contradicting the Bell-CHSH inequality of VNHV theory. But, says a less naive Hidden-Variables advocate, \"Because of 482(H1), we cannot simultaneously measure all the observables of which expectations feature in (I2). You must tell me what values of a and 0 are to be used in some experiment. This information should be built into the whole system from the start. Indeed, Exercise 481Ga tells me that, for the given a and 0, from the moment of their creation, the two RVs 14,1) and 1432) have joint probability mass function satisfying P (kal) = 1; R(02) =- 1) = P (R,(„1) = —1; R( ) = —1) = z sin2(a — 0), P \u0000 = 1; R(02) = —1) = P (k1) = —1; 1432) = 1) = 2 cos2(a — /3). You have convinced me that I cannot arrange this simultaneously for all a and 0. But I need only arrange it for the particular pair (a, 0) used in the experiment.\" An official spokesperson replies: \"Yes, but I haven't told you the main thing about the brilliant Aspect experiment, and the myriad other experiments which confirm his findings. You see, on each experiment, the decision about which directions a and /3 were to be used was not made until after the creation of the entangled pair. Before its own polarization is tested, neither photon could learn from information travelling at the speed of light of the angle of measurement of polarization of the other photon. So, we must have non-locality.\" To which the Hidden-Variables advocate can of course reply: \"I admit that that seems very disturbing for my attempt to give a realistic picture of what is happening. But you now have to tell me the full details of how the choice of directions is made ... \" And there we leave them. 10.5. Spin and the Dirac equation \u0000 485 Ia. A way of thinking about the quantum Bell—CHSH identity. For a single photon, pc, = (cos 2a)po + (sin 2a)p1.7 whence .■, 19 ,47,- = po + For our entangled pair of photons in state VV, with probability 1, \u0000 (4) + p(4) p(21 7 \u0000 .v--19(17r) p(217r \u0000 (PO1) — Pq) P82:r \u0000 NP(1 PC2?ir = A/7 whence it is reasonable to claim that in state W, the mathematical observable described by the self-adjoint operator ( (1) \u0000 (1) \u0000 P \\ (2) \u0000 ( Po (1) \u0000 P (1) \u0000 (2) !,7r ) ,171- + \u0000 )P —L7r is, with probability 1, equal to —20. (It is hard to imagine that this mathematical observable could be measured in the real world.) Since the expectation of an RV which is equal to —20 with probability 1, is obviously —20, we see a motivation for the Bell—CHSH identity. For further discussion, see Subsection 491A. 10.5 Spin and the Dirac equation Until now, spin has just been part of a mathematical 'game', so we should take a brief look at some Physics which greatly clarifies its real role. Anyone writing on this must feel tempted to include the lovely quote from Dirac: \"A great deal of my work is just playing with equations and seeing what they give. I don't suppose that applies so much to other physicists; I think it's a peculiarity of myself that I like to play about with equations, just looking for beautiful mathematical relations which maybe don't have any physical meaning at all. Sometimes they do.\" In this book, we have already seen several pieces of fine Mathematics: the Central Limit Theorem, the associated asymptotic results for MLEs and for the Likelihood Ratio, the F-test theorem for Linear Models, the Ergodic Theorem for the Gibbs sampler, the Strong Law, the Martingale-Convergence Theorems, the Universality result for quantum gates. Now, the Mathematics surrounding the Dirac equation matches any of these (though not some of the Mathematics mentioned at the end of Chapter 9); and as for the 'Sometimes they do', well, the equation signalled one of the most dramatic developments in the history of Physics. Inevitably, the true picture is rather more complicated than the one I now present, but as a first approximation, this will do, I think. 486 \u0000 10: Quantum Probability and Quantum Computing Dirac found a remarkable modification of SchrOdinger's equation for a free particle (one on which no force acts) which is compatible with Special Relativity Theory (Which subject I do not assume that you know). Dirac's equation is not however compatible with the law of conservation of angular momentum unless one postulates the existence of an intrinsic angular momentum or spin for the particle which miraculously 'balances things out'. This intrinsic angular momentum has no classical analogue. Let me mention a little of the rest of the story, even though, sadly, we do not study it in this book. In analogous fashion to that in which we are forced to introduce intrinsic angular momentum, consideration of the movement of an electron in an electro-magnetic field forces us to postulate the existence of an intrinsic magnetic moment of the electron, which again corresponds to (a constant multiple) of its spin. This is stunning enough, but Dirac's equation also predicts the existence of anti-particles; and the positron, the anti- particle of the electron, was discovered by Anderson just a few years after Dirac produced his equation. And there is much more to the story: it leads naturally on to quantum fields, etc. So the Dirac equation is truly wonderful. Notation. Since we are now considering Physics, • the modified Planck constant h will no longer be considered to be 1; • c will now denote the speed of light (not 2- 1); • a typical wave function will be denoted by //), not v, so that leaves v free to denote a velocity; • the Hamiltonian will be denoted by H (not 9-0, the Hadamard gate no longer being of interest to us; • Schrodinger's equation will be written 87/) in — at = Ho; • the position of a particle moving on R will be represented by the observable Q, where (Q0)(x) = (x) and its momentum by the observable P := —ihd/dx. I shall call Q quantum position, P quantum momentum, and H quantum energy. ► A. The Heisenberg picture of evolution of observables. Up to now, we have used the Schrodinger picture in which the state evolves according to Schrodinger's equation in at = HO, so that OM = e —itH/ n0(0). In the Schrodinger picture, a physical observable such as the momentum of the particle is represented by the same operator A at all times. In the equivalent Heisenberg picture, the state V, remains fixed, and the observable evolves. To have compatibility, the expectation of an observable at time t has to be the same in the two pictures, so we must have (Schrodinger) (OM, A(0)0(t)) = (CO), A(t)0(0)) (Heisenberg). 10.5. Spin and the Dirac equation \u0000 487 Thus, we need (0(0), A(t)0 (0)) = (e-'11160 (0), A(0)e-' 11/60 (0)) = (0(0), eitili nA(0)e-' 11/60(0))• Heisenberg therefore postulates that A(t) = eitH/hA(0)e-itH/h so that on 'differentiating by parts', iH itH/h eitH/6A.(0)e-itH/ h \u0000 dt \u0000 = \u0000 A(0)e h Thus, we are led to Heisenberg's equation ih 1A(t) \u0000 [A(t), H] \u0000 A(t)11 H A(t). \u0000 (Al) We note especially that if A(0) commutes with H, then A(t) = A(0) so that A(0) is an invariant of the motion. In particular, H(t) = H, the law of conservation of quantum energy. Of course, the Schrodinger and Heisenberg pictures only apply to situations where no measurement is made, so interpretation is again rather bizarre. B. Free particle: non-relativistic theory. For a particle of mass mo moving freely (that is, not subject to any force) at velocity v on IR, Newton tells us that its classical momentum is p = my and that its classical energy is Zmov2 = p2/(2m 0). Force is rate of change of momentum, so, since our particle moves freely, P = dp/dt = 0, and p is constant, the law of conservation of classical momentum. Since the Hamiltonian is quantum energy, we write \u0000 P2 \u0000 h2 d2 H = \u0000 \u0000 2m0 \u0000 2m0 dx2. We take P(0) = P, Q(0) = Q. Since P obviously commutes with H, we have P(t)= P, the law of conservation of quantum momentum. In regard to the quantum position Q(t) at time t, it is natural to guess from Newton's theory that Q(t) = Q + mo 1tP. This checks with 112 \u0000 d2v, \u0000 d2 \u0000 h2 d0 2m0 x dx2 dx2 (x)] \u0000 mo dx = ihmo l = \u0000 PO. For a particle of mass mo moving freely at velocity v in 1[13, Newton says that p = my and the lack of force implies that p = 0. Since the 'orbital' angular momentum 488 \u0000 10: Quantum Probability and Quantum Computing E of the particle about the origin was defined by Newton to be q x p, where q is the particle's position and x the vector product, we have dt dt = q x if) + 4 x p = 0, because p = 0 and q is parallel to p. In the quantum context, the quantum position is the 3-component vector observable Q = (Q., (2y, Qz)T and the quantum momentum is P = (Px, Py, Pz)T, where for 7b : R3 —> C, we have \u0000 (Q 4)(x, y, z) = x/P(x, y, z), \u0000 (PrIP) (x, Y, z) = -iiit, etc. The quantum orbital angular momentum is the 3-component vector observable L = QxP = (Lx,L y, Lz)T = —hi(yaz — zay, zax — xaz, xay — yax)T, \u0000 (B1) where ax = a/ax as usual. Of course, H is now 1 \u0000 (p2 ± p2 p2) _ 27Tio \u0000 x \u0000 Y \u0000 x i where A is the familiar Laplace operator 82 \u0000 82 \u0000 82 A = \u0000 + \u0000 + ax2 ay 2 az2 You should check that [Lx, H] = 0, so that in the Heisenberg picture, L(t) = L(0) = L, and L is an invariant of the motion, yielding the law of conservation of quantum angular momentum. If k is the unit vector along the z-axis, then x ) \u0000 x cos t — y sin t R(t,k) ( y \u0000 = \u0000 xsint + y cost \u0000 , z \u0000 z and a at z(x cos t — y sin t, x sin t + y cos t, z) L 0 = (xay — yax )0. This points to the perfect (Tie-algebra') tie-up between angular momentum and our earlier study of SO(3), but spin as intrinsic angular momentum is something different. ► C. Energy in Special Relativity Theory. Suppose that a free particle has 'rest mass' mo: this is the mass of the particle as measured by an observer at rest relative to the particle. (If Observer 1 reckons that the particle is moving freely, he will regard the particle as having constant velocity, and an Observer 2, moving at some constant velocity relative to Observer 1, will also believe the particle to be moving freely.) For a particle of rest mass me moving at velocity v relative to us, we regard it as having A2 '` \u0000 A, 2mo 10.5. Spin and the Dirac equation \u0000 489 • mass m mo (1 iiv1i2/c2) • momentum p = my, • energy (wait for it!) E = mc2. Juggling with the algebra produces (with the strange I discussed later) \u0000 E 2 = 7464 + \u0000 E = \u0000 TrIF,c4 c2IIP112 • \u0000 (C1) \u0000 POP D. The Dirac equation. \u0000 An essential property of the original Schrodinger equation is that because it is linear and of first order in at and H is self-adjoint, then 110(t)112 = II0(0)112, yielding conservation of probability densities. Now in Special Relativity, space and time are treated on the same footing. Dirac therefore sought an equation which is linear and of first order in all partial derivatives. The obstacle to achieving this is obviously caused by the square root in equation (C1). However, we already know that the anticommutation relations \u0000 {o-r , o-s } := o-ro-s \u0000 0-sur = 26rs12 ) \u0000 r,s E Ix, y, zl, (where /2 is the identity 2 x 2 matrix) for the Pauli matrices imply that for numbers Px,Py,Pz we have (pxa x +Pyay +Pzaz) 2 = 1114 21-2, so that pzu x pyo-y p zo-z may be regarded as a 'square root' of 11P112• Dirac therefore had the idea of writing \\ 2 2 \u0000 a \u0000 a y + cPzaz) (74c4 + c211012) \u0000 = (moc a t ± cp x + cpy x where, for some n, the a, are postulated to be self-adjoint n x n matrices satisfying the anticommutation relations {ar, as} := ara s a sa, = 215rsin \u0000 T,S \u0000 y, z}), \u0000 (D1) and thus producing the Dirac equation HD VI, HD :=-- \u0000 + CPxax + c-Pyay + CPzaz, \u0000 (D2) where IP : R3 —> Cn. (Note that HD is (formally) self-adjoint because the a's are, the P's are, and the a's commute with the P's.) We assume for now that such matrices a, exist. I skip discussion of the fact that the Dirac equation transforms (from one observer to another) in the appropriate (`Lorentz covariant') manner for Relativity Theory. See references at the end of this section. 490 \u0000 10: Quantum Probability and Quantum Computing • • E. Electron spin as intrinsic angular momentum. Once more, let L be the orbital angular momentum given by 488(B1). Then [Lx, HD] = —ihf(yaz — Zay)C(ayay + azaz) — C(Ceyay + CV zaz)(Yaz — Zay)} = ihc(a yaz — azay), so the law of conservation of quantum orbital angular momentum about the origin fails for our free particle. This is a situation which must be remedied. But now define S = (Sr , Sy, Sz )T to be the 3-component observable of self-adjoint matrices S = —lih(aya z, a za x, a xa y). (Note that aya, = 2 [a y, az].) Then [Sr , at] = [Sr, a n = 0 (r E {x, y, z}), [Si , ay] = iha z and likewise for cyclic permutations of x, y, z, whence [Sx, HD] = —ihc(ayas — as ay), and so [TT, HD] = 0 \u0000 (r E {x, y, z}), where T := L + S. Thus, T is an invariant of the motion determined by the Dirac Hamiltonian HD, and this is the true law of conservation of quantum angular momentum. Adding in the intrinsic angular momentum, or spin, S, has saved the day, assuming that the a's exist. So, how do we find the a's? This question was answered by Clifford in 1880! We take n = 4 and a t = a-, ®I, a r = a-, 03) ar , (r E {x, y, z}); indeed, this is essentially the only choice (see below). You check that these a's have all the required properties. The variable S is exactly what we had in our SU(2) picture except for the mysterious (but understood) 10'. F. A bizarre feature of the Dirac equation. Since [Px, HD] = 0, the law of conservation of momentum holds for the Dirac equation. But what does the equation say about the position of the particle? Because the orbital angular momentum is not preserved for the Dirac equation, it cannot be the case now that Q (t) is always a scalar multiple of P (t), even though 4(t) is a scalar multiple of p(t) for our free particle in Special Relativity. Check that we have [Qx, HD] = ihca x, whence, dQ x(t) = cax when t = O. dt Now, cax is a mathematical observable which can only be 'measured' as —c or c, so things are truly rather bizarre. Note too that [ax, HD] 0, so that ax is not invariant under H. 10.6. Epilogue \u0000 491 For discussion of this point, and of the associated phenomenon of Zitterbegewung (`jittering'), see Thaller [225] and other books mentioned at the end of the next subsection. G. What next? There I must end my bit of the 'Dirac' story. You are probably intrigued that the quantum energy is HD = moc2at ± • • • where at can only be measured as +1 or -1. Does this point to the existence of negative energy? In classical Special Relativity, one has to take the positive square-root sign in 489(C1). But it seems that with quantum theory, all things are possible. I cannot do better than hand you over to my Swansea colleague David Olive for a better geometric picture of what we have done, for the remarkable essential uniqueness of the choice of the a's, and for some of the still more fascinating remainder of the story. See Olive [179]. For extensive studies of the Dirac equation (including its Lorentz covariance and its bizarre behaviour in regard to position observables), see Thaller [225], Bjorken and Drell [24], Davydov [56], Messiah [163]. A fine account of some quantum-field theory by Fradkin [82] is available on the Web. See di Francesco, Mathieu and Senechal [64] for the definitive version. 10.6 Epilogue ►► A. Quantum Theory and 'Realism'. As the quart-ph website and many other web sites reveal, many people refuse to accept non-locality, and continually put forward 'realistic and local explanations' (some more realistic than others!) of the results obtained by Aspect and others. Some of the objections in regard to 'detection' in such experiments might be considered definitively countered by Rowe et al. [203]. However, most physicists believe that we have to accept that Quantum Theory just is completely counter-intuitive. I believe that the Mathematics of self-adjoint operators, etc, is all there is to understand. That Mathematics has never been faulted by experiment. Of course, it will be very interesting to see Penrose's development of his gravity theory of R-mode quantum mechanics, and perhaps even more interesting to see what M-theory eventually has to say about it all; but many mysteries will remain if we try to explain things in ordinary language. A small part of me would like to see a more 'sensible' explanation of the weird phenomena of entanglement, etc. Some physicists and mathematicians with deep understanding of the orthodox theory have sought such explanations. Amongst the best known attempts are the Bohm and Bohm-Hiley theory ([25, 26]) of quantum potential, and Nelson's theory [171] of stochastic mechanics. These theories agree with Schrodinger's equation for a particle moving under the influence of some force: indeed the quantum potential and the stochastic mechanics are derived from Schrodinger's equation. 492 \u0000 10: Quantum Probability and Quantum Computing But the Bohm(—Hiley) theory gives the particle a classical position and velocity, and the Nelson theory gives it a classical position. So these are hidden-variable theories, but each postulates some kind of underlying quantum fluctuation as an extra, non-classical, force. Influences are not local in these theories. Feynman's path-integral formulation [77] is another way, familiar to probabilists because of the `Feynman—Kac formula', of thinking about Schrodinger's equation. Everett's (in)famous many-worlds interpretation is something in which I do not believe: it is the least parsimonious theory ever! I should not be so flippant: many people who have thought long and hard about Quantum Theory, especially those in Quantum Computing, are tending to embrace the many-worlds theory. Sorry, folks: I cannot accept it. There are a number of attempts to see Quantum Theory in terms of Quantum Information Theory. See, for example, Zeilinger [239] and Bouwmeester, Ekert and Zeilinger [28], and a forthcoming book by Kelbert and Suhov. Fundamentally, I prefer to stick to the orthodox theory, regarding Mathematics as the correct language of Nature, and not needing to have it interpreted. The operator formulation applies in exactly the same way to all situations: to the finite-dimensional situation for Quantum Computing exactly as for the position-momentum case; and I consider it very much more elegant than any of its 'realistic' alternatives. Nature rejoices in Pure Mathematics: in group representations, Clifford algebras, etc. She sees SO(3) as an abstract group, utilizing all its representations, not just the SO(3) and SU(2) representations we have studied. I am sure that She loves the Parseval duality between position and momentum which 'realistic' theories lose. Thought. It always puzzles me that people want things 'explained' in English or Chinese or some other ethnic language, when such languages are extremely crude compared with that of Mathematics in situations where the latter applies. Why try to 'improve' on a perfect mathematical picture such as the 'orthodox' one of the quantum world? People will presumably accept that I cannot describe Beethoven's C sharp minor String Quartet in words. [[Incidentally, I do worry about Lewis Thomas's idea, quoted in Dawkins [57], that we should advertise our achievements on Earth by broadcasting into space Bach and only Bach. What makes Bach's music great is what cannot be explained by its 'mathematics', and an alien civilization aware that the music is the creation of intelligent life, is unlikely to be able to appreciate that. I can easily program the computer to write a fugue with 100 parts, with complex stretti and cancrisans statements galore, and all the other tricks of the art of fugue. But it won't be music. Far better to continue broadcasting the fact that we know the Riemann Hypothesis!]] Decoherence. \u0000 It would be quite wrong to leave this topic without mentioning decoherence, which recognizes the role played by the environment, which constantly `observes' a quantum system. The theory of decoherence answers two key questions. Given the weird phenomena of the quantum world, why does the everyday world generally act in a sane way? (If the environment is continually observing the world at microscopic level, why aren't states jumping to eigenstates of all sorts of different operators all the time?) On the other hand, we need to answer the question: how can it sometimes happen 10.6. Epilogue \u0000 493 that we are able to witness quantum phenomena on a 'macroscopic' scale? Read Zurek [240], and Giulini, Joos, Kiefer, Kupsch, Stamatescu and Zeh [97]. B. And finally ... . I wrote much of my account of entanglement at Bath, finishing that first draft on 3rd July 1997. I went down to the library at Bradford on Avon that morning to find a novel to free my mind from this topic. On the library shelves, however, I happened to spot the novel The Schrodinger Cat Trilogy by Robert A Wilson [236]. This extraordinary book was written in 1979, three years before the famous Aspect experiment. As mentioned in the quotation, Clauser and Aspect had already done some important experiments, as had Wilson, Lowe and Butt ([237]).) The following (except for bits within [ ] brackets) are verbatim quotations from the book (the punctuation is verbatim, too!): \"It wasn't Einstein\", Williams was droning along, \"and it wasn't Heisenberg or dear old Schrodinger who drove the last nail in the coffin of common sense. It was John S. Bell who published his memorable Theorem in 1964, nearly twenty years ago\", and blah, blah, blah. [Hugo's fantasy about a different kind of entanglement with the girl in front follows here, but is not relevant to our thinking now.] Williams continues to transmit to bored faces: \"Bell's Theorem basically deals with non-locality. That is, it shows no local explanation can account for the known facts of quantum mechanics. Urn perhaps I should clarify that. A local explanation is one that assumes that things seemingly separate in space and time are really separate. Um? Yes. It assumes, that is to say, that space and time are independent of our primate nervous systems. Do I have your attention, class? But Bell is even more revolutionary. He offers us two choices if we try to keep locality, [etc, etc. ...] Yes, Mr Naranga?\" [Mr Naranga asks if this topic is going to be on the examination. ] \"No you needn't worry about that Mr Naranga we wouldn't dream of asking anything hard on the examination I believe the last examination with a hard question at this university was in the survey of mathematics course in 1953 yes Mr Lee?\" [Mr Lee asks if it is possible that the quantum connection is not immediate and unmitigated.] \"Ah Mr Lee how did you ever land at this university there are times when I suspect you of actually seeking an education but I'm afraid your canny intellect has run aground. Recent experiments by Clauser and Aspect shut that door forever. The quantum connection is immediate, unmitigated, and I might say omnipresent as the Thomist God.\" [Later in the novel] Bohr also added nearly as much to quantum theory as Planck, Einstein, or Schrodinger, and his model of the atom [...]. Bohr himself, however, had never believed it; nor had he believed any of his other theories. Bohr invented what is called the Copenhagen interpretation, which 494 \u0000 10: Quantum Probability and Quantum Computing holds in effect that a physicist shouldn't believe anything but measurements in his laboratory. Everything else — the whole body of mathematics and the theory relating one measurement to another — Bohr regarded as a model for how the human mind works, not of how the universe works. Williams loved Bohr for the Copenhagen interpretation, which had made it possible for him to study physics seriously, even devoutly, without believing a word of it. Robert A Wilson What more can I say?! APPENDIX A SOME PREREQUISITES and ADDENDA Appendix Al. '0' notation For functions f and g on N, we write f (n) = 0 (g(n)) if the ratio f (n)/ g(n) remains bounded as n oo, that is, if for some constant K and some integer no, we have If (n)I < KIg(n)I fora > no. Thus, \u0000 ln(n) = 0(n), n 4 -= 0(en), sin(n) = 0(1), \u0000 (n \u0000 oo). For functions f and g on (0, oo) we write f (x) = 0(g(x)) as x 4. 0 if for some b > 0 we have If (x)I < K Ig(x) I for 0 < x < b. The modification for the situation where x rather than x 4. 0 is now obvious. Examples: X2 = 0(x), sin(x) = 0(x), \u0000 (x \u0000 0). We define f (x) = 0(g(x)) as x — > oo in the obviously analogous way. We can write 0(n) = 0(n2) in that if f (n) = 0(n), then f (n) = 0(n2). But we cannot write 0(n2) =- 0(n). We have (for example) —30(n) = 0(n). We have 0(n) 0(n) = 0(n) in that if f (n) = 0(n) and h(n) = 0(n), then (f h)(n) = 0(n); but we would have to be extremely careful about trying to extend this idea to infinite sums. Appendix A2. Results of 'Taylor' type I am perfectly happy if you assume throughout the following that f has derivatives of all orders on [—a, a]. I play things as I do (still making stronger assumptions than necessary) only because the following is the kind of thing beloved of some analysts and therefore featuring in some courses on Analysis which you may be enjoying. Suppose that for some a > 0, f is a continuous function on [—a, a] for which (0) exists. Since {f(y) — f (0)} /y (0) and f is continuous, it is obvious that for some Ko we have I f (y) — f (0)1 < Koly I for y E [ — a, a]. In other words, f (y) = f (0) ± 0(y) \u0000 (y \u0000 0). 496 \u0000 Appendix A. Some Prerequisites and Addenda Suppose now that f'(x) exists and is continuous on [—a, a] and that f\" (0) exists. Then, on applying the result just obtained to f', we have \u0000 (y) = (0) +O(y) \u0000 (y \u0000 0). \u0000 (ApA 2.1) But, for x > 0, we can integrate this over [0, x] to obtain f (x) = f (0) + \u0000 (0) + 0(x 2), \u0000 (x \u0000 0). \u0000 (ApA 2.2) Remember what's happening here in the integration. Concentrate first on y > 0. The meaning of (ApA 2.1) is that for some b > 0 and some K1, the 0(y) term is dominated on the y-interval [0, b] by Ki y, and so, for 0 < x < b its integral over [0, x] is dominated by 2K1x2. The case when y < 0 is, of course, strictly analogous. If f\" exists and is continuous on [—a, a] and f\"'(0) exists, then, by applying result (ApA 2.2) to f', we have (Y) = f( 0) + Yf\"(0) + 0 (y2), \u0000 (y \u0000 0), and we can integrate this to get \u0000 f (x) = f (0) + x (0) + 2x 2 f\"(0) + 0(x3) \u0000 (x \u0000 0). \u0000 (ApA 2.3) In particular, 111(1 + x) = x — 2x2 + 0(x3), \u0000 (x — > 0). \u0000 (ApA 2.4) Appendix A3. 'co' notation For functions f and g on N, we write f (n) = o(g(n)) if the ratio f (n) I g(n) \u0000 0 as n —> co. Examples: ln(n) = o(n), n 4 = o(en), sin(n) o(1), \u0000 (n —> oo). Similarly, x 2 = o(x), sin(x) \u0000 o(x), \u0000 (x —> 0). Appendix A4. Countable and uncountable sets An infinite set S is called countable (or countably infinite) if S may be put in one-one correspondence with the set N = {1, 2, 3, ...} of natural numbers; that is, if we can find a way of labelling S as the set of points in a sequence: S = \u0000 s 2, .931 • • 4 ; otherwise, S is called uncountable. We shall also call finite sets countable, so that every set is either countable or uncountable. The set Q of all rational numbers is countable. We see that Q fl (0, 1) consists of the points Appendix A4. Countable and uncountable sets \u0000 497 Call the above sequence qi , qz, q3, ..., and we then have n (0, co) = {1, qt, q i 1, q2, q2-1 , • • .}. It is now obvious that Q is countable. The set R is uncountable. Indeed, its subset H consisting of the irrationals in (0, 1) is already uncountable. Any point of H may be written uniquely as x =- CF(a i , az, a3, \u0000 := 1 an -=- an (x) E N, al + 1 1 az + \u0000 1 a3 + in the sense that the fraction pr,,(x)1q,,(x) (in lowest terms) obtained by truncating the fraction at the nth stage converges to x. Thus H is in one-one correspondence with the set of all sequences of elements of N. Suppose that H could be labelled as H — {x i , X2, X3, . . .} and that x; — CF(a ji , ajz, ai3, ...). Define x := CF(aii + 1, azz + 1, a33 + 1, ...). Then, for every n, x differs from xn in the nth place, so x H, a contradiction. Thus H is uncountable. In connection with the Strong Law of Large Numbers and the whole philosophy of Probability, it is important that the set of all strictly increasing sequences of positive integers is uncountable. This italicized result is true because being given a strictly increasing sequence b1, b2, b3, • - is the same as being given the sequence Ci, C2 7 C3 7 • • • 7 where c1 = b1, c2 = c2 \u0000 c1, c3 = b3 \u0000 b21 • 5 so the set of strictly increasing sequences of positive integers is in one-one correspondence with the set of all sequences of positive integers; and we already know that the latter is uncountable from the continued-fraction story. Note. One of the standard theorems in any elementary course on set theory is that a countable union U Cm of countable sets Cn is countable. But the standard proof (by Cantor) assumes that each Cn is not merely countable but counted, that is, already equipped with a labelling as a sequence. In many applications, there is some natural labelling of each Cn, and then Cantor's argument applies. However, in the abstract, where we merely know that there exists a labelling of each Cn as a sequence, the standard theorem relies on the Axiom of Choice. See below. Cohen has shown that one can have a system consistent with all the 'other' rules of set theory in which the standard theorem is false, and that indeed one can have such a system in which R (which is definitely uncountable, as we have seen) is a countable union of countable sets. 498 \u0000 Appendix A. Some Prerequisites and Addenda Interesting Digression. Continued fractions give the best rational approximations to x. For every x in H, the following is true: for every n, we have pTh(x) \u0000 x qn(x) 1 K qr,(x) 2 (ApA 4.1) with K = 1; and amongst any two consecutive values of n, at least one satisfies (ApA 4.1) with K = 2. 'Conversely', for every x in H, any fraction p/q such that 1 q — x <K q2 \u0000 (ApA 4.2) with K = 2 is one of the continued-fraction approximations. To round off the story: for every x in H, amongst any three consecutive values of n, at least one satisfies (ApA 4.1) with K = 'A but for the particular value x = CF(1, 1, 1, ...) = \u0000 — 1), for any K > -\\/, only finitely many fractions p/q satisfy (ApA 4.2). See Hardy and Wright [108]. These results suggest investigating the rate of growth of qn(x). An amazing result of Levy and Khinchine states that if we choose a point X in [0, 1] uniformly at random, then (P(X E H) = 1 and) with probability 1, 7r2 n Vqn (X) exp \u0000 121n 2 ) but that is another story from Probability. See, for example, Billingsley [21]. Appendix A5. The Axiom of Choice The discussion here is heuristic, and does not go into axiomatizations of the whole of set theory. The 'Axiom of Choice' is usually assumed in Mathematics. (Recall that it is needed for the Banach—Tarski Paradox.) It says the following: the Cartesian product of a collection of non-empty sets is non-empty. Let's give an equivalent statement: Let C be a collection of nonempty sets. Then there exists a function f on C such that, for each set S in the collection, f (S) E S. In other words, we can make a simultaneous choice, choosing one element from each set S in C. (The described functions f are exactly what make up the Cartesian product.) Bertrand Russell liked to say: \"To choose one sock from each of infinitely many pairs of socks requires the Axiom of Choice, but for shoes the Axiom is not needed.\" The point here is that there is a well-defined particular element in a pair of shoes, the left one; but the same is not true for socks. If, for example, one takes a countable union of 'abstract' countably infinite sets and wishes to apply Cantor's argument, then one has to make a simultaneous choice, choosing one labelling for each set from the (uncountably many) different labellings of that set. Appendix A6. A non-Borel subset of [0, 1] \u0000 499 For a useful site about the Axiom of Choice, see http://www.math.vanderbilt.edurschectex/ccc/choice.html prepared by Eric Schecter. Appendix A6. A non-Borel subset of [0, 1] Return to the continued-fraction story. Let V be the set of irrational x in [0, 1] for which there exists an increasing subsequence (nr(x)) of N such that an,( x)(x) divides an,±i (x)(x) for every r. Then V is not a Borel subset of [0, 1]. Again, the crucial point is that the set of all subsequences of the sequence of positive integers is uncountable; but to prove that V is not Borel is very difficult. The set V is `Lebesgue measurable' with measure 1. Appendix A7. Static variables in 'C' Static variables cannot be changed except via functions in the file where they are set up. Here I give a header file H . h, a 'module' file M. c which defines the function up 0 , and a program file P. c which defines a function down() and uses it within main 0 . Because the variable a in M. c is static, its value is preserved within M. c. The variable a in P .h is different. When up () is called, it uses the values of a and C2 within M. c, keeping them fixed between 'visits to M. c'; whereas down 0 deals with the values of a and C2 in P. c. This way, you cannot get into trouble by using the same symbol in different files. It is a bit like Object Oriented Programming. /* Header H.h \u0000 defines C1=1 */ #if defined H_h /* \u0000 Three of four lines \u0000 */ #else \u0000 /* to avoid DOUBLE definition \u0000 */ #define H_h \u0000 /* of Cl, once in M.c, once in P.c */ #define C1 1 \u0000 /* C1 to be used by both M.c and P.c */ void up(); \u0000 /* optional reminder of function--in module */ #endif \u0000 /* The 'fourth line of four' */ /* Module M.c \u0000 cc -c M.c -o M.o */ #include \"H.h\" #include <stdio.h> static const C2 = 2; static int a = 0; void up(){ a = a + C2 + Cl; printf(\"%ild\", a); } /* Program P.c \u0000 cc P.c M.o -o P */ 500 \u0000 Appendix A. Some Prerequisites and Addenda #include \"H.h\" const C2 = -4; int a = 0; void down(){ a = a + Cl + C2; printf(\"%4d\\n\",a); } int main(){ int i; for(i=0; i< 3; i++){up(); down();} return 0; } Results: 3 -3 6 -6 9 -9 Appendix A8. A 'non-uniqueness' example for moments Let h be an even infinitely differentiable function on Irk which (for some (5 and a with 0 < b < a) is zero on (-oo, -a] U [- 8, U [a, co) and non-zero elsewhere. Let g be the real function g(a) := f el\"h(x) dx. Then (justification being standard Fourier theory) 1 = — e -is' g(a) da, and h(n)(x) = \u0000 flii(—iare-ixag(a) da. But h(n)(0) = 0 for (n = 0, 1, 2, ...), so that fa xr g(x) dx = f g(a) da = O. It is now clear that we can choose a constant K such that K g+ and Kg- are pdfs with the same cumulants but which, since they have different supports, correspond to different distributions. Simon ([213]) treats the moment problem with characteristic elegance. The above example was suggested by his paper. You will find there many other references for `moment problems'. h(x) 27r R Appendix A9. Proof of a Two-Envelopes' result \u0000 501 Appendix A9. Proof of a 'Two-Envelopes' result Result 400(M4) — under the 'bounded-ratio' assumption — is an immediate consequence of the following result. Suppose that (X1, Y1), (X2, Y2), (X3, Y3), ... are IID RVs with values in (0, oo)2 each with the same distribution as a (0, oo) 2-valued RV (X, Y). Suppose that for some finite constant K and some constant c, we have, with probability 1, Y < K X, and E (Y I X) > cX. Then, for some absolute constant L with c < L < K, Yi + Y2 + • + Yn lim sup \u0000 = L, a.s.. ± X2 ± • ± X, Proof. By the 'Independence Golden Rule' 402(Nb), E \u0000 ,X2) = E (Yi 1)(1) whence, E (Yi +Y2 I X1, X2) > c(Xi + X2), a.s.. Hence by the 'taking out what is known' Golden Rule 389(E1), E Yi ± Y2 X1 + X2 X1 , X2 ) = \u0000 1 E (Yi + Y21, X2) > c, a.s., X l + X2 so that by obvious extension and the main Golden Rule 390F2, ( Y1 ± Y2 ± \u0000 Yn > c. ± X2 + • ' ' Xn) The remainder of the proof uses results found (for example) in [W]. Because Y < K X, the ratio in the last equation above is bounded within [0, K]. We may therefore use the `Reverse Fatou Lemma' [W; 5.4(b)] to deduce that 1E sup \u0000 + Y2 + \u0000 Yn > lira E \u0000 + Y2 + \u0000 Yn \u0000 i sup \u0000 > c. x1+X2+-•-+Xn) \u0000 + X2 + • • + xn But the lim sup is in the tail o-algebra [W; 4.10 — 4.12] of the independent sequence (X1, Y1), (X2, Y2), (X3, Y3), ..., whence, by Kolmogorov's Zero-One Law [W; 4.11, 14.3], the urn sup is a deterministic constant L (possibly infinity). That c < L < K is now obvious. ❑ > cXi , a.s., APPENDIX B DISCUSSION OF SOME SELECTED EXERCISES `Discussion' signifies that this chapter contains further hints for certain exercises and, in the case of just a few of them, some extra information to 'fill out the picture'. I regard reading the solution to a problem without having tried really hard to do it as a learning opportunity missed. I therefore hope that you will consult this chapter mainly for confirmation of your own answers and for the occasional extra discussion. Section 1.1 130a. In (b), Prob(ii)= 3n(n — 1)/n3. The answer to (c) is Th(2) 2) = (r 1 n— 1 n— 2 \u0000 n— r+ 2 n'' 2) n the left-hand side by 'counting', the right by 'conditioning'. It is intuitively obvious that any tendency of birthdays not to be spread uniformly throughout the year will increase the chance that some two out of r people will have the same birthday. Suppose that for the huge underlying population the actual proportion of birthdays falling on the ith day in the year is pi. Then, the probability that r people chosen randomly from the whole population will have all birthdays different will be E*pilpi2 • • • Pi, the * signifying that the sum is over all n Pr r-tuples (i1, i2, \u0000 , ir ) of distinct values. This function can be shown to take its maximum value when each pi is 1/n. If you know about Lagrange multipliers, you will be able to prove this. 16Ra. Using the binomial theorem and ignoring 0.01k for k > 2, we have 1 \u0000 1 \u0000 1 \u0000 x \u0000 1 1 — 0.99x + \u0000 = 1 (1 0.01) + -x- R-2, 1 [1 x x 0.01] + \u0000 = \u0000 + 100 x B: Discussion of selected exercises \u0000 503 The last expression is minimized when 1/100 - 1/x2 = 0, so x = 10. A calculator shows that x = 11 is very slightly better for the correct expression, giving expected number of tests per person equal to 0.1956, an 80.44% saving. Section 1.2 22Gb. God tosses a coin N times per minute. A customer arrives when God gets a Tail. On average A people arrive per minute, so, for the coin, q P \u0000 N (Tail) = — A \u0000 ' \u0000 p := P \u0000 A (Head) = 1 - q = 1 - N . An interval c without an arrival corresponds to n = Nc Heads in a row. 22Gc. (a) We wait for the first Head. The next result (H or T with probability 2 each) decides whether or not HH or HT occurs first. Thus, the answer is (b) The only way in which HH precedes TH is if the first two tosses produce HH. Think about it! The answer is 1. 22Gd. What is the chance that the first toss after the first Head produces a Head? What does that tell you about a key case? Section 2.2 41G. IP(exactly one professor gets the right hat) = Po,n-1. The answer to Part (b) is (1/r!)po,n-r• Section 3.2 54Ea. f R(r) = FR(r) = 2r. This makes sense. We should have fR(r) oc r because a circle of radius r has perimeter proportional to r; and of course if fR(r) cc r, then we must have f R(r) = 2r because f has to integrate to 1 over [0, 1]. 56G. The needle crosses the line if and only if Y < sin O. Buffon did the first Monte- Carlo calculation! Section 3.3 58Dc. The answer is 'obviously' 11 times the expected number of Heads on the first two tosses, and so is 3/2. You check that this tallies with the formula for px at Exercise 52Da. Section 3.4 64Ja. Let the points be (X, Y) and (U,V). Then E(XU) = 0 by symmetry, and, from Exercise 54Ea, E (X2 + Y2) = 1, giving the answer. 70Jb. I do give the full solution to this important problem. We use throughout the fact thatE(W 2)=pyy +cr,. Part (a). We have MSE = (ity - c)2 + 4, and the desired result is immediate. Part (b). Now, MSE = {Ay - (cektx + 0)}2 Var(Y - aX) = {AY (cektx + 13)}2 + 4 - 2apuxay + ce24. 504 \u0000 B: Discussion of selected exercises Clearly, we must take p= µY —aitx, and then MSE = QY — 2apo-xo-y + a2oX \u0000 (ao.x _ pay)2 ± 4 (1 p 2) so that we choose a = pay/ax and then MSE = (1—p2). If p = ±1, then MSE = 0, so that, with probability 1, Y = aX + 0 for the chosen a, /3. You could, of course, have done as \u0000 a — a (msE) = 0 = — ap (MSE). 70Ka. We can imagine that someone just gave out the hats at random. Thus, whenever i j, we have E (Xi Xi ) = E (Xi X2). Note that since X, = 0 or 1, we have X? = Xl and E (Xi X2) = P(Xi X2 = 1) = lP(X1 = 1, X2 = 1). If one professor has the right hat, then it is more likely that another has. 71Kd. Show that p(Y, Z) > h(a) and choose the best a. Section 4.1 76E. The intuitive idea for the later part is that for final configurations for which i is isolated, 'what happens to the left of i is independent of what happens to the right of i'. Think about this. This leads to Pi,n = Pi,iPn—i+1,n—i+1 = PiPn—i+1, the pn—i+1,n—i-F1 being the probability that i ends up isolated if we start with the row i + 1, \u0000 , n of sites. Of course, it is false that the probability that [i — 1, i] ends up occupied is (1 — pi)Pn—i+1, for now 'there is interference between the two sides of i'. I have warned you that one of the most common mistakes is to assume independence when it is not present. Because you may have niggling doubts about the 'partial independence' claimed above, I give the following separate argument which will surely clinch the matter for you. Decomposing the event F that site i ends up isolated according to which of the positions (compatible with F) [1, 2], \u0000 [2, 3], \u0000 , \u0000 [i — 2, i — 1], [i + 1, i + 2], [i + 2, i + 3], ... , [n — 1, 4 the first driver occupies, we find that (n — 1)pi,n = (Pi- 2,n- 2 \u0000 • • + P1,n —i+1) \u0000 (Pi,i Pi,i+1 \u0000 • • • + Pi,n- 2) Let S(k) be the proposition that for 1 < m < k and 1 < i < rn, \u0000 = PiPm—i+1. Then S(1) is true. If we assume that S(n — 1) is true then we have, using the earlier recurrence relation for the pk, for 1 < i < n, (n 1)Pi,n = (Pi-2 + • • • + Pl)Pn-i+1 + Pi (P1 • • • ± Pn-i+1) = (i 1)PiPn-i+1 ± Pi (n i)Pn-i+1, B: Discussion of selected exercises \u0000 505 so that pi,. = Pip.-z+1 and S(n) is true. Hence S(n) is true for all n by induction. 90Pa. Here's a full solution. We make the table H(i). H {John, Mary} P(H) = P(H n K) = P(H n K n Si ) = P(H n K n Si n E2) = { aa,aa} p4 0 0 0 { aa,aA } 4p3 q 0 0 0 { aa,AA} 2p2q2 0 0 0 { aA,aA} 4p2q2 4p2q2 3p2q2 4p2q2 {aA,AA} 4pq3 4pq3 4pq3 0 {AA,AA} q4 q4 q4 0 Table H(i): Re: children of John and Mary For the {aA, aA} row, for instance, we have P(K H) = 1, P(Si IH n K) = 4, P(E2 IllnKn S1) P(E2 H) = and the General Multiplication Rule allows us to fill in the entries. Hence, P(K) = 4p2q2 + 4pq3 + q4 = q2 (4p2 + 4pq q 2) q2 ( 2p ± q) 2 q2(1 + p) 2 Similarly, P(K n Si) = 3p2 q2 4pq3 q4 q2 (3p2 4pq q2 ) q2 (3p q) \u0000 q2 + 2 p) . Thus, the answer to Part (a) is P(SiLIK) = 1 + 2p (1 + p) 2 The answer to Part (b) is clearly 4 because, given that their first child is exceptional and that John and Mary themselves are standard, they must both be aA. We have P(K n Sl n E2) = 4p2q2, whence the answer to Part (c) is 2 2 \u0000 3p2 P(E2 K n Si ) = TIP q q2 (1 ± 2p) \u0000 4(1 + 2p) • Section 4.2 97D. Convince yourself that we can assume that A beats B, C beats D, and A beats C 506 \u0000 B: Discussion of selected exercises without affecting the answer. [[First label the people as —2, —1,0,1,2, and let W(i, j) be the winner of the `i versus j' game. Let A = W(W(-2, —1), W(1,2)), B = 2/A, C = W(—A, — B), D = 2/C, to label the people as A,B,C,D,E. This nonsense cannot affect the probabilities for the remaining games.]] AB C DE A — W W B L — C L — W D L — E Table H(ii): 'Five-Nations Table' The W in the '(A,B)' position in Table H(ii) signifies that A Wins against B, so B Loses against A: the table is antisymmetric. There are 27 = 128 ways of completing the table, all equiprobable by independence. But, as you can check, there are only 3 ways of completing the table such that each team wins two games. 100J. In the last part, P(BP) = p-28 . The event n (Bp) occurs if and only if for no prime p is it true that p divides both X and Y, that is, if and only if H = 1. Thus, P(H -= 1) = 11(1 P-28) = C(2s)-1. You argue that P(H = m) -= m-2sP(H = 1). Section 4.3 103C. With probability 1, precisely one of X1, X2, . , X„ is the largest, and, by symmetry, each has chance 1/n of being the largest. Hence ]P(En ) = 1/n. Consider JP(E1 n E3 n E6), remembering that if E1 n E3 n E6 occurs, then E4 (for example) may also occur. Think of how many ways the Xk's may be arranged in increasing order consistent with (E1 n E3 n E6). We must have X l < X3 < X6. There are 2 places where we can 'insert X2' : we have either X1 < X2 < X3 < X6 or X2 < X1 < X3 < X6. (We cannot have X2 > X3 or else E3 would not occur.) For each of the two arrangements above, there are 4 places where we can insert X4. Then there will be 5 places where we can insert X5, so that P(Ei n E3 n E6) = 2 x 4 x 5 -= 1 x 3 —1 6 x — 1 = P(Ei )P(E3)11/(E6)• 6! B: Discussion of selected exercises \u0000 507 You can now see why the desired result is true. 103D. I leave you to relate this to the previous exercise! Discussion. (This discussion, which assumes a number of results, illustrates that even a simple probability model can be very rich in terms of the mathematics it supports.) The situation where, as here, P(Li = oo) = 0 but in which the tail of the distribution of L1 is so large that E (L1) = oo, is surprisingly common: we shall see several more examples. Suppose that one repeated the whole experiment, with strings of convoys for each experiment. Let L1 (k) be the length of the first convoy in the kth experiment. The fact that E (L1) = oo means that instead of 'stabilizing to a finite value', the 'average length of the first convoy over the first K experiments', Ai (K) Li (1) ± Li (2) - • • ± Li (K) K := converges to infinity as K \u0000 co. It follows from the Second Borel—Cantelli Lemma 98E that, with probability 1 there will be infinitely many values of K for which L1 (K) > K ln(K), and Al (K) is clearly greater than ln(K) for those values. Of much greater interest is what happens to the sequence L1, L2, ... of lengths of successive convoys in a single experiment. With probability 1, C (n) \u0000 1 ln(n) where C(n) is the number of convoys formed by the first n cars. Roughly speaking, therefore, L1 + L2 + Ln en. Convoys are tending (rapidly) to get longer and longer because they are headed by slower and slower drivers. 116Ya. As I said, this is very instructive from the point of view of understanding 'about w', but it is quite tricky. Recall that we assume that F is continuous and strictly increasing. Let x,,,, where r E N and 0 < i < r, be the unique solution of the equation F(xi,r) = i/r. Then, because there are only countably many (where r E N and 0 < i < r), the set G of those w for which Fn(xi,r,Y(w)) _+ F(xi,r) whenever r E N and 0 < i < r, is an event of probability 1. We need to prove that for w E G, Fn(x;Y (w)) \u0000 F(x) uniformly over x E R. So, let w E G, and let E > 0 be given. Choose an integer 7.0 such that 7.0 > 2/E. The set of points {xi,r0 , x2 ,To 7 • • • , XT0-177.0 is finite, so, by definition of G, there will exist no (w) such that for n > no (w), we have Fn(xi,ro;Y(w)) F(xi,r0)1 < 1E whenever 0 < < ro and n > no(w). 508 \u0000 B: Discussion of selected exercises We now make the obvious definitions xo,r„, = —oo, x7.0,r0 = co. 'Every kind of F' is 0 at —oo and 1 at oo. For any x E 18, we have for some i with 1 < i < Xi-1,r° \u0000 X FT,(X2,-1,r0; Y(W)) \u0000 Fn(X; Y(W)) < Fri(Xx,r0; Y ( W)) F(xi_i,ro) \u0000 < \u0000 F(x) \u0000 < \u0000 F(xi,ro). So, for n > no (w), we have (with the lack of moduli being deliberate) Fn(x;Y(w)) — F(x) < Fn(xi,ro;Y(w)) — F (xi-1,r° ) = Fn(xi,r 0;Y(w)) — F(xi,ro ) F(xi,ro) — F(xi-i,r 0) < and, by an analogous argument, Fn(x;Y (w)) — F(x) > —E. The result is proved. To do the general case is a bit messy. There is a neat trick, but it would not be helpful to present it here. Section 4.4 123Ga. Let x = Po (hit A). Then x = + 4x 2, so that x = 1 or x = 1. You use the argument at 119Da to prove that x = 3. (That x cannot be 1 can also be seen from the argument at the end of this solution. Next, y := Po (hit A before B) = 1.1 + 1.0 + 11Pc (hit A before B) = + 1xY, so that y = 3/10. We have Po (hit both A and B) = 2y x x2 = A. Now let f i := Pi (hit H before either A or B). Then, fo = Po (hit D before either A or B).fn • But r := 'Po (hit D before either A or B) = 1 + 1xr, and r = 3/11. The remaining equation needed to find fo is that fD = 1 + Ifo + If the current reduced word has length at least 1, then its length will increase by W after the next step, where JP(W = 1) = 1 and JP(W = —1) = 1. So, the answer B: Discussion of selected exercises \u0000 509 regarding c is that c = E (W) = 2. But do give some thought to the difficulty that the Random Walk may visit 0 (the word of length 0) several times. The length forms a reflecting Random Walk with p = 4 on the set Z+. 126La. To prove that P(B is never in the lead) = 1 is actually easier than the first part. Section 4.6 1391c. Here's my program. This one does use pointers as a neat way of choosing (X, Y) in the disc via InDisc (&X , &Y), &X being the address at which X is stored. Sorry! /* SimDisc.c cc SimDisc.c RNG.o -o SimDisc -lm */ #include <stdio.h> #include \"RNG.h\" #define N 100000 double ssq(double a, double b){return a*a + b*b;} void InDisc(double *x, double *y){ do{*x = 2.0*Unif()-1; *y = 2.0*Unif()-1;} while (ssq(*x, *y) >1); } int main(){ long i; double sumDsq = 0.0, X,Y,U,V; setseeds(); for(i=0; i<N; i++){ InDisc(&X,&Y); InDisc(&U,&V); sumDsq += ssq(X-U, Y-V); } ShowOldSeeds(); printf(\"\\nEstimated mean of D squared = %6.4f\\n\", sumDsq/N); return 0; } /*RESULTS: Generator used was Wichmann-Hill(3278, 29675,9671) Estimated mean of D squared = 1.0012 */ Section 5.3 149Da. We have 00 r(z) = f \u0000 = • =_- \u0000 f yo(y)dy, a+ 1 0° \u0000 AKxK—le —As ea. \u0000 \u0000 dx = f(K) what? x co (A c ).pcxx—i e —(A—cx)s r(K) \u0000 dx. 510 \u0000 B: Discussion of selected exercises on putting x = 2y2 and using the symmetry of cp. 150Eb. We have, for a < A, Section 6.2 177Da. We have, with roughly 95% probability, IY - Al < You can say that you are 95% confident that (A — Vobs)2 < 4A/n and that A therefore lies between the roots of the obvious quadratic, or, in more rough- and-ready fashion, use [Yobs Yobs] n Yobs — \u0000 9 7 Yobs + 7/ 178Eb. For the ith individual chosen, let {1 if the person says he/she will vote for Party 1, with Y. the analogous Variable for Party 2. Then E (Yi — \u0000 = P2 — P1, E [(Yi — Xi)2] = E (Yi2) + E (JO — 2E (XiYi) = P2 +pi — 0, and Var(Y, — Xi) = p2 + pi — (pi —p2) 2. Note the sense of this when P1 = 0. The variables Y1 — X1, Y2 — X2, ... are IID and so S2 — S1 P2 + — (Th. —p2) 2 77, \u0000 IN ) (p2 pi, :-= 0 otherwise, As a rough and ready guide, you could pretend in estimating the variance that pi is Si pact vn. B: Discussion of selected exercises \u0000 511 Section 6.3 186Ga. We have P(M < m) = ll(Y1 < m; ...; Yr, < m) = (B) n which tells Zeus how to choose M. Tyche lets Z1 = M and then chooses n — 1 Variables Z2, Z3, • Zn independently, each with the U[0, M] distribution. She then lets Y1, , Yr, be a random permutation of Z1, Z2, • • • , Zn • Section 6.4 188Bc. Let T = T(Y). Then the desired unbiasedness property says that 1 e — 1 Y‘ •/-(y) \u0000 = (1 — e —°), y=1 so that oo By \u0000 9 2n Er (y)-7 = 2 cosh 0 — 2 = 2 2_, \u0000 Y! \u0000 (2n)! y=1 \u0000 n=1 Section 6.5 198Fa. We have \u0000 f Inf (h(z ))h\" \u0000 f (h(z))h, (z) dz g(h(z))h' (z) rh(z)) h(z)) ) f (h(z))/t/ (z) dz = f in (1 (Y) (y) f(y) dy, g g the last by change of variables. 199Ha. We have \u0000 E0 h(Y) (,o(Y ) \u0000 )E0 (ii'(17 h(Y) )Y) = \u0000 h (y) \u0000 dy = • • • Section 6.6 206Gd. By differentiation, f ,,(y 17) \u0000 1 L Y ; 00 \u0000 — But if Y has pdf f (y 7) and Z := (Y — 00)1 -y, so that Y = h(Z) := 00 + -y Z , then fz(z) = fy (h(z))11,' (z) =1g(z)7 = g(z). 213La. We have IP(N = k; O E dB; Y E dy) = p(k)71-k(0)1hd(O; y)dOdy. 512 \u0000 B: Discussion of selected exercises It is all easy if you keep a cool head. 212Kc. For a Frequentist, Y„,±1 - Y (where Y is the average of Y1, - - Y 2, • • , Yn) has the normal distribution of mean 0 and known variance o-g(n-1 + 1). 221Pb. Find the appropriate inequality which must be satisfied by a mixture. Section 7.4 263Ea. You check that (with t := min yi and t + a = max yi) lhd(9; y) = 1[0_1,0_1 21(t)1[0_ ,e+21(t + a), so that (T, A) is Sufficient for O. You may assume that this pair is Minimal Sufficient (how could it not be?). Clearly A has the distribution of the range of an IID sample, each U[0, 1]. So, let U1, U2, . . . , Un be IID each U[0,1]. Let V := min Uz, W = max (Note. Whether we deal with open or closed intervals is irrelevant in the following discussion.) Then, for 0 < v < w < 1, P(V > v, W < w) = P(U„, E (v, w) for every i) = (w - v)n. Hence, ll \u0000 P(V > v; W dw) \u0000 ( (w - v) n dw = n(w - v)n-l dw, a \u0000 11)(7 E dv; W E dw) \u0000 - 0vn(w - v)n-1 dvdw = n(n - 1)(w - v)n-2dvdw, \u0000 fv,w (v,w) \u0000 n(n - 1)(w - v)n-2 /{0<v<„,<1}• Let B = W - V, C = V. Then v = c, w = b + v, the Jacobian is 1 and so c) = n(n - i_)bn 2-/(0<b<1, 0<c<1-6}) whence 1—b \u0000 fB(b) = (f \u0000 f B,c (b, ode) i(0 ,1) (b) = n(n - 1)bn-2 (1 - b)I (04) (b), giving the required pdf of A. Back to T, A, Y. We now know about Stage 1 where Tyche chooses A. Since the conditional probability of C given B is U[0, 1 - .13], the conditional distribution of T - (9 - given A is U[0, 1 - A]. Thus, conditionally on A = T is uniform on [9- 2, 0 +1- dths]; and we now understand Stage 2. For Stage 3, Tyche chooses numbers zrt 4.ct, \u0000 znaa2 independently and uniformly on (tobs, tabs + aobs ) and then lets yTbs yps, \u0000 y _ nobs t- ue a random permutation of t obs tobs + a \u0000 z1 ztct, z rt, \u0000 zai.ct2. B: Discussion of selected exercises \u0000 513 Section 8.3 327La. We have Ho : IL = pt101-E 1 00±o-G, HA : p, = pl 01 -Fcto 1 +1 00±o-G, U = [1] 0 [1] + [1] 0 [1]± = [1] 0 RI c , Pu = P - mean ± PB eff) W = U+ W I 0 [1], Z = 1111 0 [1], Z I U, PZ = PA eff- 336Pa. If we change notation and think of the errors for successive months as E1, E2, E3, . . ., then it might be better to use an IID sequence ni , 72 , 773 , ... each N(0, a2), let c e (0,1) be an unknown parameter, and then let E1 = 7117 En = EEn-i + V1 — C2 Tin, the sort of autoregressive model used in books on the theory and practice of Time Series. Chatfield [38] and Brockwell and Davis [33] are fine introductions. Box and Jenkins [29], Brillinger [32], Brockwell and Davis [34], and Priestley [188] are classics. For important new ideas, see Tong [229]. Our geometry can cope with the time-series model, but at the cost of considerable complexity. Of course, WinBUGS can deal with it. APPENDIX C TABLES The following tables are included. • Table of Distribution Function 4 of the standard normal N(0, 1) distribution, calculated as explained in Subsection 158C. • Table of upper percentage points for t,,, Student's t distribution with v degrees of freedom, listing t such that IP(T > t) = 77 or ID(ITI > t) > 71 for various values of i and v, where T — ty. This table was calculated as explained in Subsection 253G. • Table of upper percentage points for the )( 2 distribution with v degrees of freedom, calculated as explained at 149Ea, using the fact that xi2, = Gamma( 1 v, rat e 2). • Table of upper 5% points for Fisher's F,,, distribution, calculated as explained at 252F and 210Id. I am reasonably sure that the tables are correct! Tables \u0000 515 Table of Distribution Function (I) of the standard normal N(0, 1) distribution 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767 2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936 1r) \u0000t-- 00 CS 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986 3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990 3.1 0.9990 0.9991 0.9991 0.9991 0.9992 0.9992 0.9992 0.9992 0.9993 0.9993 Example. We have 4,(1.63) = 4,(1.6 + 0.03) = 0.9484. One can use linear interpolation: 4)(1.634) \u0000 4,(1.63) + 0.4 {4,(1.64) - 4,(1.63)} , not that such accuracy is ever relevant in practice! 'Tail percentage points' may be read off from the last row of the 't' table, since the toi and N(0, 1) distributions are the same. 516 \u0000 Tables Upper percentage points for ti,, Student's t-distribution with 11 degrees of freedom P(ITI > t) 50.0% 20.0% 10.0% 5.0% 2.0% 1.0% 0.2% P(T > t) 25.0% 10.0% 5.0% 2.5% 1.0% 0.5% 0.1% V I t t t t t I 1 1.000 3.08 6.31 12.71 31.82 63.66 318.31 2 0.816 1.89 2.92 4.30 6.96 9.92 22.33 3 0.765 1.64 2.35 3.18 4.54 5.84 10.21 4 0.741 1.53 2.13 2.78 3.75 4.60 7.17 5 0.727 1.48 2.02 2.57 3.36 4.03 5.89 6 0.718 1.44 1.94 2.45 3.14 3.71 5.21 7 0.711 1.41 1.89 2.36 3.00 3.50 4.79 8 0.706 1.40 1.86 2.31 2.90 3.36 4.50 9 0.703 1.38 1.83 2.26 2.82 3.25 4.30 10 0.700 1.37 1.81 2.23 2.76 3.17 4.14 11 0.697 1.36 1.80 2.20 2.72 3.11 4.02 12 0.695 1.36 1.78 2.18 2.68 3.05 3.93 13 0.694 1.35 1.77 2.16 2.65 3.01 3.85 14 0.692 1.35 1.76 2.14 2.62 2.98 3.79 15 0.691 1.34 1.75 2.13 2.60 2.95 3.73 16 0.690 1.34 1.75 2.12 2.58 2.92 3.69 17 0.689 1.33 1.74 2.11 2.57 2.90 3.65 18 0.688 1.33 1.73 2.10 2.55 2.88 3.61 19 0.688 1.33 1.73 2.09 2.54 2.86 3.58 20 0.687 1.33 1.72 2.09 2.53 2.85 3.55 21 0.686 1.32 1.72 2.08 2.52 2.83 3.53 22 0.686 1.32 1.72 2.07 2.51 2.82 3.51 23 0.685 1.32 1.71 2.07 2.50 2.81 3.49 24 0.685 1.32 1.71 2.06 2.49 2.80 3.47 25 0.684 1.32 1.71 2.06 2.49 2.79 3.45 30 0.683 1.31 1.70 2.04 2.46 2.75 3.39 35 0.682 1.31 1.69 2.03 2.44 2.72 3.34 40 0.681 1.30 1.68 2.02 2.42 2.70 3.31 45 0.680 1.30 1.68 2.01 2.41 2.69 3.28 50 0.679 1.30 1.68 2.01 2.40 2.68 3.26 55 0.679 1.30 1.67 2.00 2.40 2.67 3.25 60 0.679 1.30 1.67 2.00 2.39 2.66 3.23 65 0.678 1.29 1.67 2.00 2.39 2.65 3.22 70 0.678 1.29 1.67 1.99 2.38 2.65 3.21 75 0.678 1.29 1.67 1.99 2.38 2.64 3.20 Do 0.678 1.28 1.64 1.96 2.33 2.58 3.09 Example. If T has the t16 distribution, then the value t such that P(T > \u0000 = 2.5% (equivalently such that IP(ITI > t) = 5%) is t = 2.12. Tables \u0000 517 Upper percentage points for x2 = Gamma(lv, mean 2), the x2 distribution with v degrees of freedom v 99.0% 97.5% 95.0% 50.0% 10.0% 5.0% 2.5% 1.0% 1 (0.0002) ( 0.001) ( 0.004) 0.45 2.71 3.84 5.02 6.63 2 ( 0.020) ( 0.051) ( 0.103) 1.39 4.61 5.99 7.38 9.21 3 ( 0.115) ( 0.216) ( 0.352) 2.37 6.25 7.81 9.35 11.34 4 ( 0.297) ( 0.484) ( 0.711) 3.36 7.78 9.49 11.14 13.28 5 ( 0.554) ( 0.831) ( 1.145) 4.35 9.24 11.07 12.83 15.09 6 ( 0.872) 1.24 1.64 5.35 10.64 12.59 14.45 16.81 7 1.24 1.69 2.17 6.35 12.02 14.07 16.01 18.48 8 1.65 2.18 2.73 7.34 13.36 15.51 17.53 20.09 9 2.09 2.70 3.33 8.34 14.68 16.92 19.02 21.67 10 2.56 3.25 3.94 9.34 15.99 18.31 20.48 23.21 11 3.05 3.82 4.57 10.34 17.28 19.68 21.92 24.73 12 3.57 4.40 5.23 11.34 18.55 21.03 23.34 26.22 13 4.11 5.01 5.89 12.34 19.81 22.36 24.74 27.69 14 4.66 5.63 6.57 13.34 21.06 23.68 26.12 29.14 15 5.23 6.26 7.26 14.34 22.31 25.00 27.49 30.58 16 5.81 6.91 7.96 15.34 23.54 26.30 28.85 32.00 17 6.41 7.56 8.67 16.34 24.77 27.59 30.19 33.41 18 7.01 8.23 9.39 17.34 25.99 28.87 31.53 34.81 19 7.63 8.91 10.12 18.34 27.20 30.14 32.85 36.19 20 8.26 9.59 10.85 19.34 28.41 31.41 34.17 37.57 21 8.90 10.28 11.59 20.34 29.62 32.67 35.48 38.93 22 9.54 10.98 12.34 21.34 30.81 33.92 36.78 40.29 23 10.20 11.69 13.09 22.34 32.01 35.17 38.08 41.64 24 10.86 12.40 13.85 23.34 33.20 36.42 39.36 42.98 25 11.52 13.12 14.61 24.34 34.38 37.65 40.65 44.31 30 14.95 16.79 18.49 29.34 40.26 43.77 46.98 50.89 35 18.51 20.57 22.47 34.34 46.06 49.80 53.20 57.34 40 22.16 24.43 26.51 39.34 51.81 55.76 59.34 63.69 45 25.90 28.37 30.61 44.34 57.51 61.66 65.41 69.96 50 29.71 32.36 34.76 49.33 63.17 67.50 71.42 76.15 55 33.57 36.40 38.96 54.33 68.80 73.31 77.38 82.29 60 37.49 40.48 43.19 59.33 74.40 79.08 83.30 88.38 65 41.44 44.60 47.45 64.33 79.97 84.82 89.18 94.42 70 45.44 48.76 51.74 69.33 85.53 90.53 95.02 100.43 75 49.48 52.94 56.05 74.33 91.06 96.22 100.84 106.39 80 53.54 57.15 60.39 79.33 96.58 101.88 106.63 112.33 90 61.75 65.65 69.13 89.33 107.57 113.15 118.14 124.12 100 70.06 74.22 77.93 99.33 118.50 124.34 129.56 135.81 (Brackets surround entries which are 'misaligned' through being given to more than 2 decimal places.) Example. If H has the x?.3 distribution with v = 13 degrees of freedom, then the value c such that 1P(1/ > c) = 5% is 22.36. Important Note. See 164Ka. 518 \u0000 Tables Upper 5% points for the F((num) vl, (den) zi2) distribution vl 1 2 3 4 5 6 7 8 9 10 1 161.45 199.50 215.71 224.58 230.16 233.99 236.77 238.88 240.54 241.88 2 18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.38 19.40 3 10.13 9.55 9.28 9.12 9.01 8.94 8.89 8.85 8.81 8.79 4 7.71 6.94 6.59 6.39 6.26 6.16 6.09 6.04 6.00 5.96 5 6.61 5.79 5.41 5.19 5.05 4.95 4.88 4.82 4.77 4.74 6 5.99 5.14 4.76 4.53 4.39 4.28 4.21 4.15 4.10 4.06 7 5.59 4.74 4.35 4.12 3.97 3.87 3.79 3.73 3.68 3.64 8 5.32 4.46 4.07 3.84 3.69 3.58 3.50 3.44 3.39 3.35 9 5.12 4.26 3.86 3.63 3.48 3.37 3.29 3.23 3.18 3.14 v2 10 4.96 4.10 3.71 3.48 3.33 3.22 3.14 3.07 3.02 2.98 11 4.84 3.98 3.59 3.36 3.20 3.09 3.01 2.95 2.90 2.85 12 4.75 3.89 3.49 3.26 3.11 3.00 2.91 2.85 2.80 2.75 13 4.67 3.81 3.41 3.18 3.03 2.92 2.83 2.77 2.71 2.67 14 4.60 3.74 3.34 3.11 2.96 2.85 2.76 2.70 2.65 2.60 15 4.54 3.68 3.29 3.06 2.90 2.79 2.71 2.64 2.59 2.54 20 4.35 3.49 3.10 2.87 2.71 2.60 2.51 2.45 2.39 2.35 30 4.17 3.32 2.92 2.69 2.53 2.42 2.33 2.27 2.21 2.16 60 4.00 3.15 2.76 2.53 2.37 2.25 2.17 2.10 2.04 1.99 120 3.92 3.07 2.68 2.45 2.29 2.18 2.09 2.02 1.96 1.91 co 3.84 3.00 2.60 2.37 2.21 2.10 2.01 1.94 1.88 1.83 vi. 11 12 13 14 15 20 30 60 120 oo 1 242.98 243.91 244.69 245.36 245.95 248.01 250.10 252.20 253.25 254.31 2 19.40 19.41 19.42 19.42 19.43 19.45 19.46 19.48 19.49 19.50 3 8.76 8.74 8.73 8.71 8.70 8.66 8.62 8.57 8.55 8.53 4 5.94 5.91 5.89 5.87 5.86 5.80 5.75 5.69 5.66 5.63 5 4.70 4.68 4.66 4.64 4.62 4.56 4.50 4.43 4.40 4.36 6 4.03 4.00 3.98 3.96 3.94 3.87 3.81 3.74 3.70 3.67 7 3.60 3.57 3.55 3.53 3.51 3.44 3.38 3.30 3.27 3.23 8 3.31 3.28 3.26 3.24 3.22 3.15 3.08 3.01 2.97 2.93 9 3.10 3.07 3.05 3.03 3.01 2.94 2.86 2.79 2.75 2.71 v2 10 2.94 2.91 2.89 2.86 2.85 2.77 2.70 2.62 2.58 2.54 11 2.82 2.79 2.76 2.74 2.72 2.65 2.57 2.49 2.45 2.40 12 2.72 2.69 2.66 2.64 2.62 2.54 2.47 2.38 2.34 2.30 13 2.63 2.60 2.58 2.55 2.53 2.46 2.38 2.30 2.25 2.21 14 2.57 2.53 2.51 2.48 2.46 2.39 2.31 2.22 2.18 2.13 15 2.51 2.48 2.45 2.42 2.40 2.33 2.25 2.16 2.11 2.07 20 2.31 2.28 2.25 2.22 2.20 2.12 2.04 1.95 1.90 1.84 30 2.13 2.09 2.06 2.04 2.01 1.93 1.84 1.74 1.68 1.62 60 1.95 1.92 1.89 1.86 1.84 1.75 1.65 1.53 1.47 1.39 120 1.87 1.83 1.80 1.78 1.75 1.66 1.55 1.43 1.35 1.25 cx) 1.79 1.75 1.72 1.69 1.67 1.57 1.46 1.32 1.22 1.00 APPENDIX D A SMALL SAMPLE OF THE LITERATURE These days, there are so many brilliant young (and youngish) probabilists and statisticians around that I am not going to single any out. Tributes to people who helped develop the fields are more 'classical'. But, of course, many of the books mentioned are modern. The Bibliography is fairly extensive, though still a small fraction of what it could/should have been. For everyone, there are many books in the Bibliography of great interest. And for those determined to strive, to seek, to find, but not to yield, the Bibliography contains — for inspiration — some books and papers (several of which were mentioned in the main text) which are at a significantly more advanced level than this book. Probability In the early days, Probability Theory was developed by Cardano, Galileo, Pascal, de Moivre, two Bernoullis, Bayes, Laplace, Gauss, Markov, Tchebychev. In the 20th century, the Measure Theory of Borel and Lebesgue allowed Borel, Wiener, Kolmogorov [140], Doob [66] and others to put Probability on a rigorous basis and to extend its scope dramatically. Levy [151, 152] provided much of the intuition and inspiration. Volume 1 of Feller's book [75], a little more challenging than this present one, will always hold a special place in the hearts of probabilists. It is quite challenging, and should perhaps be read after, or in conjunction with, this one. Pitman [184] is a good route-in as prerequisite for the Probability in this book, should you need one. At about the same level as this book are Grimmett and Stirzaker [103], Volume 1 of Karlin and Taylor [125], Ross [201, 202]. 520 \u0000 A small sample of the literature There are a lot of books on the measure-theoretic foundations of Probability. Neveu [172] has real class and French elegance, Breiman [30] is one of the all- time greats, and Williams [235] is mischievous. Other favourites on Probability: Chow and Teicher [41], Chung [43], Durrett [69], Fristedt and Gray [85], Laha and Rohatgi [146], Renyi [193]. The sad death of E T Jaynes means that only some of a fascinating book is complete. See [117]. (Skip his remarks about Feller, though.) Genetics. Elizabeth Thompson, daughter of Edward who so inspired many of us 'tutees' at Oxford, is a very considerable authority on Pedigree Analysis in Genetics. See [227, 228]. I have already enthused about [101] which shows in particular that there is much more to Genetics than Maths. Richard Dawkins' books have also had my very strong recommendation. Fisher wrote one of the most important books on Genetics since Darwin, now updated as [80], work further developed by W D Hamilton and others. Genetics has provided motivation for much Probability of a type too advanced for this book: measure-valued diffusions, etc. Find out on the Web what people such as Donald Dawson, Peter Donnelly, Stewart Ethier, Steven Evans, Thomas Kurtz, Edwin Perkins, ..., are doing. The FKPP equation of Fisher and of Kolmogorov, Petrovskii and Piscunov was a big motivating factor for branching processes in Probability and for much work in Analysis. Interestingly, the probabilistic methods of Bramson and others have here been able to beat the analytical at their own game. Of course, this is the era of the Human Genome Project about which you can find much information via the website: http://www.ornl.gov/hgmis/ . Random walks, Markov chains, stochastic processes. For these important topics, see Feller [75], Kemeny and Snell [131], Norris [176], Spitzer [216]. Markov chains play an important part in queueing theory; see Cox and Smith [52], and Kelly's fascinating book [130]. For stochastic processes generally and applications, see Bremaud [31], Cinlar [45], Cox and Miller [50], Volume 2 of Karlin and Taylor [125]. The theory of martingales is due principally to Doob [66], with some key contributions from Levy and Meyer. The books, Neveu [173], Hall and Heyde [107], Williams [235] and Rogers and Williams [199] give an idea of the scope, Dellacherie and Meyer [60] the definitive account. Martingales form the foundation for the very important technique of stochastic calculus created by the great Japanese mathematician Ito. Oksendal [178] is a nice introduction, as are Chung and (Ruth) Williams [44] and Durrett [70]. Revuz and Yor [194], Karatzas and Shreve [123], and Rogers and Williams [199], which also begin at A small sample of the literature \u0000 521 the beginning, take the theory further. For references for martingales in finance, see Subsection 421K. I became really enthusiastic about some topics in Probability, especially Statistical Mechanics, at the end of Chapter 9. It will surprise some people to realize that the topics I find most interesting (and have for a long time found most interesting) in Probability are ones which do not have to do with martingales! But of course, such topics are too hard for me to have been able to do anything with them, in spite of determined efforts. Frequentist Statistics Early Frequentist Statistics owed a lot to Galton, two Pearsons, 'Student' (W S Gosset), Neyman. The greatest of statisticians was Sir Ronald Fisher, and key work originated by him on ANOVA was developed by Yates and coworkers at the Rothamsted Experimental Station. As stated earlier, we owe several key concepts — sufficiency, ancillarity, likelihood, F tests, ... — to Fisher. See [79] for some of his ideas. Huge contributions to Statistics were later made by Bartlett, Cox, Tukey, Wald, Wilks and others. A great introductory book on Frequentist Theory is Freedman, Pisani and Purves [83]: how that book makes you think! Amongst classics are Kendall and Stuart (now modernized with the help of Ord and Arnold as [132, 133]), Cox and Hinkley [49] (which also has a substantial Bayesian section), Lehmann [149, 150]. I always liked Silvey [212] as a succinct account. See also Casella and Berger [37], Garthwaite and Jolliffe [88], Kalbfleisch [122]. On regression, see Draper and Smith [68], Montgomery and Peck [167], Myers [170], Rousseeuw and Leroy [200], Ryan [205] and Weisberg [234]. See further references under 320Fc. On ANOVA, which cannot really be separated from Experimental Design, see Cochran and Cox [46], Cox [48], Fisher and Bennett [79], Huitema [113], Mead [161], Snedecor and Cochran [214], Scheffe [207]. Cox and Reid [51] is an important recent book which, amongst many other things, explains how Galois fields and finite geometries feature in Experimental Design. Rosemary Bailey has in preparation a book showing how several other pieces of interesting mathematics also feature there. The theory of Linear Models expands to Multivariate Analysis (see Anderson [5], Chatfield and Collins [39], Everitt and Dunn [73], Fahrmeir and Tutz [74], Johnson and Wichern [121], Krzanowski and Marriott [143], Manly [156], Mardia, Kent and Bibby [157], Murtagh and Heck [169], Scott [210], Seber [211], for example). It is interesting to look at a book such as Tabachnick and Fidell [223], written for social scientists, and therefore with 522 \u0000 A small sample of the literature a somewhat different emphasis. The fact that its Contents section runs to 21 pages listing chapters on Multiple Regression, Canonical Correlation, Multiway Frequency Analysis, Analysis of Covariance (ANCOVA), Multivariate Analysis of Covariance (MANCOVA), Discriminant Factor Analysis, Principal Components and Factor Analysis, Structural Equation Modelling, etc, gives a clue to the degree of expansion the subject has witnessed. That book surveys some of the Frequentist computing packages available in 1996. The theory of Generalized Linear Models (see Dobson [65], McCullagh and Nelder [160]) keeps some features of Linear-Model theory, but allows a certain degree of non-linearity and the use of non-normal distributions. GENSTAT, GLIM, S-PLUS are amongst packages which can implement the theory. I gave references for time series at the end of Appendix B. Bayesian Statistics That the world has now to a large extent embraced the Bayesian view of Statistics is due to de Finetti [58], Good [99], Jeffreys [118], Savage [206], and, perhaps above all, to the missionary zeal (and good sense) of Lindley. Bayesian Statistics is well served in the literature: see Bernardo and Smith [17], Carlin and Louis [36], Lindley [153], Gelman, Carlin, Stern and Rubin [90], O'Hagan [177]. A nice introduction is provided by Lee [148]. I am very much looking forward to Draper [67]. The series of volumes [16] edited by Bernardo, Berger, Dawid and Smith provide the authoritative way of keeping up to date. The Bayesian approach was always elegant (if one accepts its philosophy). However, it only became of real practical use for complex models with the advent of MCMC computing. I have illustrated this both with bare-hands programs and via WinBUGS . Some key references were given at the end of Subsection 268C. See also Gamerman [87], Robert [197], Robert and Casella [198]. As is so often the case, we owe the original ideas to physicists. The structure of WinBUGS relies heavily on the theory of graphical models as in Lauritzen [147] (building inevitably on the work of physicists on Markov random fields). I have explained that MLwiN is a user-friendly package capable of doing some 'classical' and some MCMC work. I have probably been unfair to Decision Theory (Frequentist and Bayesian), but I cannot get to like it. See Berger [14], Chernoff and Moses [40], the relevant chapter of Cox and Hinkley [49], de Groot [59], the very influential Ferguson [76], French and Insua [84]. Some references for Differential Geometry in Statistics were given at 380Na. A small sample of the literature \u0000 523 On the question of model choice, see Akaike [2], Burnham and Anderson [35], Christensen [42] Gelfand and Ghosh [89], Mallows [155], Schwarz [209], Spiegelhalter, Best and Carlin [215], and references contained in these publications. See also the discussion at 320Fc for the case of Regression. Quantum Theory Amongst the great names of early Quantum Theory are Planck, Einstein, Schrodinger, Heisenberg, Bohr, Dirac, and, of course, though less early, Feynman. The rigorous probabilistic theory is due principally to von Neumann. Feynman's place as the greatest expositor of Science is likely to remain: see Volume III of [78] and the magical [77] which presents the alternative 'path- integral' formulation of standard Quantum Theory. I did greatly enjoy the popular books, Greene [100] and Penrose [183]. Just look at the magic-dodecahedron version of entanglement in the latter. As a guide to the mathematical theory, one of my favourites is Isham [115], and I am fond of the classic accounts, Davydov [56], Messiah [163], Merzbacher [162], Schiff [208] . For the full mathematical theory, see the great Reed and Simon [191], and Mackey [154]. For references for the Dirac equation, see the end of Section 10.5. For some connections between Quantum Theory and Probability and Statistics, see Helstrom [111], Holevo [112], Streater [219], and current work of Barndorff-Nielsen and Gill which you may track via the Web. The books by Meyer [165] and Parthasarathy [181] are of much interest to probabilists as they present the Hudson—Parthasarathy stochastic calculus, a non-commutative version of Ito's. Quantum computing The idea of quantum computing goes back to Manin and, later and independently, Feynman. Deutsch made a striking contribution, but what made the subject really take off was Schor's algorithm (which is presented in the Preskill, Gruska, and Nielsen and Chuang references now to be mentioned.) Most of what I have learnt about quantum computing has been from Preskill's marvellous notes [187], from Gruska's fine book [105], and from very many papers under Quantum Physics at the Los Alamos National Laboratory archive site http: //www.arXiv.org mirrored as http://uk.arXiv.org 524 \u0000 A small sample of the literature and mirrored at other sites too. I refer to these as quant - ph. I don't myself add quant - ph to the site address because there are lots of other interesting things on the site. After I had essentially completed my account, a superb book, Nielsen and Chuang [174], appeared. Read it. Also read Bouwmeester, Ekert and Zeilinger [28]. S- PLUS, R, Minitab, 14-TEX, Postscript, C, Emacs, WinEdt As already remarked, S-PLUS (see Krause and Olson [142], Venables and Ripley [232]) is the most popular package amongst academic statisticians, and R, of which a free version is available ([190]), contains many features of S-PLUS. For Minitab, see [204]. For L9TEX (Lamport [144]), I found Kopka and Daly [141] very useful. I had earlier learnt TEX from the master in Knuth [139]. I found Postscript ([1]) fun and, after a time, surprisingly easy to use, sometimes via a C-to-Postscript converter I wrote. I learnt 'C' from Kernighan and Richie [136] and Kelley and Pohl [129]. I did once learn something of Object Oriented Programming but haven't found it of any real use for my work. (OOPs, I'll get some flak for that! But static-variable modules seem to do things much more neatly.) I used the (free) Emacs and (very-low-cost) WinEdt (correct spelling!) to prepare the LATEX file. Both are excellent, and both downloadable from the Web. I particularly like WinEdt's 'live' correction of spelling. Bibliography [1] Adobe Systems, Inc, Postscript Language Reference Manual (second edition), Addison Wesley, Reading, Mass., 1993. [2] Akaike, H., Prediction and Entropy, Chapter 1 of [7]. [3] Aldous, D., Probability Approximations via the Poisson Clumping Heuristic, Springer-Verlag, Berlin, Heidelberg, New York, 1989. [4] Amari, S., Differential-Geometrical Methods in Statistics, Springer-Verlag, Berlin, Heidelberg, New York, 1985. [5] Anderson, T.W., An Introduction to Multivariate Statistical Analysis, Wiley, New York, 1958. [6] Athreya, K.B. and Ney, P., Branching Processes, Springer-Verlag, Berlin, Heidelberg, New York, 1989. [7] Atkinson, A.C. and Fienberg, S.E. (editors), A Celebration of Statistics : the ISI Centenary Volume, Springer-Verlag, Berlin, Heidelberg, New York, 1985. [8] Barndorff-Nielsen, O.E. and Cox, D.R., Asymptotic Techniques for Use in Statistics, Chapman and Hall, London, 1989. [9] Barndorff-Nielsen, O.E. and Cox, D.R., Inference and Asymptotics, Chapman and Hall, London, 1994. [10] Barnett, V. and Lewis, T.: Outliers in Statistical Data, Wiley, Chichester, New York, 1984. [11] Barron, A.R., Entropy and the Central Limit Theorem, Ann. Prob. 14, 336-343, 1986. [12] Baxter, R.S., Exactly Solvable Models in Statistical Mechanics, Academic Press, New York, 1982. [13] Baxter, R.J. and Enting, I.G., 399th solution of the Ising model, J. Phys. A., 11, 2463-2473,1978. 525 526 \u0000 Bibliography [14] Berger, J.0., Statistical Decision Theory and Bayesian Analysis, second edition, Springer-Verlag, Berlin, Heidelberg, New York, 1985. [15] Bernardo, J.M. and Berger, J.O., On the development of reference priors, in Volume 4 of [16], 1992. [16] Bernardo, J.M., Berger, J.O., Dawid, A.P., and Smith, A.F.M. (editors), Bayesian Statistics, several volumes, Oxford University Press. [17] Bernardo, J.M. and Smith, A.F.M., Bayesian Theory, Wiley, Chichester, New York, 1994. Further volumes are due. [18] Besag, J., The statistical analysis of dirty pictures (with discussion), J. Royal Stat. Soc., B, 48, 259-302, 1986. [19] Besag, J., Green, P., Higdon, D., and Mengerson, K., Bayesian computation and stochastic systems (with discussion), Statistical Science, 10, 3-66, 1995. [20] Besag, J. and Higdon, D., Bayesian analysis of agricultural field experiments, J. Royal Stat. Soc., 61, 691-746, 1999. [21] Billingsley, P., Ergodic Theory and Information, Wiley, Chichester, New York, 1965. [22] Billingsley, P., Convergence of Probability Measures, Wiley, Chichester, New York, 1968. [23] Bingham, N.H. and Kiesel, R., Risk-neutral Valuation: Pricing and Hedging of Financial Derivatives, Springer-Verlag, Berlin, Heidelberg, New York, 1998. [24] Bjorken, J.D. and Drell, S.D., Relativistic Quantum Mechanics, McGraw—Hill, New York, Toronto, 1964. [25] Bohm, D., Wholeness and the Implicate Order, Routledge (Taylor and Francis), London, New York, 1980. [26] Bohm, D. and Hiley, B.J., The Undivided Universe, Routledge (Taylor and Francis), London, New York, 1993, 1995. [27] Borcherds, R.E., What is moonshine?, Documenta Mathematica (extra volume of) Proc. Intern. Cong. Math. , 1998, available from http: //xxx . lanl .gov/abs/math.QA/9809110. [28] Bouwmeester, D., Ekert, A.K. and Zeilinger, A., The Physics of Quantum Information. Quantum Cryptography, Quantum Teleportation, Quantum Computation, Springer-Verlag, Berlin, Heidelberg, New York, 2000. [29] Box, G.E.P. and Jenkins, G. M., Time Series Analysis, Forecasting and Control, Holden—Day, San Francisco, 1970. Bibliography \u0000 527 [30] Breiman, L., Probability, SIAM, Philadelphia, 1992 (originally, Addison—Wesley, Reading, Mass., 1968). [31] Bremaud, P., An Introduction to Probabilistic Modelling, Springer-Verlag, Berlin, Heidelberg, New York, 1988. [32] Brillinger, D.R., Time Series: Data Analysis and Theory, Holt, New York, 1975. [33] Brockwell, P.J. and Davis, R.A., An Introduction to Time Series and Forecasting, Springer-Verlag, Berlin, Heidelberg, New York, 1997. [34] Brockwell, P.J. and Davis, R.A., Time Series: Theory and Methods, Springer- Verlag, Berlin, Heidelberg, New York, 1987. [35] Burnham, K.P. and Anderson, D.R., Model Selection and Inference, Springer, Berlin, Heidelberg, New York, 1998. [36] Carlin, B.P. and Louis, T.A., Bayes and Empirical Bayes Methods for Data Analysis, Chapman and Hall, London, 2000. [37] Casella, G. and Berger, R.L., Statistical Inference, Wadsworth, Belmont, CA, 1990. [38] Chatfield, C., The Analysis of Time Series: Theory and Practice, Chapman and Hall, London, 1975. [39] Chatfield, C. and Collins, A.J., Introduction to Multivariate Analysis, Chapman and Hall, London, 1980. [40] Chernoff, H. and Moses, L., Elementary Decision Theory, Dover, Mineola, NY, 1987. [41] Chow, Y.S. and Teicher, H., Probability Theory: Independence, \u0000 Interchange- ability, Martingales (second edition), Springer-Verlag, Berlin, Heidelberg, New York, 1988. [42] Christensen, R., Analysis of Variance, Design, and Regression: Applied Statistical Methods, Chapman and Hall, London, 1996. [43] Chung, K.L., A Course in Probability Theory, Academic Press, New York, 1974. [44] Chung, K.L. and Williams, R.I., Introduction to Stochastic Integration, Birkhauser, Boston, 1983. [45] cinlar, E., Introduction to Stochastic Processes, Prentice—Hall, Englewood Cliffs, NJ, 1975. [46] Cochran, W.G. and Cox, G.M., Experimental Designs (second edition), Wiley, New York, 1957. [47] Cox, D.A., Primes of the form x 2 + ny2, Wiley, New York, 1989. 528 \u0000 Bibliography [48] Cox, D.R., Planning of Experiments, Wiley, New York, 1992. [49] Cox, D.R. and Hinkley, D.V., Theoretical Statistics, Chapman and Hall, London, 1974. Problems and Solutions in Theoretical Statistics, Chapman and Hall, London, 1978. [50] Cox, D.R. and Miller, H.D., The Theory of Stochastic Processes, Chapman and Hall, London, 1965. [51] Cox, D.R. and Reid, N., The Theory of the Design of Experiments, Chapman and Hall, London, 2000. [52] Cox D.R. and Smith, W.L., Queues, Methuen, London, 1961. [53] Daniels, H.E., Saddlepoint approximations in statistics, Ann. Math. Statist., 25, 631-650, 1954. [54] Davis, B., Reinforced random walk, Probab. Th. Rel. Fields 84, 203-229, 1990. [55] Davison, A.C. and Hinkley, D.V., Bootstrap Methods and Their Application, Cambridge University Press, 1997. [56] Davydov, A.S., Quantum Mechanics, Pergamon, Oxford, 1965. [57] Dawkins, R., River out of Eden, Weidenfeld and Nicholson, London, 1995. [58] de Finetti, B., Theory of Probability: a Critical Introductory Treatment (two volumes), Wiley, Chichester, New York, 1974. [59] de Groot, M., Optimal Statistical Decisions, McGraw—Hill, New York, 1970. [60] Dellacherie, C. and Meyer, P.-A., Probabilites et Potentiel (several volumes), Hermann, Paris. English translation, Probability and Potential, published by North—Holland, Amsterdam. [61] Deuschel, J.-D. and Stroock, D.W., Large Deviations, Academic Press, Boston, 1989. [62] Diaconis, P., Recent progress on de Finetti's notion of exchangeability, in Volume 3 of [16]. [63] Diaconis, P., Group Representations in Probability and Statistics, Institute of Math. Stat., Hayward, California, 1988. [64] di Francesco, P., Mathieu, P. and Senechal, D., Conformal Field Theory (two volumes), Springer, Berlin, Heidelberg, New York, 1997. [65] Dobson, A.J., An Introduction to Statistical Modelling, Chapman and Hall, London, 1983. [66] Doob, J.L., Stochastic Processes, Wiley, New York, 1953. Bibliography \u0000 529 [67] Draper, D., Bayesian Hierarchical Modeling (to appear). [68] Draper, N.R. and Smith, H., Applied Regression Analysis, Wiley, New York, 1981. [69] Durrett, R., Probability: Theory and Examples, second edition, Duxbury Press, Pacific Grove CA, 1996. [70] Durrett, R., Stochastic Calculus: A Practical Introduction, CRC Press, London, 1996. [71] Efron, B. and Tibshirani, R.J., An Introduction to the Bootstrap, Chapman and Hall, New York, 1993. [72] Einstein, A., Podolsky, B. and Rosen, N, Can quantum-mechanical description of physical reality be considered complete?, Phys. Rev. 41, 777-780, 1935. [73] Everitt, B.S. and Dunn, G., Applied Multivariate Data Analysis, Edward Arnold, London, 1992. [74] Fahrmeir, L. and Tutz, G., Multivariate Statistical Modelling based on Generalized Linear Models, Springer-Verlag, Berlin, Heidelberg, New York, 1994. [75] Feller, W., Introduction to Probability Theory and its Applications, Vol.], Wiley, New York, 1957. [76] Ferguson, T.S., Mathematical Statistics: A Decision Theoretic Approach, Academic Press, New York, 1967. [77] Feynman, R.P., QED: The Strange Story of Light and Matter, Princeton University Press, 1985; Penguin, London, 1990. [78] Feynman, R.P., Leighton, R.B., Sands, M., The Feynman Lectures on Physics, 2 volumes, Addison—Wesley, Reading, Mass., 1979. [79] Fisher, R.A. and Bennett, J.H. (editor), Statistical Methods, Experimental Design, and Scientific Inference, Oxford University Press, 1990. [80] Fisher, R.A. and Bennett, J.H. (editor), The Genetical Theory of Natural Selection: A Complete Variorum Edition, Oxford University Press, 1990. [81] Flury, B., A First Course in Multivariate Statistics, Springer-Verlag, Berlin, Heidelberg, New York, 1997. [82] Fradkin, E., Physics 483 Course on Quantum Field Theory at University of Illinois, http://w3.physics.uiuc.edu/ efradkin/phys483/physics483.html [83] Freedman, D., Pisani, R. and Purves, R., Statistics, third edition, Norton, New York, London, 1998. 530 \u0000 Bibliography [84] French, S. and Insua, D.R., Statistical Decision Theory, Arnold, London, 2000. [85] Fristedt, B., and Gray, L., A Modern Approach to Probability Theory and its Applications, Springer-Verlag, Berlin, Heidelberg, New York, 1996. [86] Frieden, B.R., Physics from Fisher Information, Cambridge University Press, 1998. [87] Gamerman, D., Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference, Chapman and Hall, London, 1997. [88] Garthwaite, P.H. and Jolliffe, I.T., Statistical Inference, Prentice Hall, Englewood Cliffs, NJ, 1995. [89] Gelfand, A.E. and Ghosh, S.K., Model choice: a minimum posterior predictive loss approach, Biometrika, 85, 1-1, 1998. [90] Gelman, A., Carlin, J.B., Stern, H.S. and Rubin, D.B., Bayesian Data Analysis, Chapman and Hall, London, 1995. [91] Geman, S. and Geman, D., Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images, IEEE Trans. Pattn. Anal. Mach. Intel., 6, 721- 741, 1984. [92] Gershenfeld, N. and Chuang, I.L., Quantum computing with molecules, Sci. Am., June, 1998. [93] Gilks, W.R., Full conditional distributions, in [94]. [94] Gilks, W.R., Richardson, S. and Spiegelhalter, D., Markov Chain Monte Carlo in Practice, Chapman and Hall, London, 1996. [95] Gilks, W.R. and Roberts, G.O., Strategies for improving MCMC, in [94]. [96] Gilmour, S.G., The interpretation of Mallows's Cp statistic, J. Royal Stat. Soc., Series D (The Statistician), 45, 49-56, 1996. [97] Giulini, D., Joos, E., Kiefer, C., Kupsch, J., Stamatescu, I.-O. and Zeh, H.D., Decoherence and the Appearance of a Classical World in Quantum Theory, Springer-Verlag, Berlin, Heidelberg, New York, 1996. [98] Goddard, P., The work of Richard Ewen Borcherds, Documenta Mathematica (extra volume of) Proc. Intern. Cong. Math. 1998, 99-108, available from http://xxx.lanl.gov/abs/math.QA/9808136 [99] Good, U., Probability and the Weighing of Evidence, Griffin, London, 1950. [100] Greene, B., The Elegant Universe, Vintage, Random House, London, 2000. (originally published by Jonathan Cape, 1999). Bibliography \u0000 531 [101] Griffiths, A.J.F., Lewontin, R.C., Gelbart, W.M.G., and Miller, J., Modern Genetic Analysis, Freeman, New York, 1999. [102] Grimmett, G., Percolation, second edition, Springer-Verlag, Berlin, Heidelberg, New York, 1999. [103] Grimmett, G. and Stirzaker, D.R., Probability and Random Processes, Oxford University Press, 1992. [104] Grover, L., A fast quantum mechanical algorithm for database search, Proc. 28th ACM Symp. Theory Comp., 212-219, ACM Press, New York, 1996. [105] Gruska, J., Quantum Computing, McGraw—Hill, London, 1999. [106] Hall, P., Introduction to the Theory of Coverage Processes, John Wiley, New York, 1988. [107] Hall, P. and Heyde, C.C., Martingale Limit Theory and its Applications, Academic Press, New York, 1980. [108] Hardy, G.H. and Wright, E.M., An Introduction to the Theory of Numbers, fourth edition, Oxford University Press, 1960. [109] Harris, T.E., The Theory of Branching Processes, Springer-Verlag, Berlin, Heidelberg, New York, 1963. [110] Hastings, W.K., Monte Carlo sampling methods using Markov chains and their applications, Biometrika, 57, 97-109,1970. [111] Helstrom, C.W., Quantum Detection and Estimation Theory, Academic Press, New York, 1976. [112] Holevo, A.S., Probabilistic and Statistical Aspects of Quantum Theory, North— Holland, Amsterdam, 1982. [113] Huitema, B.E., The Analysis of Variance and Alternatives, Wiley, New York, 1980. [114] Hunt, P.J. and Kennedy, J.E., Financial Derivatives in Theory and Practice, Wiley, Chichester, New York, 2000. [115] Isham, C.J., Lectures on Quantum Theory, Imperial College Press, London, and World Scientific, 1995. [116] Itzykson, C. and Drouffe, J.M., Statistical Field Theory (two volumes), Cambridge University Press, 1991. [117] Jaynes, E.T., Probability Theory: The Logic of Science, http://omega.albany.edu:8008/JaynesBook [118] Jeffreys, H., Theory of Probability, Oxford University Press, 1961. 532 \u0000 Bibliography [119] Jennison, C. and Turnbull, B.W., Group Sequential Methods with Applications to Clinical Trials, Chapman and Hall, London, 2000. [120] Johnson, N.L. and Kotz, S., Distributions in Statistics (several volumes), Wiley, New York, 1969—. [121] Johnson, R.A. and Wichern, D.W., Applied Multivariate Statistical Analysis (fourth edition), Prentice—Hall, Englewood Cliffs, NJ, 1998. [122] Kalbfleisch, J.G., Probability and Statistical Inference: Volume 1: Statistical Inference, Springer-Verlag, Berlin, Heidelberg, New York, 1985. [123] Karatzas, I. and Shreve, S.E., Brownian Motion and Stochastic Calculus, Springer- Verlag, Berlin, Heidelberg, New York, 1987. [124] Karatzas, I. and Shreve, S.E., Methods of Mathematical Finance, Springer-Verlag, Berlin, Heidelberg, New York, 1998. [125] Karlin, S. and Taylor, H.M., A First Course in Stochastic Processes, Academic Press, New York, 1975. A Second Course in Stochastic Processes, Academic Press, New York, 1982. [126] Kass, R.E. and Vos, P.W., Geometrical Foundations of Asymptotic Inference, Wiley, New York, 1997. [127] Kasteleyn, P.W., The statistics of dimers on a lattice, I, The number of dimer arrangements on a quadratic lattice, Physica, 27, 1209-1225, 1961. [128] Kaufman, B., Crystal statistics II: partition function evaluated by spinor analysis, Phys. Rev., 76, 1232-1243, 1949. [129] Kelley, A. and Pohl, I., A Book on C, second edition, Benjamin/Cummings, Redwood City, CA, 1990. [130] Kelly, F.P., Reversibility and Stochastic Networks, Wiley, Chichester, New York, 1979. [131] Kemeny, J.G. and Snell, J.L., Finite Markov Chains, Van Nostrand, Princeton, NJ, 1959. [132] Kendall, M.G., Stuart, A. and Ord, J.K., (`Kendall's Advanced Theory of Statistics, Volume 1'), Distribution Theory, Arnold, London, 1994. [133] Kendall, M.G., Stuart, A., Ord, J.K. and Arnold, S., (`Kendall's Advanced Theory of Statistics, Volume 2A'), Classical Inference and the Linear Model, Arnold, London, 1999. [134] Kendall, D.G., Branching processes since 1873, J. London Math. Soc., 41, 385- 406, 1966. Bibliography \u0000 533 [135] Kendall, D.G., The genealogy of genealogy: branching processes before (and after) 1873, Bull. London Math. Soc. 7, 225-253, 1975. [136] Kernigan, B.W. and Richie, D.M., The C Programming Language (second edition), Prentice Hall, Englewood Cliffs, NJ, 1988. [137] Kingman, J.F.C., Martingales in the OK Corral, Bull. London Math. Soc. 31, 601- 606, 1999. [138] Knill, E., Laflamme, R. and Milburn, G.J., A scheme for efficient quantum computation with linear optics, Nature 409, 46-52, 2001. [139] Knuth, D.E., The TEXbook, Addison Wesley, Reading, Mass., 1987. [140] Kolmogorov, A. N. Foundations of the Theory of Probability (translated from the German), second edition, Chelsea, New York, 1956. [141] Kopka, H. and Daly, P.W., A Guide to JTEX2e: Document Preparation for Beginners and Advanced Users, second edition, Addison—Wesley, Harlow and Reading, Mass., 1995. [142] Krause, A. and Olson, M., The Basics of S and S-PLUS, Springer-Verlag, Berlin, Heidelberg, New York, 1997. [143] Krzanowski, W.J. and Marriott, F.H.C., Multivariate Analysis (two volumes), Edward Arnold, London, Wiley, New York, 1994. [144] Lamport, L., LATEX — A Document Preparation System, second edition, Addison Wesley, Reading, Mass., 1994. [145] Langlands, R., Pouliot, Ph. and Saint—Aubin, Y., Conformal invariance in two- dimensional percolation theory, Bull. Amer. Math. Soc., 30, 1-61, 1994. [146] Laha, R. and Rohatgi, V., Probability Theory, Wiley, New York, 1979. [147] Lauritzen, S., Graphical Models, Oxford University Press, 1996. [148] Lee, P.M., Bayesian Statistics, Arnold, London, 1997. [149] Lehmann, E.L., Testing Statistical Hypotheses, second edition, Springer-Verlag, Berlin, Heidelberg, New York, 1997. [150] Lehmann, E.L., Theory of Point Estimation, second edition, Springer-Verlag, Berlin, Heidelberg, New York, 1998. [151] Levy, P., Theorie de l'Addition des Variables Aleatoires, Gauthier—Villars, Paris, 1954. [152] Levy, P., Processus Stochastiques et Mouvement Brownien, Gauthier—Villars, Paris, 1965. 534 \u0000 Bibliography [153] Lindley, D.V., Introduction to Probability and Statistics from a Bayesian Viewpoint (two volumes), Cambridge University Press, 1965. [154] Mackey, G.W., The Mathematical Foundations of Quantum Mechanics, Benjamin, New York, 1963. [155] Mallows, C.L., Some comments on Cp, Technometrics, 15, 661-675,1973. [156] Manly, B.F.J., Multivariate Statistical Methods: A Primer, Chapman and Hall, London, 1994. [157] Mardia, K.V., Kent, J.T. and Bibby, J.M., Multivariate Analysis, Academic Press, New York, 1980. [158] Marriott, P. and Salmon, M (editors), Applications of Differential Geometry to Econometrics, Cambridge University Press, 2000. [159] McCullagh, P., Tensor Methods in Statistics, Chapman and Hall, London, 1987. [160] McCullagh, P. and Nelder, J.A., Generalized Linear Models, Chapman and Hall, London, 1983. [161] Mead, R., The Design of Experiments: Statistical Principles for Practical Application, Cambridge University Press, 1988. [162] Merzbacher, E., Quantum Mechanics, Wiley, New York, 1970. [163] Messiah, A., Quantum Mechanics (two volumes), Dunod, Paris; North—Holland, Amsterdam, Wiley, New York. [164] Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E., Equations of state calculations by fast computing machine, J. Chem. Phys., 21, 1087-1091,1953. [165] Meyer, P.-A., Quantum Theory for Probabilists, Springer Lecture Notes, Berlin, Heidelberg, New York, 1993. [166] MLwiN is available at a cost from the site http: //www . ioe . ac .uk/mlwin/. [167] Montgomery, D.C. and Peck, E.A., Introduction to Linear Regression Analysis, Wiley, New York, 1992. [168] Murray, M. K. and J. W. Rice, Differential Geometry and Statistics, Chapman and Hall, London, 1993. [169] Murtagh, F. and Heck, A., Multivariate Data Analysis, Kluwer, Dordrecht, 1987. [170] Myers, R.H., Classical and Modern Regression with Applications (second edition), PWS-Kent, Boston, 1990. [171] Nelson, E. Quantum Fluctuations, Princeton University Press, 1985. Bibliography \u0000 535 [172] Neveu, J., Bases Mathematiques Du Calcul des Probabilites, Masson, Paris 1967. English translation: Mathematical Foundations of the Theory of Probability, Holden—Day, San Francisco. [173] Neveu, J., Discrete-parameter Martingales, North—Holland, Amsterdam, 1975. [174] Nielsen, M.A. and Chuang, I.L., Quantum Theory and Quantum Information, Cambridge University Press, 2000. [175] Nobile, A. and Green, P.J., Bayesian analysis of factorial experiments by mixture modelling, Biometrika, 87, 15-35, 2000. [176] Norris, J.R., Markov Chains, Cambridge University Press, 1997. [177] O'Hagan, A., Bayesian Inference (`Kendall's Advanced Theory of Statistics, Volume 2B'), Arnold, London, 1994. [178] Oksendal, B., Stochastic Differential Equations, Springer-Verlag, Berlin, Heidelberg, New York, 1985. [179] Olive, D.I., The relativistic electron, in Electron, a Centenary Volume (edited by M. Springford), Cambridge University Press, 1997. [180] Onsager, L., Crystal statistics, I: a two-dimensional model with an order-disorder transition, Phys. Rev., 65, 117-149, 1944. [181] Parthasarathy, K.R., An Introduction to Quantum Stochastic Calculus, Birkhauser, Basel, Boston, 1992. [182] Pemantle, R., Vertex-reinforced random walk. Prob. Theor and Rel. Fields 92, 117-136, 1990. [183] Penrose, R., Shadows of the Mind: a Search for the Missing Science of Consciousness, Oxford University Press, 1994. [184] Pitman, J.W., Probability, Springer-Verlag, Berlin, Heidelberg, New York, 1993. [185] Polya, G, How to Solve it, Penguin, London, 1990. [186] Polya, G., Mathematics and Plausible Reasoning: I. Induction and Analogy in Mathematics, Princeton University Press, 1954. [187] Preskill, J., Physics 229: Advanced Mathematical Methods of Physics — Quantum Computing and Quantum Information, California Institute of Technology, 1998. Available at http: //www. theory. caltech. edu/people/preskill/ph229. [188] Priestley, M., Spectral Analysis and Time Series (two volumes), Academic Press, London, New York, 1994. 536 \u0000 Bibliography [189] Propp, J.G. and Wilson, D.B., Exact sampling with coupled Markov chains and applications to statistical mechanics, Random Structures and Algorithms, 9, 223- 252,1996. [190] R is available free from http : //clan .r-project . org. [191] Reed, M. and Simon, B., Methods of Modern Mathematical Physics (four volumes), Academic Press, New York. [192] Reid, N., Saddlepoint methods and statistical inference (with discussion), Stat. Sci., 3, 213-238,1988. [193] Renyi, A., Probability Theory, North-Holland, Amsterdam, 1970. [194] Revuz, D. and Yor, M., Continuous Martingales and Brownian Motion, Springer- Verlag, Berlin, Heidelberg, New York, 1991. [195] Ripley, B.D., Modelling spatial patterns, J. Royal Statist. Soc., Ser. B, 39 172-212, 1977. [196] Ripley, B.D., Stochastic Simulation, Wiley, Chichester, New York, 1987. [197] Robert, C.P., Discretization and MCMC Convergence Assessment, Springer- Verlag, Berlin, Heidelberg, New York, 1998. [198] Robert, C.P. and Casella, G., Monte Carlo Statistical Methods, Springer-Verlag, Berlin, Heidelberg, New York, 1999. [199] Rogers, L.C.G. and Williams, D., Diffusions, Markov Processes, and Martingales (two volumes), Cambridge University Press, 2000 (originally published by Wiley). [200] Rousseeuw, P.J. and Leroy, A.M.: Robust Regression and Outlier Detection, Wiley, Chichester, New York, 1987. [201] Ross, S.M., Stochastic Processes, Wiley, New York, 1983. [202] Ross, S.M., Introduction to Probability Models, Academic Press, 1981. [203] Rowe, M.A., Kielpinski, D., Meyer, V., Sackett, C.A., Itano, W.M., Monroe, C. and Wineland, D.J., Experimental violation of a Bell's inequality with efficient detection, Nature, 409, 791-793, February, 2001. [204] Ryan, B.F. and Joiner, B.L., MINITAB Handbook, Duxbury Press, Pacific Grove, Ca, 2000. [205] Ryan, T.P., Modern Regression Methods, Wiley, New York, 1996. [206] Savage, L.J., The Foundations of Statistics (second edition), Dover, New York, 1972. (First edition, Wiley, 1954.) [207] Scheffe, H., The Analysis of Variance, Wiley, New York, 1958. Bibliography \u0000 537 [208] Schiff, L.I., Quantum Mechanics, second edition, McGraw—Hill, New York, 1955. [209] Schwarz, G., Estimating the dimension of a model, Ann. Stat., 6, 461-464,1978. [210] Scott, D.W., Multivariate Density Estimation: Theory, Practice, and Visualization, Wiley, New York, 1992. [211] Seber, G.A.F., Multivariate Observations, Wiley, New York, 1984. [212] Silvey, S.D., Statistical Inference, Chapman and Hall, London, 1970. [213] Simon, B., The classical moment problem as a self-adjoint finite difference operator, Advances in Math. 137, 82-203,1998. [214] Snedecor, G.W. and Cochran, W.G., Statistical Methods, Iowa State University Press, Ames, 1980. [215] Spiegelhalter, D., Best, N. and Carlin, B., Bayesian deviance, the effective number of parameters, and the comparison of arbitrarily complex models (to appear), 2001. [216] Spitzer, F., Principles of Random Walk, Van Nostrand, Princeton, NJ, 1964. [217] Stern, C. and Sherwood, E.R., The Origin of Genetics: A Mendel Source Book, Freeman, San Francisco, 1966. [218] Stigler, S.M., The 1988 Neyman Memorial Lecture: A Galtonian perspective on shrinkage estimators, Statistical Science, 5, 147-155,1990. [219] Streater, R.F., Classical and quantum probability, J. Math. Phys., 41, 3556-3603, 2000. [220] Streater, R.F. and Wightman, A.S., PCT, Spin and Statistics, and all that, Benjamin/Cummings, Reading, Mass., 1978. [221] Stroock, D.W., Probability Theory, an Analytic View, Cambridge University Press, 1993. [222] Stoyan, D., Kendall, W.S. and Mecke, J., Stochastic Geometry and its Applications, Wiley, Chichester, New York, 1987. [223] Tabachnick, B.G. and Fidell, L.S., Using Multivariate Statistics, third edition, HarperCollins, New York, 1996. [224] Temperley, H.N.V. and Fisher, M.E., Dimer problem in statistical mechanics — an exact result, Phil. Mag., 6, 1061-1063,1961. [225] Thaller, B., The Dirac Equation, Springer-Verlag, Berlin, Heidelberg, New York, 1992. 538 \u0000 Bibliography [226] Thompson, C.J., Mathematical Statistical Mechanics, Princeton University Press, 1979. [227] Thompson, E.A., Pedigree Analysis in Human Genetics, Johns Hopkins University Press, Baltimore, 1986. [228] Thompson, E.A. and Cannings, C., Genealogical and Genetic Structures, Cambridge University Press, 1981. [229] Tong, H., Non-Linear Time Series, A Dynamical System Approach, Oxford University Press, 1990 . [230] Tukey, J., The philosophy of multiple comparisons, Statistical Science, 6, 100- 116, 1991. [231] Varadhan, S.R.S., Large Deviations and Applications, SIAM, Philadelphia, 1084. [232] Venables, W.N. and Ripley, B., Modern Applied Statistics with S-PLUS; Volume 1: Data Analysis, third edition, Springer-Verlag, Berlin, Heidelberg, New York 1999. [233] Wagon, S., The Banach-Tarski Paradox, Cambridge University Press, 1985. [234] Weisberg, S., Applied Linear Regression, second edition, Wiley, New York, 1985. [235] Williams, D., Probability with Martingales, Cambridge University Press, 1991. [236] Wilson, R.A., Schrodinger's Cat Trilogy, Dell, New York, 1979, Orbit, 1990. [237] Wilson, A.R., Lowe, J. and Butt, D.K., Measurement of the relative planes of polarisation of annihilation quanta as a function of separation distance, J. Phys. G Nuclear Physics, 2, 613-623, 1976. [238] WinBUGS is available from http : //www .mrc-bsu . cam. ac . uk/bugs /welcome . shtml. [239] Zeilinger, A., A foundational principle for quantum mechanics, Foundations of Physics, 29, 631643, 1999. [240] Zurek, W.H., Decoherence and the transition from quantum to classical, Physics Today, 44, 36 \u0000 14, 1991. Index Z := {O,1,2,...} N := {1,2,3,...} A ® B for matrices, 325, 460 B(K, L), 209 Cx (a), 153 Fx , 50 IF, 49 Lx (a), 155 Mx (a), 146 Pt+, 299 Pu, 299 R(8, A), 475 U ® V for subspaces, 326 U-L, 299 At, 444 Cov(X, Y), 68 E(X), 56, 61 F(K), 148 R co, wact, 36 P(B I A), 7, 73 RK, 324 Var(X), 66 y 7 yobs yobs, 169 x y for vectors, 325, 460 vt, 448 dev, Dev, 226 40; y), 182 cp(x),(1,(x), 146 (P x (a), 166 lhd, 171 lr, LR, 224 (x, y), 296 complex case, 449 px, 56 R- (0 I y°11, 200 p(X, Y), 68 43 r l, 61 G2, 65 R.(10, A), 475 ax, ay, az, 474 sobs, Sn, Sn, an-1, 170 {F, G} anticommutator, 479 c + [a, b], c[a, b], etc, 305 fx, 53 gx (a), 143 Px, 51 q(x, x), Q(X, Y), etc, 304 App(f g), 197 BVN(px, py; ate , ay; p), 369 Ent (f z ), 197 F:`.,s(n), 305 MVNn(pt, V), 367 SN(U), 297 SO(3), 475 SRW(p), 117 SU(2), 473 power(0), 225 Actual outcome C+J\"t , 36, 48, 169 Actual statistics, 48 Addition rule, 6, 39, 42 Adjoint, 323 Hermitian, 444 Akaike Information Criterion (AIC), 236, 303, 378 Aliasing problem, 333 Almost surely, 44 Ancillary statistics, 186, 263, 279 ANCOVA example, 336 ANOVA a trailer, 289 introduction, 29 modifications, 358 539 540 \u0000 Index on WinBUGS , 333 the Bayesian view, 333 theory, 326 with interaction and replication, 331 ANOVA table, 307, 312 Aspect experiment, 483 Asymptotic normality of MLEs, 193 Auto-regressive model mentioned, 513 Axiom of Choice, 498 Axiomatization full, 43 Ballot theorem, 126 martingale proof, 414 Banach—Tarski Paradox, 43 Barron's identity, 199 Basis, 296 Bayes' formula, 77 for events, 75 in probability, 259 in Statistics multi-parameter, 266 one-parameter, 200 simple case, 8 Bayes' Theorem, 75 Bayesian filter change-point detection, 84 Bayesian hypothesis testing see Bayesian significance testing 234 Bayesian posterior density, 200 Bayesian priors, 200 multiparameter reference difficulties with, 381 philosophy, 219 reference, 204, 436 important warning, 276 multiparameter, 379 use of imaginary results, 217 Your choice, 215 Bayesian significance testing, 214, 219 sharp hypotheses, 234 Behrens—Fisher problem, 308 Bell inequalities, 483 Berry—Esseen Theorem, 163 Binomial coefficients, 9, 20-21 Birthdays problem, 13 true story, 26 Black—Scholes formula, 421 Borel sets and functions, 45 Borel—Cantelli Lemma first, 45 proof, 60 second, 97 Bounded-convergence theorem, 65 Boys-and-Girls Principle, 432, 435 Boys-and-Girls problem, 100 Branching process, 394 martingale, 418 typed, 398 Buffon's needle, 56 Canonical Commutation Relation (CCR), 455 Weyl's formulation, 458 Car convoys, 103 discussion, 507 Car parking, 76 Car-and-Goats problem, 15, 73 Cauchy—Schwarz inequality, 66 Central Limit Theorem (CLT), 156 and entropy, 198 for medians, 165 Multivariate, 374 Change-pdf trick, 147 Change-point-detection filter, 84 Characteristic functions, 166 Chi-Squared Principle (ChiSqP), 345 for discrete variables, 345 proof, 377 Choleski decomposition, 366 Classical Probability as commutative Quantum Probability, 461 Clifford algebra, 490 CNOT gate, 463 Coin tossing rigorous model, 110 Conditional expectation E (X I A), 385 Index \u0000 541 E (X I Y), 403 E (X I Y), Y discrete, 388 E (X 1 g), 405 and independence, 402 fundamental property, 390 Conditional Mean Formula, 390 Conditional pdfs, 258 Conditional probability definition, 73 motivation, 6 recursive calculation, 82-87 Conditional uniformity and sufficiency, 435 for Poisson processes, 435 Conditional variance, 391 Conditional Variance Formula, 392 Conditioning need for care with, 260 Confidence Intervals (CIs) Bayesian, 200 examples, 175-181 Frequentist Definition, 173 Frequentist Interpretation, 174 Confidence Regions and hypothesis tests, 226 Conformal field theory, 439 Confounding, 179, 357 Conjugate priors, 207 Constant-risk principle, 411 Continued fractions, 497 Continuous RV (so called), 53 Contours for BVN, 371 Convex function, 64 Convolution, 251 Correlation coefficient, 68, 371 as measure of association, 371 Statistics of, 372 Countable sets, 496 Coupon collecting, 432 Covariance, 68 Cramer—Rao Theorem, 190 Credible interval, 172 Cromwell's dictum, 216 Crystallization, 36, 48 Cumulant generating function, 153 Cumulants, 153 Cumulative Distribution Function, 50 de Finetti's Theorem, 220 martingale proof, 419 Decoherence, 492 Degrees of freedom, 306, 312, 327 Density matrices in Quantum Theory, 452 for subsystems, 467 Deutsch's problem, 442, 446 Deviance, 226 chi-squared results for, 230, 345 Deviance Information Criterion see also Akaike Information Criterion Bayesian, 378 Difference equations, 118, 387 Differential Geometry in Statistics, 379 Dimension, 296 Dimerization, 76 Dirac equation, 489 Discrete Random Variable, 51 Distribution Function Fx of X, 50 Distribution-free tests, 317 Edgeworth expansions, 163 Effective prior sample size, 202 Efficiency, 188 Einstein—Podolsky—Rosen paradox, 481 Empirical Distribution Function (EDF), 116 Entanglement, 465, 480 Entangling circuits, 465 Entropy, 196 and Fisher information, 198 and the CLT, 198 EPR—Bohm paradox, 481 Ergodic property, 269 Estimators and estimates, 187 Euler's formula, 101 European option, 422 Events, 36-39 Exchangeability, 220 Exclusion principle mentioned, 473 542 \u0000 Index Expectation, fix or E (X), of X general case, 59 Exponential distribution lack-of-memory property, 82 Exponential family of distributions, 184 Extension-to-Basis Principle, 296, 297 F-test, 301 and choice of model, 303 sharp-hypothesis nature, 302 Fiducial school mentioned, 311 Fisher and Mendel, 346 Fisher information, 189 and entropy, 198 and the Uncertainty Principle, 459 Fisher's z-transform, 373 Fisher—Behrens problem, 308 Fisher-information metric, 380 Five-nations problem, 97 Four-liars problem, 19 solution, 79 Fourier inversion formula, 457 Fourier transform, 457 Frequentist prior, 172 Fundamental model, 44 and coin tossing, 110 universality, 112 Fundamental Theorem of Statistics, 116 Galton—Watson branching process, 394 Gambler's ruin, 117, 413 Gamma function F(•), 148 Gaussian integral, 146 General Linear Model, 336 Generating functions general comments on, 142 Genetics, 87-96 Gibbs sampler, 268 see also WinBUGS Cauchy case, 273 for N(p, prec p), 268 hierarchical t model, 351 sticking phenomenon, 353 Glivenko—Cantelli Theorem, 116 Goodness of fit, 339-348 Grover's algorithm, 446 circuitry, 467 Hadamard gate, 463 Hamiltonian, 445 Hardy—Weinberg law, 89 Hat-matching problem, 40, 70 Hazard functions, 80-82 Hedging strategy, 422 Heisenberg picture, 486 Heisenberg Uncertainty Principle see Uncertainty Principle Hidden variables, 483 Hierarchical models, 215 tu, v random, 348 History Fri, 128 History FT, 129 Hitting probabilities, 118 Hypothesis testing, 222 sequential, 416 Hypothesis tests and confidence regions, 226 IID Random Variables, 103 Inclusion-exclusion principle general, 40 proof, 58 Independence, 8, 96-103, 241 Independence means multiply for expectations, 101 for MGFs, 152 for pgfs, 144 for probabilities, 96 Indicator function, 49 Individual Confidence Intervals, 328 Interaction, 331, 360 Interarrival times, 428 Interchange circuit, 464 Intervals notation for, 305 Jacobian, 243-246 Jacobian Theorem, 246 Jensen's inequality, 64 Joint pdf, 246 Joint pmf, 240 Index \u0000 543 Kolmogorov Statistic, 339 distribution, 342 Kullback—Leibler relative entropy, 197 Kurtosis, 154 Lack-of-memory property, 82 Laplace transform, 155 Law of the iterated logarithm, 116 Least-squares methods, 69 see also Linear Regression Leverage, 316 Likelihood function, 181 examples, 182 Likelihood-ratio martingale, 409 Likelihood-ratio test, 223 Linear Algebra, 295-297 Linear Model General, 336 Linear regression a trailer, 288 Bayesian view, 332 goodness of fit, 355 in Probability, 69 in Statistics, 312 introduction, 29 on WinBUGS , 332 Linear transformation, 323 Location parameter Bayesian theory, 202 CIs for, 186, 264 special case, 263 Log-likelihood function, 182 examples, 182 Logic gates, 445 Logistic regression, 357 Long-term-relative-frequency idea, 5 logical flaws in, 25 resolution, 115 Lung cancer and smoking, 179 Marginal pmf, 241 Markov Chain Monte Carlo (MCMC) see Gibbs sampler see also WinBUGS Markov's inequality, 106 Martingale `Markovian', 409 `product', 408 `sum', 408 definition, 406 Likelihood-ratio, 409 Martingale-convergence theorem, 418 Marx brothers, 41 Maximum absolute deviation, 330 Maximum-entropy distributions, 198 Maximum-Likelihood Estimators (MLEs), 192 asymptotic normality, 193, 375 Mean see Expectation Mean-square-error, 69 Measure theory, 42-46 policy on, 23 Median, 72 Minimal-sufficient statistic, 262 Minimum-Variance Bound, 190 Mixtures of conjugates, 213 Mode, 210 Model choice, 378 in polynomial regression, 320 introduction to AIC, etc, 236 references, 522 Moment Generating Function (MGF), 146 Moment problem non-uniqueness, 500 Moments, 151 Monotone-convergence theorem, 60 Monster group, 439 Moonshine, 439 Multilinear regression remarks on, 322 Multiplication rule general, 7 Multiply means independence in Quantum Theory, 461 Multivariate normal conditioning, 369 pdf, 367 National Lottery, 10, 18 Nelder's dictum, 319 No-cloning theorem, 471 544 \u0000 Index Non-Borel set, 499 Non-linear regression, 322 Non-locality, 481 Non-orthogonality, 316 Non-parametric Statistics, 317 Normal integral, 146 Normal-sampling theorem, 305 a trailer, 286 NOT gate, 463 Notational conflicts, 170 Null event, 44 0(•) notation, 495 o(•) notation, 496 Observable, 444 OK Corral, 410 Option pricing, 421 Order statistics, 107 Oriented parallelogram, 243 Orthogonality implies independence within MVN, 368 Orthonormal basis, 296 Orthonormality principle, 297 2-dimensional story, 283 for projections, 300 Outliers, 340 p-value of data, 224 Parallel-axis result, 67 Parseval's formula, 457 Parsimonious models, 321 Partitioned matrices, 368 Pascal's triangle, 20 Paths counting, 125 Patterns waiting for, 19, 21, 22, 413 Pauli Exclusion Principle mentioned, 473 Pauli matrices crz , cry , az , 474 Pdf of a sum, 251 Pearson Statistic, 346, 374 Perfect simulation mentioned, 275 Permutations, 9 Perpendicular projections, 299 characterization, 324 Phase, 445 Point Estimation, 187 Poisson Line Process (PLP), 437 Poisson Point Process (PPP), 434 rigorous construction, 436 Poisson process definitions, 427 large gaps in, 22 martingale characterization, 433 Polarization, 482 Polya's urn, 76 and martingale convergence, 418 Bayesian isomorphism, 79 main martingale, 409 other martingales, 410 Polynomial regression, 318 Positive-definite symmetric matrices, 366 Possible outcome w, 36 Power function for test, 225 Pre-experiments, 202 Pre-priors, 202 Pre-Statistics, 48 Predicting new observations, 211, 314 Predictive probabilities, 211 Principle of parsimony, 236, 303, 321 Probability addition rule extended, 39 full, 42 simple, 39 monotone-convergence properties, 43 Probability and Statistics notational conflicts, 170 their interrelation, 27 Probability density function (pdf), 53 transformation rules, 55 Probability generating function (pgf), 143 Probability mass function (pmf), 51 Pure-product states, 461 quant-ph, 523 Quantile-quantile plots, 340 Index \u0000 545 Quantum circuit, 463 Quantum computers NMR implementation, 470 Quantum gate, 463 Quantum Probability and 'Realism', 491 and Classical Probability, 461 fundamental postulate, 450 Quaternions, 474 Qubits, 444 Random effects in ANOVA, 358 Random sum generating functions, 393 variance, 392 Random Variable definition, 47 fractal, 123 need for measurability, 48 Random Walk expected number of visits, 120 on Z2, 121 on fractal, 122 on free group, 122 Simple Random Walk SRW, 117 Random-effects model, 358 Random-number generator, 130 C code, 134 Range of a transformation, 323 Realization, 36 Record problem, 103 Reference priors see under Bayesian priors Reflection principle, 125 Regular pdfs, 172 Rejection sampling, 137 Relativity, 488 Replication, 331 Residual, 28 Residuals, 285, 286, 288, 291, 305 looking at, 356, 360 Return probabilities, 118 Reverse martingales, 419 Riemann zeta-function, 100 Robustness of methods, 354 Rotations, 475 RSS, 305, 312, 327 rss 28, see also RSS Rules for expectations, 62, 70 Saddle-point method some references, 164 Sample Mean mean and variance, 104 Sample Median, 108 CLT for, 165 use of, 109 Sampling with replacement, 99 Sampling without replacement, 71 Scale parameter, 206, 227 Scheffe's Lemma, 161 Schor's algorithm mentioned, 442 Schrodinger picture, 486 Schrodinger's equation, 445 score, 189 SD line, 371 Self-adjoint, 324 Hermitian case, 444 Sequential testing, 416 Sharp hypotheses, 233, 234 Sharp hypothesis F-test and, 302 Shrinkage (mentioned), 284 Signed area, 243 Significance level of data, 224 Significance level of test, 224 Simpson's paradox, 179 Simulating from Beta distributions, 250 exponential distributions, 136 Gamma distributions, 256 normal distributions polar method, 250 rejection sampling, 137 t distributions, 254, 256 two-sided exponential distributions, 136 Simulation example, 32 546 \u0000 Index F-inverse principle, 50 ratio method, 255 Simultaneous Confidence Intervals, 277, 328 Size of test, 224 Skewness, 154 Smoking and lung cancer, 179 Span, spanning set, 295 Spearman's rank-correlation coefficient, 317 Special distributions Bernoulli, 51 Beta(K, L), 209 binomial, 10 Bivariate Normal, 369 Cauchy, 55 chi-squared, 152 non-central, 153 exponential E(rate A) or E(A), 54 Fr,s, 252 Gamma(K, mean au), 149 Gamma(K, rate A), 149 geometric, 53 hypergeometric, 17-18 log-normal, 148 Multivariate Normal, 367 negative binomial, 53 normal Ka, o-2), 54 normal N(9, prec p), 207 Poisson, 51 standard normal SN(U) on subspace U, 298 Standardized t, StandT,,, 253 Student's t,,, 253 uniform U[a, b], 54 Weibull, 82 Special Relativity, 488 Spectral Theorem, 449 for symmetric matrices, 365 Spin, 477 as intrinsic angular momentum, 490 spinor representation, 478 Standard deviation definition, 68 Standardized form of an RV , 156 Static variables in C, 499 Statistical Mechanics some references, 439 Statistics and Probability their interrelation, 27 Sticking phenomenon for Gibbs sampler, 353 Stirling's formula, 11 accurate version, 148 Stochastic process discrete-time , 406 Stopping time, 128 Stopping-time principle, 412 Strong Law of Large Numbers (SLLN), 113 for coin tossing, 111 martingale proof, 420 proof of special case, 113 Strong Markov principle, 129 Studentized range, 329 Sufficient statistic Bayesian version, 203 first definition, 183 Frequentist interpretation, 183-186 minimal, 262 Sums of independent RVs Gamma, 152 normal, 152 Poisson, 144 Superposition, 444 versus probabilistic mixture, 452 Tchebychev's inequality, 105 Tensor products, 324, 460 Test for disease, 3-4, 7, 8 The Chi-Squared Principle (ChiSqP) for goodness of fit, 347 Time series some references, 513 Tower property, 402 Transforming observations, 148 Two-Envelopes 'Paradox', 16, 399, 404, 501 Two-sample problem free, 308 Index \u0000 547 paired, 308 pooled, 311 Type I and Type II errors, 226 Unbiased Estimator, 188 of variance, 104 Uncertainty Principle and Fisher information, 459 as Mathematics general case, 453 as Physics, 454 for position and momentum, 454 Uncountable sets, 496 Unitary matrices, 445 Universality for quantum gates, 469 Upper percentage points, 304 Variance as measure of spread, 67 definition, 66 of sum of independent RVs, 102 Waiting for patterns, 19, 21, 22, 413 Waiting-time paradox, 434 Weak Law of Large Numbers (WLLN), 107 Weierstrass approximation theorem proof of, 109 Welch statistic mentioned, 311 Wilks' Theorem, 345 proof, 377 WinBUGS for ANOVA, 333 for Cauchy, 272 for 'hierarchical t' model, 352 for modified ANOVA models, 358 for normal, 270 for regression, 332 Zitterbegewung, 491","libVersion":"0.3.2","langs":""}